<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[leetcode-721:Accounts Merge(账户合并)]]></title>
    <url>%2F2020%2F06%2F07%2Fleetcode-721%2F</url>
    <content type="text"><![CDATA[题目链接：721. Accounts Merge 题目描述 题目难度：Medium Given a list accounts, each element accounts[i] is a list of strings, where the first element accounts[i][0] is a name, and the rest of the elements are emails representing emails of the account. Now, we would like to merge these accounts. Two accounts definitely belong to the same person if there is some email that is common to both accounts. Note that even if two accounts have the same name, they may belong to different people as people could have the same name. A person can have any number of accounts initially, but all of their accounts definitely have the same name. After merging the accounts, return the accounts in the following format: the first element of each account is the name, and the rest of the elements are emails in sorted order. The accounts themselves can be returned in any order. Example 1: 12345678Input: accounts = [[&quot;John&quot;, &quot;johnsmith@mail.com&quot;, &quot;john00@mail.com&quot;], [&quot;John&quot;, &quot;johnnybravo@mail.com&quot;], [&quot;John&quot;, &quot;johnsmith@mail.com&quot;, &quot;john_newyork@mail.com&quot;], [&quot;Mary&quot;, &quot;mary@mail.com&quot;]]Output: [[&quot;John&quot;, &apos;john00@mail.com&apos;, &apos;john_newyork@mail.com&apos;, &apos;johnsmith@mail.com&apos;], [&quot;John&quot;, &quot;johnnybravo@mail.com&quot;], [&quot;Mary&quot;, &quot;mary@mail.com&quot;]]Explanation: The first and third John&apos;s are the same person as they have the common email &quot;johnsmith@mail.com&quot;.The second John and Mary are different people as none of their email addresses are used by other accounts.We could return these lists in any order, for example the answer [[&apos;Mary&apos;, &apos;mary@mail.com&apos;], [&apos;John&apos;, &apos;johnnybravo@mail.com&apos;], [&apos;John&apos;, &apos;john00@mail.com&apos;, &apos;john_newyork@mail.com&apos;, &apos;johnsmith@mail.com&apos;]] would still be accepted. Note: The length of accounts will be in the range [1, 1000]. The length of accounts[i] will be in the range [1, 10]. The length of accounts[i][j] will be in the range [1, 30]. Solution1并查集 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; public List&lt;List&lt;String&gt;&gt; accountsMerge(List&lt;List&lt;String&gt;&gt; accounts) &#123; DSU dsu = new DSU(); Map&lt;String, String&gt; emailToName = new HashMap(); Map&lt;String, Integer&gt; emailToID = new HashMap(); int id = 0; for (List&lt;String&gt; account: accounts) &#123; String name = ""; for (String email: account) &#123; if (name == "") &#123; name = email; continue; &#125; emailToName.put(email, name); if (!emailToID.containsKey(email)) &#123; emailToID.put(email, id++); &#125; dsu.union(emailToID.get(account.get(1)), emailToID.get(email)); &#125; &#125; Map&lt;Integer, List&lt;String&gt;&gt; ans = new HashMap(); for (String email: emailToName.keySet()) &#123; int index = dsu.find(emailToID.get(email)); ans.computeIfAbsent(index, x-&gt; new ArrayList()).add(email); &#125; for (List&lt;String&gt; component: ans.values()) &#123; Collections.sort(component); component.add(0, emailToName.get(component.get(0))); &#125; return new ArrayList(ans.values()); &#125;&#125;class DSU &#123; int[] parent; public DSU() &#123; parent = new int[10001]; for (int i = 0; i &lt;= 10000; ++i) parent[i] = i; &#125; public int find(int x) &#123; if (parent[x] != x) parent[x] = find(parent[x]); return parent[x]; &#125; public void union(int x, int y) &#123; parent[find(x)] = find(y); &#125;&#125; Solution212345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public List&lt;List&lt;String&gt;&gt; accountsMerge(List&lt;List&lt;String&gt;&gt; accounts) &#123; Map&lt;String, List&lt;Integer&gt;&gt; map = new HashMap&lt;&gt;(); for (int j = 0; j &lt; accounts.size(); j++)&#123; List&lt;String&gt; list = accounts.get(j); for (int i = 1; i &lt; list.size(); i++)&#123; map.computeIfAbsent(list.get(i), k -&gt; new ArrayList&lt;&gt;()).add(j); &#125; &#125; boolean[] visited = new boolean[accounts.size()]; List&lt;List&lt;String&gt;&gt; res = new ArrayList&lt;&gt;(); for (String s: map.keySet())&#123; Set&lt;String&gt; set = new TreeSet&lt;&gt;(); dfs(set, map, visited, map.get(s) , accounts); if (set.size() &gt; 0)&#123; List&lt;String&gt; curList = new ArrayList&lt;&gt;(); // add name curList.add(accounts.get(map.get(s).get(0)).get(0)); curList.addAll(set); res.add(curList); &#125; &#125; return res; &#125; public void dfs(Set&lt;String&gt; res, Map&lt;String, List&lt;Integer&gt;&gt; map, boolean[] visited, List&lt;Integer&gt; indexList, List&lt;List&lt;String&gt;&gt; accounts)&#123; for (Integer i: indexList)&#123; if (visited[i])&#123; continue; &#125; visited[i] = true; List&lt;String&gt; subList = accounts.get(i).subList(1, accounts.get(i).size()); res.addAll(subList); // visit the rest emails in the current account for (String s: subList)&#123; dfs(res, map, visited, map.get(s), accounts); &#125; &#125; &#125;&#125; 注：以上解法均来自于LeetCode]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-438:Find All Anagrams in a String]]></title>
    <url>%2F2020%2F05%2F16%2Fleetcode-438%2F</url>
    <content type="text"><![CDATA[题目链接：Find All Anagrams in a String 题目描述 题目难度：Medium Given a string s and a non-empty string p, find all the start indices of p‘s anagrams in s. Strings consists of lowercase English letters only and the length of both strings s and p will not be larger than 20,100. The order of output does not matter. Example 1: 123456789Input:s: &quot;cbaebabacd&quot; p: &quot;abc&quot;Output:[0, 6]Explanation:The substring with start index = 0 is &quot;cba&quot;, which is an anagram of &quot;abc&quot;.The substring with start index = 6 is &quot;bac&quot;, which is an anagram of &quot;abc&quot;. Example 2: 12345678910Input:s: &quot;abab&quot; p: &quot;ab&quot;Output:[0, 1, 2]Explanation:The substring with start index = 0 is &quot;ab&quot;, which is an anagram of &quot;ab&quot;.The substring with start index = 1 is &quot;ba&quot;, which is an anagram of &quot;ab&quot;.The substring with start index = 2 is &quot;ab&quot;, which is an anagram of &quot;ab&quot;. Solution1234567891011121314151617public class Solution &#123; public List&lt;Integer&gt; findAnagrams(String s, String p) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); if (s == null || s.length() == 0 || p == null || p.length() == 0) return list; int[] hash = new int[256]; //character hash for (char c : p.toCharArray()) &#123; hash[c]++; &#125; int left = 0, right = 0, count = p.length(); while (right &lt; s.length()) &#123; if (hash[s.charAt(right++)]-- &gt;= 1) count--; if (count == 0) list.add(left); if (right - left == p.length() &amp;&amp; hash[s.charAt(left++)]++ &gt;= 0) count++; &#125; return list; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-208:Implement Trie (Prefix Tree,前缀树或者字典树)]]></title>
    <url>%2F2020%2F04%2F22%2Fleetcode-208%2F</url>
    <content type="text"><![CDATA[题目链接：Prefix Tree 题目描述 题目难度：Medium Implement a trie with insert, search, and startsWith methods. Example: 12345678Trie trie = new Trie();trie.insert(&quot;apple&quot;);trie.search(&quot;apple&quot;); // returns truetrie.search(&quot;app&quot;); // returns falsetrie.startsWith(&quot;app&quot;); // returns truetrie.insert(&quot;app&quot;); trie.search(&quot;app&quot;); // returns true Note: You may assume that all inputs are consist of lowercase letters a-z. All inputs are guaranteed to be non-empty strings. Solution参考：https://leetcode.com/problems/implement-trie-prefix-tree/discuss/58832/AC-JAVA-solution-simple-using-single-array 1234567891011121314151617181920212223242526272829303132333435363738394041424344class TrieNode &#123; public boolean isWord; public TrieNode[] children = new TrieNode[26]; public TrieNode() &#123;&#125;&#125;public class Trie &#123; private TrieNode root; public Trie() &#123; root = new TrieNode(); &#125; public void insert(String word) &#123; TrieNode ws = root; for(int i = 0; i &lt; word.length(); i++)&#123; char c = word.charAt(i); if(ws.children[c - 'a'] == null)&#123; ws.children[c - 'a'] = new TrieNode(); &#125; ws = ws.children[c - 'a']; &#125; ws.isWord = true; &#125; public boolean search(String word) &#123; TrieNode ws = root; for(int i = 0; i &lt; word.length(); i++)&#123; char c = word.charAt(i); if(ws.children[c - 'a'] == null) return false; ws = ws.children[c - 'a']; &#125; return ws.isWord; &#125; public boolean startsWith(String prefix) &#123; TrieNode ws = root; for(int i = 0; i &lt; prefix.length(); i++)&#123; char c = prefix.charAt(i); if(ws.children[c - 'a'] == null) return false; ws = ws.children[c - 'a']; &#125; return true; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-207:Course Schedule(课程表)]]></title>
    <url>%2F2020%2F04%2F22%2Fleetcode-207%2F</url>
    <content type="text"><![CDATA[题目链接：Course Schedule 题目描述 题目难度：Medium There are a total of numCourses courses you have to take, labeled from 0 to numCourses-1. Some courses may have prerequisites, for example to take course 0 you have to first take course 1, which is expressed as a pair: [0,1] Given the total number of courses and a list of prerequisite pairs, is it possible for you to finish all courses? Example 1: 1234Input: numCourses = 2, prerequisites = [[1,0]]Output: trueExplanation: There are a total of 2 courses to take. To take course 1 you should have finished course 0. So it is possible. Example 2: 12345Input: numCourses = 2, prerequisites = [[1,0],[0,1]]Output: falseExplanation: There are a total of 2 courses to take. To take course 1 you should have finished course 0, and to take course 0 you should also have finished course 1. So it is impossible. Constraints: The input prerequisites is a graph represented by a list of edges, not adjacency matrices. Read more about how a graph is represented. You may assume that there are no duplicate edges in the input prerequisites. 1 &lt;= numCourses &lt;= 10^5 AC代码代码来自LeetCode 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution &#123; //状态表， 0 : 初始状态，没有被访问 1 : 正在被访问 2 : 访问完毕 int state[]; public boolean canFinish(int numCourses, int[][] prerequisites) &#123; if(numCourses == 0 || prerequisites==null) return true; state = new int[numCourses]; LinkedList&lt;Integer&gt; adj[] = new LinkedList[numCourses]; for(int[] pair:prerequisites)&#123; int v = pair[0]; if(adj[v] == null)&#123; adj[v] = new LinkedList&lt;Integer&gt;(); &#125; adj[v].add(pair[1]); &#125; for(int i=0;i&lt;numCourses;i++)&#123; // i 没有被访问，则访问 if(state[i]==0)&#123; //在访问 i 的过程中出现了环，则表示失败 if(!topologySort(i,adj)) return false; &#125; &#125; return true; &#125; public boolean topologySort(int source, List&lt;Integer&gt; adj[])&#123; List&lt;Integer&gt; neighbours = adj[source]; //将状态变为正在被访问状态 state[source] = 1; if(neighbours!=null)&#123; for(int i : neighbours)&#123; //没有访问则访问 if(state[i]==0)&#123; if(!topologySort(i,adj)) return false; //出现了环，失败 &#125;else if(state[i]==1) return false; &#125; &#125; //将状态变为访问完毕状态 state[source]=2; return true; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-205:Isomorphic Strings]]></title>
    <url>%2F2020%2F04%2F21%2Fleetcode-205%2F</url>
    <content type="text"><![CDATA[题目链接：Isomorphic Strings 题目描述 题目难度：Easy Given two strings s and t, determine if they are isomorphic. Two strings are isomorphic if the characters in s can be replaced to get t. All occurrences of a character must be replaced with another character while preserving the order of characters. No two characters may map to the same character but a character may map to itself. Example 1: 12Input: s = &quot;egg&quot;, t = &quot;add&quot;Output: true Example 2: 12Input: s = &quot;foo&quot;, t = &quot;bar&quot;Output: false Example 3: 12Input: s = &quot;paper&quot;, t = &quot;title&quot;Output: true Note:You may assume both s and t have the same length. Solution1234567891011public class Solution &#123; public boolean isIsomorphic(String s, String t) &#123; int m1[] = new int[256], m2[] = new int[256], n = s.length(); for (int i = 0; i &lt; n; ++i) &#123; if (m1[s.charAt(i)] != m2[t.charAt(i)]) return false; m1[s.charAt(i)] = i + 1; m2[t.charAt(i)] = i + 1; &#125; return true; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-204:Count Primes(计算质数个数)]]></title>
    <url>%2F2020%2F04%2F21%2Fleetcode-204%2F</url>
    <content type="text"><![CDATA[题目链接：Count Primes 题目描述 题目难度：Easy Count the number of prime numbers less than a non-negative number, n. Example: 123Input: 10Output: 4Explanation: There are 4 prime numbers less than 10, they are 2, 3, 5, 7. Solution112345678910111213class Solution &#123; public int countPrimes(int n) &#123; if(n &lt;= 1) return 0; boolean arrays[] = new boolean[n]; int count = 0; for(int i = 2;i &lt; n;i++) &#123; if(arrays[i] == false) count++; int j = 2; while(j * i &lt; n) arrays[i * j++] = true; &#125; return count; &#125; Solution2Solution1的改进版本，有3倍的速度提升 arrays[i] == false表示 *i * 是质数 12345678910111213141516171819class Solution &#123; public int countPrimes(int n) &#123; if(n &lt; 3) return 0; boolean arrays[] = new boolean[n]; int count = 1; // 2 是质数 //除2之外的偶数一定不是质数 for(int i = 3;i &lt; n;i += 2) &#123; if(arrays[i] == false) count++; int j = 3; while(j * i &lt; n) &#123; arrays[i * j] = true; j += 2; &#125; &#125; return count; &#125;&#125; Solution3相较于Solution2，速度有所提升 12345678910111213141516public int countPrimes(int n) &#123; if (n &lt; 3) return 0; boolean arrays[] = new boolean[n]; int count = 1; // 2 是质数 for (int i = 3; i &lt; n; i += 2) &#123; if (arrays[i] == false) count++; long j = i; if (j * i &gt; Integer.MAX_VALUE) continue; while (j * i &lt; n) &#123; arrays[i * (int) j] = true; j += 2; &#125; &#125; return count; &#125; Solution4相较于Solution3，速度有所提升 12345678910111213141516171819202122class Solution &#123; public int countPrimes(int n) &#123; if (n &lt; 3) return 0; boolean[] isPrime = new boolean[n]; /*初始质数的个数为 n / 2，即去掉除2之外的所有的偶数，然后再加上数字1，然后从 3 开始把非质数去掉，剩下的就是质数的个数。如 n = 10，即n的值为[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]，则 primes = 10 / 2 = 5，可能是质数的数为[2, 3, 5, 7, 9]。 */ int primes = n / 2; //把 i 的 k 倍的数字标记为非质数，其实 k 为 &gt;= j 的奇数 for (int i = 3; i * i &lt; n; i += 2) &#123; // i 不是质数，则 i 的整数倍的数字已经被标记为非质数了 if (isPrime[i]) continue; for (int j = i * i; j &lt; n; j += 2 * i) &#123; if (!isPrime[j]) &#123; primes--; isPrime[j] = true; &#125; &#125; &#125; return primes; &#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-203:Remove Linked List Elements]]></title>
    <url>%2F2020%2F04%2F21%2Fleetcode-203%2F</url>
    <content type="text"><![CDATA[题目链接：Remove Linked List Elements 题目描述 题目难度：Easy Remove all elements from a linked list of integers that have value val. Example: 12Input: 1-&gt;2-&gt;6-&gt;3-&gt;4-&gt;5-&gt;6, val = 6Output: 1-&gt;2-&gt;3-&gt;4-&gt;5 Solution1234567891011121314151617181920212223242526272829303132/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode removeElements(ListNode head, int val) &#123; if(head == null) return head; //删除链表开头值为val的连续结点 while(head != null &amp;&amp; head.val == val) head = head.next; if(head == null) return head; ListNode curNode = head; ListNode nextNode = head.next; while(nextNode != null) &#123; //nextNode 始终指向值不为 val 的结点 while(nextNode != null &amp;&amp; nextNode.val == val) nextNode = nextNode.next; //末尾节点值为 val 的情况 if(nextNode == null) &#123; curNode.next = null; return head; &#125; curNode.next = nextNode; curNode = curNode.next; nextNode = nextNode.next; &#125; return head; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-190:Reverse Bits]]></title>
    <url>%2F2020%2F04%2F21%2Fleetcode-190%2F</url>
    <content type="text"><![CDATA[题目链接：Reverse Bits 题目描述 题目难度：Easy Reverse bits of a given 32 bits unsigned integer. Example 1: 123Input: 00000010100101000001111010011100Output: 00111001011110000010100101000000Explanation: The input binary string 00000010100101000001111010011100 represents the unsigned integer 43261596, so return 964176192 which its binary representation is 00111001011110000010100101000000. Example 2: 123Input: 11111111111111111111111111111101Output: 10111111111111111111111111111111Explanation: The input binary string 11111111111111111111111111111101 represents the unsigned integer 4294967293, so return 3221225471 which its binary representation is 10111111111111111111111111111111. Note: Note that in some languages such as Java, there is no unsigned integer type. In this case, both input and output will be given as signed integer type and should not affect your implementation, as the internal binary representation of the integer is the same whether it is signed or unsigned. In Java, the compiler represents the signed integers using 2’s complement notation. Therefore, in Example 2 above the input represents the signed integer -3 and the output represents the signed integer -1073741825. Solution参考：My 3ms pure C solution 12345678uint32_t reverseBits(uint32_t n) &#123; uint32_t m = 0; for (int i = 0; i &lt; 32; i++, n &gt;&gt;= 1) &#123; m &lt;&lt;= 1; m |= n &amp; 1; &#125; return m;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-174:Dungeon Game]]></title>
    <url>%2F2020%2F04%2F20%2Fleetcode-174%2F</url>
    <content type="text"><![CDATA[题目链接：Dungeon Game 题目描述 题目难度：Hard The demons had captured the princess (P) and imprisoned her in the bottom-right corner of a dungeon. The dungeon consists of M x N rooms laid out in a 2D grid. Our valiant knight (K) was initially positioned in the top-left room and must fight his way through the dungeon to rescue the princess. The knight has an initial health point represented by a positive integer. If at any point his health point drops to 0 or below, he dies immediately. Some of the rooms are guarded by demons, so the knight loses health (negative integers) upon entering these rooms; other rooms are either empty (0’s) or contain magic orbs that increase the knight’s health (positive integers). In order to reach the princess as quickly as possible, the knight decides to move only rightward or downward in each step. Write a function to determine the knight’s minimum initial health so that he is able to rescue the princess. For example, given the dungeon below, the initial health of the knight must be at least 7 if he follows the optimal path RIGHT-&gt; RIGHT -&gt; DOWN -&gt; DOWN. -2 (K) -3 3 -5 -10 1 10 30 -5 (P) Note: The knight’s health has no upper bound. Any room can contain threats or power-ups, even the first room the knight enters and the bottom-right room where the princess is imprisoned. Solution1dfs，深搜，从左上角到右下角走，自顶向下，超时了 1234567891011121314151617181920212223class Solution &#123; int min = Integer.MAX_VALUE; public int calculateMinimumHP(int[][] dungeon) &#123; if(dungeon == null || dungeon.length == 0) return min; calculateMinimumHPCore(dungeon, 0, 0, 0, dungeon[0][0]); return min; &#125; private void calculateMinimumHPCore(int[][] dungeon, int i, int j, int cur, int curMin) &#123; if(i &gt; dungeon.length - 1 || j &gt; dungeon[0].length - 1) return; curMin = Math.min(curMin, cur + dungeon[i][j]); if(i == dungeon.length - 1 &amp;&amp; j == dungeon[0].length - 1) &#123; cur += dungeon[i][j]; min = Math.min(min, Math.max( (curMin &lt;= 0 ? (0 - curMin + 1) : 1), (cur &lt;= 0 ? 0 - cur + 1 : 1))); return; &#125; calculateMinimumHPCore(dungeon, i + 1, j, cur + dungeon[i][j], curMin); calculateMinimumHPCore(dungeon, i, j + 1, cur + dungeon[i][j], curMin); &#125;&#125; Solution2mem[i][j]表示，把[i, j]所在的位置当作K，此时所需的最小初始健康值。 这种解法是自底向上，从P出发到K。 12345678910111213141516171819202122232425class Solution &#123; public int calculateMinimumHP(int[][] dungeon) &#123; if (dungeon.length == 0 || dungeon[0].length == 0) &#123; return 0; &#125; return helper(dungeon, 0, 0, new int[dungeon.length][dungeon[0].length]); &#125; //从[i, j]出发所需的最小初始健康值 private int helper(int[][] d, int i, int j, int[][] mem) &#123; if (i &gt;= d.length || j &gt;= d[0].length) &#123; return Integer.MAX_VALUE; &#125; if (mem[i][j] &gt; 0) &#123; return mem[i][j]; &#125; int min = Math.min(helper(d, i + 1, j, mem), helper(d, i, j + 1, mem)); if (min == Integer.MAX_VALUE) &#123; min = 1; &#125; mem[i][j] = Math.max(min - d[i][j], 1); return mem[i][j]; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-191:Number of 1 Bits(二进制中1的个数)]]></title>
    <url>%2F2020%2F04%2F20%2Fleetcode-191%2F</url>
    <content type="text"><![CDATA[题目链接：Number of 1 Bits 题目描述 题目难度：Easy Write a function that takes an unsigned integer and return the number of ‘1’ bits it has (also known as the Hamming weight). Example 1: 123Input: 00000000000000000000000000001011Output: 3Explanation: The input binary string 00000000000000000000000000001011 has a total of three &apos;1&apos; bits. Example 2: 123Input: 00000000000000000000000010000000Output: 1Explanation: The input binary string 00000000000000000000000010000000 has a total of one &apos;1&apos; bit. Example 3: 123Input: 11111111111111111111111111111101Output: 31Explanation: The input binary string 11111111111111111111111111111101 has a total of thirty one &apos;1&apos; bits. Note: Note that in some languages such as Java, there is no unsigned integer type. In this case, the input will be given as signed integer type and should not affect your implementation, as the internal binary representation of the integer is the same whether it is signed or unsigned. In Java, the compiler represents the signed integers using 2’s complement notation. Therefore, in Example 3 above the input represents the signed integer -3. AC代码112345public class Solution &#123; public int hammingWeight(int n) &#123; return Integer.bitCount(n); &#125;&#125; AC代码2参考：https://www.nowcoder.com/questionTerminal/8ee967e43c2c4ec193b040ea7fbb10b8 如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。举个例子：一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现减1的结果是把最右边的一个1开始的所有位都取反了。这个时候如果我们再把原来的整数和减去1之后的结果做与运算，从原来整数最右边一个1那一位开始所有位都会变成0。如1100&amp;1011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。 12345678910public class Solution &#123; public int hammingWeight(int n) &#123; int count = 0; while(n!= 0)&#123; count++; n = n &amp; (n - 1); &#125; return count; &#125;&#125; AC代码3n 每次都是无符号右移一位，可避免当n为负数时，最高位总是1，从而引发死循环。 123456789101112public class Solution &#123; public int hammingWeight(int n) &#123; int count = 0; while(n != 0)&#123; if((n &amp; 1) != 0)&#123; count++; &#125; n = n &gt;&gt;&gt; 1; &#125; return count; &#125;&#125; AC代码412345678910111213public class Solution &#123; // you need to treat n as an unsigned value public int hammingWeight(int n) &#123; int numOfOne = 0; while(n != 0)&#123; numOfOne++; //n &amp; -n 表示的是 n 从右到左第一个1所在的位置 //如n = 6,那么 n &amp; -n 就是 2 n = n &amp; (~(n &amp; -n)); &#125; return numOfOne; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-189:Rotate Array]]></title>
    <url>%2F2020%2F04%2F20%2Fleetcode-189%2F</url>
    <content type="text"><![CDATA[题目链接：Rotate Array 题目描述 题目难度：Easy Given an array, rotate the array to the right by k steps, where k is non-negative. Example 1: 123456Input: [1,2,3,4,5,6,7] and k = 3Output: [5,6,7,1,2,3,4]Explanation:rotate 1 steps to the right: [7,1,2,3,4,5,6]rotate 2 steps to the right: [6,7,1,2,3,4,5]rotate 3 steps to the right: [5,6,7,1,2,3,4] Example 2: 12345Input: [-1,-100,3,99] and k = 2Output: [3,99,-1,-100]Explanation: rotate 1 steps to the right: [99,-1,-100,3]rotate 2 steps to the right: [3,99,-1,-100] Note: Try to come up as many solutions as you can, there are at least 3 different ways to solve this problem. Could you do it in-place with O(1) extra space? AC代码12345678910111213141516171819202122class Solution &#123; public void rotate(int[] nums, int k) &#123; if(nums == null || nums.length &lt;= 1) return; k %= nums.length; rorate(nums, 0, nums.length - k - 1); rorate(nums, nums.length - k, nums.length - 1); rorate(nums, 0, nums.length - 1); &#125; private void rorate(int[] nums, int start, int end) &#123; int temp = 0; while(start &lt; end) &#123; temp = nums[start]; nums[start] = nums[end]; nums[end] = temp; start++; end--; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-187:Repeated DNA Sequences]]></title>
    <url>%2F2020%2F04%2F20%2Fleetcode-187%2F</url>
    <content type="text"><![CDATA[题目链接：Repeated DNA Sequences 题目描述 题目难度：Medium All DNA is composed of a series of nucleotides abbreviated as A, C, G, and T, for example: “ACGAATTCCG”. When studying DNA, it is sometimes useful to identify repeated sequences within the DNA. Write a function to find all the 10-letter-long sequences (substrings) that occur more than once in a DNA molecule. Example: 123Input: s = &quot;AAAAACCCCCAAAAACCCCCCAAAAAGGGTTT&quot;Output: [&quot;AAAAACCCCC&quot;, &quot;CCCCCAAAAA&quot;] AC代码112345678910111213141516171819202122class Solution &#123; public List&lt;String&gt; findRepeatedDnaSequences(String s) &#123; List&lt;String&gt; resList = new ArrayList&lt;&gt;(); Set&lt;Integer&gt; words = new HashSet&lt;&gt;(); Set&lt;Integer&gt; doublewords = new HashSet&lt;&gt;(); char[] maps = new char[26]; //maps['A' - 'A'] = 0; maps['C' - 'A'] = 1; maps['G' - 'A'] = 2; maps['T' - 'A'] = 3; for(int i = 0;i + 9 &lt; s.length();i++)&#123; // 注意是 i + 9 int val = 0; for(int j = 0;j &lt; 10;j++)&#123; val &lt;&lt;= 2; val |= maps[s.charAt(j + i) - 'A']; &#125; if(!words.add(val) &amp;&amp; doublewords.add(val)) resList.add(s.substring(i,i + 10)); &#125; return resList; &#125;&#125; AC代码21234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public List&lt;String&gt; findRepeatedDnaSequences(String s)&#123; if(s.length() &lt;= 10) return new ArrayList&lt;&gt;(); //注意到字符只有ACGT, 因此考虑映射成二进制数00,01,10,11, 也就是0,1,2,3 //尽量避免使用hash, 因此字符'X'--&gt;X-'A' //'T'-'A' = n-1 int[] map = new int['T' - 'A' + 1]; map['A' - 'A'] = 0; map['C' - 'A'] = 1; map['G' - 'A'] = 2; map['T' - 'A'] = 3; byte[] counts = new byte[1048577];//20位数最大为1048575, 作为序号 List&lt;String&gt; ans = new ArrayList&lt;&gt;(); //构建长度为10的数字 int x = 0; for(int i = 0 ; i &lt; 10 ; i++)&#123; x = x &lt;&lt; 2; x = x | map[s.charAt(i) - 'A']; &#125; counts[x] = 1; for(int i = 10 ; i &lt; s.length() ; i++)&#123; //将最前面的2位, 置为0 x = x &amp; (~ (3 &lt;&lt; 18)); x = x &lt;&lt; 2; //尾部添加新字符 x = x | map[s.charAt(i) - 'A']; if(counts[x] == 1)&#123; ans.add(s.substring(i - 9 , i + 1)); counts[x] = 2; &#125; if(counts[x] == 0)&#123; counts[x] = 1; &#125; &#125; return ans; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-179:Largest Number]]></title>
    <url>%2F2020%2F04%2F20%2Fleetcode-179%2F</url>
    <content type="text"><![CDATA[题目链接： Largest Number 题目描述 题目难度：Medium Given a list of non negative integers, arrange them such that they form the largest number. Example 1: 12Input: [10,2]Output: &quot;210&quot; Example 2: 12Input: [3,30,34,5,9]Output: &quot;9534330&quot; Note: The result may be very large, so you need to return a string instead of an integer. AC代码12345678910111213141516171819class Solution &#123; public String largestNumber(int[] nums) &#123; StringBuilder sb = new StringBuilder(); Integer arrays[] = new Integer[nums.length]; for(int i = 0;i &lt; nums.length;i++) arrays[i] = nums[i]; Arrays.sort(arrays, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return -(String.valueOf(o1) + String.valueOf(o2)).compareTo(String.valueOf(o2) + String.valueOf(o1)); &#125; &#125; ); if(arrays[0] == 0) return "0"; for(int i : arrays) sb.append(i); return sb.toString(); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-173:Binary Search Tree Iterator(BST迭代器)]]></title>
    <url>%2F2020%2F04%2F19%2Fleetcode-173%2F</url>
    <content type="text"><![CDATA[题目链接：Binary Search Tree Iterator 题目描述 题目难度：Medium Implement an iterator over a binary search tree (BST). Your iterator will be initialized with the root node of a BST. Calling next() will return the next smallest number in the BST. Example: 12345678910BSTIterator iterator = new BSTIterator(root);iterator.next(); // return 3iterator.next(); // return 7iterator.hasNext(); // return trueiterator.next(); // return 9iterator.hasNext(); // return trueiterator.next(); // return 15iterator.hasNext(); // return trueiterator.next(); // return 20iterator.hasNext(); // return false Note: next() and hasNext() should run in average O(1) time and uses O(h) memory, where h is the height of the tree. You may assume that next() call will always be valid, that is, there will be at least a next smallest number in the BST when next() is called. AC代码11234567891011121314151617181920212223242526272829303132333435363738394041/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class BSTIterator &#123; List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); int index = -1; public BSTIterator(TreeNode root) &#123; inorder(root); &#125; public void inorder(TreeNode root)&#123; if(root == null) return; inorder(root.left); list.add(root.val); inorder(root.right); &#125; /** @return the next smallest number */ public int next() &#123; return list.get(++index); &#125; /** @return whether we have a next smallest number */ public boolean hasNext() &#123; return index &lt; list.size()-1; &#125;&#125;/** * Your BSTIterator object will be instantiated and called as such: * BSTIterator obj = new BSTIterator(root); * int param_1 = obj.next(); * boolean param_2 = obj.hasNext(); */ AC代码2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class BSTIterator &#123; private Stack&lt;TreeNode&gt; stack; public BSTIterator(TreeNode root) &#123; stack = new Stack&lt;&gt;(); TreeNode cur = root; while(cur != null)&#123; stack.push(cur); if(cur.left != null) cur = cur.left; else break; &#125; &#125; /** @return whether we have a next smallest number */ public boolean hasNext() &#123; return !stack.isEmpty(); &#125; /** @return the next smallest number */ public int next() &#123; TreeNode node = stack.pop(); TreeNode cur = node; // traversal right branch if(cur.right != null)&#123; cur = cur.right; while(cur != null)&#123; stack.push(cur); if(cur.left != null) cur = cur.left; else break; &#125; &#125; return node.val; &#125;&#125;/** * Your BSTIterator object will be instantiated and called as such: * BSTIterator obj = new BSTIterator(root); * int param_1 = obj.next(); * boolean param_2 = obj.hasNext(); */]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-169:Majority Element]]></title>
    <url>%2F2020%2F04%2F19%2Fleetcode-169%2F</url>
    <content type="text"><![CDATA[题目链接：Majority Element 题目描述 题目难度：Easy Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array. Example 1: 12Input: [3,2,3]Output: 3 Example 2: 12Input: [2,2,1,1,1,2,2]Output: 2 AC代码“同归于尽法” 123456789101112131415161718class Solution &#123; public int majorityElement(int[] nums) &#123; if(nums == null || nums.length == 0) return -1; int count = 1; int curNum = nums[0]; int i = 1; for(; i &lt; nums.length; i++) &#123; if(nums[i] == curNum) count++; else count--; if(count == 0) &#123; curNum = nums[i + 1]; &#125; &#125; return curNum; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-168:Excel Sheet Column Title]]></title>
    <url>%2F2020%2F04%2F19%2Fleetcode-168%2F</url>
    <content type="text"><![CDATA[题目链接：Excel Sheet Column Title 题目描述 题目难度：Easy Given a positive integer, return its corresponding column title as appear in an Excel sheet. For example: 123456781 -&gt; A2 -&gt; B3 -&gt; C...26 -&gt; Z27 -&gt; AA28 -&gt; AB ... Example 1: 12Input: 1Output: "A" Example 2: 12Input: 28Output: "AB" Example 3: 12Input: 701Output: "ZY" AC代码11234567891011121314151617181920class Solution &#123; public String convertToTitle(int n) &#123; StringBuilder sb = new StringBuilder(); while(n != 0) &#123; int a = n % 26; if(a == 0) &#123; sb.append("Z"); n = n / 27; &#125; else &#123; sb.append((char)(a + 64)); n /= 26; &#125; &#125; return sb.reverse().toString(); &#125;&#125; AC代码2123456789101112public class Solution &#123; public String convertToTitle(int n) &#123; StringBuilder result = new StringBuilder(); while (n &gt; 0) &#123; n--; result.insert(0, (char) ('A' + n % 26)); n /= 26; &#125; return result.toString(); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-167:Two Sum II - Input array is sorted]]></title>
    <url>%2F2020%2F04%2F19%2Fleetcode-167%2F</url>
    <content type="text"><![CDATA[题目链接：Two Sum II - Input array is sorted 题目描述 题目难度：Easy Given an array of integers that is already sorted in ascending order, find two numbers such that they add up to a specific target number. The function twoSum should return indices of the two numbers such that they add up to the target, where index1 must be less than index2. Note: Your returned answers (both index1 and index2) are not zero-based. You may assume that each input would have exactly one solution and you may not use the same element twice. Example: 123Input: numbers = [2,7,11,15], target = 9Output: [1,2]Explanation: The sum of 2 and 7 is 9. Therefore index1 = 1, index2 = 2. AC代码12345678910111213141516class Solution &#123; public int[] twoSum(int[] numbers, int target) &#123; int[] res = new int[2]; int left = 0, right = numbers.length - 1; int curSum = numbers[left] + numbers[right]; while(curSum != target) &#123; if(curSum &gt; target) right--; else if(curSum &lt; target) left++; curSum = numbers[left] + numbers[right]; &#125; res[0] = left + 1; res[1] = right + 1; return res; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[位图]]></title>
    <url>%2F2020%2F04%2F19%2Fbitmap-1%2F</url>
    <content type="text"><![CDATA[位图简介位图可以用于无重复数字的排序，核心思想是用一个bit来表示某一个数字是否存在（1：存在，0：不存在）。位图排序的时间复杂度为O(n)，空间复杂度为O(n位)。 C程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546//位图排序#include &lt;iostream&gt;#include &lt;bitset&gt;#define WIDTHWORD 32 //一个整数的宽度是32bit#define SHIFT 5 #define MASK 0x1F //0x1f == 31#define N 100 //对100个无重复的整数排序using namespace std; //申请一个N位的bitmapint bitmap[1 + N / WIDTHWORD]; //将bitmap的第value设置为1void set(int value) &#123; bitmap[value &gt;&gt; SHIFT] |= (1 &lt;&lt; (value &amp; MASK));&#125; //清除bitmap第value位上的1:设置为0void clear(int value) &#123; bitmap[value &gt;&gt; SHIFT] &amp;= ~(1 &lt;&lt; (value &amp; MASK));&#125; //测试bitmap第value位是否为1int test(int value) &#123; return bitmap[value &gt;&gt; SHIFT] &amp; (1 &lt;&lt; (value &amp; MASK));&#125; int main() &#123; int a[] = &#123;12, 5, 1, 89, 64, 49, 77, 91, 3, 0, 32, 50, 99&#125;; int length = sizeof(a) / sizeof(int); //将bitmap所有位设置为0 for (int i = 0; i &lt; N; ++i) &#123; clear(i); &#125; //bitmap中将待排序数组中值所在的位设置为1 for (int i = 0; i &lt; length; i++) set(a[i]); //输出排序后的结果 for (int i = 0; i &lt; N; ++i) &#123; if (test(i)) cout &lt;&lt; i &lt;&lt; " "; &#125;&#125; 使用C++的bitset1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;bitset&gt;#define N 100using namespace std; int main() &#123; int a[] = &#123;12, 5, 1, 89, 64, 49, 77, 91, 3, 0, 32, 50, 99&#125;; int length = sizeof(a) / sizeof(int); //直接使用C++bitset，申请Nbit的空间，每一位均设置为0 bitset&lt;N&gt; bitmap; //遍历待排序数组，将bitmap中对应位设置为1 for (int i = 0; i &lt; length; i++) bitmap.set(a[i], 1); //输入排序结果 for (int i = 0; i &lt; N; ++i) &#123; if (bitmap[i]) cout &lt;&lt; i &lt;&lt; " "; &#125;&#125; 重复元素数组的排序如果待排序数组中每个整数重复出现次数不超过10次，那么可以用4位表示一个整数，用这四位统计该数出现次数，然后排序后输出该数时，输出m次即可。 其他用途1、找出不重复数： 在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数 2、判断某数是否存在于海量整数中： 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 参考： bitmap对海量无重复的整数排序]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>bitmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-164:Maximum Gap(最大间距)]]></title>
    <url>%2F2020%2F04%2F19%2Fleetcode-164%2F</url>
    <content type="text"><![CDATA[题目链接：Maximum Gap 题目描述 题目难度：Hard Given an unsorted array, find the maximum difference between the successive elements in its sorted form. Return 0 if the array contains less than 2 elements. Example 1: 1234Input: [3,6,9,1]Output: 3Explanation: The sorted form of the array is [1,3,6,9], either (3,6) or (6,9) has the maximum difference 3. Example 2: 123Input: [10]Output: 0Explanation: The array contains less than 2 elements, therefore return 0. Note: You may assume all elements in the array are non-negative integers and fit in the 32-bit signed integer range. Try to solve it in linear time/space. 尝试因为在这个题目中，出现重复的数字对于结果来说是没有影响的。比如[3, 6, 9, 1] 和 [3, 9, 6, 1, 9, 1]的结果都是3,所以有一种解决办法就是位图，核心思想是用一个bit位来表示某个数字是否存在。位图一般用来解决无重复数字的排序问题，时间复杂度为O(n)，空间复杂度为O(n)。代码如下： 12345678910111213141516171819202122232425262728293031class Solution &#123; private static final int WIDTHWORD = 32; //一个整数的宽度是32bit private static final int SHIFT = 5; private static final int MASK = 0x1F; //0x1f == 31 public int maximumGap(int[] nums) &#123; if(nums == null || nums.length &lt; 2) return 0; int max = nums[0]; int maxDifference = 0, preNum = -1; int len = nums.length; for(int i = 1;i &lt; len;i++) if(nums[i] &gt; max) max = nums[i]; int bitMap[] = new int[1 + max / WIDTHWORD]; for (int i = 0; i &lt; len; i++) bitMap[nums[i] &gt;&gt; SHIFT] |= (1 &lt;&lt; (nums[i] &amp; MASK)); for (int i = 0; i &lt;= max; ++i) &#123; if ( (bitMap[i &gt;&gt; SHIFT] &amp; (1 &lt;&lt; (i &amp; MASK))) != 0) &#123; if(preNum == -1) preNum = i; else &#123; maxDifference = (i - preNum) &gt; maxDifference ? (i - preNum) : maxDifference; preNum = i; &#125; &#125; &#125; return maxDifference; &#125;&#125; 结果是正确的，但是遇到某些case会超时，如以下case： 12&gt; [119458430,410028302,15982430,357241335,694698590,529536210,10295004,340726408,376705042,5648398,425888724,86150790,141550201,28628951,75242355,441374731,33363303,102347550,225909314,214275444,244909062,855305835,237845472,3355885,51545952,135311344,157929786,527907114,89782308,94647333,8910464,344171577,116390552,346980625,278264060,268489590,804080832,716690440,2426580,19004370,133272093,252996210,468692992,199336776,293697212,140484060,252870691,515767290,68435274,17093532,442238940,512174759,59982768,110430496,423197815,133049526,396334614,195512240,698061558,439133815,18761859,245630732,34904140,395302910,130676087,431466938,16627844,166879053,12372060,49183701,610095167,702709920,268394716,414536880,3427575,6691692,491903478,483507682,3070270,85414784,5099136,90587601,142649685,118404720,842001390,558266077,312767292,544397826,191075058,526146660,110684881,517521627,30493560,48581148,740481424,50160000,13157144,19385314,163198805,26886400,133712000,912151324,48210912,133547250,52982906,197472724,208125504,837346,709414284,396336348,60213406,623044591,17630886,147180747,387038799,752489738,19244553,246993366,12712240,38527412,735167847,5650645,41681390,88521660,506419704,112878845,217291473,78666952,270974445,67984895,380932470,243841688,8850110,666691696,304193860,43211280,198116823,33948796,13335525,28420812,21050712,756482508,128778464,170400077,298966045,241786051,215049569,14209914,79150986,24352440,2116400,65035575,775059750,57782506,571417902,466282355,17766714,538008622,665264180,2117532,178667473,755760948,641674494,685749764,378972351,660147070,438379281,23188440,706912830,5572941,274989216,99784281,857173344,600184575,301751644,214164810,659915900,383952912,8481186,162511173,100142682,549049700,209229028,37982520,382106825,138857280,245934684,535442040,468179010,54055316,32480100,690569635,84640120,33317120,5642820,154232160,285539970,119883575,89168962,370189908,70960941,500751504,195606720,11929730,400311912,18992646,87712940,16851726,128856126,319801524,197542912,208377480,454448880,765892862,64000432,110419260,162102932,13562620,390161466,801080960,272334192,470694965,237498086,202663747,257755446,235713269,203990625,129525760,35067109,229728632,291737083,376510302,78974950,98342706,8990602,666869410,1254669,78094600,196383867,31448198,148435928,309242010,82166032,23200666,197811280,628895010,325339577,20031264,5071729,291573105,154858160,354686068,525529860,193712025,291968541,78746580,155359008,452569520,419772492,434216667,8105543,229222994,27549552,37935126,113570474,922373616,639252950,493639960,476380630,431031908,31790762,210544806,17778084,119418138,206927570,36597600,163780056,543271210,447340280,544898816,611803668,604361730,247409239,410379840,335820,47584368,657907250,256298595,175246445,103401100,985822236,82407754,375456872,277085610,20940032,7602130,263623668,624148511,31428831,41502413,627479996,178545044,20512233,452471045,12947552,110520912,205450345,229625910,48527220,111079904,595135620,402737010,18232903,5500680,279678301,73265530,110349876,68149872,30556512,29704584,594177147,254648576,59403834,29461485,99749100,14924800,104989500,272741678,19435955,197495298,46066566,22311681,94362997,197386436,23533699,455842044,483385025,59460616,831293700,6408891,628465310,4115578,394652304,32231760,35254602,44157960,427490670,51320565,927766971,117541914,310797000,321006786,522594000,60382323,440867925,72305472,87839850,222566545,81488225,304458750,299897136,153407224,171569840,858642093,129862580,216602945,190715364,661243282,323834118,460776228,370760012,39348248,73766112,146395197,236149479,325768597,252750196,628500093,438397270,907074,535412620,205098718,87239609,185756064,40103341,385671138,18138120,528855629,257931315,106414084,774346496,92061346,345273908,182427657,8189075,72743840,90180272,211858911,2360232,831433071,9551980,148009276,283078048,320771004,900209310,98962920,24433302,679558806,2613915,395736396,17177988,251675190,75214429,112437630,179743448,317896579,109450689,240366413,190951350,53456996,63545394,765989516,608856880,430709760,687381972,46899156,220609950,18909450,30726138,522034125,114667420,26237256,448349745,937314708,11764714,53221326,441120358,18030324,84336252,388939775,557245150,61863260,2612727,462836595,226766828,90428355,454333440]&gt; 因为在位图中，需要遍历从0开始到数组中的最大值，才能对数组进行排序。因此如果数组中有特别大的数字，那么就会较为耗时。 AC代码1一种更有效的方法是基数排序。在这里参考了I solved it using radix sort，以下代码是对二进制形式的数字进行基数排序：int类型的二进制表示为32位，因此从低位到高位需要32轮循环。在第 i 次循环中，把该位为1的放入onebucket，把该位为0的放入zerobucket，num然后依次加入zerobucket、onebucket。32次循环之后，num中的元素就是从小到大的结果。 1234567891011121314151617181920212223242526class Solution: # @param num, a list of integer # @return an integer def maximumGap(self, num): if len(num) &lt; 2: return 0 num = self.radixSort(num) res = 0 for i in range(1, len(num)): res = max(num[i] - num[i - 1], res) return res def radixSort(self, num): for i in range(31): onebucket = [] zerobucket = [] needle = 1 &lt;&lt; i for j in range(len(num)): if num[j] &amp; needle != 0: onebucket.append(num[j]) else: zerobucket.append(num[j]) num = [] num += zerobucket num += onebucket return num AC代码2参考：[bucket sort] JAVA solution with explanation, O(N) time and space Suppose there are N elements in the array, the min value is min and the max value is max. Then the maximum gap will be no smaller than ceiling[(max - min ) / (N - 1)]. Let gap = ceiling[(max - min ) / (N - 1)]. We divide all numbers in the array into n-1 buckets, where k-th bucket contains all numbers in [min + (k-1)gap, min + kgap). Since there are n-2 numbers that are not equal **min* or *max*** and there are n-1 buckets, at least one of the buckets are empty. We only need to store the largest number and the smallest number in each bucket. After we put all the numbers into the buckets. We can scan the buckets sequentially and get the max gap. 123456789101112131415161718192021222324252627282930313233343536373839404142public class Solution &#123;public int maximumGap(int[] num) &#123; if (num == null || num.length &lt; 2) return 0; // get the max and min value of the array int min = num[0]; int max = num[0]; for (int i:num) &#123; min = Math.min(min, i); max = Math.max(max, i); &#125; // the minimum possibale gap, ceiling of the integer division int gap = (int)Math.ceil((double)(max - min)/(num.length - 1)); int[] bucketsMIN = new int[num.length - 1]; // store the min value in that bucket int[] bucketsMAX = new int[num.length - 1]; // store the max value in that bucket Arrays.fill(bucketsMIN, Integer.MAX_VALUE); Arrays.fill(bucketsMAX, Integer.MIN_VALUE); // put numbers into buckets for (int i:num) &#123; //there in no min and max not in bucket if (i == min || i == max) continue; int idx = (i - min) / gap; // index of the right position in the buckets bucketsMIN[idx] = Math.min(i, bucketsMIN[idx]); bucketsMAX[idx] = Math.max(i, bucketsMAX[idx]); &#125; // scan the buckets for the max gap int maxGap = Integer.MIN_VALUE; int previous = min; for (int i = 0; i &lt; num.length - 1; i++) &#123; if (bucketsMIN[i] == Integer.MAX_VALUE &amp;&amp; bucketsMAX[i] == Integer.MIN_VALUE) // empty bucket continue; // min value minus the previous value is the current gap maxGap = Math.max(maxGap, bucketsMIN[i] - previous); // update previous bucket value previous = bucketsMAX[i]; &#125; maxGap = Math.max(maxGap, max - previous); // updata the final max value gap return maxGap; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-165:Compare Version Numbers(比较版本号)]]></title>
    <url>%2F2020%2F04%2F19%2Fleetcode-165%2F</url>
    <content type="text"><![CDATA[题目链接：Compare Version Numbers 题目描述 题目难度：Medium Compare two version numbers version1 and version2. If *version1* &gt; *version2* return 1; if *version1* &lt; *version2* return -1;otherwise return 0. You may assume that the version strings are non-empty and contain only digits and the . character. The . character does not represent a decimal point and is used to separate number sequences. For instance, 2.5 is not “two and a half” or “half way to version three”, it is the fifth second-level revision of the second first-level revision. You may assume the default revision number for each level of a version number to be 0. For example, version number 3.4 has a revision number of 3 and 4 for its first and second level revision number. Its third and fourth level revision number are both 0. Example 1: 12Input: version1 = &quot;0.1&quot;, version2 = &quot;1.1&quot;Output: -1 Example 2: 12Input: version1 = &quot;1.0.1&quot;, version2 = &quot;1&quot;Output: 1 Example 3: 12Input: version1 = &quot;7.5.2.4&quot;, version2 = &quot;7.5.3&quot;Output: -1 Example 4: 123Input: version1 = &quot;1.01&quot;, version2 = &quot;1.001&quot;Output: 0Explanation: Ignoring leading zeroes, both “01” and “001&quot; represent the same number “1” Example 5: 123Input: version1 = &quot;1.0&quot;, version2 = &quot;1.0.0&quot;Output: 0Explanation: The first version number does not have a third level revision number, which means its third level revision number is default to &quot;0&quot; Note: Version strings are composed of numeric strings separated by dots . and this numeric strings may have leading zeroes. Version strings do not start or end with dots, and they will not be two consecutive dots. AC代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution &#123; public int compareVersion(String version1, String version2) &#123; if(version1 == null &amp;&amp; version2 == null) return 0; if(version1 == null) return -1; if(version2 == null) return 1; if(version1.equals(version2)) return 0; String[] strs1 = version1.split("\\."); String[] strs2 = version2.split("\\."); int len = (strs1.length &gt; strs2.length) ? strs2.length : strs1.length; int i = 0; int s1 = 0; int s2 = 0; for(;i &lt; len;) &#123; s1 = dropLeadingZeroes(strs1[i]); s2 = dropLeadingZeroes(strs2[i]); if(s1 &gt; s2) return 1; else if(s1 &lt; s2) return -1; else i++; &#125; if( (i == strs2.length) &amp;&amp; (i == strs1.length) ) return 0; else if( i == strs1.length ) &#123; while( (i &lt; strs2.length) &amp;&amp; dropLeadingZeroes(strs2[i]) == 0) i++; if(i == strs2.length) return 0; else return -1; &#125; else &#123; while( (i &lt; strs1.length) &amp;&amp; dropLeadingZeroes(strs1[i]) == 0) i++; if(i == strs1.length) return 0; else return 1; &#125; &#125; //去掉 s 的前导0，并转换为 int // 另一种写法：return Integer.parseInt(s); 这样写速度较慢 private int dropLeadingZeroes(String s) &#123; if(s == null || s.length() == 0 || s.charAt(0) != '0') return Integer.parseInt(s); int i = 1; while(i &lt; s.length()) &#123; if(s.charAt(i) == '0') i++; else return Integer.parseInt(s.substring(i)); &#125; if(i == s.length()) return 0; return 0; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-154:Find Minimum in Rotated Sorted Array II]]></title>
    <url>%2F2020%2F04%2F18%2Fleetcode-154%2F</url>
    <content type="text"><![CDATA[题目链接：Find Minimum in Rotated Sorted Array II 题目描述 题目难度：Hard Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand. (i.e., [0,1,2,4,5,6,7] might become [4,5,6,7,0,1,2]). Find the minimum element. The array may contain duplicates. Example 1: 12Input: [1,3,5]Output: 1 Example 2: 12Input: [2,2,2,0,1]Output: 0 Note: This is a follow up problem to Find Minimum in Rotated Sorted Array. Would allow duplicates affect the run-time complexity? How and why? AC代码参考：Beats 100% Binary Search with Explanations 解释： 设 lo = 0, hi = nums.length - 1，最小值 min 一定在 [lo, mi] 之间，设mi = lo + (hi - lo) / 2; lo &lt;= mi &lt;= hi ， 如果 ( nums[mi] &gt; nums[hi] ) ，结合 mi &lt;= hi 的条件，那么说明 min 在 [mi, hi] 之间，则 hi 不变，lo的值从mi + 1位置开始（索引为 mi 的元素不可能是最小值，因为 nums[mi] &gt; nums[hi]） 如果 (nums[mi] &lt; nums[lo]) ，结合 lo &lt;= mi 的条件，说明 min 在 [lo, hi] 之间，则 lo 不变，hi的值从mi位置开始（索引为 mi 的元素有可能是最小值） 否则，也就是如果 nums[lo] &lt;= nums[mi] &lt;= nums[hi] ，则 min 可能在 [lo, hi)之间(只有nums[lo] == nums[mi] == nums[hi] 的时候索引为 hi 的元素才是最小值，这个时候不考虑索引为 hi 的元素结果也是正确的)，此时 hi = hi - 1。 1234567891011121314151617181920class Solution &#123; public int findMin(int[] nums) &#123; int lo = 0, hi = nums.length - 1; while (lo &lt; hi) &#123; int mi = lo + (hi - lo) / 2; if (nums[mi] &gt; nums[hi]) &#123; lo = mi + 1; &#125; else if (nums[mi] &lt; nums[lo]) &#123; hi = mi; lo++; &#125; else &#123; // nums[lo] &lt;= nums[mi] &lt;= nums[hi] hi--; &#125; &#125; return nums[lo]; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-153:Find Minimum in Rotated Sorted Array(旋转数组中的最小元素)]]></title>
    <url>%2F2020%2F04%2F18%2Fleetcode-153%2F</url>
    <content type="text"><![CDATA[题目链接：Find Minimum in Rotated Sorted Array 题目描述 题目难度：Medium Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand. (i.e., [0,1,2,4,5,6,7] might become [4,5,6,7,0,1,2]). Find the minimum element. You may assume no duplicate exists in the array. Example 1: 12Input: [3,4,5,1,2] Output: 1 Example 2: 12Input: [4,5,6,7,0,1,2]Output: 0 AC代码参考：Java solution with binary search The minimum element must satisfy one of two conditions: 1) If rotate, A[min] &lt; A[min - 1]; 2) If not, A[0]. Therefore, we can use binary search: check the middle element, if it is less than previous one, then it is minimum. If not, there are 2 conditions as well: If it is greater than both left and right element, then minimum element should be on its right, otherwise on its left. 1234567891011121314151617181920212223class Solution &#123; public int findMin(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return 0; &#125; if (nums.length == 1) &#123; return nums[0]; &#125; int start = 0, end = nums.length - 1; while (start &lt; end) &#123; int mid = (start + end) / 2; if (mid &gt; 0 &amp;&amp; nums[mid] &lt; nums[mid - 1]) &#123; return nums[mid]; &#125; if (nums[start] &lt;= nums[mid] &amp;&amp; nums[mid] &gt; nums[end]) &#123; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; return nums[start]; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-162:Find Peak Element]]></title>
    <url>%2F2020%2F04%2F18%2Fleetcode-162%2F</url>
    <content type="text"><![CDATA[题目链接：Find Peak Element 题目描述 题目难度：Medium A peak element is an element that is greater than its neighbors. Given an input array nums, where nums[i] ≠ nums[i+1], find a peak element and return its index. The array may contain multiple peaks, in that case return the index to any one of the peaks is fine. You may imagine that nums[-1] = nums[n] = -∞. Example 1: 123Input: nums = [1,2,3,1]Output: 2Explanation: 3 is a peak element and your function should return the index number 2. Example 2: 1234Input: nums = [1,2,1,3,5,6,4]Output: 1 or 5 Explanation: Your function can return either index number 1 where the peak element is 2, or index number 5 where the peak element is 6. Note: Your solution should be in logarithmic complexity. AC代码123456789101112131415class Solution &#123; public int findPeakElement(int[] nums) &#123; if(nums == null || nums.length == 0) return -1; if(nums.length == 1) return 0; int len = nums.length; if(nums[0] &gt; nums[1]) return 0; if(nums[len - 1] &gt; nums[len - 2]) return len - 1; for(int i = 1; i &lt; len - 1;i++) &#123; if( (nums[i] &gt; nums[i - 1]) &amp;&amp; (nums[i] &gt; nums[i + 1]) ) return i; &#125; return -1; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-155:Min Stack]]></title>
    <url>%2F2020%2F04%2F18%2Fleetcode-155%2F</url>
    <content type="text"><![CDATA[题目链接：Min Stack 题目描述 题目难度：Easy Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) – Push element x onto stack. pop() – Removes the element on top of the stack. top() – Get the top element. getMin() – Retrieve the minimum element in the stack. Example: 12345678MinStack minStack = new MinStack();minStack.push(-2);minStack.push(0);minStack.push(-3);minStack.getMin(); --&gt; Returns -3.minStack.pop();minStack.top(); --&gt; Returns 0.minStack.getMin(); --&gt; Returns -2. AC代码1234567891011121314151617181920212223242526272829303132333435363738class MinStack &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); Stack&lt;Integer&gt; minStack = new Stack&lt;&gt;(); /** initialize your data structure here. */ public MinStack() &#123; &#125; public void push(int x) &#123; stack.push(x); if( (minStack.isEmpty()) || (x &lt;= minStack.peek()) ) minStack.push(x); else minStack.push(minStack.peek()); &#125; public void pop() &#123; stack.pop(); minStack.pop(); &#125; public int top() &#123; return stack.peek(); &#125; public int getMin() &#123; return minStack.peek(); &#125;&#125;/** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.push(x); * obj.pop(); * int param_3 = obj.top(); * int param_4 = obj.getMin(); */]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-151:Reverse Words in a String]]></title>
    <url>%2F2020%2F04%2F17%2Fleetcode-151%2F</url>
    <content type="text"><![CDATA[题目链接：Reverse Words in a String 题目描述 题目难度：Medium Given an input string, reverse the string word by word. Example 1: 12Input: &quot;the sky is blue&quot;Output: &quot;blue is sky the&quot; Example 2: 123Input: &quot; hello world! &quot;Output: &quot;world! hello&quot;Explanation: Your reversed string should not contain leading or trailing spaces. Example 3: 123Input: &quot;a good example&quot;Output: &quot;example good a&quot;Explanation: You need to reduce multiple spaces between two words to a single space in the reversed string. Note: A word is defined as a sequence of non-space characters. Input string may contain leading or trailing spaces. However, your reversed string should not contain leading or trailing spaces. You need to reduce multiple spaces between two words to a single space in the reversed string. Follow up: For C programmers, try to solve it in-place in O(1) extra space. AC代码11234567891011121314class Solution &#123; public String reverseWords(String s) &#123; if(s == null || s.length() == 0) return s; String strs[] = s.trim().split("\\s+"); System.out.println(strs.length); StringBuilder sb = new StringBuilder(); for(int i = strs.length - 1;i &gt;= 0;i--) &#123; if(i == 0) sb.append(strs[i]); else sb.append(strs[i]).append(" "); &#125; return sb.toString(); &#125;&#125; AC代码2123456789101112131415161718class Solution &#123; public String reverseWords(String s) &#123; StringBuilder sb = new StringBuilder(); int r = s.length() - 1, l = 0; while(r &gt;= 0) &#123; if(s.charAt(r) != ' ') &#123; //can not find index return -1 l = s.lastIndexOf(' ', r); //public StringBuilder append(CharSequence s,int start,int end) sb.append(s, l + 1, r + 1); sb.append(" "); r = l; &#125; r--; &#125; return sb.toString().trim(); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-150:Evaluate Reverse Polish Notation(根据后缀表达式计算结果)]]></title>
    <url>%2F2020%2F04%2F17%2Fleetcode-150%2F</url>
    <content type="text"><![CDATA[题目链接：Evaluate Reverse Polish Notation 题目描述 题目难度:Medium Evaluate the value of an arithmetic expression in Reverse Polish Notation. 中缀表达式：运算符号在操作数之间，如 1 + 2 后缀表达式：运算符号在操作数之后，如1 2 + 前缀表达式：运算符号在操作数之前，如+ 1 2 Valid operators are +, -, *, /. Each operand may be an integer or another expression. Note: Division between two integers should truncate toward zero. The given RPN expression is always valid. That means the expression would always evaluate to a result and there won’t be any divide by zero operation. Example 1: 123Input: ["2", "1", "+", "3", "*"]Output: 9Explanation: ((2 + 1) * 3) = 9 Example 2: 123Input: ["4", "13", "5", "/", "+"]Output: 6Explanation: (4 + (13 / 5)) = 6 Example 3: 12345678910Input: ["10", "6", "9", "3", "+", "-11", "*", "/", "*", "17", "+", "5", "+"]Output: 22Explanation: ((10 * (6 / ((9 + 3) * -11))) + 17) + 5= ((10 * (6 / (12 * -11))) + 17) + 5= ((10 * (6 / -132)) + 17) + 5= ((10 * 0) + 17) + 5= (0 + 17) + 5= 17 + 5= 22 AC代码1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123; public int evalRPN(String[] tokens) &#123; int result = 0; if(tokens == null || tokens.length == 0) return result; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int i = 0; int operand1 = 0, operand2 = 0; while(i &lt; tokens.length) &#123; if( (tokens[i].equals("+")) || (tokens[i].equals("-")) || (tokens[i].equals("*")) || (tokens[i].equals("/")) ) &#123; //注意：先退栈的是操作数2，然后是操作数1 operand2 = stack.pop(); operand1 = stack.pop(); result = calc(operand1, operand2, tokens[i]); stack.add(result); &#125; else stack.add(Integer.parseInt(tokens[i])); i++; &#125; return stack.pop(); &#125; private int calc(int operand1, int operand2, String simble) &#123; int result = 0; switch(simble) &#123; case "+" : result = operand1 + operand2; break; case "-" : result = operand1 - operand2; break; case "*" : result = operand1 * operand2; break; case "/" : result = operand1 / operand2; break; &#125; return result; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-148:Sort List]]></title>
    <url>%2F2020%2F04%2F17%2Fleetcode-148%2F</url>
    <content type="text"><![CDATA[题目链接：Sort List 题目描述 题目难度：Medium Sort a linked list in O(n log n) time using constant space complexity. Example 1: 12Input: 4-&gt;2-&gt;1-&gt;3Output: 1-&gt;2-&gt;3-&gt;4 Example 2: 12Input: -1-&gt;5-&gt;3-&gt;4-&gt;0Output: -1-&gt;0-&gt;3-&gt;4-&gt;5 AC代码二路归并排序 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode sortList(ListNode head) &#123; if (head == null || head.next == null) return head; ListNode prev = null, slow = head, fast = head; while (fast != null &amp;&amp; fast.next != null) &#123; prev = slow; slow = slow.next; fast = fast.next.next; &#125; prev.next = null; ListNode l1 = sortList(head); ListNode l2 = sortList(slow); return merge(l1, l2); &#125; private ListNode merge(ListNode l1, ListNode l2) &#123; int val1 = 0, val2 = 0; ListNode fakeHead = new ListNode(0); ListNode curNode = fakeHead; while(l1 != null &amp;&amp; l2 != null) &#123; val1 = l1.val; val2 = l2.val; if(val1 &lt; val2) &#123; curNode.next = l1; l1 = l1.next; &#125; else &#123; curNode.next = l2; l2 = l2.next; &#125; curNode = curNode.next; curNode.next = null; &#125; if(l1 != null) curNode.next = l1; if(l2 != null) curNode.next = l2; return fakeHead.next; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-145:Binary Tree Postorder Traversal(后序遍历)]]></title>
    <url>%2F2020%2F04%2F16%2Fleetcode-145%2F</url>
    <content type="text"><![CDATA[题目链接：Binary Tree Postorder Traversal 题目描述 题目难度:Hard Given a binary tree, return the postorder traversal of its nodes’ values. Example: 12345678Input: [1,null,2,3] 1 \ 2 / 3Output: [3,2,1] Follow up: Recursive solution is trivial, could you do it iteratively? AC代码1递归解法 123456789101112131415161718192021222324/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; resList = new ArrayList&lt;&gt;(); postorderTraversalCore(resList, root); return resList; &#125; private void postorderTraversalCore(List&lt;Integer&gt; list, TreeNode root) &#123; if(root == null) return; if(root.left != null) postorderTraversalCore(list, root.left); if(root.right != null) postorderTraversalCore(list, root.right); list.add(root.val); &#125;&#125; AC代码2非递归解法 思路看这里 12345678910111213141516171819202122232425262728293031323334/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); if(root == null) return list; Stack&lt;TreeNode&gt; s = new Stack&lt;&gt;(); TreeNode last = null; while (!s.isEmpty() || root!=null)&#123; while (root != null)&#123; s.push(root); root = root.left; &#125; if(!s.isEmpty())&#123; TreeNode t = s.pop(); if(t.right == null || last == t.right)&#123; list.add(t.val); last = t; &#125;else&#123; s.push(t); root = t.right; &#125; &#125; &#125; return list; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-143:Reorder List]]></title>
    <url>%2F2020%2F04%2F16%2Fleetcode-143%2F</url>
    <content type="text"><![CDATA[题目链接：Reorder List 题目描述 题目难度：Medium Given a singly linked list L: L0→L1→…→L**n-1→Ln,reorder it to: L0→L**n→L1→L**n-1→L2→L**n-2→… You may not modify the values in the list’s nodes, only nodes itself may be changed. Example 1: 1Given 1-&gt;2-&gt;3-&gt;4, reorder it to 1-&gt;4-&gt;2-&gt;3. Example 2: 1Given 1-&gt;2-&gt;3-&gt;4-&gt;5, reorder it to 1-&gt;5-&gt;2-&gt;4-&gt;3. 解题思路 将整个链表从中间切一刀，得到两个链表：链表1和链表2 将链表2翻转，得到链表2’ 交叉的方式合并链表1和链表2’ 如：有一个链表为1-&gt;2-&gt;3-&gt;4-&gt;5,切分得到链表11-&gt;2-&gt;3和链表24-&gt;5，翻转链表2，得到链表2’5-&gt;4，合并链表1和链表2’得到1-&gt;5-&gt;2-&gt;4-&gt;3。 AC代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public void reorderList(ListNode head) &#123; if(head == null || head.next == null || head.next.next == null) return; int len = 0; ListNode curNode = head; while(curNode != null)&#123; len++; curNode = curNode.next; &#125; ListNode list1Head = head; curNode = head; ListNode list2Head = head; int step = (len / 2) + (len % 2); for(int i = 0; i &lt; step;i++) &#123; list2Head = list2Head.next; if(i == step - 1) curNode.next = null; curNode = curNode.next; &#125; list2Head = reverse(list2Head); ListNode list2TmpNode = list2Head; //合并两链表 for(int i = 0;i &lt; len / 2;i++) &#123; list2TmpNode = list2Head; list2Head = list2Head.next; list2TmpNode.next = list1Head.next; list1Head.next = list2TmpNode; list1Head = list1Head.next.next; &#125; &#125; //翻转链表 private ListNode reverse(ListNode head) &#123; if(head == null) return head; ListNode newHead = new ListNode(0); ListNode curNode = head; ListNode nextNode = head.next; while(curNode != null) &#123; nextNode = curNode.next; curNode.next = newHead.next; newHead.next = curNode; curNode = nextNode; &#125; return newHead.next; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-144:Binary Tree Preorder Traversal(先序遍历)]]></title>
    <url>%2F2020%2F04%2F16%2Fleetcode-144%2F</url>
    <content type="text"><![CDATA[题目链接：leetcode-144:Binary Tree Preorder Traversal 题目描述 题目难度：Medium Given a binary tree, return the preorder traversal of its nodes’ values. Example: 12345678Input: [1,null,2,3] 1 \ 2 / 3Output: [1,2,3] Follow up: Recursive solution is trivial, could you do it iteratively? AC代码1递归解法 123456789101112131415161718192021222324/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; resList = new ArrayList&lt;&gt;(); preorderTraversalCore(resList, root); return resList; &#125; private void preorderTraversalCore(List&lt;Integer&gt; list, TreeNode root) &#123; if(root == null) return; list.add(root.val); if(root.left != null) preorderTraversalCore(list, root.left); if(root.right != null) preorderTraversalCore(list, root.right); &#125;&#125; AC代码2非递归解法 1234567891011121314151617181920212223242526272829/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; resList = new ArrayList&lt;&gt;(); if(root == null) return resList; Stack&lt;TreeNode&gt; s = new Stack&lt;&gt;(); while (!s.isEmpty() || root != null)&#123; while(root != null)&#123; resList.add(root.val); s.push(root); root = root.left; &#125; if(!s.isEmpty())&#123; TreeNode t = s.pop(); root = t.right; &#125; &#125; return resList; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伽罗华域运算及C语言实现]]></title>
    <url>%2F2020%2F04%2F16%2FGalois-Field%2F</url>
    <content type="text"><![CDATA[伽罗华域（Galois Field）简介在数学中，有限域（或称伽罗华域）是一个包含有限元素的域。与其他域一样，有限域是进行加减乘除运算都有定义并且满足特定规则的集合。其中加法和乘法必须满足交换、结合和分配的规律。加法和乘法具有封闭性，即加法和乘法结果仍然是域中的元素。 伽罗华域一般用GF(2M)表示，这个域中含有2M个元素。GF(2M)上的四则运算是基于多项式运算的，一般都是这种结构f(x) = x6 + x4 + x2 + x + 1。 本原多项式 （primitive polynomial）是一种特殊的不可约多项式。当一个域上的本原多项式确定了，这个域上的运算也就确定了。本原多项式一般通过查表得知，同一个域往往有多个本原多项式。 当M=8时，即GF(2M)域，其中比较常见的一个本原多项式为P(x) = x8 + x4 + x3 + x2 + 1。GF(28)域也是计算机领域用的比较多的一种域，因为一字节等于8比特。AES加密中列混合变换中用到了伽罗华域运算。 AES加解密算法中，使用的不可约多项式（irreducible polynomial）为P(x) = x8 + x4 + x3 + x + 1，所以下面主要讨论AES算法中用到的GF(28)域，多项式为P(x) = x8 + x4 + x3 + x + 1乘法的实现。 运算规则加法在伽罗华域中，加法是模2运算，也就是忽略进位的加法，等同于计算机中的XOR异或，即1^1=0, 1^0=1, 0^0=0。 乘法伽罗华域中的乘法是基于多项式运算的，比如：5=00000101b=(22+1)，对应多项式为 x2 + 1。 举例：3*7=(x+1)(x2+x+1)=x3+x2+x+x2+ x + 1= x3+(x2+x2)+(x+x)+1=x3+1 模2加法中相同项相加为0。 在相乘得到的多项式结果中，如果x的次数大于7，就需要对多项式在GF(28)上关于本原多项式P(x)求余数，即modP(x)。因为加法为模2加法，相同项相加为0，所以加法可以当成减法来计算。 基于多项式P(x) = x8 + x4 + x3 + x2 + 1 129 * 5 = (x7 + 1)(x2 + 1) = (x9 + x7 + x2 + 1) = (x9 + x7 + x2 + 1) + (x *P(x)) = (x9 + x7 + x2 + 1) + (x9 + x5 + x4 + x3 + x) = (x7 + x5 + x4 + x3 + x2 + x + 1) = 10111111b = 0xBF = 191 基于多项式P(x) = x8 + x4 + x3 + x + 1 129 * 5 = (x7 + 1)(x2 + 1) = (x9 + x7 + x2 + 1) = (x9 + x7 + x2 + 1) + (x *P(x)) = (x9 + x7 + x2 + 1) + (x9 + x5 + x4 + x2 + x) = (x7 + x5 + x4 + x + 1) = 10110011b = 0xB3 = 179 推导因为在AES算法列混合环节中用到了伽罗华域乘法，所以接下来的代码实现使用AES算法指定的不可约多项式P(x) = x8 + x4 + x3 + x + 1进行分析。 GMul(2, v)为了方便编程我们先找找规律，假设函数GMul(u, v)表示伽罗华域乘法，先看与2相乘的伽罗华域计算，即GMul(2, v)，v、u不分左右： 2∗7=(x) * (x2+x+1)=x3+x2+x 可以看出伽罗华域中一个数与2相乘等于这个数左移一位 看上式，假如v对应的多项式x的次数大于7，即v的最高位为1，也就是v&gt;&gt;7 == 1的话就进行 modP(x) 化简，比如： 2 * 129 = (x) * (x7 + 1) = (x8 + x) = (x8 + x)+ P(x) = (x8 + x) + (x8 + x4 + x3 + x + 1) = (x4 + x3 + 1 ) = 00011001b = 0x19 = 00000010^ 00011011 = 0x02 ^ 0x1b = (129 &lt;&lt; 1) ^ 0x1b 2 * 176= (x) * (x7 + x5 + x4) = (x8 + x6 + x5) = (x8 + x6 + x5) + (P(x)) = (x8 + x6 + x5) + (x8 + x4 + x3 + x + 1) = (x6 + x5 + x4 + x3 + x + 1) = 01111011b= 0x7B = 01100000^ 00011011 = 0x60 ^ 0x1b = (176 &lt;&lt; 1) ^ 0x1b 从上面的3个例子可以总结一个规律： GMul(3, v)3∗v=(2+1)∗v=GMul(2,v) + *v= GMul(2,v) ^ v GMul(4, v)4∗v=2∗2∗v= GMul(2, GMul(2, v)) GMul(7, v)7∗v=00000111=(2∗2+2+1)= GMul(2, GMul(2, v)) ^ GMul(2, v) ^ v GMul(8, v)8∗v=(2∗2∗2)∗v=GMul(2, GMul(2, GMul(2, v)))8=2^3，循环3次GMul(2,v) GMul(11111111b, v)11111111b=27+26+25+24+23+22+21+1所以相当于：GMul(2,v)循环7次 ^ GMul(2,v)循环6次 … GMul(2,v)循环1次 ^ v 代码实现GMul(2, v)根据上面总结的公式可以很容易用代码实现GMul(2, v) 12345678uint8_t GMul2(uint8_t v)&#123; int flag = (v &amp; 0x80); v &lt;&lt;= 1; if (flag) &#123; v ^= 0x1B; /* P(x) = x^8 + x^4 + x^3 + x + 1 */ &#125; return v;&#125; GMul(u, v)根据上述对GMul(u, v)的总结，可以用以下代码实现： 123456789101112131415uint8_t GMul(uint8_t u, uint8_t v) &#123; uint8_t p = 0; for (int i = 0; i &lt; 8; ++i) &#123; if (u &amp; 0x01) &#123; p ^= v; &#125; v = GMul2(v); //调用GMul(2，v) u &gt;&gt;= 1; &#125; return p;&#125; 为了代码整洁可以写成一个通用的函数，AES算法实现中列混合环节可以直接调用下面的函数： 12345678910111213141516171819uint8_t GMul(uint8_t u, uint8_t v) &#123; uint8_t p = 0; for (int i = 0; i &lt; 8; ++i) &#123; if (u &amp; 0x01) &#123; p ^= v; &#125; int flag = (v &amp; 0x80); v &lt;&lt;= 1; if (flag) &#123; v ^= 0x1B; /* P(x) = x^8 + x^4 + x^3 + x + 1 */ &#125; u &gt;&gt;= 1; &#125; return p;&#125; 上述代码的思路和快速幂的解法类似。 快速幂代码： 123456789101112/* 求 num 的 n 次方*/int mypow(int num, int n)&#123; int ans = 1; while(n != 0) &#123; if((n % 2) == 1) ans = ans * num; num = num * num; n = n &gt;&gt; 1; &#125; return ans;&#125; 参考 版权声明：本文为CSDN博主「Coder Liming」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/shaosunrise/article/details/80174210]]></content>
      <categories>
        <category>mathematics</category>
      </categories>
      <tags>
        <tag>伽罗华域</tag>
        <tag>Galois_Field</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AES加密算法的介绍与C实现]]></title>
    <url>%2F2020%2F04%2F16%2FAES128%2F</url>
    <content type="text"><![CDATA[对称加密1、什么是对称加密？对称加密就是指，加密和解密使用同一个密钥的加密方式。 2、对称加密的工作过程发送方使用密钥将明文数据加密成密文，然后发送出去，接收方收到密文后，使用同一个密钥将密文解密成明文读取。 3、对称加密的优点加密计算量小、速度块，适合对大量数据进行加密的场景。 4、对称加密的两大不足 密钥传输问题：如上所说，由于对称加密的加密和解密使用的是同一个密钥，所以对称加密的安全性就不仅仅取决于加密算法本身的强度，更取决于密钥是否被安全的保管，因此加密者如何把密钥安全的传递到解密者手里，就成了对称加密面临的关键问题。（比如，我们客户端肯定不能直接存储对称加密的密钥，因为被反编译之后，密钥就泄露了，数据安全性就得不到保障，所以实际中我们一般都是客户端向服务端请求对称加密的密钥，而且密钥还得用非对称加密算法加密后再传输。） 密钥管理问题：随着密钥数量的增多，密钥的管理问题会逐渐显现出来。比如我们在加密用户的信息时，不可能所有用户都用同一个密钥加密解密。这样一旦密钥泄漏，就相当于泄露了所有用户的信息，因此需要为每一个用户单独的生成一个密钥并且管理，这样密钥管理的代价也会非常大。 AES加密算法简介如今有很多的对称加密算法，如DES加密算法、3DES加密算法等，但是因为AES加密算法的安全性要高于DES和3DES，所以AES已经成为了主要的对称加密算法。 AES加密算法就是众多对称加密算法中的一种，它的英文全称是Advanced Encryption Standard，翻译过来是高级加密标准，它是用来替代之前的DES加密算法的。AES算法是当前最流行的对称加密算法，也是一种分组加密算法，分组密码就是把明文分为固定长度的一组一组，每次加密一组数据，直到加密完整个明文数据。AES算法根据分组长度可以分为AES128, AES192，AES256，其所要求的秘钥长度和加密轮数也各不相同。 对称加密算法也就是加密和解密用相同的密钥，具体的加密流程如下图： 下面简单介绍下各个部分的作用与意义： 明文P 没有经过加密的数据。 密钥K 用来加密明文的密码，在对称加密算法中，加密与解密的密钥是相同的。密钥为接收方与发送方协商产生，但不可以直接在网络上传输，否则会导致密钥泄漏，通常是通过非对称加密算法加密密钥，然后再通过网络传输给对方，或者直接面对面商量密钥。密钥是绝对不可以泄漏的，否则会被攻击者还原密文，窃取机密数据。 AES加密函数 设AES加密函数为E，则 C = E(K, P),其中P为明文，K为密钥，C为密文。也就是说，把明文P和密钥K作为加密函数的参数输入，则加密函数E会输出密文C。 密文C 经加密函数处理后的数据 AES解密函数 设AES解密函数为D，则 P = D(K, C),其中C为密文，K为密钥，P为明文。也就是说，把密文C和密钥K作为解密函数的参数输入，则解密函数会输出明文P。 在这里简单介绍下对称加密算法与非对称加密算法的区别。 对称加密算法 加密和解密用到的密钥是相同的，这种加密方式加密速度非常快，适合经常发送数据的场合。缺点是密钥的传输比较麻烦。 非对称加密算法 加密和解密用的密钥是不同的，这种加密方式是用数学上的难解问题构造的，通常加密解密的速度比较慢，适合偶尔发送数据的场合。优点是密钥传输方便。常见的非对称加密算法为RSA、ECC和EIGamal。 实际中，一般是通过RSA加密AES的密钥，传输到接收方，接收方解密得到AES密钥，然后发送方和接收方用AES密钥来通信。 AES的基本结构AES为分组密码，分组密码也就是把明文分成一组一组的，每组长度相等，每次加密一组数据，直到加密完整个明文。在AES标准规范中，分组长度只能是128位，也就是说，每个分组为16个字节（每个字节8位）。密钥的长度可以使用128位、192位或256位。密钥的长度不同，推荐加密轮数也不同，如下表所示： AES 密钥长度（32位比特字) 分组长度(32位比特字) 加密轮数 AES-128 4 4 10 AES-192 6 4 12 AES-256 8 4 14 轮数在下面介绍，这里实现的是AES-128，也就是密钥的长度为128位，加密轮数为10轮。 AES算法主要可以分为秘钥扩展、字节替换、行移位、列混合和轮秘钥加这5个步骤。 秘钥扩展（KeyExpansions：给定的初始秘钥一般比较短，比如16字节，而算法如果进行10轮运算的话就需要16x(10+1)字节长度的秘钥，需要对原始秘钥进行秘钥扩展。 字节替换（SubBytes）：一个非线性的替换步骤，根据查表把一个字节替换为另一个字节。 行移位（ShiftRows）：将数据矩阵的每一行循环移位一定长度。 列混合（MixColumns）：将数据矩阵乘以一个固定的矩阵（不是普通的矩阵相乘，而是伽罗华域中的乘法运算），增加混淆程度。 轮秘钥加（AddRoundKey）:将数据矩阵与秘钥矩阵进行异或操作。 上面说到，AES的加密公式为C = E(K,P)，在加密函数E中，会执行一个轮函数，并且执行10次这个轮函数。这个轮函数的前9次执行的操作是一样的，只有第10次有所不同。也就是说，一个明文分组会被加密10轮。AES的核心就是实现一轮中的所有操作。 AES的处理单位是字节，128位的输入明文分组P和输入密钥K都被分成16个字节，分别记为P = P0 P1 … P15 和 K = K0 K1 … K15。如，明文分组为P = abcdefghijklmnop,其中的字符a对应P0，p对应P15。一般地，明文分组用字节为单位的正方形矩阵描述，称为状态矩阵。在算法的每一轮中，状态矩阵的内容不断发生变化，最后的结果作为密文输出。该矩阵中字节的排列顺序为从上到下、从左至右依次排列，如下图所示： 现在假设明文分组P为”abcdefghijklmnop”，则对应上面生成的状态矩阵图如下： 上图中，0x61为字符a的十六进制表示。可以看到，明文经过AES加密后，已经面目全非。 类似地，128位密钥也是用字节为单位的矩阵表示，矩阵的每一列被称为1个32位比特字。通过密钥编排函数该密钥矩阵被扩展成一个44个字组成的序列W[0],W[1], … ,W[43],该序列的前4个元素W[0],W[1],W[2],W[3]是原始密钥，用于加密运算中的初始密钥加;后面40个字分为10组，每组4个字（128比特）分别用于10轮加密运算中的轮密钥加，如下图所示： 上图中，设K = “abcdefghijklmnop”，则K0 = a, K15 = p, W[0] = K0 K1 K2 K3 = “abcd”。 AES的整体结构如下图所示，其中的W[0,3]是指W[0]、W[1]、W[2]和W[3]串联组成的128位密钥。加密的第1轮到第9轮的轮函数一样，包括4个操作：字节代换、行位移、列混合和轮密钥加。最后一轮迭代不执行列混合。另外，在第一轮迭代之前，先将明文和原始密钥进行一次异或加密操作。 上图也展示了AES解密过程，解密过程仍为10轮，每一轮的操作是加密操作的逆操作。由于AES的4个轮操作都是可逆的，因此，解密操作的一轮就是顺序执行逆行移位、逆字节代换、轮密钥加和逆列混合。同加密操作类似，最后一轮不执行逆列混合，在第1轮解密之前，要执行1次密钥加操作。 下面分别介绍AES中一轮的4个操作阶段，这4分操作阶段使输入位得到充分的混淆。 一、字节代换1.字节代换操作AES的字节代换其实就是一个简单的查表操作。AES定义了一个S盒和一个逆S盒。AES的S盒： 状态矩阵中的元素按照下面的方式映射为一个新的字节：把该字节的高4位作为行值，低4位作为列值，取出S盒或者逆S盒中对应的行的元素作为输出。例如，加密时，输出的字节S1为0x12,则查S盒的第0x01行和0x02列，得到值0xc9,然后替换S1原有的0x12为0xc9。状态矩阵经字节代换后的图如下： 2.字节代换逆操作逆字节代换也就是查逆S盒来变换，逆S盒如下： 二、行移位1.行移位操作行移位是一个简单的左循环移位操作。当密钥长度为128比特时，状态矩阵的第0行左移0字节，第1行左移1字节，第2行左移2字节，第3行左移3字节，如下图所示： 2.行移位的逆变换行移位的逆变换是将状态矩阵中的每一行执行相反的移位操作，例如AES-128中，状态矩阵的第0行右移0字节，第1行右移1字节，第2行右移2字节，第3行右移3字节。 三、列混合1.列混合操作列混合变换是通过矩阵相乘来实现的，经行移位后的状态矩阵与固定的矩阵相乘，得到混淆后的状态矩阵，如下图的公式所示： 状态矩阵中的第j列(0 ≤j≤3)的列混合可以表示为下图所示： 其中，矩阵元素的乘法和加法都是定义在基于伽罗华域GF(2^8)上的二元运算,并不是通常意义上的乘法和加法。有关伽罗华域请点击这里查看。这里涉及到一些信息安全上的数学知识，不过不懂这些知识也行。其实这种二元运算的加法等价于两个字节的异或，乘法则复杂一点。对于一个8位的二进制数来说，使用域上的乘法乘以(00000010)等价于左移1位(低位补0)后，再根据情况同(00011011)进行异或运算，设S1 = (a7 a6 a5 a4 a3 a2 a1 a0)，刚0x02 * S1如下图所示： 也就是说，如果a7为1，则进行异或运算，否则不进行。类似地，乘以(00000100)可以拆分成两次乘以(00000010)的运算： 乘以(0000 0011)可以拆分成先分别乘以(0000 0001)和(0000 0010)，再将两个乘积异或： 因此，我们只需要实现乘以2的函数，其他数值的乘法都可以通过组合来实现。下面举个具体的例子,输入的状态矩阵如下： C9 E5 FD 2B 7A F2 78 6E 63 9C 26 67 B0 A7 82 E5 下面，进行列混合运算：以第一列的运算为例： 其它列的计算就不列举了，列混合后生成的新状态矩阵如下： D4 E7 CD 66 28 02 E5 BB BE C6 D6 BF 22 0F DF A5 2.列混合逆运算逆向列混合变换可由下图的矩阵乘法定义：可以验证，逆变换矩阵同正变换矩阵的乘积恰好为单位矩阵。 四、轮密钥加轮密钥加是将128位轮密钥Ki同状态矩阵中的数据进行逐位异或操作，如下图所示。其中，密钥Ki中每个字W[4i],W[4i+1],W[4i+2],W[4i+3]为32位比特字，包含4个字节，他们的生成算法下面在下面介绍。轮密钥加过程可以看成是字逐位异或的结果，也可以看成字节级别或者位级别的操作。也就是说，可以看成S0 S1 S2 S3 组成的32位字与W[4i]的异或运算。 轮密钥加的逆运算同正向的轮密钥加运算完全一致，这是因为异或的逆操作是其自身。轮密钥加非常简单，但却能够影响S数组中的每一位。 密钥扩展AES首先将初始密钥输入到一个44的状态矩阵中，如下图所示。 这个44矩阵的每一列的4个字节组成一个字，矩阵4列的4个字依次命名为W[0]、W[1]、W[2]和W[3]，它们构成一个以字为单位的数组W。例如，设密钥K为”abcdefghijklmnop”,则K0 = ‘a’,K1 = ‘b’, K2 = ‘c’,K3 = ‘d’,W[0] = “abcd”。接着，对W数组扩充40个新列，构成总共44列的扩展密钥数组。新列以如下的递归方式产生： 如果i不是4的倍数，那么第i列由如下等式确定：W[i]=W[i-4]⨁W[i-1] 如果i是4的倍数，那么第i列由如下等式确定：W[i]=W[i-4]⨁T(W[i-1])其中，T是一个有点复杂的函数。函数T由3部分组成：字循环、字节代换和轮常量异或，这3部分的作用分别如下。a. 字循环：将1个字中的4个字节循环左移1个字节。即将输入字[b0, b1, b2, b3]变换成[b1,b2,b3,b0]。b. 字节代换：对字循环的结果使用S盒进行字节代换。c. 轮常量异或：将前两步的结果同轮常量Rcon[j]进行异或，其中j表示轮数。轮常量Rcon[j]是一个字，其值见下表。 下面举个例子：设初始的128位密钥为：3C A1 0B 21 57 F0 19 16 90 2E 13 80 AC C1 07 BD那么4个初始值为：W[0] = 3C A1 0B 21W[1] = 57 F0 19 16W[2] = 90 2E 13 80W[3] = AC C1 07 BD下面求扩展的第1轮的子密钥(W[4],W[5],W[6],W[7])。由于4是4的倍数，所以：W[4] = W[0] ⨁ T(W[3])T(W[3])的计算步骤如下： 循环地将W[3]的元素移位：AC C1 07 BD变成C1 07 BD AC; 将 C1 07 BD AC 作为S盒的输入，输出为78 C5 7A 91; 将78 C5 7A 91与第一轮轮常量Rcon[1]进行异或运算，将得到79 C5 7A 91，因此，T(W[3])=79 C5 7A 91，故W[4] = 3C A1 0B 21 ⨁ 79 C5 7A 91 = 45 64 71 B0其余的3个子密钥段的计算如下：W[5] = W[1] ⨁ W[4] = 57 F0 19 16 ⨁ 45 64 71 B0 = 12 94 68 A6W[6] = W[2] ⨁ W[5] =90 2E 13 80 ⨁ 12 94 68 A6 = 82 BA 7B 26W[7] = W[3] ⨁ W[6] = AC C1 07 BD ⨁ 82 BA 7B 26 = 2E 7B 7C 9B所以，第一轮的密钥为 45 64 71 B0 12 94 68 A6 82 BA 7B 26 2E 7B 7C 9B。 AES解密在文章开始的图中，有AES解密的流程图，可以对应那个流程图来进行解密。下面介绍的是另一种等价的解密模式，流程图如下图所示。这种等价的解密模式使得解密过程各个变换的使用顺序同加密过程的顺序一致，只是用逆变换取代原来的变换。 AES分组填充模式AES采用分组密码体制，即AES加密会首先把明文切成一段一段的，而且每段数据的长度要求必须是128位16个字节，如果最后一段不够16个字节了，就需要用Padding来把这段数据填满16个字节，然后分别对每段数据进行加密，最后再把每段加密数据拼起来形成最终的密文。下面介绍几种常用的分组填充模式。 NoPaddingNoPadding是指不需要填充，也就是说数据的发送方肯定会保证最后一段数据也正好是16个字节。 ZeroPaddingZeroPadding，数据长度不对齐时使用0填充，否则不填充。 PKCS7Padding假设每个区块大小为blockSize（AES当中为16字节，128bit） 已对齐，填充一个长度为blockSize且每个字节均为blockSize的数据。 即填充 16 个字节，每个字节的值为 16 未对齐，需要补充的字节个数为n，则填充一个长度为n且每个字节均为n的数据。 AES分组密码工作模式AES算法描述怎么加密一个数据块，分组密码工作模式描述了如何重复加密比较长的多个数据块。 常见的分组密码工作模式有ECB、CBC、CFB、OFB、CTR五种，下面通过流程图分别展示了5大模式的分组密码工作加解密的流程。 ECBECB(Electronic Codebook, 电子密码本)模式是最简单的加密模式，明文消息被分成固定大小的块（分组），并且每个块被单独加密。每个块的加密和解密都是独立的，且使用相同的方法进行加密，所以可以进行并行计算，但是这种方法一旦有一个块被破解，使用相同的方法可以解密所有的明文数据，安全性比较差。适用于数据较少的情形，加密前需要把明文数据填充到块大小的整倍数。 CBCCBC(Cipher Block Chaining, 密码块链)模式中每一个分组要先和前一个分组加密后的数据进行XOR异或操作，然后再进行加密。这样每个密文块依赖该块之前的所有明文块，为了保持每条消息都具有唯一性，第一个数据块进行加密之前需要用初始化向量IV进行异或操作。CBC模式是一种最常用的加密模式，它主要缺点是加密是连续的，不能并行处理，并且与ECB一样消息块必须填充到块大小的整倍数。 在链模式如CBC中，每个分组都会影响到下一个分组的加密。这就是为了保证两个相同的普通文本分组不会生成相同的密文分组。第一个分组是个特列，因为它前面再没有其他的分组了。链模式允许定义一个额外的称为初始化向量（Initialization Vector, IV）的分组来开始这个链。这个通常会被标成可选的。但你总是需要提供一个。否则，它会用一个全是0的分组，那样会让你的数据容易受到特定的攻击的侵害。 初始向量IV的作用是使加密更加安全可靠，我们使用AES加密时需要主动提供初始向量，而且只需要提供一个初始向量就够了，后面每段数据的加密向量都是前面一段的密文。初始向量IV的长度规定为128位16个字节，初始向量的来源为随机生成。 CFBCFB(Cipher Feedback, 密码反馈)模式和CBC模式比较相似，前一个分组的密文加密后和当前分组的明文XOR异或操作生成当前分组的密文。CFB模式的解密和CBC模式的加密在流程上其实是非常相似的。 OFBOFB(Output Feedback, 输出反馈)模式将分组密码转换为同步流密码，也就是说可以根据明文长度先独立生成相应长度的流密码。通过流程图可以看出，OFB和CFB非常相似，CFB是前一个分组的密文加密后XOR当前分组明文，OFB是前一个分组与前一个明文块异或之前的流密码XOR当前分组明文。由于异或操作的对称性，OFB模式的解密和加密完全一样的流程。 CTRCTR(Counter, 计数器)。在CTR模式中，每次加密时都会生成一个不同的值来作为计数器的初始值，每个分组对应一个逐次累加的计数器，通过对计数器进行加密来生成密钥流，再将密钥流与明文分组进行异或操作得到密文分组。 C语言实现AES-128 C语言实现 秘钥长度为128bit，分组填充模式为PKCS7Padding，提供ECB和CBC两种模式。 注意的问题实际开发中使用AES加密需要注意的地方 服务端和我们客户端必须使用一样的密钥和初始向量IV。 服务端和我们客户端必须使用一样的加密模式。 服务端和我们客户端必须使用一样的Padding模式。 以上三条有一个不满足，双方就无法完成互相加解密。 同时针对对称加密密钥传输问题这个不足：我们一般采用RSA+AES加密相结合的方式，用AES加密数据，而用RSA加密AES的密钥。同时密钥和IV可以随机生成，这要是128位16个字节就行，但是必须由服务端来生成，因为如果由我们客户端生成的话，就好比我们客户端存放了非对称加密的私钥一样，这样容易被反编译，不安全，一定要从服务端请求密钥和初始向量IV。 参考 AES加密算法的详细介绍与实现 AES算法描述及C语言实现 第二篇：对称加密及AES加密算法 图解分组密码五大工作模式]]></content>
      <categories>
        <category>密码学</category>
      </categories>
      <tags>
        <tag>AES128</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-54:Spiral Matrix(顺时针旋转打印矩阵)]]></title>
    <url>%2F2020%2F04%2F15%2Fleetcode-54%2F</url>
    <content type="text"><![CDATA[题目链接：Spiral Matrix 题目描述 题目难度：Medium Given a matrix of m x n elements (m rows, n columns), return all elements of the matrix in spiral order. Example 1: 1234567Input:[ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ]]Output: [1,2,3,6,9,8,7,4,5] Example 2: 1234567Input:[ [1, 2, 3, 4], [5, 6, 7, 8], [9,10,11,12]]Output: [1,2,3,4,8,12,11,10,9,5,6,7] AC代码11234567891011121314151617181920212223242526272829class Solution &#123; public List&lt;Integer&gt; spiralOrder(int[][] matrix) &#123; List&lt;Integer&gt; resList = new ArrayList&lt;&gt;(); if(matrix == null || matrix.length == 0) return resList; int n = matrix.length * matrix[0].length; int[] dr = new int[]&#123;0, 1, 0, -1&#125;; int[] dc = new int[]&#123;1, 0, -1, 0&#125;; boolean[][] seen = new boolean[matrix.length][matrix[0].length]; int i = 0; int r = 0, c = 0, di = 0; while(i &lt; n)&#123; seen[r][c] = true; resList.add(matrix[r][c]); int cr = r + dr[di]; int cc = c + dc[di]; if(cr &gt;= 0 &amp;&amp; cr &lt; matrix.length &amp;&amp; cc &gt;= 0 &amp;&amp; cc &lt; matrix[0].length &amp;&amp; seen[cr][cc] == false)&#123; r = cr; c = cc; &#125; else&#123; di = (di + 1) % 4; r += dr[di]; c += dc[di]; &#125; i++; &#125; return resList; &#125;&#125; AC代码212345678910111213141516171819202122232425262728class Solution &#123; List&lt;Integer&gt; resList = new ArrayList&lt;&gt;(); public List&lt;Integer&gt; spiralOrder(int[][] matrix) &#123; if(matrix == null || matrix.length == 0) return resList; printMatrixCore(matrix, 0, matrix.length - 1, 0, matrix[0].length - 1); return resList; &#125; private void printMatrixCore(int[][] matrix, int startRow, int endRow, int startCol, int endCol)&#123; int i = 0; if(startRow &lt; endRow &amp;&amp; startCol &lt; endCol)&#123; for(i = startCol;i &lt;= endCol;i++) resList.add(matrix[startRow][i]); for(i = startRow + 1;i &lt;= endRow;i++) resList.add(matrix[i][endCol]); for(i = endCol - 1;i &gt;= startCol;i--) resList.add(matrix[endRow][i]); for(i = endRow - 1;i &gt; startRow;i--) resList.add(matrix[i][startCol]); printMatrixCore(matrix, startRow + 1, endRow - 1, startCol + 1, endCol - 1); &#125; else if(startRow == endRow &amp;&amp; startCol == endCol) resList.add(matrix[startRow][startCol]); else if(startRow == endRow &amp;&amp; startCol &lt; endCol)&#123; for(i = startCol;i &lt;= endCol;i++) resList.add(matrix[startRow][i]); &#125; else if(startRow &lt; endRow &amp;&amp; startCol == endCol)&#123; for(i = startRow;i &lt;= endRow;i++) resList.add(matrix[i][startCol]); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-140:Word Break II]]></title>
    <url>%2F2019%2F10%2F01%2Fleetcode-140%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/word-break-ii/ 题目描述题目难度：Medium Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, add spaces in s to construct a sentence where each word is a valid dictionary word. Return all such possible sentences. Note: The same word in the dictionary may be reused multiple times in the segmentation. You may assume the dictionary does not contain duplicate words. Example 1: 12345678Input:s = &quot;catsanddog&quot;wordDict = [&quot;cat&quot;, &quot;cats&quot;, &quot;and&quot;, &quot;sand&quot;, &quot;dog&quot;]Output:[ &quot;cats and dog&quot;, &quot;cat sand dog&quot;] Example 2: 12345678910Input:s = &quot;pineapplepenapple&quot;wordDict = [&quot;apple&quot;, &quot;pen&quot;, &quot;applepen&quot;, &quot;pine&quot;, &quot;pineapple&quot;]Output:[ &quot;pine apple pen apple&quot;, &quot;pineapple pen apple&quot;, &quot;pine applepen apple&quot;]Explanation: Note that you are allowed to reuse a dictionary word. Example 3: 12345Input:s = &quot;catsandog&quot;wordDict = [&quot;cats&quot;, &quot;dog&quot;, &quot;sand&quot;, &quot;and&quot;, &quot;cat&quot;]Output:[] Solution代码来自：https://leetcode.com/problems/word-break-ii/discuss/44167/My-concise-JAVA-solution-based-on-memorized-DFS 1234567891011121314151617181920212223242526class Solution &#123; public List&lt;String&gt; wordBreak(String s, List&lt;String&gt; wordDict) &#123; return DFS(s, wordDict, new HashMap&lt;String, LinkedList&lt;String&gt;&gt;()); &#125; // DFS function returns an array including all substrings derived from s.List&lt;String&gt; DFS(String s, List&lt;String&gt; wordDict, HashMap&lt;String, LinkedList&lt;String&gt;&gt;map) &#123; if (map.containsKey(s)) return map.get(s); LinkedList&lt;String&gt;res = new LinkedList&lt;String&gt;(); if (s.length() == 0) &#123; res.add(""); return res; &#125; for (String word : wordDict) &#123; if (s.startsWith(word)) &#123; List&lt;String&gt; sublist = DFS(s.substring(word.length()), wordDict, map); for (String sub : sublist) res.add(word + (sub.isEmpty() ? "" : " ") + sub); &#125; &#125; map.put(s, res); return res;&#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-139:Word Break]]></title>
    <url>%2F2019%2F09%2F28%2Fleetcode-139%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/word-break/ 题目描述题目难度：Medium Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, determine if s can be segmented into a space-separated sequence of one or more dictionary words. Note: The same word in the dictionary may be reused multiple times in the segmentation. You may assume the dictionary does not contain duplicate words. Example 1: 123Input: s = &quot;leetcode&quot;, wordDict = [&quot;leet&quot;, &quot;code&quot;]Output: trueExplanation: Return true because &quot;leetcode&quot; can be segmented as &quot;leet code&quot;. Example 2: 1234Input: s = &quot;applepenapple&quot;, wordDict = [&quot;apple&quot;, &quot;pen&quot;]Output: trueExplanation: Return true because &quot;applepenapple&quot; can be segmented as &quot;apple pen apple&quot;. Note that you are allowed to reuse a dictionary word. Example 3: 12Input: s = &quot;catsandog&quot;, wordDict = [&quot;cats&quot;, &quot;dog&quot;, &quot;sand&quot;, &quot;and&quot;, &quot;cat&quot;]Output: false Solution112345678910111213141516public class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; dict) &#123; boolean[] f = new boolean[s.length() + 1]; f[0] = true; //填充数组 f 的 1 到 s.length() 的元素 for(int i=1; i &lt;= s.length(); i++)&#123; for(int j=0; j &lt; i; j++)&#123; if(f[j] &amp;&amp; dict.contains(s.substring(j, i)))&#123; f[i] = true; break; &#125; &#125; &#125; return f[s.length()]; &#125;&#125; Solution2123456789101112131415161718192021222324//Time: O(MN): N - wordDict length, M - s.length()//Space: O(M) class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; return canBreak(s, 0, wordDict, new int[s.length()]); &#125; private boolean canBreak(String s, int index, List&lt;String&gt; wordDict, int[] memo)&#123; if(index == s.length()) return true; if(memo[index] != 0) return memo[index] &gt; 0 ? true : false; for(String word : wordDict)&#123; if(index + word.length() &lt;= s.length() &amp;&amp; s.substring(index, index+word.length()).equals(word))&#123; if(canBreak(s, index+word.length(), wordDict, memo)) &#123; memo[index] = 1; return true; &#125; &#125; &#125; memo[index] = -1; return false; &#125;&#125; 代码来自：Leetcode]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-134:Gas Station(加油站)]]></title>
    <url>%2F2019%2F09%2F28%2Fleetcode-134%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/gas-station/ 题目描述题目难度：Medium There are N gas stations along a circular route, where the amount of gas at station i is gas[i]. You have a car with an unlimited gas tank and it costs cost[i] of gas to travel from station i to its next station (i+1). You begin the journey with an empty tank at one of the gas stations. Return the starting gas station’s index if you can travel around the circuit once in the clockwise direction, otherwise return -1. Note: If there exists a solution, it is guaranteed to be unique. Both input arrays are non-empty and have the same length. Each element in the input arrays is a non-negative integer. Example 1: 1234567891011121314Input: gas = [1,2,3,4,5]cost = [3,4,5,1,2]Output: 3Explanation:Start at station 3 (index 3) and fill up with 4 unit of gas. Your tank = 0 + 4 = 4Travel to station 4. Your tank = 4 - 1 + 5 = 8Travel to station 0. Your tank = 8 - 2 + 1 = 7Travel to station 1. Your tank = 7 - 3 + 2 = 6Travel to station 2. Your tank = 6 - 4 + 3 = 5Travel to station 3. The cost is 5. Your gas is just enough to travel back to station 3.Therefore, return 3 as the starting index. Example 2: 12345678910111213Input: gas = [2,3,4]cost = [3,4,3]Output: -1Explanation:You can&apos;t start at station 0 or 1, as there is not enough gas to travel to the next station.Let&apos;s start at station 2 and fill up with 4 unit of gas. Your tank = 0 + 4 = 4Travel to station 0. Your tank = 4 - 3 + 2 = 3Travel to station 1. Your tank = 3 - 3 + 3 = 3You cannot travel back to station 2, as it requires 4 unit of gas but you only have 3.Therefore, you can&apos;t travel around the circuit once no matter where you start. Solution非常经典的一道题。可以转换成求最大连续和做，但是有更简单的方法。基于一个数学定理： 1如果一个数组的总和非负，那么一定可以找到一个起始位置，从他开始绕数组一圈，累加和一直都是非负的 （证明貌似不难，以后有时间再补） 有了这个定理，判断到底是否存在这样的解非常容易，只需要把全部的油耗情况计算出来看看是否大于等于0即可。 那么如何求开始位置在哪？ 注意到这样一个现象： 假如从位置i开始，i+1，i+2…，一路开过来一路油箱都没有空。说明什么？说明从i到i+1，i+2，…肯定是正积累。 现在突然发现开往位置j时油箱空了。这说明什么？说明从位置i开始没法走完全程(废话)。那么，我们要从位置i+1开始重新尝试吗？不需要！为什么？因为前面已经知道，位置i肯定是正积累，那么，如果从位置i+1开始走更加没法走完全程了，因为没有位置i的正积累了。同理，也不用从i+2，i+3，…开始尝试。所以我们可以放心地从位置j+1开始尝试。 12345678910111213141516int canCompleteCircuit(vector&lt;int&gt; &amp;gas, vector&lt;int&gt; &amp;cost) &#123; int start = 0; // 起始位置 int remain = 0; // 当前剩余燃料 int debt = 0; // 前面没能走完的路上欠的债 for (int i = 0; i &lt; gas.size(); i++) &#123; remain += gas[i] - cost[i]; if (remain &lt; 0) &#123; debt += remain; start = i + 1; remain = 0; &#125; &#125; return remain + debt &gt;= 0 ? start : -1;&#125; 参考： Leetcode#134 Gas station]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-137:Single Number II]]></title>
    <url>%2F2019%2F09%2F26%2Fleetcode-137%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/single-number-ii/ 题目描述题目难度：Medium Given a non-empty array of integers, every element appears three times except for one, which appears exactly once. Find that single one. Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory? Example 1: 12Input: [2,2,3,2]Output: 3 Example 2: 12Input: [0,1,0,1,0,1,99]Output: 99 Solution代码来源：leetcode 12345678public int singleNumber(int[] A) &#123; int ones = 0, twos = 0; for(int i = 0; i &lt; A.length; i++)&#123; ones = (ones ^ A[i]) &amp; ~twos; twos = (twos ^ A[i]) &amp; ~ones; &#125; return ones;&#125; 核心思想：对一个相同的数字经过三次运算之后，ones 和 twos 的值保持不变 1234567891011int ones = 0, twos = 0;int a = 5;//第一次ones = (ones ^ a) &amp; ~twos; // 5twos = (twos ^ a) &amp; ~ones; // 0//第二次ones = (ones ^ a) &amp; ~twos; // 0twos = (twos ^ a) &amp; ~ones; // 5//第三次ones = (ones ^ a) &amp; ~twos; // 0twos = (twos ^ a) &amp; ~ones; // 0]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-132:Palindrome Partitioning II]]></title>
    <url>%2F2019%2F09%2F25%2Fleetcode-132%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/palindrome-partitioning-ii/ 题目描述题目难度：Hard Given a string s, partition s such that every substring of the partition is a palindrome. Return the minimum cuts needed for a palindrome partitioning of s. Example: 123Input: &quot;aab&quot;Output: 1Explanation: The palindrome partitioning [&quot;aa&quot;,&quot;b&quot;] could be produced using 1 cut. Solution1This can be solved by two points: cut[i] is the minimum of cut[j - 1] + 1 (j &lt;= i), if [j, i] is palindrome. If [j, i] is palindrome, [j + 1, i - 1] is palindrome, and c[j] == c[i]. The 2nd point reminds us of using dp (caching). 1234a b a | c c j i j-1 | [j, i] is palindrome cut(j-1) + 1 代码来自：Leetcode 12345678910111213141516171819202122class Solution &#123; public int minCut(String s) &#123; char[] c = s.toCharArray(); int n = c.length; // cut[i] 表示下标从 0 到 i 的元素组成的子数组的最小切割数（子问题） int[] cut = new int[n]; // pal[i][j] 表示下标从 0 到 i 的元素组成的子数组是否为回文序列 boolean[][] pal = new boolean[n][n]; for(int i = 0; i &lt; n; i++) &#123; int min = i; for(int j = 0; j &lt;= i; j++) &#123; //j + 1 &gt;= i - 1 相当于 i - j &lt;= 2，即 i 和 j 的距离不超过2 if(c[j] == c[i] &amp;&amp; (j + 1 &gt;= i - 1 || pal[j + 1][i - 1])) &#123; pal[j][i] = true; min = j == 0 ? 0 : Math.min(min, cut[j - 1] + 1); &#125; &#125; cut[i] = min; &#125; return cut[n - 1];&#125;&#125; Solution2123456789101112131415161718192021222324252627class Solution &#123; public int minCut(String s) &#123; if (s==null || s.length()&lt;=1) return 0; int n = s.length(); //每个位置的最小切割数 int[] dp = new int[n+1]; for (int i = 0; i&lt;dp.length; ++i) &#123; //初始化 dp 为每一个位置的最大切割数 dp[i] = i-1; &#125; for (int i = 0; i&lt;n; ++i) &#123; //回文串是奇数的情况 helper(i, i, dp, s); //回文串是偶数的情况 helper(i, i+1, dp, s); &#125; return dp[n]; &#125; private void helper(int l, int r, int[] dp, String s) &#123; while (l&gt;=0 &amp;&amp; r&lt;s.length() &amp;&amp; s.charAt(l) == s.charAt(r)) &#123; dp[r+1] = Math.min(dp[r+1], dp[l] + 1); l--; r++; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并查集]]></title>
    <url>%2F2019%2F09%2F22%2Funion-find%2F</url>
    <content type="text"><![CDATA[来看一个实例，杭电1232畅通工程 首先在地图上给你若干个城镇，这些城镇都可以看作点，然后告诉你哪些对城镇之间是有道路直接相连的。最后要解决的是整幅图的连通性问题。比如随意给你两个点，让你判断它们是否连通，或者问你整幅图一共有几个连通分支，也就是被分成了几个互相独立的块。像畅通工程这题，问还需要修几条路，实质就是求有几个连通分支。如果是1个连通分支，说明整幅图上的点都连起来了，不用再修路了；如果是2个连通分支，则只要再修1条路，从两个分支中各选一个点，把它们连起来，那么所有的点都是连起来的了；如果是3个连通分支，则只要再修两条路…… 以下面这组数据输入数据来说明 4 21 34 3 第一行告诉你，一共有4个点，2条路。下面两行告诉你，1、3之间有条路，4、3之间有条路。那么整幅图就被分成了1-3-4和2两部分。只要再加一条路，把2和其他任意一个点连起来，畅通工程就实现了，那么这个这组数据的输出结果就是1。好了，现在编程实现这个功能吧，城镇有几百个，路有不知道多少条，而且可能有回路。 这可如何是好？ 我以前也不会呀，自从用了并查集之后，嗨，效果还真好！我们全家都用它！ 并查集由一个整数型的数组和两个函数构成。数组pre[]记录了每个点的前导点是什么，函数find是查找，join是合并。 12345678910111213141516171819202122int pre[1000 ];int find(int x) //查找根节点&#123; int r=x; while ( pre[r] != r ) //返回根节点 r r=pre[r]; int i=x , j ; while( i != r ) //路径压缩 &#123; j = pre[ i ]; // 在改变上级之前用临时变量 j 记录下他的值 pre[ i ]= r ; //把上级改为根节点 i=j; &#125; return r ;&#125;void join(int x,int y) //判断x y是否连通， //如果已经连通，就不用管了 //如果不连通，就把它们所在的连通分支合并起,&#123; int fx=find(x),fy=find(y); if(fx!=fy) pre[fx ]=fy;&#125; 为了解释并查集的原理，我将举一个更有爱的例子。 话说江湖上散落着各式各样的大侠，有上千个之多。他们没有什么正当职业，整天背着剑在外面走来走去，碰到和自己不是一路人的，就免不了要打一架。但大侠们有一个优点就是讲义气，绝对不打自己的朋友。而且他们信奉“朋友的朋友就是我的朋友”，只要是能通过朋友关系串联起来的，不管拐了多少个弯，都认为是自己人。这样一来，江湖上就形成了一个一个的群落，通过两两之间的朋友关系串联起来。而不在同一个群落的人，无论如何都无法通过朋友关系连起来，于是就可以放心往死了打。但是两个原本互不相识的人，如何判断是否属于一个朋友圈呢？ 我们可以在每个朋友圈内推举出一个比较有名望的人，作为该圈子的代表人物，这样，每个圈子就可以这样命名“齐达内朋友之队”“罗纳尔多朋友之队”……两人只要互相对一下自己的队长是不是同一个人，就可以确定敌友关系了。 但是还有问题啊，大侠们只知道自己直接的朋友是谁，很多人压根就不认识队长，要判断自己的队长是谁，只能漫无目的的通过朋友的朋友关系问下去：“你是不是队长？你是不是队长？”这样一来，队长面子上挂不住了，而且效率太低，还有可能陷入无限循环中。于是队长下令，重新组队。队内所有人实行分等级制度，形成树状结构，我队长就是根节点，下面分别是二级队员、三级队员。每个人只要记住自己的上级是谁就行了。遇到判断敌友的时候，只要一层层向上问，直到最高层，就可以在短时间内确定队长是谁了。由于我们关心的只是两个人之间是否连通，至于他们是如何连通的，以及每个圈子内部的结构是怎样的，甚至队长是谁，并不重要。所以我们可以放任队长随意重新组队，只要不搞错敌友关系就好了。于是，门派产生了。 下面我们来看并查集的实现。 int pre[1000]; 这个数组，记录了每个大侠的上级是谁。大侠们从1或者0开始编号（依据题意而定），pre[15]=3就表示15号大侠的上级是3号大侠。如果一个人的上级就是他自己，那说明他就是掌门人了，查找到此为止。也有孤家寡人自成一派的，比如欧阳锋，那么他的上级就是他自己。每个人都只认自己的上级。比如胡青牛同学只知道自己的上级是杨左使。张无忌是谁？不认识！要想知道自己的掌门是谁，只能一级级查上去。 find这个函数就是找掌门用的，意义再清楚不过了（路径压缩算法先不论，后面再说）。 1234567int find(int x) //查找我（x）的掌门 &#123; int r=x; //委托 r 去找掌门 while (pre[r]!=r) //如果r的上级不是r自己（也就是说找到的大侠他不是掌门 = =） r=pre[r] ; // r 就接着找他的上级，直到找到掌门为止。 return r ; //掌门驾到~~~ &#125; 再来看看join函数，就是在两个点之间连一条线，这样一来，原先它们所在的两个板块的所有点就都可以互通了。这在图上很好办，画条线就行了。但我们现在是用并查集来描述武林中的状况的，一共只有一个pre[]数组，该如何实现呢？ 还是举江湖的例子，假设现在武林中的形势如图所示。虚竹小和尚与周芷若MM是我非常喜欢的两个人物，他们的终极boss分别是玄慈方丈和灭绝师太，那明显就是两个阵营了。我不希望他们互相打架，就对他俩说：“你们两位拉拉勾，做好朋友吧。”他们看在我的面子上，同意了。这一同意可非同小可，整个少林和峨眉派的人就不能打架了。这么重大的变化，可如何实现呀，要改动多少地方？其实非常简单，我对玄慈方丈说：“大师，麻烦你把你的上级改为灭绝师太吧。这样一来，两派原先的所有人员的终极boss都是师太，那还打个球啊！反正我们关心的只是连通性，门派内部的结构不要紧的。”玄慈一听肯定火大了：“我靠，凭什么是我变成她手下呀，怎么不反过来？我抗议！”抗议无效，上天安排的，最大。反正谁加入谁效果是一样的，我就随手指定了一个。这段函数的意思很明白了吧？ 123456void join(int x,int y) //我想让虚竹和周芷若做朋友 &#123; int fx=find(x),fy=find(y); //虚竹的老大是玄慈，芷若MM的老大是灭绝 if(fx!=fy) //玄慈和灭绝显然不是同一个人 pre[fx ]=fy; //方丈只好委委屈屈地当了师太的手下啦 &#125; 再来看看路径压缩算法。建立门派的过程是用join函数两个人两个人地连接起来的，谁当谁的手下完全随机。最后的树状结构会变成什么胎唇样，我也完全无法预计，一字长蛇阵也有可能。这样查找的效率就会比较低下。最理想的情况就是所有人的直接上级都是掌门，一共就两级结构，只要找一次就找到掌门了。哪怕不能完全做到，也最好尽量接近。这样就产生了路径压缩算法。 设想这样一个场景：两个互不相识的大侠碰面了，想知道能不能揍。 于是赶紧打电话问自己的上级：“你是不是掌门？” 上级说：“我不是呀，我的上级是谁谁谁，你问问他看看。” 一路问下去，原来两人的最终boss都是东厂曹公公。 “哎呀呀，原来是记己人，西礼西礼，在下三营六组白面葫芦娃!” “幸会幸会，在下九营十八组仙子狗尾巴花！” 两人高高兴兴地手拉手喝酒去了。 “等等等等，两位同学请留步，还有事情没完成呢！”我叫住他俩。 “哦，对了，还要做路径压缩。”两人醒悟。 白面葫芦娃打电话给他的上级六组长：“组长啊，我查过了，其习偶们的掌门是曹公公。不如偶们一起及接拜在曹公公手下吧，省得级别太低，以后查找掌门麻环。” “唔，有道理。” 白面葫芦娃接着打电话给刚才拜访过的三营长……仙子狗尾巴花也做了同样的事情。 这样，查询中所有涉及到的人物都聚集在曹公公的直接领导下。每次查询都做了优化处理，所以整个门派树的层数都会维持在比较低的水平上。路径压缩的代码，看得懂很好，看不懂也没关系，直接抄上用就行了。总之它所实现的功能就是这么个意思。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>并查集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-130:Surrounded Regions]]></title>
    <url>%2F2019%2F09%2F22%2Fleetcode-130%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/surrounded-regions/ 题目描述题目难度：Medium Given a 2D board containing &#39;X&#39; and &#39;O&#39; (the letter O), capture all regions surrounded by &#39;X&#39;. A region is captured by flipping all &#39;O&#39;s into &#39;X&#39;s in that surrounded region. Example: 1234X X X XX O O XX X O XX O X X After running your function, the board should be: 1234X X X XX X X XX X X XX O X X Explanation: Surrounded regions shouldn’t be on the border, which means that any &#39;O&#39; on the border of the board are not flipped to &#39;X&#39;. Any &#39;O&#39; that is not on the border and it is not connected to an &#39;O&#39; on the border will be flipped to &#39;X&#39;. Two cells are connected if they are adjacent cells connected horizontally or vertically. Solution思路：从最外围的四个方向当中的&#39;O&#39;出发，用dfs把遇到的&#39;O&#39;都置为true，最后把坐标为false且为&#39;O&#39;的元素置为&#39;X&#39;。 代码来自Leetcode 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public void solve(char[][] board) &#123; if(board.length == 0) return; int rows = board.length; int cols = board[0].length; boolean[][] visited = new boolean[rows][cols]; for(int c=0; c&lt;cols; c++) &#123; if(board[0][c] == 'O' &amp;&amp; !visited[0][c]) bfs(board, 0, c, rows, cols, visited); if(board[rows-1][c] == 'O' &amp;&amp; !visited[rows - 1][c]) bfs(board, rows-1, c, rows, cols, visited); &#125; for(int r=0; r&lt;rows; r++) &#123; if(board[r][0] == 'O' &amp;&amp; !visited[r][0]) bfs(board, r, 0, rows, cols, visited); if(board[r][cols-1] == 'O' &amp;&amp; !visited[r][cols - 1]) bfs(board, r, cols-1, rows, cols, visited); &#125; for(int r=1; r&lt;rows-1; r++) &#123; for(int c=1; c&lt;cols-1; c++) &#123; if(!visited[r][c] &amp;&amp; board[r][c] == 'O') &#123; board[r][c] = 'X'; &#125; &#125; &#125; &#125; public void bfs(char[][] board, int r, int c, int rows, int cols, boolean[][] visited) &#123; visited[r][c] = true; if(r-1 &gt;= 0 &amp;&amp; !visited[r-1][c] &amp;&amp; board[r-1][c] == 'O') bfs(board, r-1, c, rows, cols, visited); if(r+1 &lt; rows &amp;&amp; !visited[r+1][c] &amp;&amp; board[r+1][c] == 'O') bfs(board, r+1, c, rows, cols, visited); if(c+1 &lt; cols &amp;&amp; !visited[r][c+1] &amp;&amp; board[r][c+1] == 'O') bfs(board, r, c+1, rows, cols, visited); if(c-1 &gt;= 0 &amp;&amp; !visited[r][c-1] &amp;&amp; board[r][c-1] == 'O') bfs(board, r, c-1, rows, cols, visited); &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-129:Sum Root to Leaf Numbers]]></title>
    <url>%2F2019%2F09%2F22%2Fleetcode-129%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/sum-root-to-leaf-numbers/ 题目描述题目难度：Medium Given a binary tree containing digits from 0-9 only, each root-to-leaf path could represent a number. An example is the root-to-leaf path 1-&gt;2-&gt;3 which represents the number 123. Find the total sum of all root-to-leaf numbers. Note: A leaf is a node with no children. Example: 123456789Input: [1,2,3] 1 / \ 2 3Output: 25Explanation:The root-to-leaf path 1-&gt;2 represents the number 12.The root-to-leaf path 1-&gt;3 represents the number 13.Therefore, sum = 12 + 13 = 25. Example 2: 123456789101112Input: [4,9,0,5,1] 4 / \ 9 0 / \5 1Output: 1026Explanation:The root-to-leaf path 4-&gt;9-&gt;5 represents the number 495.The root-to-leaf path 4-&gt;9-&gt;1 represents the number 491.The root-to-leaf path 4-&gt;0 represents the number 40.Therefore, sum = 495 + 491 + 40 = 1026. Solution1234567891011121314151617181920212223242526272829/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; int res = 0; public int sumNumbers(TreeNode root) &#123; if(root == null) return res; sumNumbersCore(root, 0); return res; &#125; private void sumNumbersCore(TreeNode root, int cur)&#123; cur = cur * 10 + root.val; if(root.left == null &amp;&amp; root.right == null)&#123; res += cur; return; &#125; if(root.left != null) sumNumbersCore(root.left, cur); if(root.right != null) sumNumbersCore(root.right, cur); &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-126:Word Ladder II]]></title>
    <url>%2F2019%2F09%2F18%2Fleetcode-126%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/word-ladder-ii/ 题目描述题目难度：Hard Given two words (beginWord and endWord), and a dictionary’s word list, find all shortest transformation sequence(s) from beginWord to endWord, such that: Only one letter can be changed at a time Each transformed word must exist in the word list. Note that beginWord is not a transformed word. Note: Return an empty list if there is no such transformation sequence. All words have the same length. All words contain only lowercase alphabetic characters. You may assume no duplicates in the word list. You may assume beginWord and endWord are non-empty and are not the same. Example 1: 12345678910Input:beginWord = &quot;hit&quot;,endWord = &quot;cog&quot;,wordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]Output:[ [&quot;hit&quot;,&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;cog&quot;], [&quot;hit&quot;,&quot;hot&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]] Example 2: 12345678Input:beginWord = &quot;hit&quot;endWord = &quot;cog&quot;wordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;]Output: []Explanation: The endWord &quot;cog&quot; is not in wordList, therefore no possible transformation. Solution代码来自：LeetCode 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Solution &#123; public List&lt;List&lt;String&gt;&gt; findLadders(String beginWord, String endWord, List&lt;String&gt; wordList) &#123; List&lt;List&lt;String&gt;&gt; res = new ArrayList&lt;&gt;(); Set&lt;String&gt; dict = new HashSet&lt;&gt;(wordList); if(!dict.contains(endWord))&#123; return res; &#125; Set&lt;String&gt; begin = new HashSet&lt;&gt;(); Set&lt;String&gt; end = new HashSet&lt;&gt;(); begin.add(beginWord); end.add(endWord); Map&lt;String, List&lt;String&gt;&gt; map = new HashMap&lt;&gt;(); bfs(begin, end, dict, map, false); List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(beginWord); dfs(beginWord, endWord, res, list, map); return res; &#125; //function: 从 begin 中的任意元素通过变换为 dict 中的任意元素可到达 end 中的任意元素 // map 表示从 key 变换一次可达的 list //reverse : false 表示从 begin 到达 end， true 表示从 end 到达 begin private void bfs(Set&lt;String&gt; begin, Set&lt;String&gt; end, Set&lt;String&gt; dict, Map&lt;String, List&lt;String&gt;&gt; map, boolean reverse)&#123; if(begin.isEmpty())&#123; return; &#125; if(begin.size() &gt; end.size())&#123; bfs(end, begin, dict, map, !reverse); return; &#125; boolean finish = false; Set&lt;String&gt; temp = new HashSet&lt;&gt;(); dict.removeAll(begin); for(String w : begin)&#123; char[] array = w.toCharArray(); for(int i = 0; i &lt; array.length; i++)&#123; char old = array[i]; for(char c = 'a'; c &lt;= 'z'; c++)&#123; array[i] = c; String newWord = new String(array); if(dict.contains(newWord))&#123; if(end.contains(newWord))&#123; finish = true; &#125;else&#123; temp.add(newWord); &#125; String key = reverse ? newWord : w; String val = reverse ? w : newWord; if(!map.containsKey(key))&#123; map.put(key, new ArrayList&lt;&gt;()); &#125; map.get(key).add(val); &#125; &#125; array[i] = old; &#125; &#125; if(!finish)&#123; bfs(temp, end, dict, map, reverse); &#125; &#125; //function: 求从 begin 到达 end 的所有路径 private void dfs(String begin, String end, List&lt;List&lt;String&gt;&gt; res, List&lt;String&gt; list, Map&lt;String, List&lt;String&gt;&gt; map)&#123; if(begin.equals(end))&#123; res.add(new ArrayList&lt;&gt;(list)); return; &#125; if(!map.containsKey(begin))&#123; return; &#125; for(String str : map.get(begin))&#123; list.add(str); dfs(str, end, res, list, map); list.remove(list.size() - 1); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-128:Longest Consecutive Sequence(最长的连续序列)]]></title>
    <url>%2F2019%2F09%2F18%2Fleetcode-128%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/longest-consecutive-sequence/ 题目描述题目难度：Hard Given an unsorted array of integers, find the length of the longest consecutive elements sequence. Your algorithm should run in O(n) complexity. Example: 123Input: [100, 4, 200, 1, 3, 2]Output: 4Explanation: The longest consecutive elements sequence is [1, 2, 3, 4]. Therefore its length is 4. Solution1234567891011121314151617class Solution &#123; public int longestConsecutive(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; Arrays.sort(nums); int res = 1; int preNum = nums[0]; int curLen = 1; for(int i = 1;i &lt; nums.length;i++)&#123; if(nums[i] == preNum) continue; else if(nums[i] == preNum + 1) curLen++; else curLen = 1; preNum = nums[i]; res = Math.max(res, curLen); &#125; return res; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-127:Word Ladder]]></title>
    <url>%2F2019%2F09%2F18%2Fleetcode-127%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/word-ladder/ 题目描述题目难度：Medium Given two words (beginWord and endWord), and a dictionary’s word list, find the length of shortest transformation sequence from beginWord to endWord, such that: Only one letter can be changed at a time. Each transformed word must exist in the word list. Note that beginWord is not a transformed word. Note: Return 0 if there is no such transformation sequence. All words have the same length. All words contain only lowercase alphabetic characters. You may assume no duplicates in the word list. You may assume beginWord and endWord are non-empty and are not the same. Example 1: 123456789Input:beginWord = &quot;hit&quot;,endWord = &quot;cog&quot;,wordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]Output: 5Explanation: As one shortest transformation is &quot;hit&quot; -&gt; &quot;hot&quot; -&gt; &quot;dot&quot; -&gt; &quot;dog&quot; -&gt; &quot;cog&quot;,return its length 5. Example 2: 12345678Input:beginWord = &quot;hit&quot;endWord = &quot;cog&quot;wordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;]Output: 0Explanation: The endWord &quot;cog&quot; is not in wordList, therefore no possible transformation. Solution代码来自：Leetcode 利用BFS 12345678910111213141516171819202122232425262728293031323334353637public class Solution &#123; public int ladderLength(String beginWord, String endWord, List&lt;String&gt; wordList)&#123; Set&lt;String&gt; beginSet = new HashSet&lt;&gt;(); Set&lt;String&gt; endSet = new HashSet&lt;&gt;(); beginSet.add(beginWord); endSet.add(endWord); Set&lt;String&gt; dict = new HashSet&lt;&gt;(wordList); if (!dict.contains(endWord)) return 0; return search(beginSet, endSet, dict,1); &#125; //function: 从 beginSet 中的任意元素到达 endSet 中的任意元素的最小距离，dict 是中间转换集合， cnt是当前变换次数 private int search (Set&lt;String&gt; beginSet, Set&lt;String&gt; endSet, Set&lt;String&gt; dict, int cnt)&#123; if (beginSet.isEmpty() || endSet.isEmpty()) return 0; cnt++; dict.removeAll(beginSet); Set&lt;String&gt; nextSet = new HashSet&lt;&gt;(); for (String str : beginSet)&#123; char[] chs = str.toCharArray(); for (int i = 0; i &lt; chs.length; i++)&#123; char c = chs[i]; for (char j = 'a'; j &lt; 'z'; j++)&#123; chs[i] = j; String tmp = new String(chs); if (!dict.contains(tmp)) continue; if (endSet.contains(tmp)) return cnt; nextSet.add(tmp); &#125; chs[i] = c; &#125; &#125; //从 nextSet 中的元素到达 endSet 中的元素，还是反过来效果都是一样的，也就是说 nextSet 和 endSet 可以互换，但是相对来说 nextSet 应该选取元素更少的集合，这样在 search 函数中就可以尽可能的减少string 的创建 return endSet.size() &gt; nextSet.size() ? search(nextSet, endSet, dict, cnt) : search(endSet, nextSet, dict, cnt); &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-124:Binary Tree Maximum Path Sum]]></title>
    <url>%2F2019%2F09%2F17%2Fleetcode-124%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/binary-tree-maximum-path-sum/ 题目描述题目难度：Hard Given a non-empty binary tree, find the maximum path sum. For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Example 1: 1234567Input: [1,2,3] 1 / \ 2 3Output: 6 Example 2: 123456789Input: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7Output: 42 Solution代码来自Leetcode 123456789101112131415161718192021222324252627/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */public class Solution &#123; int maxValue; public int maxPathSum(TreeNode root) &#123; maxValue = Integer.MIN_VALUE; maxPathDown(root); return maxValue; &#125; //function:求出从 node 结点开始向下延伸所经过结点的最大值 private int maxPathDown(TreeNode node) &#123; if (node == null) return 0; int left = Math.max(0, maxPathDown(node.left)); int right = Math.max(0, maxPathDown(node.right)); maxValue = Math.max(maxValue, left + right + node.val); return Math.max(left, right) + node.val; &#125;&#125; 题意及代码可参考：https://segmentfault.com/a/1190000018983149]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-117:Populating Next Right Pointers in Each Node II]]></title>
    <url>%2F2019%2F09%2F16%2Fleetcode-117%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/populating-next-right-pointers-in-each-node-ii/ 题目描述题目难度：Medium Given a binary tree 123456struct Node &#123; int val; Node *left; Node *right; Node *next;&#125; Populate each next pointer to point to its next right node. If there is no next right node, the next pointer should be set to NULL. Initially, all next pointers are set to NULL. Example: 12345Input: &#123;&quot;$id&quot;:&quot;1&quot;,&quot;left&quot;:&#123;&quot;$id&quot;:&quot;2&quot;,&quot;left&quot;:&#123;&quot;$id&quot;:&quot;3&quot;,&quot;left&quot;:null,&quot;next&quot;:null,&quot;right&quot;:null,&quot;val&quot;:4&#125;,&quot;next&quot;:null,&quot;right&quot;:&#123;&quot;$id&quot;:&quot;4&quot;,&quot;left&quot;:null,&quot;next&quot;:null,&quot;right&quot;:null,&quot;val&quot;:5&#125;,&quot;val&quot;:2&#125;,&quot;next&quot;:null,&quot;right&quot;:&#123;&quot;$id&quot;:&quot;5&quot;,&quot;left&quot;:null,&quot;next&quot;:null,&quot;right&quot;:&#123;&quot;$id&quot;:&quot;6&quot;,&quot;left&quot;:null,&quot;next&quot;:null,&quot;right&quot;:null,&quot;val&quot;:7&#125;,&quot;val&quot;:3&#125;,&quot;val&quot;:1&#125;Output: &#123;&quot;$id&quot;:&quot;1&quot;,&quot;left&quot;:&#123;&quot;$id&quot;:&quot;2&quot;,&quot;left&quot;:&#123;&quot;$id&quot;:&quot;3&quot;,&quot;left&quot;:null,&quot;next&quot;:&#123;&quot;$id&quot;:&quot;4&quot;,&quot;left&quot;:null,&quot;next&quot;:&#123;&quot;$id&quot;:&quot;5&quot;,&quot;left&quot;:null,&quot;next&quot;:null,&quot;right&quot;:null,&quot;val&quot;:7&#125;,&quot;right&quot;:null,&quot;val&quot;:5&#125;,&quot;right&quot;:null,&quot;val&quot;:4&#125;,&quot;next&quot;:&#123;&quot;$id&quot;:&quot;6&quot;,&quot;left&quot;:null,&quot;next&quot;:null,&quot;right&quot;:&#123;&quot;$ref&quot;:&quot;5&quot;&#125;,&quot;val&quot;:3&#125;,&quot;right&quot;:&#123;&quot;$ref&quot;:&quot;4&quot;&#125;,&quot;val&quot;:2&#125;,&quot;next&quot;:null,&quot;right&quot;:&#123;&quot;$ref&quot;:&quot;6&quot;&#125;,&quot;val&quot;:1&#125;Explanation: Given the above binary tree (Figure A), your function should populate each next pointer to point to its next right node, just like in Figure B. Note: You may only use constant extra space. Recursive approach is fine, implicit stack space does not count as extra space for this problem. Solution112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/*// Definition for a Node.class Node &#123; public int val; public Node left; public Node right; public Node next; public Node() &#123;&#125; public Node(int _val,Node _left,Node _right,Node _next) &#123; val = _val; left = _left; right = _right; next = _next; &#125;&#125;;*/class Solution &#123; public Node connect(Node root) &#123; Node head = null; //head of the next level Node prev = null; //the leading node on the next level Node cur = root; //current node of current level while (cur != null) &#123; while (cur != null) &#123; //iterate on the current level //left child if (cur.left != null) &#123; if (prev != null) &#123; prev.next = cur.left; &#125; else &#123; head = cur.left; &#125; prev = cur.left; &#125; //right child if (cur.right != null) &#123; if (prev != null) &#123; prev.next = cur.right; &#125; else &#123; head = cur.right; &#125; prev = cur.right; &#125; //move to next node cur = cur.next; &#125; //move to next level cur = head; head = null; prev = null; &#125; return root; &#125;&#125; Solution212345678910111213141516171819202122232425262728293031323334353637383940414243444546/*// Definition for a Node.class Node &#123; public int val; public Node left; public Node right; public Node next; public Node() &#123;&#125; public Node(int _val,Node _left,Node _right,Node _next) &#123; val = _val; left = _left; right = _right; next = _next; &#125;&#125;;*/class Solution &#123; public Node connect(Node root) &#123; Node iter = root; while (iter != null) &#123; Node dummy = new Node(); Node current = dummy; while (iter != null) &#123; if (iter.left != null) &#123; current.next = iter.left; current = current.next; &#125; if (iter.right != null) &#123; current.next = iter.right; current = current.next; &#125; iter = iter.next; &#125; iter = dummy.next; &#125; return root; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-87:Scramble String]]></title>
    <url>%2F2019%2F09%2F16%2Fleetcode-87%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/scramble-string/ 题目描述题目难度：Hard Given a string s1, we may represent it as a binary tree by partitioning it to two non-empty substrings recursively. Below is one possible representation of s1 = &quot;great&quot;: 1234567 great / \ gr eat / \ / \g r e at / \ a t To scramble the string, we may choose any non-leaf node and swap its two children. For example, if we choose the node &quot;gr&quot; and swap its two children, it produces a scrambled string &quot;rgeat&quot;. 1234567 rgeat / \ rg eat / \ / \r g e at / \ a t We say that &quot;rgeat&quot; is a scrambled string of &quot;great&quot;. Similarly, if we continue to swap the children of nodes &quot;eat&quot; and &quot;at&quot;, it produces a scrambled string &quot;rgtae&quot;. 1234567 rgtae / \ rg tae / \ / \r g ta e / \ t a We say that &quot;rgtae&quot; is a scrambled string of &quot;great&quot;. Given two strings s1 and s2 of the same length, determine if s2 is a scrambled string of s1. Example 1: 12Input: s1 = &quot;great&quot;, s2 = &quot;rgeat&quot;Output: true Example 2: 12Input: s1 = &quot;abcde&quot;, s2 = &quot;caebd&quot;Output: false Solution1234567891011121314151617181920212223class Solution &#123; public boolean isScramble(String s1, String s2) &#123; if(s1 == null || s2 == null) return false; if(s1.equals(s2)) return true; if(s1.length() != s2.length()) return false; int[] letters = new int[26]; int len = s1.length(); for(int i = 0; i &lt; len; i++)&#123; letters[s1.charAt(i)-'a']++; letters[s2.charAt(i)-'a']--; &#125; for(int i = 0; i &lt; 26; i++)&#123; if(letters[i]!= 0) return false; &#125; for(int i = 1; i &lt; len; i++)&#123; if(isScramble(s1.substring(0,i), s2.substring(0,i)) &amp;&amp; isScramble(s1.substring(i), s2.substring(i))) return true; if(isScramble(s1.substring(0,i), s2.substring(len-i)) &amp;&amp; isScramble(s1.substring(i), s2.substring(0,len-i))) return true; &#125; return false; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-81:Search in Rotated Sorted Array II]]></title>
    <url>%2F2019%2F09%2F15%2Fleetcode-81%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/search-in-rotated-sorted-array-ii/ 题目描述题目难度：Medium Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand. (i.e., [0,0,1,2,2,5,6] might become [2,5,6,0,0,1,2]). You are given a target value to search. If found in the array return true, otherwise return false. Example 1: 12Input: nums = [2,5,6,0,0,1,2], target = 0Output: true Example 2: 12Input: nums = [2,5,6,0,0,1,2], target = 3Output: false Follow up: This is a follow up problem to Search in Rotated Sorted Array, where nums may contain duplicates. Would this affect the run-time complexity? How and why? 解题思路 参考自：My 8ms C++ solution (o(logn) on average, o(n) worst case) The idea is the same as the previous one without duplicates 123451) everytime check if targe == nums[mid], if so, we find it.2) otherwise, we check if the first half is in order (i.e. nums[left]&lt;=nums[mid]) and if so, go to step 3), otherwise, the second half is in order, go to step 4)3) check if target in the range of [left, mid-1] (i.e. nums[left]&lt;=target &lt; nums[mid]), if so, do search in the first half, i.e. right = mid-1; otherwise, search in the second half left = mid+1;4) check if target in the range of [mid+1, right] (i.e. nums[mid]&lt;target &lt;= nums[right]), if so, do search in the second half, i.e. left = mid+1; otherwise search in the first half right = mid-1; The only difference is that due to the existence of duplicates, we can have nums[left] == nums[mid] and in that case, the first half could be out of order (i.e. NOT in the ascending order, e.g. [3 1 2 3 3 3 3]) and we have to deal this case separately. In that case, it is guaranteed that nums[right] also equals to nums[mid], so what we can do is to check if nums[mid]== nums[left] == nums[right] before the original logic, and if so, we can move left and right both towards the middle by 1. and repeat. Solution1234567891011121314151617181920212223242526272829303132class Solution &#123; public boolean search(int[] nums, int target) &#123; if (nums == null || nums.length == 0) &#123; return false; &#125; int start = 0; int end = nums.length - 1; while (start &lt;= end) &#123; // 这种做法比 mid = (end + start) / 2 要好，因为 end + start 的值可能会大于 int 类型的最大值 int mid = start + (end - start) / 2; if(nums[mid] == target) return true; if( (nums[start] == nums[mid]) &amp;&amp; (nums[end] == nums[mid]) ) &#123;++start; --end;&#125; else if (nums[start] &lt;= nums[mid]) &#123; if (nums[start] &lt;= target &amp;&amp; target &lt; nums[mid]) &#123; end = mid - 1; &#125; else &#123; start = mid + 1; &#125; &#125; else &#123; if (nums[mid] &lt; target &amp;&amp; target &lt;= nums[end]) &#123; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; &#125; return false; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-72:Edit Distance]]></title>
    <url>%2F2019%2F09%2F15%2Fleetcode-72%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/edit-distance/ 题目描述题目难度：Hard Given two words word1 and word2, find the minimum number of operations required to convert word1 to word2. You have the following 3 operations permitted on a word: Insert a character Delete a character Replace a character Example 1: 123456Input: word1 = &quot;horse&quot;, word2 = &quot;ros&quot;Output: 3Explanation: horse -&gt; rorse (replace &apos;h&apos; with &apos;r&apos;)rorse -&gt; rose (remove &apos;r&apos;)rose -&gt; ros (remove &apos;e&apos;) Example 2: 12345678Input: word1 = &quot;intention&quot;, word2 = &quot;execution&quot;Output: 5Explanation: intention -&gt; inention (remove &apos;t&apos;)inention -&gt; enention (replace &apos;i&apos; with &apos;e&apos;)enention -&gt; exention (replace &apos;n&apos; with &apos;x&apos;)exention -&gt; exection (replace &apos;n&apos; with &apos;c&apos;)exection -&gt; execution (insert &apos;u&apos;) 思路 以下解释及Solution1代码均来自：https://leetcode.com/problems/edit-distance/discuss/25849/Java-DP-solution-O(nm) Let following be the function definition :- f(i, j) := minimum cost (or steps) required to convert first i characters of word1 to first j characters of word2 Case 1: word1[i] == word2[j], i.e. the ith the jth character matches. f(i, j) = f(i - 1, j - 1) Case 2: word1[i] != word2[j], then we must either insert, delete or replace, whichever is cheaper f(i, j) = 1 + min { f(i, j - 1), f(i - 1, j), f(i - 1, j - 1) } f(i, j - 1) represents insert operation f(i - 1, j) represents delete operation f(i - 1, j - 1) represents replace operation Here, we consider any operation from word1 to word2. It means, when we say insert operation, we insert a new character after word1 that matches the jth character of word2. So, now have to match i characters of word1 to j - 1 characters of word2. Same goes for other 2 operations as well. Note that the problem is symmetric. The insert operation in one direction (i.e. from word1 to word2) is same as delete operation in other. So, we could choose any direction. Above equations become the recursive definitions for DP. Base Case: f(0, k) = f(k, 0) = k Solution1非递归写法。 123456789101112131415161718192021222324252627public class Solution &#123; public int minDistance(String word1, String word2) &#123; int m = word1.length(); int n = word2.length(); int[][] cost = new int[m + 1][n + 1]; for(int i = 0; i &lt;= m; i++) cost[i][0] = i; for(int i = 1; i &lt;= n; i++) cost[0][i] = i; for(int i = 0; i &lt; m; i++) &#123; for(int j = 0; j &lt; n; j++) &#123; if(word1.charAt(i) == word2.charAt(j)) cost[i + 1][j + 1] = cost[i][j]; else &#123; int a = cost[i][j]; int b = cost[i][j + 1]; int c = cost[i + 1][j]; cost[i + 1][j + 1] = a &lt; b ? (a &lt; c ? a : c) : (b &lt; c ? b : c); cost[i + 1][j + 1]++; &#125; &#125; &#125; return cost[m][n]; &#125;&#125; Solution2递归写法。 12345678910111213141516171819class Solution &#123; public int minDistance(String word1, String word2) &#123; int[][] dp = new int[word1.length() + 1][word2.length() + 1]; return minHelper(word1, word2, dp, word1.length(), word2.length()); &#125; //function: 求 dp[i][j] private int minHelper(String word1, String word2, int[][]dp, int i, int j) &#123; if (dp[i][j] != 0) return dp[i][j]; if (i == 0) return dp[0][j] = j; if (j == 0) return dp[i][0] = i; if (word1.charAt(i - 1) == word2.charAt(j - 1)) return dp[i][j] = minHelper(word1, word2, dp, i - 1, j - 1); int insert = minHelper(word1, word2, dp, i-1, j); int replace = minHelper(word1, word2, dp, i-1, j - 1); int delete = minHelper(word1, word2, dp, i, j - 1); return dp[i][j] = Math.min(insert,Math.min(replace,delete)) + 1; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串s1最少删除多少个字符使得s1包含字符串s2]]></title>
    <url>%2F2019%2F09%2F14%2FMinimum-number-of-deletions%2F</url>
    <content type="text"><![CDATA[题目描述给定两非空字符串s1和s2，s1的长度大于s2的长度，问s1最少删除多少个字符使得s1包含字符串s2 Solution1234567891011121314151617public int helper(String s1, String s2)&#123; int min = Integer.MAX_VALUE; if(s1.contains(s2)) return 0; int j = 0; int i = 0; int start = 0; while(i &lt; s1.length())&#123; if(s1.charAt(i) == s2.charAt(j)) j++; i++; if(j == s2.length()) &#123; min = Math.min(min, i - start); j = 0; start = i; &#125; &#125; return min - s2.length(); &#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-71:Simplify Path(简化路径/路径压缩)]]></title>
    <url>%2F2019%2F09%2F14%2Fleetcode-71%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/simplify-path/ 题目描述题目难度：Medium Given an absolute path for a file (Unix-style), simplify it. Or in other words, convert it to the canonical path. In a UNIX-style file system, a period . refers to the current directory. Furthermore, a double period .. moves the directory up a level. For more information, see: Absolute path vs relative path in Linux/Unix Note that the returned canonical path must always begin with a slash /, and there must be only a single slash / between two directory names. The last directory name (if it exists) must not end with a trailing /. Also, the canonical path must be the shortest string representing the absolute path. Example 1: 123Input: &quot;/home/&quot;Output: &quot;/home&quot;Explanation: Note that there is no trailing slash after the last directory name. Example 2: 123Input: &quot;/../&quot;Output: &quot;/&quot;Explanation: Going one level up from the root directory is a no-op, as the root level is the highest level you can go. Example 3: 123Input: &quot;/home//foo/&quot;Output: &quot;/home/foo&quot;Explanation: In the canonical path, multiple consecutive slashes are replaced by a single one. Example 4: 12Input: &quot;/a/./b/../../c/&quot;Output: &quot;/c&quot; Example 5: 12Input: &quot;/a/../../b/../c//.//&quot;Output: &quot;/c&quot; Example 6: 12Input: &quot;/a//b////c/d//././/..&quot;Output: &quot;/a/b/c&quot; Solution1123456789101112131415161718192021222324252627282930313233public String simplifyPath(String path) &#123; LinkedList&lt;String&gt; list = new LinkedList(); int i = 0; int len = path.length(); while(i &lt; len)&#123; while(i &lt; len &amp;&amp; path.charAt(i) == '/') i++; if(i == len) break; char c = path.charAt(i); int j = i; if(c == '.' &amp;&amp; i + 1 == len) break; // /. 结束的情况 if(c == '.' &amp;&amp; j + 1 &lt; len &amp;&amp; path.charAt(j + 1) == '/') i++; // /./的情况 else if(c == '.' &amp;&amp; j + 1 &lt; len &amp;&amp; path.charAt(j + 1) == '.' &amp;&amp; (j + 2 == len || path.charAt(j + 2) == '/')) &#123; // /../的情况 if (list.size() != 0) list.remove(list.size() - 1); i = i + 2; &#125; else &#123; StringBuilder sb = new StringBuilder(); while (c != '/')&#123; sb.append(c); i++; if(i == len) break; c = path.charAt(i); &#125; list.add(sb.toString()); &#125; &#125; StringBuilder sb = new StringBuilder(); sb.append("/"); for(int k = 0;k &lt; list.size();k++) if(k == list.size() - 1) sb.append(list.get(k)); else sb.append(list.get(k) + "/"); return sb.toString(); &#125; Solution2代码来自：https://leetcode.com/problems/simplify-path/discuss/25686/Java-10-lines-solution-with-stack 1234567891011public String simplifyPath(String path) &#123; Deque&lt;String&gt; stack = new LinkedList&lt;&gt;(); Set&lt;String&gt; skip = new HashSet&lt;&gt;(Arrays.asList("..",".","")); for (String dir : path.split("/")) &#123; if (dir.equals("..") &amp;&amp; !stack.isEmpty()) stack.pop(); else if (!skip.contains(dir)) stack.push(dir); &#125; String res = ""; for (String dir : stack) res = "/" + dir + res; return res.isEmpty() ? "/" : res;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机图形学、数字图像处理、计算机视觉之间的区别与联系]]></title>
    <url>%2F2019%2F09%2F14%2FCV-CG-DIP%2F</url>
    <content type="text"><![CDATA[本文转载自：【学习笔记】计算机图形学、数字图像处理、计算机视觉之间的区别与联系 三者之间既有区别，又有联系，不确切的描述： 计算机图形学≈画图 计算机视觉≈看图 数字图像处理≈看图前沐浴更衣焚香做好各种仪式，然后再看图 计算机图形学（Computer Graphics）讲的是图形，也就是图形的构造方式，是一种从无到有的概念，从数据得到图像。是给定关于景象结构、表面反射特性、光源配置及相机模型的信息，生成图像。 计算机视觉（Computer Vision）是给定图象，从图象提取信息，包括景象的三维结构，运动检测，识别物体等。 数字图像处理（Digital Image Processing）是对已有的图像进行变换、分析、重构，得到的仍是图像。 模式识别（PR）本质就是分类，根据常识或样本或二者结合进行分类，可以对图像进行分类，从图像得到数据。 Computer Graphics和Computer Vision是同一过程的两个方向。Computer Graphics将抽象的语义信息转化成图像，Computer Vision从图像中提取抽象的语义信息。Image Processing探索的是从一个图像或者一组图像之间的互相转化和关系，与语义信息无关。总之，计算机图形学是计算机视觉的逆问题，两者从最初相互独立的平行发展到最近的融合是一大趋势。图像模式的分类是计算机视觉中的一个重要问题，模式识别中的许多方法可以应用于计算机视觉中。 区别：Computer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb颜色等。输出的是图像，即二维像素数组。Computer Vision，简称 CV。输入的是图像或图像序列，通常来自相机或usb摄像头。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。Digital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。 联系：CG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。CV 更是大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理。最后还要提到近年来的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。 简单点说：1 计算机视觉，里面人工智能的东西更多一些，不仅仅是图像处理的知识，还涵盖了人工智能，机器学习等领域知识；2，计算机图形学，主要涉及图形成像及游戏类开发，如opengl等，还有就是视频渲染等；3，图像处理，这个主要针对图像图像的基本处理，如图像检索或则图像识别，压缩，复原等等操作。 计算机图形学和数字图像处理是比较老的技术。计算机视觉要迟几十年才提出。计算机图形学和数字图像处理的区别在于图形和图像。图形是矢量的、纯数字式的。图像常常由来自现实世界的信号产生，有时也包括图形。而图像和图形都是数据的简单堆积，图像是像素的叠加，图形则是基本图元的叠加。计算机视觉要从图像中整理出一些信息或统计数据，也就是说要对计算机图像作进一步的分析。计算机图形学的研究成果可以用于产生数字图像处理所需要的素材，计算机视觉需要以数字图像处理作为基础。计算机视觉与数字图像处理的这种关系类似于物理学和数学的关系。 另外，如果不是浙江大学的或者中科院计算所的，不建议做计算机图形学这一方向，难度太大（图形比图像虽然表面上只高一维，但实际上工作量大了好多倍；其次，图像，国内外差距目前已经很小，好发重要期刊；图形，除上面两个单位和微软外，国内外差距很大，不好发重要期刊） 数字图像处理主要是对已有的图像，比如说可见光的图像、红外图像、雷达成像进行噪声滤除、边缘检测、图像恢复等处理，就像用ps 处理照片一样的。人脸识别啊、指纹识别啊、运动物体跟踪啊，都属于图像处理。去噪有各种滤波算法；其他的有各种时频变化算法，如傅里叶变化，小波变换等，有很多这方面的书籍。图形学主要研究如何生成图形的，像用autoCAD作图，就是图形学中算法的应用。各种动漫软件中图形算法的生成等。]]></content>
      <categories>
        <category>计算机视觉</category>
        <category>数字图像处理</category>
        <category>计算机图形学</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>数字图像处理</tag>
        <tag>计算机图形学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-410:Split Array Largest Sum(分割数组最大和)]]></title>
    <url>%2F2019%2F09%2F14%2Fleetcode-410%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/split-array-largest-sum/ 题目描述题目难度：Hard Given an array which consists of non-negative integers and an integer m, you can split the array into m non-empty continuous subarrays. Write an algorithm to minimize the largest sum among these m subarrays. Note:If n is the length of array, assume the following constraints are satisfied: 1 ≤ n ≤ 1000 1 ≤ m ≤ min(50, n) Examples: 1234567891011Input:nums = [7,2,5,10,8]m = 2Output:18Explanation:There are four ways to split nums into two subarrays.The best way is to split it into [7,2,5] and [10,8],where the largest sum among the two subarrays is only 18. 问题分析 The answer is between maximum value of input array numbers and sum of those numbers. Use binary search to approach the correct answer. We have l = max number of array; r = sum of all numbers in the array;Every time we do mid = (l + r) / 2; Use greedy to narrow down left and right boundaries in binary search.3.1 Cut the array from left.3.2 Try our best to make sure that the sum of numbers between each two cuts (inclusive) is large enough but still less than mid. 3.3 We’ll end up with two results: either we can divide the array into more than m subarrays or we cannot.If we can, it means that the mid value we pick is too small because we’ve already tried our best to make sure each part holds as many non-negative numbers as we can but we still have numbers left. So, it is impossible to cut the array into m parts and make sure each parts is no larger than mid. We should increase m. This leads to l = mid + 1;If we can’t, it is either we successfully divide the array into m parts and the sum of each part is less than mid, or we used up all numbers before we reach m. Both of them mean that we should lower mid because we need to find the minimum one. This leads to r = mid - 1; 参考自：https://leetcode.com/problems/split-array-largest-sum/discuss/89817/Clear-Explanation%3A-8ms-Binary-Search-Java Solution12345678910111213141516171819202122232425262728293031323334353637public class Solution &#123; public int splitArray(int[] nums, int m) &#123; int max = 0; long sum = 0; // int 数组元素之和可能超过 int 类型的最大值 for (int num : nums) &#123; max = Math.max(num, max); sum += num; &#125; if (m == 1) return (int)sum; //binary search long l = max; long r = sum; while (l &lt;= r) &#123; long mid = (l + r)/ 2; if (valid(mid, nums, m)) &#123; r = mid - 1; &#125; else &#123; l = mid + 1; &#125; &#125; return (int)l; &#125; public boolean valid(long target, int[] nums, int m) &#123; int count = 1; long total = 0; for(int num : nums) &#123; total += num; if (total &gt; target) &#123; total = num; count++; if (count &gt; m) &#123; return false; &#125; &#125; &#125; return true; &#125;&#125; 参考自：https://leetcode.com/problems/split-array-largest-sum/discuss/89817/Clear-Explanation%3A-8ms-Binary-Search-Java]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组循环右移k位]]></title>
    <url>%2F2019%2F09%2F14%2FArray-loop-right-shift%2F</url>
    <content type="text"><![CDATA[题目描述将一个长度为n的数组A的元素循环右移k位,比如数组 1, 2, 3, 4, 5循环右移3位之后变成3, 4, 5, 1, 2 Solution123456789101112131415161718public void rightShift(int[] arr, int n)&#123; if(arr == null || n &lt; 0) return; n %= arr.length; reverse(arr, 0, arr.length - n - 1); reverse(arr, arr.length - n, arr.length - 1); reverse(arr, 0, arr.length - 1); &#125;public void reverse(int[] arr, int start, int end)&#123; int temp; while (start &lt; end)&#123; temp = arr[start]; arr[start] = arr[end]; arr[end] = temp; start++; end--; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【微软面试题】在二叉搜索树中找最小的大于某个key值的节点]]></title>
    <url>%2F2019%2F09%2F13%2FMinimum-value-greater-than-k%2F</url>
    <content type="text"><![CDATA[本文转载自：【微软面试题】在二叉搜索树中找最小的大于某个key值的节点 在二叉搜索树中找最小的大于某个key值的节点 如 8 / \ 6 12 / / 2 11 14 key = 8 返回11key = 1 返回2key = 16 返回NULL 1234567891011121314151617181920212223242526272829303132333435363738struct TreeNode&#123; int val; TreeNode* left; TreeNode* right;&#125;;// 迭代实现TreeNode * FindCeiling(TreeNode *root, int key)&#123; TreeNode * ceiling = NULL; TreeNode * current = root; while(current) &#123; if(current-&gt;val &lt;= key) current = current-&gt;right; else &#123; ceiling = current; current = current-&gt;left; &#125; &#125; return ceiling;&#125;// 递归实现TreeNode * FindCeiling(TreeNode *root, int key)&#123; if(root == NULL) return NULL; if(root-&gt;val &lt;= key) return FindCeiling(root-&gt;right, key); else &#123; TreeNode *ceiling = FindCeiling(root-&gt;left, key); return ceiling ? ceiling : root; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节跳动面试题 —— 水壶问题]]></title>
    <url>%2F2019%2F09%2F13%2FKettle-problem%2F</url>
    <content type="text"><![CDATA[原题：给你一个装满水的 8 升满壶和两个分别是 5 升、3 升的空壶，请想个优雅的办法，使得其中一个水壶恰好装 4 升水，每一步的操作只能是倒空或倒满。 理解了这个题目的意思之后，我们的第一个方法肯定就是使用强大的脑力来进行暴力破解法，瓶子里的水在我们的脑子里颠三倒四，但是脑子有可能没那么清晰，想了几步之后就开始出现记忆错乱，然后就不得不开始慢慢重播。 甚至到最后好不容易搞定了，但是怎么走过来的步骤又给忘记的一干二净 —— 智商好像受到了点小小的侮辱！ 这道题其实有一道非常科学的解决方法 —— 广度遍历，我们将三个瓶子的状态标示为一个数。 8 0 0 然后开始拓展这个数的所有可能的状态，第一步这个数可以变为，括号里的数是上一步的数字 3 5 0（8 0 0） 、 5 0 3（8 0 0） 然后继续拓展第二步所有可能的状态，并且不得和之前的状态出现重复（这叫剪枝） 0 5 3（3 5 0）、3 2 3（3 5 0）、5 3 0（5 0 3） 继续第三步 6 2 0（3 2 3）、2 3 3（5 3 0） 我们发现状态变少了，这是怎么回事呢？这是因为剪枝约束 —— 不得出现和之前重复的状态，就好比下象棋，如果我不动我还能活，但是必须动就会被将死的感觉一样。 继续第四步 6 0 2（6 2 0）、2 5 1（2 3 3） 继续第五步，怎么还没出现 4 这个数字呢，好着急啊！ 1 5 2（6 0 2）、7 0 1（2 5 1） 继续第六步 1 4 3（1 5 2） 总算搞定了，这就是算法的停止条件，出现第一个数字 4。所以最终的路径就是 1 4 3 &lt;– 1 5 2 &lt;– 6 0 2 &lt;– 6 2 0 &lt;– 3 2 3 &lt;– 3 5 0 &lt;– 8 0 0 代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package com.csk.test;import java.util.ArrayList;import java.util.List;public class Test&#123; //状态类，arr数组的三个属性分别表示三个壶当前的水量 static class Status&#123; int[] arr = new int[3]; Status(int x, int y, int z)&#123; arr[0] = x; arr[1] = y; arr[2] = z; &#125; Status(Status s)&#123; arr[0] = s.arr[0]; arr[1] = s.arr[1]; arr[2] = s.arr[2]; &#125; /* @Override public int hashCode() &#123; return arr[0] + arr[1] + arr[2]; &#125; */ @Override public boolean equals(Object obj) &#123; Status status = (Status)obj; return arr[0] == status.arr[0] &amp;&amp; arr[1] == status.arr[1] &amp;&amp; arr[2] == status.arr[2]; &#125; &#125; //保存可以到达最终结果的每一次状态变化的list private static List&lt;List&lt;Status&gt;&gt; resList = new ArrayList&lt;&gt;(); private static int[] caps = &#123;8, 5, 3&#125;; //每一个壶的最大容量 public static void main(String[] args) &#123; List&lt;Status&gt; list = new ArrayList&lt;&gt;(); Status s = new Status(8, 0, 0); list.add(new Status(s)); helper(s, list); System.out.println(resList.size()); List&lt;Status&gt; curStatus; for(int i = 0;i &lt; resList.size();i++) &#123; curStatus = resList.get(i); System.out.println(i + " : "); for(Status status : curStatus) System.out.println(status.arr[0] + " " + status.arr[1] + " " + status.arr[2]); &#125; &#125; private static void helper(Status status, List&lt;Status&gt; list)&#123; if(status.arr[0] == 4 || status.arr[1] == 4 || status.arr[2] == 4)&#123; resList.add(new ArrayList&lt;&gt;(list)); return; &#125; for(int i = 0;i &lt; 3;i++)&#123; for(int j = 0;j &lt; 3;j++) &#123; if (i == j) continue; int temp; if (status.arr[i] == 0 || status.arr[j] == caps[j]) continue; if(status.arr[i] &gt;= caps[j] - status.arr[j]) temp = caps[j] - status.arr[j]; else temp = status.arr[i]; status.arr[i] -= temp; status.arr[j] += temp; Status s = new Status(status); if (list.contains(s)) &#123; status.arr[i] += temp; status.arr[j] -= temp; continue; &#125; else &#123; list.add(s); helper(status, list); list.remove(list.size() - 1); status.arr[i] += temp; status.arr[j] -= temp; &#125; &#125; &#125; &#125;&#125; 注： 除代码之外的其他部门转载自：字节跳动面试题 —— 水壶问题]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小船过河]]></title>
    <url>%2F2019%2F09%2F11%2FBoat-crossing-the-river%2F</url>
    <content type="text"><![CDATA[本文转载自：小船过河问题 小船过河问题POJ1700是一道经典的贪心算法例题。 题目大意是只有一艘船，能乘2人，船的运行速度为2人中较慢一人的速度，过去后还需一个人把船划回来，问把n个人运到对岸，最少需要多久。 先将所有人过河所需的时间按照升序排序，我们考虑把单独过河所需要时间最多的两个旅行者送到对岸去，有两种方式： 最快的和次快的过河，然后最快的将船划回来；次慢的和最慢的过河，然后次快的将船划回来，所需时间为：t[0]+2*t[1]+t[n-1]; 最快的和最慢的过河，然后最快的将船划回来，最快的和次慢的过河，然后最快的将船划回来，所需时间为：2*t[0]+t[n-2]+t[n-1]。 算一下就知道，除此之外的其它情况用的时间一定更多。每次都运送耗时最长的两人而不影响其它人，问题具有贪心子结构的性质。 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;bool cmp(int const &amp;x, int const &amp;y)&#123; return x &lt; y;//由小到大排列&#125;int main()&#123; int N = 8;//一共N个人要过河 int spendTime[8] = &#123; 3,7,5,9,4,2,6,1 &#125;;//每个人过河的速度 int sum = 0; sort(spendTime, spendTime + 8, cmp); while (N&gt;3) &#123; sum = min(sum + spendTime[1]* 2 + spendTime[0] + spendTime[N - 1],//第一种过河方式 sum + spendTime[0] * 2 + spendTime[N - 1] + spendTime[N - 2]);//第二种过河方式 N -= 2;//两个两个的运人 &#125; if (N == 3) sum += spendTime[0] + spendTime[1] + spendTime[2]; else if (N == 2)sum += spendTime[1]; else sum += spendTime[0]; cout &lt;&lt; "一共用时：" &lt;&lt; sum &lt;&lt; endl; system("pause"); return 0;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-230:Kth Smallest Element in a BST(二叉搜索树中第k小的结点)]]></title>
    <url>%2F2019%2F09%2F11%2Fleetcode-230%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/kth-smallest-element-in-a-bst/ 题目描述题目难度：Medium Given a binary search tree, write a function kthSmallest to find the kth smallest element in it. Note:You may assume k is always valid, 1 ≤ k ≤ BST’s total elements. Example 1: 1234567Input: root = [3,1,4,null,2], k = 1 3 / \ 1 4 \ 2Output: 1 Example 2: 123456789Input: root = [5,3,6,2,4,null,null,1], k = 3 5 / \ 3 6 / \ 2 4 / 1Output: 3 Follow up:What if the BST is modified (insert/delete operations) often and you need to find the kth smallest frequently? How would you optimize the kthSmallest routine? Solution112345678910111213141516171819202122232425262728/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; int res = 0; int cur = 0; public int kthSmallest(TreeNode root, int k) &#123; helper(root, k); return res; &#125; public void helper(TreeNode root, int k) &#123; if(root == null) return; helper(root.left, k); cur++; if(cur == k) &#123; res = root.val; return; &#125; helper(root.right, k); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-464:Can I Win]]></title>
    <url>%2F2019%2F09%2F10%2Fleetcode-464%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/can-i-win/ 题目描述题目难度：Medium In the “100 game,” two players take turns adding, to a running total, any integer from 1..10. The player who first causes the running total to reach or exceed 100 wins. What if we change the game so that players cannot re-use integers? For example, two players might take turns drawing from a common pool of numbers of 1..15 without replacement until they reach a total &gt;= 100. Given an integer maxChoosableInteger and another integer desiredTotal, determine if the first player to move can force a win, assuming both players play optimally. You can always assume that maxChoosableInteger will not be larger than 20 and desiredTotal will not be larger than 300. Example 12345678910111213Input:maxChoosableInteger = 10desiredTotal = 11Output:falseExplanation:No matter which integer the first player choose, the first player will lose.The first player can choose an integer from 1 up to 10.If the first player choose 1, the second player can only choose integers from 2 up to 10.The second player will win by choosing 10 and get a total = 11, which is &gt;= desiredTotal.Same with other integers chosen by the first player, the second player will always win. Solution12345678910111213141516171819202122232425public class Solution &#123; public boolean canIWin(int maxChoosableInteger, int desiredTotal) &#123; if (desiredTotal &lt;= 0) return true; if (maxChoosableInteger * (maxChoosableInteger + 1) / 2&lt;desiredTotal) return false; return dfs(desiredTotal, 0, new HashMap&lt;Integer, Boolean&gt;(), maxChoosableInteger); &#125; private boolean dfs(int total, int state, HashMap&lt;Integer, Boolean&gt; hashMap, int maxChoosableInteger) &#123; int key = state; if (hashMap.containsKey(key)) return hashMap.get(key); for (int i = 0;i &lt; maxChoosableInteger;i++) &#123; if ((state &amp; (1 &lt;&lt; i)) == 0) &#123; state |= (1 &lt;&lt; i); if (total &lt;= i+1 || !dfs(total - (i + 1), state, hashMap, maxChoosableInteger)) &#123; hashMap.put(key, true); state ^= (1 &lt;&lt; i); return true; &#125; state ^= (1 &lt;&lt; i); &#125; &#125; hashMap.put(key, false); return false; &#125;&#125; Solution212345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public boolean canIWin(int maxChoosableInteger, int desiredTotal) &#123; //特殊情况1:可选择的最大数大于等于期望值 if (desiredTotal &lt;= maxChoosableInteger) &#123; return true; &#125; //特殊情况2:可选择的数加和小于期望值 if (((1 + maxChoosableInteger) / 2 * maxChoosableInteger) &lt; desiredTotal) &#123; return false; &#125; return canIWin(desiredTotal, 0, maxChoosableInteger, new int[1 &lt;&lt; maxChoosableInteger]); &#125; private boolean canIWin(int target, int path, int maxInt, int[] memo) &#123; // HashMap&lt;Integer, Boolean&gt; map) &#123; // if (map.containsKey(path)) return map.get(path); if (memo[path] != 0) return memo[path] == 1 ? true : false; //选择一个未被选择过的数 for (int i = maxInt; i &gt;= 1; i--) &#123; if ((path &amp; (1 &lt;&lt; (maxInt - i))) != 0) continue; if (i &gt;= target) &#123; //map.put(path, true); memo[path] = 1; return true; &#125; boolean canWin = canIWin(target - i, path | (1 &lt;&lt; (maxInt - i)), maxInt, memo); //map); if (!canWin) &#123; //map.put(path, true); memo[path] = 1; return true; &#125; &#125; //map.put(path, false); memo[path] = -1; return false; &#125; &#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-575:Distribute Candies(分糖果)]]></title>
    <url>%2F2019%2F09%2F10%2Fleetcode-575%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/distribute-candies/ 题目描述题目难度:Easy Given an integer array with even length, where different numbers in this array represent different kinds of candies. Each number means one candy of the corresponding kind. You need to distribute these candies equally in number to brother and sister. Return the maximum number of kinds of candies the sister could gain. Example 1: 123456Input: candies = [1,1,2,2,3,3]Output: 3Explanation:There are three different kinds of candies (1, 2 and 3), and two candies for each kind.Optimal distribution: The sister has candies [1,2,3] and the brother has candies [1,2,3], too. The sister has three different kinds of candies. Example 2: 1234Input: candies = [1,1,2,3]Output: 2Explanation: For example, the sister has candies [2,3] and the brother has candies [1,1]. The sister has two different kinds of candies, the brother has only one kind of candies. Note: The length of the given array is in range [2, 10,000], and will be even. The number in given array is in range [-100,000, 100,000]. Solution12345678public class Solution &#123; public int distributeCandies(int[] candies) &#123; Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); for(int i : candies) set.add(i); return set.size() &gt;= candies.length / 2 ? candies.length / 2 : set.size(); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-135:Candy(分糖果)]]></title>
    <url>%2F2019%2F09%2F10%2Fleetcode-135%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/candy/ 题目描述题目难度：Hard There are N children standing in a line. Each child is assigned a rating value. You are giving candies to these children subjected to the following requirements: Each child must have at least one candy. Children with a higher rating get more candies than their neighbors. What is the minimum candies you must give? Example 1: 123Input: [1,0,2]Output: 5Explanation: You can allocate to the first, second and third child with 2, 1, 2 candies respectively. Example 2: 1234Input: [1,2,2]Output: 4Explanation: You can allocate to the first, second and third child with 1, 2, 1 candies respectively. The third child gets 1 candy because it satisfies the above two conditions. 思路初始化每个人分配的糖果数量都是1。 （1）从前往后遍历ratings数组，如果发现当前位置的值比前一个位置的值要大，需要更新当前位置candies数组的值为前一个位置candies数组的值加1。 （2）从后往前遍历ratings数组，如果发现当前位置的值比后一个位置的值要大且当前位置candies数组的值小于等于后一个位置candies数组的值，那么更新当前位置candies数组的值为后一个位置candies数组的值加1。 时间复杂度和空间复杂度均是O(n)，其中n为孩子的数量。 借鉴自：LeetCode135——分发糖果 Solution123456789101112131415161718192021222324public class Solution &#123; public int candy(int[] ratings) &#123; int n = ratings.length; int[] candies = new int[n]; for (int i = 0; i &lt; n; i++) &#123; candies[i] = 1; //每人至少发一颗糖 &#125; for(int i = 1; i &lt; n; i++)&#123; //从前往后遍历ratings数组 if(ratings[i] &gt; ratings[i - 1])&#123; candies[i] = candies[i - 1] + 1; &#125; &#125; for(int i = n - 2; i &gt;= 0; i--)&#123; //从后往前遍历ratings数组 if(ratings[i] &gt; ratings[i + 1] &amp;&amp; candies[i] &lt;= candies[i + 1])&#123; candies[i] = candies[i + 1] + 1; &#125; &#125; int sum = 0; for (int i = 0; i &lt; n; i++) &#123; sum += candies[i]; &#125; return sum; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-35:Search Insert Position(寻找插入位置)]]></title>
    <url>%2F2019%2F09%2F07%2Fleetcode-35%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/search-insert-position/ 题目描述题目难度：Easy Given a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order. You may assume no duplicates in the array. Example 1: 12Input: [1,3,5,6], 5Output: 2 Example 2: 12Input: [1,3,5,6], 2Output: 1 Example 3: 12Input: [1,3,5,6], 7Output: 4 Example 4: 12Input: [1,3,5,6], 0Output: 0 Solution1递归解法，代码来自Leetcode 12345678910111213141516171819202122232425class Solution &#123; public int searchInsert(int[] nums, int target) &#123; return searchInsert(nums, target, 0, nums.length-1); &#125; private int searchInsert(int[] nums, int target, int low, int high) &#123; int mid = (low+high)/2; if (target &lt; nums[mid]) &#123; if (mid == 0 || target &gt; nums[mid-1]) &#123; return mid; &#125; return searchInsert(nums, target, low, mid-1); &#125; if (target &gt; nums[mid]) &#123; if (mid == nums.length-1 || target &lt; nums[mid+1]) &#123; return mid+1; &#125; return searchInsert(nums, target, mid+1, high); &#125; return mid; &#125;&#125; Solution2非递归解法 1234567891011121314class Solution &#123; public int searchInsert(int[] nums, int target) &#123; if(nums == null || nums.length == 0) return -1; int low = 0,high = nums.length - 1,mid = 0; while(low &lt;= high)&#123; mid = (high + low) / 2; if(nums[mid] &gt; target) high = mid - 1; else if(nums[mid] &lt; target) low = mid + 1; else return mid; &#125; return low; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-350:Intersection of Two Arrays II(两个数组的交集)]]></title>
    <url>%2F2019%2F09%2F05%2Fleetcode-350%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/intersection-of-two-arrays-ii/ 题目描述题目难度：Easy Given two arrays, write a function to compute their intersection. Example 1: 12Input: nums1 = [1,2,2,1], nums2 = [2,2]Output: [2,2] Example 2: 12Input: nums1 = [4,9,5], nums2 = [9,4,9,8,4]Output: [4,9] Note: Each element in the result should appear as many times as it shows in both arrays. The result can be in any order. Follow up: What if the given array is already sorted? How would you optimize your algorithm? What if nums1‘s size is small compared to nums2‘s size? Which algorithm is better? What if elements of nums2 are stored on disk, and the memory is limited such that you cannot load all elements into the memory at once? Solution1代码来自：https://blog.csdn.net/codingisforlife/article/details/84309011 123456789101112131415161718192021public int[] intersect(int[] nums1, int[] nums2) &#123; if(nums1 == null || nums1.length == 0) return nums1; if(nums2 == null || nums2.length == 0) return nums2; int len1 = nums1.length; int len2 = nums2.length; boolean[] bl = new boolean[len2]; ArrayList&lt;Integer&gt; al=new ArrayList&lt;Integer&gt;(); for(int i = 0;i &lt; len1;i++) &#123; for(int j = 0;j &lt; len2;j++) &#123; if(nums1[i] == nums2[j] &amp;&amp; bl[j] == false) &#123; al.add(nums1[i]); bl[j] = true; break; &#125; &#125; &#125; int[] in = new int[al.size()]; int e=0; for(int i:al) in[e++] = i; return in; &#125; Solution2123456789101112131415161718192021public class Solution &#123; public int[] intersect(int[] nums1, int[] nums2) &#123; if(nums1 == null || nums1.length == 0) return nums1; if(nums2 == null || nums2.length == 0) return nums2; HashMap&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for(int i : nums1) map.put(i,map.getOrDefault(i,0) + 1); for(int i : nums2)&#123; if(map.containsKey(i) &amp;&amp; map.get(i) &gt; 0)&#123; map.put(i,map.get(i) - 1); list.add(i); &#125; &#125; int j = 0; int[] arrs = new int[list.size()]; for(int i : list) arrs[j++] = i; return arrs; &#125;&#125; Solution3代码来自LeetCode 12345678910111213141516171819202122class Solution &#123; public int[] intersect(int[] nums1, int[] nums2) &#123; if(nums1 == null || nums1.length == 0) return nums1; if(nums2 == null || nums2.length == 0) return nums2; Arrays.sort(nums1); Arrays.sort(nums2); int i = 0; int j = 0; int k = 0; while(i &lt; nums1.length &amp;&amp; j &lt; nums2.length)&#123; if(nums1[i] &lt; nums2[j]) i++; else if(nums1[i] &gt; nums2[j]) j++; else if(nums1[i] == nums2[j]) &#123; nums1[k++] = nums1[i++]; j++; &#125; &#125; return Arrays.copyOfRange(nums1, 0, k); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-51:N-Queens(N-皇后问题)]]></title>
    <url>%2F2019%2F09%2F03%2Fleetcode-51%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/n-queens/ 题目描述题目难度：Hard The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Given an integer n, return all distinct solutions to the n-queens puzzle. Each solution contains a distinct board configuration of the n-queens’ placement, where &#39;Q&#39; and &#39;.&#39; both indicate a queen and an empty space respectively. Example: 12345678910111213Input: 4Output: [ [&quot;.Q..&quot;, // Solution 1 &quot;...Q&quot;, &quot;Q...&quot;, &quot;..Q.&quot;], [&quot;..Q.&quot;, // Solution 2 &quot;Q...&quot;, &quot;...Q&quot;, &quot;.Q..&quot;]]Explanation: There exist two distinct solutions to the 4-queens puzzle as shown above. Solution代码来源于Leetcode 123456789101112131415161718192021222324252627282930313233343536public class _51SolveNQueens &#123; int total = 0; //n皇后问题共有多少种可能性 int[] sol; //每一行哪一列可以放皇后 public List&lt;List&lt;String&gt;&gt; solveNQueens(int n) &#123; sol = new int[n]; List&lt;List&lt;String&gt;&gt; res = new ArrayList(); DFS(res, n, 0, 0, 0, 0); return res; &#125; //col，d1，d2 分别表示 row 行中列，右斜对角线，左斜对角线不能放置皇后的位置 private void DFS(List&lt;List&lt;String&gt;&gt; res, int N, int row, int col, int d1, int d2) &#123; int avl = ((1 &lt;&lt; N) - 1) &amp; ~(col | d1 | d2); //available positions, bitmask while (avl != 0) &#123; int p = avl &amp; -avl; //取出p从右往左第一个1的位置 avl ^= p; //相当于 avl = avl - p sol[row] = p; if (row == N - 1) &#123; List&lt;String&gt; list = new ArrayList(); for (int i = 0; i &lt; N; i++) &#123; StringBuilder sb = new StringBuilder(); for (int c = 0; c &lt; N; c++) &#123; if ((1 &lt;&lt; c) == sol[i]) sb.append("Q"); else sb.append("."); &#125; list.add(sb.toString()); &#125; total++; res.add(list); &#125; else &#123; DFS(res, N, row + 1, col | p, (d1 | p) &gt;&gt; 1, (d2 | p) &lt;&lt; 1); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer:二进制中1的个数]]></title>
    <url>%2F2019%2F09%2F03%2Fjianzhioffer-num-of-1-binary%2F</url>
    <content type="text"><![CDATA[题目描述输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 Solution1JDK 自带的实现 12345public class Solution &#123; public int NumberOf1(int n) &#123; return Integer.bitCount(n); &#125;&#125; 其源码 12345678910111213141516171819/** * Returns the number of one-bits in the two's complement binary * representation of the specified &#123;@code int&#125; value. This function is * sometimes referred to as the &lt;i&gt;population count&lt;/i&gt;. * * @param i the value whose bits are to be counted * @return the number of one-bits in the two's complement binary * representation of the specified &#123;@code int&#125; value. * @since 1.5 */ public static int bitCount(int i) &#123; // HD, Figure 5-2 i = i - ((i &gt;&gt;&gt; 1) &amp; 0x55555555); i = (i &amp; 0x33333333) + ((i &gt;&gt;&gt; 2) &amp; 0x33333333); i = (i + (i &gt;&gt;&gt; 4)) &amp; 0x0f0f0f0f; i = i + (i &gt;&gt;&gt; 8); i = i + (i &gt;&gt;&gt; 16); return i &amp; 0x3f; &#125; Solution2链接：https://www.nowcoder.com/questionTerminal/8ee967e43c2c4ec193b040ea7fbb10b8来源：牛客网 12345678910public class Solution &#123; public int NumberOf1(int n) &#123; int count = 0; while(n!= 0)&#123; count++; n = n &amp; (n - 1); &#125; return count; &#125;&#125; 解释：如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。举个例子：一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现减1的结果是把最右边的一个1开始的所有位都取反了。这个时候如果我们再把原来的整数和减去1之后的结果做与运算，从原来整数最右边一个1那一位开始所有位都会变成0。如1100&amp;1011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。 Solution3n 每次都是无符号右移一位，可避免当n为负数时，最高位总是1，从而引发死循环。 123456789101112public class Solution &#123; public int NumberOf1(int n) &#123; int count = 0; while(n != 0)&#123; if((n &amp; 1) != 0)&#123; count++; &#125; n = n &gt;&gt;&gt; 1; &#125; return count; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-300:Longest Increasing Subsequence(LIS,最长上升子序列)]]></title>
    <url>%2F2019%2F08%2F29%2Fleetcode-300%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/longest-increasing-subsequence/ 题目描述题目难度：Medium Given an unsorted array of integers, find the length of longest increasing subsequence. Example: 123Input: [10,9,2,5,3,7,101,18]Output: 4 Explanation: The longest increasing subsequence is [2,3,7,101], therefore the length is 4. Note: There may be more than one LIS combination, it is only necessary for you to return the length. Your algorithm should run in O(n2) complexity. Follow up: Could you improve it to O(n log n) time complexity? Solution1:动态规划法（时间复杂度O(N^2))设长度为N的数组为{a0，a1, a2, …an-1)，则假定以aj结尾的数组序列的最长递增子序列长度为L(j)，则L(j)={ max(L(i))+1, i&lt;j且a[i]&lt;a[j] }。也就是说，我们需要遍历在j之前的所有位置i(从0到j-1)，找出满足条件a[i]&lt;a[j]的L(i)，求出max(L(i))+1即为L(j)的值。最后，我们遍历所有的L(j)（从0到N-1），找出最大值即为最大递增子序列。时间复杂度为O(N^2)。 例如给定的数组为{5，6，7，1，2，8}，则L(0)=1, L(1)=2, L(2)=3, L(3)=1, L(4)=2, L(5)=4。所以该数组最长递增子序列长度为4，序列为{5，6，7，8}。 1234567891011121314151617class Solution &#123; public int lengthOfLIS(int[] nums) &#123; int max = 0; if(nums == null || nums.length == 0) return max; int len = nums.length; int[] dp = new int[len]; for(int i = 1;i &lt; len;i++) &#123; for (int j = 0; j &lt; i; j++) &#123; if(nums[j] &lt; nums[i])&#123; dp[i] = Math.max(dp[i], dp[j] + 1); max = Math.max(max, dp[i]); &#125; &#125; &#125; return max + 1; &#125;&#125; Solution2:O(NlgN）算法 假设存在一个序列d[1..9] ={ 2，1 ，5 ，3 ，6，4， 8 ，9， 7}，可以看出来它的LIS长度为5。下面一步一步试着找出它。我们定义一个序列B，然后令 i = 1 to 9 逐个考察这个序列。此外，我们用一个变量Len来记录现在最长算到多少了 首先，把d[1]有序地放到B里，令B[1] = 2，就是说当只有1一个数字2的时候，长度为1的LIS的最小末尾是2。这时Len=1 然后，把d[2]有序地放到B里，令B[1] = 1，就是说长度为1的LIS的最小末尾是1，d[1]=2已经没用了，很容易理解吧。这时Len=1 接着，d[3] = 5，d[3]&gt;B[1]，所以令B[1+1]=B[2]=d[3]=5，就是说长度为2的LIS的最小末尾是5，很容易理解吧。这时候B[1..2] = 1, 5，Len＝2 再来，d[4] = 3，它正好加在1,5之间，放在1的位置显然不合适，因为1小于3，长度为1的LIS最小末尾应该是1，这样很容易推知，长度为2的LIS最小末尾是3，于是可以把5淘汰掉，这时候B[1..2] = 1, 3，Len = 2 继续，d[5] = 6，它在3后面，因为B[2] = 3, 而6在3后面，于是很容易可以推知B[3] = 6, 这时B[1..3] = 1, 3, 6，还是很容易理解吧？ Len = 3 了噢。 第6个, d[6] = 4，你看它在3和6之间，于是我们就可以把6替换掉，得到B[3] = 4。B[1..3] = 1, 3, 4， Len继续等于3 第7个, d[7] = 8，它很大，比4大，嗯。于是B[4] = 8。Len变成4了 第8个, d[8] = 9，得到B[5] = 9，嗯。Len继续增大，到5了。 最后一个, d[9] = 7，它在B[3] = 4和B[4] = 8之间，所以我们知道，最新的B[4] =7，B[1..5] = 1, 3, 4, 7, 9，Len = 5。 于是我们知道了LIS的长度为5。 注意，这个1,3,4,7,9不是LIS，它只是存储的对应长度LIS的最小末尾。有了这个末尾，我们就可以一个一个地插入数据。虽然最后一个d[9] = 7更新进去对于这组数据没有什么意义，但是如果后面再出现两个数字 8 和 9，那么就可以把8更新到d[5], 9更新到d[6]，得出LIS的长度为6。 然后应该发现一件事情了：在B中插入数据是有序的，而且是进行替换而不需要挪动——也就是说，我们可以使用二分查找，将每一个数字的插入时间优化到O(logN)~于是算法的时间复杂度就降低到了O(NlogN)～！ 12345678910111213141516171819202122232425262728293031323334class Solution &#123; public static int lengthOfLIS(int[] nums) &#123; int max = 0; if(nums == null || nums.length == 0) return max; int[] arr = new int[nums.length]; int index; arr[0] = nums[0]; int len = 1; for(int i = 1;i &lt; nums.length;i++) &#123; if(arr[len - 1] &lt; nums[i]) arr[len++] = nums[i]; else&#123; index = insertLocation(arr, len - 1,nums[i]); arr[index] = nums[i]; &#125; &#125; return len; &#125; /** * 返回 target 在 arr 数组[0, end]中的插入位置 */ private static int insertLocation(int[] arr, int end, int target)&#123; int left = 0; int right = end; int mid; while(left &lt;= right)&#123; mid = left + ((right - left) &gt;&gt; 1); if(arr[mid] &gt; target) right = mid - 1; else if(arr[mid] &lt; target) left = mid + 1; else return mid; &#125; return left; &#125;&#125; 参考：最长递增子序列]]></content>
      <categories>
        <category>Leetcode Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[子矩阵的最大累加和]]></title>
    <url>%2F2019%2F08%2F26%2Fsubarray-max-sum%2F</url>
    <content type="text"><![CDATA[题目描述给定一个矩阵matrix，其中的值有正负0，返回子矩阵的最大累加和。 思路一维数组的子数组最大和好求，详见：leetcode-53:Maximum Subarray 二维数组子数组最大和，可以将二维数组进行降维或者压缩，使得二维数组变成一维数组，然后按照一维数组的方式进行求最大值。 如下是一个二维数组： 123456789[[1 , 2 , 0], //第一行[2 , 3 , -1], //第二行[4 , -5 , 3] //第三行] 首先单独每一行都可以求得子数组最大值； 然后可以将第一行和第二行对应列分别加起来，形成一个新的行： [3 , 5, -1],对于这一行又可以求得子数组最大值。类似的，还可以把第二行和第三行进行降维，可以对第一行和第二行和第三行进行降维，求子数组最大和。 在计算的过程中不断更新最大值，即为所求。 Solution123456789101112131415161718192021222324//子矩阵的最大累加和问题public static int maxSum(int[][] m)&#123; if(m == null || m.length == 0 || m[0].length == 0)&#123;//判断矩阵非空 return 0; &#125; int rows = m.length; int cols = m[0].length; int[] s = new int[cols];//累加数组 int cur = 0; int max = Integer.MIN_VALUE; for(int i = 0;i != m.length;i++)&#123;//从第i行元素开始，往下查找所有子矩阵 for(int p = 0;p &lt; cols;p++) s[p] = 0; //累加数组清0 for(int j = i;j != m.length;j++)&#123; cur = 0; for(int k = 0;k != cols;k++)&#123; s[k] += m[j][k];//每一步的累加数组（叠加每一列） cur += s[k]; max = Math.max(cur, max);//每一步的最大子矩阵的累加和 cur = cur &gt; 0 ? cur : 0; &#125; &#125; &#125; return max;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-547:Friend Circles(并查集的应用)]]></title>
    <url>%2F2019%2F08%2F25%2Fleetcode-547%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/friend-circles/ 题目描述题目难度：Medium There are N students in a class. Some of them are friends, while some are not. Their friendship is transitive in nature. For example, if A is a direct friend of B, and B is a direct friend of C, then A is an indirect friend of C. And we defined a friend circle is a group of students who are direct or indirect friends. Given a N*N matrix M representing the friend relationship between students in the class. If M[i][j] = 1, then the ith and jthstudents are direct friends with each other, otherwise not. And you have to output the total number of friend circles among all the students. Example 1: 123456Input: [[1,1,0], [1,1,0], [0,0,1]]Output: 2Explanation:The 0th and 1st students are direct friends, so they are in a friend circle. The 2nd student himself is in a friend circle. So return 2. Example 2: 123456Input: [[1,1,0], [1,1,1], [0,1,1]]Output: 1Explanation:The 0th and 1st students are direct friends, the 1st and 2nd students are direct friends, so the 0th and 2nd students are indirect friends. All of them are in the same friend circle, so return 1. Note: N is in range [1,200]. M[i][i] = 1 for all students. If M[i][j] = 1, then M[j][i] = 1. Solution1234567891011121314151617181920212223class Solution &#123; public int findCircleNum(int[][] M) &#123; int res = 0; if(M == null || M.length == 0) return res; int N = M.length; int[] bcj = new int[N]; for(int i = 0;i &lt; N;i++) bcj[i] = i; for(int i = 0;i &lt; N;i++)&#123; for(int j = 0;j &lt; i;j++)&#123; if(M[i][j] == 1)&#123; int x = i; int y = j; while(x != bcj[x]) x = bcj[x]; while(y != bcj[y]) y = bcj[y]; bcj[x] = y; &#125; &#125; &#125; for(int i = 0;i &lt; N;i++) if(bcj[i] == i) res++; return res; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer:给定二叉树的前序和中序，重建二叉树]]></title>
    <url>%2F2019%2F08%2F25%2Fpreorder-inorder-build-binary-tree%2F</url>
    <content type="text"><![CDATA[题目描述输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 Solution12345678910111213141516171819202122232425262728293031/** * Definition for binary tree * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */public class Solution &#123; public TreeNode reConstructBinaryTree(int[] pre, int[] in) &#123; if (pre == null || in == null || pre.length != in.length) return null; java.util.HashMap&lt;Integer, Integer&gt; map = new java.util.HashMap&lt;&gt;(); for (int i = 0; i &lt; in.length; i++) map.put(in[i], i); return preIn(pre, 0, pre.length - 1, in, 0, in.length - 1, map); &#125; //找子树中的根节点 public TreeNode preIn(int[] p, int pi, int pj, int[] n, int ni, int nj, java.util.HashMap&lt;Integer, Integer&gt; map) &#123; //在给定范围找根节点，如果范围不存在，则返回null if (pi &gt; pj) return null; //前序遍历给定范围中第一个就是根节点 TreeNode head = new TreeNode(p[pi]); int index = map.get(p[pi]); //找出根节点在中序遍历中的位置，该位置往左是根节点的左子树范围，该位置往右是右子树的范围 //索引位置往左找出根节点的左孩子 head.left = preIn(p, pi + 1, pi + index - ni, n, ni, index - 1, map); //索引位置往右找出根节点的右孩子 head.right = preIn(p, pi + index - ni + 1, pj, n, index + 1, nj, map); return head; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-40:Combination Sum II]]></title>
    <url>%2F2019%2F08%2F23%2Fleetcode-40%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/combination-sum-ii/ 题目描述题目难度：Medium Given a collection of candidate numbers (candidates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target. Each number in candidates may only be used once in the combination. Note: All numbers (including target) will be positive integers. The solution set must not contain duplicate combinations. Example 1: 12345678Input: candidates = [10,1,2,7,6,1,5], target = 8,A solution set is:[ [1, 7], [1, 2, 5], [2, 6], [1, 1, 6]] Example 2: 123456Input: candidates = [2,5,2,1,2], target = 5,A solution set is:[ [1,2,2], [5]] Solution12345678910111213141516171819202122class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; combinationSum2(int[] nums, int target) &#123; List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;(); Arrays.sort(nums); backtrack(list, new ArrayList&lt;&gt;(), nums, target, 0); return list; &#125;private void backtrack(List&lt;List&lt;Integer&gt;&gt; list, List&lt;Integer&gt; tempList, int [] nums, int remain, int start)&#123; if(remain &lt; 0) return; else if(remain == 0) list.add(new ArrayList&lt;&gt;(tempList)); else&#123; for(int i = start; i &lt; nums.length; i++)&#123; if(i &gt; start &amp;&amp; nums[i] == nums[i-1]) continue; tempList.add(nums[i]); backtrack(list, tempList, nums, remain - nums[i], i + 1); tempList.remove(tempList.size() - 1); &#125; &#125;&#125; &#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-39:Combination Sum]]></title>
    <url>%2F2019%2F08%2F23%2Fleetcode-39%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/combination-sum/ 题目描述题目难度：Medium Given a set of candidate numbers (candidates) (without duplicates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target. The same repeated number may be chosen from candidates unlimited number of times. Note: All numbers (including target) will be positive integers. The solution set must not contain duplicate combinations. Example 1: 123456Input: candidates = [2,3,6,7], target = 7,A solution set is:[ [7], [2,2,3]] Example 2: 1234567Input: candidates = [2,3,5], target = 8,A solution set is:[ [2,2,2,2], [2,3,3], [3,5]] Solution123456789101112131415161718public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] nums, int target) &#123; List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;(); Arrays.sort(nums); backtrack(list, new ArrayList&lt;&gt;(), nums, target, 0); return list;&#125;private void backtrack(List&lt;List&lt;Integer&gt;&gt; list, List&lt;Integer&gt; tempList, int [] nums, int remain, int start)&#123; if(remain &lt; 0) return; else if(remain == 0) list.add(new ArrayList&lt;&gt;(tempList)); else&#123; for(int i = start; i &lt; nums.length; i++)&#123; tempList.add(nums[i]); backtrack(list, tempList, nums, remain - nums[i], i); tempList.remove(tempList.size() - 1); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-90:Subsets II]]></title>
    <url>%2F2019%2F08%2F23%2Fleetcode-90%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/subsets-ii/ 题目描述题目难度：Medium Given a collection of integers that might contain duplicates, nums, return all possible subsets (the power set). Note: The solution set must not contain duplicate subsets. Example: 12345678910Input: [1,2,2]Output:[ [2], [1], [1,2,2], [2,2], [1,2], []] Solution12345678910111213141516171819class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;(); Arrays.sort(nums); backtrack(list, new ArrayList&lt;&gt;(), nums, 0); return list; &#125; private void backtrack(List&lt;List&lt;Integer&gt;&gt; list , List&lt;Integer&gt; tempList, int [] nums, int start)&#123; list.add(new ArrayList&lt;&gt;(tempList)); for(int i = start; i &lt; nums.length; i++)&#123; if(i != start &amp;&amp; nums[i] == nums[i - 1]) continue; tempList.add(nums[i]); backtrack(list, tempList, nums, i + 1); tempList.remove(tempList.size() - 1); &#125;&#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-78:Subsets]]></title>
    <url>%2F2019%2F08%2F23%2Fleetcode-78%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/subsets/ 题目描述题目难度：Medium Given a set of distinct integers, nums, return all possible subsets (the power set). Note: The solution set must not contain duplicate subsets. Example: 123456789101112Input: nums = [1,2,3]Output:[ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []] Solution11234567891011121314151617class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; resList = new ArrayList&lt;&gt;(); if(nums == null &amp;&amp; nums.length == 0) return resList; resList.add(new ArrayList&lt;Integer&gt;()); for(int i = 0;i &lt; nums.length;i++)&#123; int size = resList.size(); for(int j = 0;j &lt; size;j++)&#123; ArrayList list = new ArrayList(resList.get(j)); list.add(nums[i]); resList.add(list); &#125; &#125; return resList; &#125;&#125; Solution21234567891011121314151617class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;(); //Arrays.sort(nums); backtrack(list, new ArrayList&lt;&gt;(), nums, 0); return list;&#125;private void backtrack(List&lt;List&lt;Integer&gt;&gt; list , List&lt;Integer&gt; tempList, int [] nums, int start)&#123; list.add(new ArrayList&lt;&gt;(tempList)); for(int i = start; i &lt; nums.length; i++)&#123; tempList.add(nums[i]); backtrack(list, tempList, nums, i + 1); tempList.remove(tempList.size() - 1); &#125;&#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美团点评-2020校招(2019.8.22)]]></title>
    <url>%2F2019%2F08%2F23%2Fmeituan-exam1%2F</url>
    <content type="text"><![CDATA[最小唯一前缀输入输入n串唯一的字符串，n为2到100之间，字符串用”,”隔开，字符串长度不超过100. 输出可以唯一区分字符串的最小前缀，用”,”隔开 样例输入1meituanapp,meituanwaimai,dianpingliren,dianpingjiehun 样例输入1meituana,meituanw,dianpingl,dianpingj Solution123456789101112131415161718192021222324252627282930313233package com.csk.meituan;import java.util.Scanner;public class Main &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); while (scanner.hasNext())&#123; String str = scanner.nextLine(); String[] strs = str.split(","); int len = strs.length; int[] lens = new int[len]; for(int i = 0;i &lt; len - 1;i++)&#123; for(int j = i + 1;j &lt; len;j++)&#123; int q = lens[i]; int p = lens[j]; while(strs[i].charAt(q) == strs[j].charAt(p))&#123; q++; p++; &#125; lens[i] = q; lens[j] = p; &#125; &#125; for(int i = 0;i &lt; len;i++)&#123; if(i == len - 1) System.out.print(strs[i].substring(0, lens[i] + 1)); else System.out.print(strs[i].substring(0, lens[i] + 1) + ","); &#125; System.out.println(); &#125; &#125;&#125; 美团骑手包裹区间分组题目描述2110年美团外卖火星第3000号配送站点有26名骑手，分别以大写字母A-Z命名，因此可以称呼这些骑手为皇家骑手特工A，皇家骑手特工B，，，皇家骑手特工Z，某美团黑珍珠餐厅的外卖流水线上会顺序产出一组包裹，美团配送调度引擎已经将包裹配送到骑手，并在包裹上粘贴好骑手名称，如RETTEBTAE代表代表一组流水线包裹共9个，同时分配给了名字A B E R T的5名骑手。请在不打乱流水线产出顺序的情况下，把这组包裹划分为尽可能多的片段，同一个骑手只会出现在其中一个片段，返回一个表示每个包裹片段的长度的列表。 输入输入数据只有一行，为一个字符串（不包含引号），长度不超过1000，只包含大写字母（`A`-\Z`）,字符之间无空格。 输出输出每个分割成片段的包裹组的长度，每个长度之间通过空格隔开。 样例输入1MPMPCPMCMDEFEGDEHINHKLIN 样例输出129 7 8` Solution123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.csk.meituan;import java.util.ArrayList;import java.util.HashMap;import java.util.Scanner;/** * Created by changsk on 2019/8/23 *///MPMPCPMCMDEFEGDEHINHKLINpublic class Main &#123; static int start = 0; static int end = 0; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); while (scanner.hasNext())&#123; String str = scanner.nextLine(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); HashMap&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); int i = 0; for(;i &lt; str.length();i++) map.put(str.charAt(i), i); i = 0; int temp_start = 0; while(i &lt; str.length())&#123; end = map.get(str.charAt(i)); temp_start = start; boolean res = helper(str, map, start, end); while(!res)&#123; res = helper(str, map, start, end); &#125; list.add(end - temp_start + 1); i = end + 1; start = i; &#125; for(int j : list) System.out.println(j); &#125; &#125; private static boolean helper(String str, HashMap&lt;Character, Integer&gt; map, int start, int _end)&#123; boolean res = true; int max = map.get(str.charAt(_end)); for(int i = start + 1;i &lt; _end;i++)&#123; if(map.get(str.charAt(i)) &gt; max)&#123; res = false; start = i; end = map.get(str.charAt(i)); break; &#125; &#125; return res; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>美团笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch原理]]></title>
    <url>%2F2019%2F08%2F22%2Fes-principle%2F</url>
    <content type="text"><![CDATA[终于有人把Elasticsearch原理讲透了！]]></content>
      <categories>
        <category>搜索</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-47:Permutations II(去重的全排列)]]></title>
    <url>%2F2019%2F08%2F19%2Fleetcode-47%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/permutations-ii/ 题目描述题目难度：Medium Given a collection of numbers that might contain duplicates, return all possible unique permutations. Example: 1234567Input: [1,1,2]Output:[ [1,1,2], [1,2,1], [2,1,1]] Solution1234567891011121314151617181920212223class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; resList = new ArrayList(); if(nums == null || nums.length == 0) return resList; Arrays.sort(nums); //这一步是必须的 permuteUniqueCore(nums, new ArrayList&lt;Integer&gt;(), resList); return resList; &#125; private void permuteUniqueCore(int[] nums, List&lt;Integer&gt; tmpList, List&lt;List&lt;Integer&gt;&gt; resList)&#123; if(tmpList.size() == nums.length) &#123; resList.add(new ArrayList(tmpList)); return; &#125; for(int i = 0; i &lt; nums.length;i++)&#123; if(tmpList.contains(nums[i])) continue; if(i != 0 &amp;&amp; nums[i] == nums[i - 1]) continue; tmpList.add(nums[i]); permuteUniqueCore(nums, tmpList, resList); tmpList.remove(tmpList.size() - 1); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-46:Permutations(全排列)]]></title>
    <url>%2F2019%2F08%2F19%2Fleetcode-46%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/permutations/ 题目描述题目难度：Medium Given a collection of distinct integers, return all possible permutations. Example: 12345678910Input: [1,2,3]Output:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] Solution1 以下解释和代码来自于 LeetCode 12345678910111213141516171819public List&lt;List&lt;Integer&gt;&gt; permute(int[] num) &#123; List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;List&lt;Integer&gt;&gt;(); if (num.length == 0) return ans; List&lt;Integer&gt; l0 = new ArrayList&lt;Integer&gt;(); l0.add(num[0]); //首先加入第一个数字 ans.add(l0); for (int i = 1; i&lt; num.length; ++i)&#123; //依次加入后续每个数字 List&lt;List&lt;Integer&gt;&gt; new_ans = new ArrayList&lt;List&lt;Integer&gt;&gt;(); //创建新list 存储加入新数字后的结果 for (int j = 0; j&lt;=i; ++j)&#123; //加入的新数字可能存储的位置 for (List&lt;Integer&gt; l : ans)&#123; List&lt;Integer&gt; new_l = new ArrayList&lt;Integer&gt;(l); new_l.add(j,num[i]); new_ans.add(new_l); &#125; &#125; ans = new_ans; &#125; return ans;&#125; Solution2DFS 12345678910111213141516171819public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;(); // Arrays.sort(nums); // not necessary backtrack(list, new ArrayList&lt;&gt;(), nums); return list;&#125; private void backtrack(List&lt;List&lt;Integer&gt;&gt; list, List&lt;Integer&gt; tempList, int [] nums)&#123; if(tempList.size() == nums.length)&#123; list.add(new ArrayList&lt;&gt;(tempList)); //注意这里的操作，是加入一个新的list，内容和 tempList 一样 &#125; else&#123; for(int i = 0; i &lt; nums.length; i++)&#123; if(tempList.contains(nums[i])) continue; // element already exists, skip tempList.add(nums[i]); backtrack(list, tempList, nums); tempList.remove(tempList.size() - 1); &#125; &#125;&#125; 自己写的代码 ，和上面的原理差不多，只不过判断重复用的是一个 boolean 数组，backtrack里面参数 len 也是没有必要的，显得很啰嗦。 12345678910111213141516171819202122232425class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; resList = new ArrayList&lt;&gt;(); if(nums == null || nums.length == 0) return resList; boolean[] seen = new boolean[nums.length]; backtrack(resList, new ArrayList&lt;Integer&gt;(), nums, seen, 0); return resList; &#125; private void backtrack(List&lt;List&lt;Integer&gt;&gt; resList, ArrayList&lt;Integer&gt; tmpList, int[] nums, boolean[] seen, int len)&#123; if(len == nums.length) &#123; resList.add(new ArrayList(tmpList)); return; &#125; for(int i = 0;i &lt; nums.length;i++)&#123; if(seen[i] == true) continue; tmpList.add(nums[i]); seen[i] = true; backtrack(resList, tmpList, nums, seen, len + 1); tmpList.remove(tmpList.size() - 1); seen[i] = false; &#125;&#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判断IP地址有效性的正则表达式]]></title>
    <url>%2F2019%2F08%2F19%2Fip-regex%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021public class Test &#123; public static String matches(String text) &#123; String res = null; if (text != null &amp;&amp; !text.isEmpty()) &#123; String regex = "^(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|[1-9])\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)$"; if (text.matches(regex)) &#123; res = text + "\n是一个合法的IP地址！"; &#125; else &#123; res = text + "\n不是一个合法的IP地址！"; &#125; &#125; return res == null ? "输入ip无效" : res; &#125; public static void main(String[] args) &#123; String ip = "127.0.0.255"; System.out.println(matches(ip)); &#125;&#125; ^:行的开头 $:行的结尾 \d:表示0到9的任意数字 X{2}:X出现两次 [0-4]:出现0,1,2,3,4都算匹配上 参考: JAVA验证IP地址的有效性]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-198:House Robber(打家劫舍)]]></title>
    <url>%2F2019%2F08%2F18%2Fleetcode-198%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/house-robber/ 题目描述题目难度：Easy You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security system connected and it will automatically contact the police if two adjacent houses were broken into on the same night. Given a list of non-negative integers representing the amount of money of each house, determine the maximum amount of money you can rob tonight without alerting the police. Example 1: 1234Input: [1,2,3,1]Output: 4Explanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4. Example 2: 1234Input: [2,7,9,3,1]Output: 12Explanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1). Total amount you can rob = 2 + 9 + 1 = 12. Solution11234567891011class Solution &#123; public int rob(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; if(nums.length == 1) return nums[0]; for(int i = 2;i &lt; nums.length;i++) if(i &gt;= 3) nums[i] += Math.max(nums[i - 2], nums[i - 3]); else nums[i] += nums[i - 2]; return Math.max(nums[nums.length - 1], nums[nums.length - 2]); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-1038:Binary Search Tree to Greater Sum Tree]]></title>
    <url>%2F2019%2F08%2F18%2Fleetcode-1038%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/binary-search-tree-to-greater-sum-tree/ 题目描述Given the root of a binary search tree with distinct values, modify it so that every node has a new value equal to the sum of the values of the original tree that are greater than or equal to node.val. As a reminder, a binary search tree is a tree that satisfies these constraints: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Example 1: 12Input: [4,1,6,0,2,5,7,null,null,null,3,null,null,null,8]Output: [30,36,21,36,35,26,15,null,null,null,33,null,null,null,8] Note: The number of nodes in the tree is between 1 and 100. Each node will have value between 0 and 100. The given tree is a binary search tree. Solution1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; int cur = 0; public TreeNode bstToGst(TreeNode root) &#123; if(root == null) return root; helper(root); return root; &#125; private void helper(TreeNode root)&#123; if(root.right != null) helper(root.right); cur += root.val; root.val = cur; if(root.left != null) helper(root.left); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[问题 1004-[递归]母牛的故事]]></title>
    <url>%2F2019%2F08%2F15%2Fstories-of-cows%2F</url>
    <content type="text"><![CDATA[题目链接：https://www.dotcpp.com/oj/problem1004.html?sid=1076912&amp;lang=1#editor 题目描述有一头母牛，它每年年初生一头小母牛。每头小母牛从第四个年头开始，每年年初也生一头小母牛。请编程实现在第n年的时候，共有多少头母牛？ 输入输入数据由多个测试实例组成，每个测试实例占一行，包括一个整数n(0&lt;n&lt;55)，n的含义如题目中描述。n=0表示输入数据的结束，不做处理。 输出对于每个测试实例，输出在第n年的时候母牛的数量。每个输出占一行。 样例输入12342450 样例输出123246 Solution12345678910111213141516171819202122232425#include&lt;iostream&gt;using namespace std;int main()&#123;int n;int fn;while(cin&gt;&gt;n&amp;&amp;n!=0)&#123; int f1=1,f2=2,f3=3,f4=4;if(n==1) cout&lt;&lt;f1&lt;&lt;endl;else if(n==2) cout&lt;&lt;f2&lt;&lt;endl;else if(n==3) cout&lt;&lt;f3&lt;&lt;endl;else if(n==4) cout&lt;&lt;f4&lt;&lt;endl;else&#123;for(int i=5;i&lt;=n;i++)&#123;fn=f2+f4;f2=f3;f3=f4;f4=fn;&#125;cout&lt;&lt;fn&lt;&lt;endl;;&#125;&#125;return 0;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-674:Longest Continuous Increasing Subsequence(最长连续递增子串)]]></title>
    <url>%2F2019%2F08%2F15%2Fleetcode-674%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/longest-continuous-increasing-subsequence/ 题目描述题目难度：Easy Given an unsorted array of integers, find the length of longest continuous increasing subsequence (subarray). Example 1: 1234Input: [1,3,5,4,7]Output: 3Explanation: The longest continuous increasing subsequence is [1,3,5], its length is 3. Even though [1,3,5,7] is also an increasing subsequence, it&apos;s not a continuous one where 5 and 7 are separated by 4. Example 2: 123Input: [2,2,2,2,2]Output: 1Explanation: The longest continuous increasing subsequence is [2], its length is 1. Note: Length of the array will not exceed 10,000. Solution12345678910111213class Solution &#123; public int findLengthOfLCIS(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; int res = 1; int max = 1; for(int i = 1;i &lt; nums.length;i++)&#123; if(nums[i] &gt; nums[i - 1]) max++; else max = 1; res = Math.max(max, res); &#125; return res; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-和为S的连续正数序列]]></title>
    <url>%2F2019%2F08%2F05%2Fjianzhioffer-continuous-positive-sequence%2F</url>
    <content type="text"><![CDATA[题目描述小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck! Solution123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;public class Solution &#123; ArrayList&lt;ArrayList&lt;Integer&gt; &gt; resList = new ArrayList&lt;&gt;(); public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindContinuousSequence(int sum) &#123; int start = 1; int end = 2; int curSum = start + end; if(sum &lt; 3) return resList; int max = (sum + 1) &gt;&gt; 1; while(start &lt; end &amp;&amp; end &lt;= max)&#123; if(curSum == sum) &#123; addList(start, end, new ArrayList&lt;Integer&gt;(end - start + 1)); curSum -= start; start++; &#125; else if(curSum &lt; sum) &#123; end++; curSum += end; &#125; else&#123; curSum -= start; start++; &#125; &#125; return resList; &#125; private void addList(int start, int end, ArrayList&lt;Integer&gt; list)&#123; if(start &gt; end) return; for(int i = start;i &lt;= end;i++)&#123; list.add(i); &#125; resList.add(list); &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-76:Minimum Window Substring]]></title>
    <url>%2F2019%2F08%2F05%2Fleetcode-76%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/minimum-window-substring/ 题目描述Given a string S and a string T, find the minimum window in S which will contain all the characters in T in complexity O(n). Example: 12Input: S = &quot;ADOBECODEBANC&quot;, T = &quot;ABC&quot;Output: &quot;BANC&quot; Note: If there is no such window in S that covers all characters in T, return the empty string &quot;&quot;. If there is such window, you are guaranteed that there will always be only one unique minimum window in S. Solution代码来自Leetcode 12345678910111213141516171819202122232425262728293031class Solution &#123; public String minWindow(String s, String t) &#123; int sLen = s.length(), tLen = t.length(), count = t.length(), start = 0, minLen = Integer.MAX_VALUE; int[] freq = new int[128]; char[] sc = s.toCharArray(); char[] tc = t.toCharArray(); for(char c: tc) &#123; freq[c]++; &#125; int l=0,r=0; while(r&lt;sLen) &#123; if(freq[sc[r++]]-- &gt; 0) &#123; count--; &#125; while(count == 0) &#123; if(minLen &gt; r-l) &#123; minLen = r-l; start = l; &#125; if(freq[sc[l++]]++ == 0) count++; &#125; &#125; return minLen == Integer.MAX_VALUE ? "" : s.substring(start,start+minLen); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于java的生产者和消费者三种实现方式]]></title>
    <url>%2F2019%2F08%2F04%2Fjava-producer-consumer%2F</url>
    <content type="text"><![CDATA[本文转载自：基于java的生产者和消费者三种实现方式 概述生产者和消费者问题是一个经典的线程同步问题。生产者（Producer）生产产品（Product），放入仓库（Repertory）；消费者（Consumer）消费产品，从仓库里获取。仓库爆满时生产者等待消费者消费，仓库为空时消费者等待生产者生产。 实现synchronized生产者Producer： 1234567891011121314public class Producer implements Runnable &#123; private static final String TAG = "Producer"; private Repertory mRepertory; Producer(Repertory repertory) &#123; mRepertory = repertory; &#125; @Override public void run() &#123; Object product = new Object(); mRepertory.produce(product); &#125;&#125; 消费者Consumer： 12345678910111213public class Consumer implements Runnable &#123; private static final String TAG = "Consumer"; private Repertory mRepertory; Consumer(Repertory repertory) &#123; mRepertory = repertory; &#125; @Override public void run() &#123; mRepertory.consume(); &#125;&#125; 仓库Repertory1： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Repertory1 implements Repertory &#123; private static final String TAG = "Repertory1"; private static final int MAX_SIZE = 100; private LinkedList&lt;Object&gt; list = new LinkedList&lt;&gt;(); @Override public synchronized void produce(Object product) &#123; while (isFull()) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; list.add(product); Log.e(TAG, Thread.currentThread().getName() + "生产了: " + product + "，还剩余：" + size()); notifyAll(); &#125; @Override public synchronized void consume() &#123; while (isEmpty()) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; Object product = list.remove(); Log.e(TAG, Thread.currentThread().getName() + "消费了: " + product + "，还剩余：" + size()); notifyAll(); &#125; public boolean isEmpty() &#123; return list.isEmpty(); &#125; public boolean isFull() &#123; return size() == MAX_SIZE; &#125; public int size() &#123; return list.size(); &#125;&#125; Lock/Condition仓库Repertory2： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Repertory2 implements Repertory &#123; private static final String TAG = "Repertory2"; private static final int MAX_SIZE = 100; private LinkedList&lt;Object&gt; list = new LinkedList&lt;&gt;(); private final Lock mLock = new ReentrantLock(); private final Condition mEmptyCondition = mLock.newCondition(); private final Condition mFullCondition = mLock.newCondition(); @Override public void produce(Object product) &#123; mLock.lock(); try &#123; while (isFull()) &#123; try &#123; mFullCondition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; list.add(product); Log.e(TAG, Thread.currentThread().getName() + "生产了: " + product + "，还剩余：" + size()); mEmptyCondition.signalAll(); &#125; finally &#123; mLock.unlock(); &#125; &#125; @Override public void consume() &#123; mLock.lock(); try &#123; while (isEmpty()) &#123; try &#123; mEmptyCondition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; Object product = list.remove(); Log.e(TAG, Thread.currentThread().getName() + "消费了: " + product + "，还剩余：" + size()); mFullCondition.signalAll(); &#125; finally &#123; mLock.unlock(); &#125; &#125; public boolean isEmpty() &#123; return list.isEmpty(); &#125; public boolean isFull() &#123; return size() == MAX_SIZE; &#125; public int size() &#123; return list.size(); &#125; &#125; BlockingDeque仓库Reprotory3： 12345678910111213141516171819202122232425262728293031323334353637383940public class Repertory3 implements Repertory &#123; private static final String TAG = "Repertory3"; private static final int MAX_SIZE = 100; private BlockingDeque&lt;Object&gt; list = new LinkedBlockingDeque&lt;&gt;(MAX_SIZE); @Override public void produce(Object product) &#123; try &#123; list.put(product); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Log.e(TAG, Thread.currentThread().getName() + "生产了: " + product); &#125; @Override public void consume() &#123; Object product = null; try &#123; product = list.take(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Log.e(TAG, Thread.currentThread().getName() + "消费了: " + product); &#125; public boolean isEmpty() &#123; return list.isEmpty(); &#125; public boolean isFull() &#123; return size() == MAX_SIZE; &#125; public int size() &#123; return list.size(); &#125; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JAVA字符串格式化-String.format()的使用]]></title>
    <url>%2F2019%2F08%2F03%2Fjava-string-format%2F</url>
    <content type="text"><![CDATA[本文转载自：JAVA字符串格式化-String.format()的使用 常规类型的格式化String类的format()方法用于创建格式化的字符串以及连接多个字符串对象。熟悉C语言的同学应该记得C语言的sprintf()方法，两者有类似之处。format()方法有两种重载形式。 format(String format, Object… args) 新字符串使用本地语言环境，制定字符串格式和参数生成格式化的新字符串。 format(Locale locale, String format, Object… args) 使用指定的语言环境，制定字符串格式和参数生成格式化的字符串。 显示不同转换符实现不同数据类型到字符串的转换，如图所示 转 换 符 说 明 示 例 %s 字符串类型 “mingrisoft” %c 字符类型 ‘m’ %b 布尔类型 true %d 整数类型（十进制） 99 %x 整数类型（十六进制） FF %o 整数类型（八进制） 77 %f 浮点类型 99.99 %a 十六进制浮点类型 FF.35AE %e 指数类型 9.38e+5 %g 通用浮点类型（f和e类型中较短的） %h 散列码 %% 百分比类型 ％ %n 换行符 %tx 日期与时间类型（x代表不同的日期与时间转换符 123456789101112131415161718public static void main(String[] args) &#123; String str=null; str=String.format("Hi,%s", "王力"); System.out.println(str); str=String.format("Hi,%s:%s.%s", "王南","王力","王张"); System.out.println(str); System.out.printf("字母a的大写是：%c %n", 'A'); System.out.printf("3&gt;7的结果是：%b %n", 3&gt;7); System.out.printf("100的一半是：%d %n", 100/2); System.out.printf("100的16进制数是：%x %n", 100); System.out.printf("100的8进制数是：%o %n", 100); System.out.printf("50元的书打8.5折扣是：%f 元%n", 50*0.85); System.out.printf("上面价格的16进制数是：%a %n", 50*0.85); System.out.printf("上面价格的指数表示：%e %n", 50*0.85); System.out.printf("上面价格的指数和浮点数结果的长度较短的是：%g %n", 50*0.85); System.out.printf("上面的折扣是%d%% %n", 85); System.out.printf("字母A的散列码是：%h %n", 'A'); &#125; 输出结果： 12345678910111213Hi,王力 Hi,王南:王力.王张 字母a的大写是：A 3&gt;7的结果是：false 100的一半是：50 100的16进制数是：64 100的8进制数是：144 50元的书打8.5折扣是：42.500000 元 上面价格的16进制数是：0x1.54p5 上面价格的指数表示：4.250000e+01 上面价格的指数和浮点数结果的长度较短的是：42.5000 上面的折扣是85% 字母A的散列码是：41 搭配转换符的标志如图所示： 标 志 说 明 示 例 结 果 + 为正数或者负数添加符号 (“%+d”,15) +15 − 左对齐 (“%-5d”,15) |15 | 0 数字前面补0 (“%04d”, 99) 0099 空格 在整数之前添加指定数量的空格 (“% 4d”, 99) | 99| , 以“,”对数字分组 (“%,f”, 9999.99) 9,999.990000 ( 使用括号包含负数 (“%(f”, -99.99) (99.990000) # 如果是浮点数则包含小数点，如果是16进制或8进制则添加0x或0 (“%#x”, 99)(“%#o”, 99) 0x630143 &lt; 格式化前一个转换符所描述的参数 (“%f和%&lt;3.2f”, 99.45) 99.450000和99.45 $ 被格式化的参数索引 (“%1$d,%2$s”, 99,”abc”) 99,abc 12345678910111213141516public static void main(String[] args) &#123; String str=null; //$使用 str=String.format("格式参数$的使用：%1$d,%2$s", 99,"abc"); System.out.println(str); //+使用 System.out.printf("显示正负数的符号：%+d与%d%n", 99,-99); //补O使用 System.out.printf("最牛的编号是：%03d%n", 7); //空格使用 System.out.printf("Tab键的效果是：% 8d%n", 7); //.使用 System.out.printf("整数分组的效果是：%,d%n", 9989997); //空格和小数点后面个数 System.out.printf("一本书的价格是：% 50.5f元%n", 49.8); &#125; 输出结果 123456格式参数$的使用：99,abc 显示正负数的符号：+99与-99 最牛的编号是：007 Tab键的效果是： 7 整数分组的效果是：9,989,997 一本书的价格是： 49.80000元 日期和事件字符串格式化在程序界面中经常需要显示时间和日期，但是其显示的 格式经常不尽人意，需要编写大量的代码经过各种算法才得到理想的日期与时间格式。字符串格式中还有%tx转换符没有详细介绍，它是专门用来格式化日期和时 间的。%tx转换符中的x代表另外的处理日期和时间格式的转换符，它们的组合能够将日期和时间格式化成多种格式。 常见日期和时间组合的格式，如图所示。 转 换 符 说 明 示 例 c 包括全部日期和时间信息 星期六 十月 27 14:21:20 CST 2007 F “年-月-日”格式 2007-10-27 D “月/日/年”格式 10/27/07 r “HH:MM:SS PM”格式（12时制） 02:25:51 下午 T “HH:MM:SS”格式（24时制） 14:28:16 R “HH:MM”格式（24时制） 14:28 123456789101112131415public static void main(String[] args) &#123; Date date=new Date(); //c的使用 System.out.printf("全部日期和时间信息：%tc%n",date); //f的使用 System.out.printf("年-月-日格式：%tF%n",date); //d的使用 System.out.printf("月/日/年格式：%tD%n",date); //r的使用 System.out.printf("HH:MM:SS PM格式（12时制）：%tr%n",date); //t的使用 System.out.printf("HH:MM:SS格式（24时制）：%tT%n",date); //R的使用 System.out.printf("HH:MM格式（24时制）：%tR",date); &#125; 输出结果： 123456全部日期和时间信息：星期一 九月 10 10:43:36 CST 2012 年-月-日格式：2012-09-10 月/日/年格式：09/10/12 HH:MM:SS PM格式（12时制）：10:43:36 上午 HH:MM:SS格式（24时制）：10:43:36 HH:MM格式（24时制）：10:43 定义日期格式的转换符可以使日期通过指定的转换符生成新字符串。这些日期转换符如图所示。 123456789101112131415161718192021222324252627public static void main(String[] args) &#123; Date date=new Date(); //b的使用，月份简称 String str=String.format(Locale.US,"英文月份简称：%tb",date); System.out.println(str); System.out.printf("本地月份简称：%tb%n",date); //B的使用，月份全称 str=String.format(Locale.US,"英文月份全称：%tB",date); System.out.println(str); System.out.printf("本地月份全称：%tB%n",date); //a的使用，星期简称 str=String.format(Locale.US,"英文星期的简称：%ta",date); System.out.println(str); //A的使用，星期全称 System.out.printf("本地星期的简称：%tA%n",date); //C的使用，年前两位 System.out.printf("年的前两位数字（不足两位前面补0）：%tC%n",date); //y的使用，年后两位 System.out.printf("年的后两位数字（不足两位前面补0）：%ty%n",date); //j的使用，一年的天数 System.out.printf("一年中的天数（即年的第几天）：%tj%n",date); //m的使用，月份 System.out.printf("两位数字的月份（不足两位前面补0）：%tm%n",date); //d的使用，日（二位，不够补零） System.out.printf("两位数字的日（不足两位前面补0）：%td%n",date); //e的使用，日（一位不补零） System.out.printf("月份的日（前面不补0）：%te",date); 输出结果 123456789101112英文月份简称：Sep 本地月份简称：九月 英文月份全称：September 本地月份全称：九月 英文星期的简称：Mon 本地星期的简称：星期一 年的前两位数字（不足两位前面补0）：20 年的后两位数字（不足两位前面补0）：12 一年中的天数（即年的第几天）：254 两位数字的月份（不足两位前面补0）：09 两位数字的日（不足两位前面补0）：10 月份的日（前面不补0）：10 和日期格式转换符相比，时间格式的转换符要更多、更精确。它可以将时间格式化成时、分、秒甚至时毫秒等单位。格式化时间字符串的转换符如图所示。 转 换 符 说 明 示 例 H 2位数字24时制的小时（不足2位前面补0） 15 I 2位数字12时制的小时（不足2位前面补0） 03 k 2位数字24时制的小时（前面不补0） 15 l 2位数字12时制的小时（前面不补0） 3 M 2位数字的分钟（不足2位前面补0） 03 S 2位数字的秒（不足2位前面补0） 09 L 3位数字的毫秒（不足3位前面补0） 015 N 9位数字的毫秒数（不足9位前面补0） 562000000 p 小写字母的上午或下午标记 中：下午英：pm z 相对于GMT的RFC822时区的偏移量 +0800 Z 时区缩写字符串 CST s 1970-1-1 00:00:00 到现在所经过的秒数 1193468128 Q 1970-1-1 00:00:00 到现在所经过的毫秒数 1193468128984 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; Date date = new Date(); //H的使用 System.out.printf("2位数字24时制的小时（不足2位前面补0）:%tH%n", date); //I的使用 System.out.printf("2位数字12时制的小时（不足2位前面补0）:%tI%n", date); //k的使用 System.out.printf("2位数字24时制的小时（前面不补0）:%tk%n", date); //l的使用 System.out.printf("2位数字12时制的小时（前面不补0）:%tl%n", date); //M的使用 System.out.printf("2位数字的分钟（不足2位前面补0）:%tM%n", date); //S的使用 System.out.printf("2位数字的秒（不足2位前面补0）:%tS%n", date); //L的使用 System.out.printf("3位数字的毫秒（不足3位前面补0）:%tL%n", date); //N的使用 System.out.printf("9位数字的毫秒数（不足9位前面补0）:%tN%n", date); //p的使用 String str = String.format(Locale.US, "小写字母的上午或下午标记(英)：%tp", date); System.out.println(str); System.out.printf("小写字母的上午或下午标记（中）：%tp%n", date); //z的使用 System.out.printf("相对于GMT的RFC822时区的偏移量:%tz%n", date); //Z的使用 System.out.printf("时区缩写字符串:%tZ%n", date); //s的使用 System.out.printf("1970-1-1 00:00:00 到现在所经过的秒数：%ts%n", date); //Q的使用 System.out.printf("1970-1-1 00:00:00 到现在所经过的毫秒数：%tQ%n", date); &#125; 输出结果 12345678910111213142位数字24时制的小时（不足2位前面补0）:11 2位数字12时制的小时（不足2位前面补0）:11 2位数字24时制的小时（前面不补0）:11 2位数字12时制的小时（前面不补0）:11 2位数字的分钟（不足2位前面补0）:03 2位数字的秒（不足2位前面补0）:52 3位数字的毫秒（不足3位前面补0）:773 9位数字的毫秒数（不足9位前面补0）:773000000 小写字母的上午或下午标记(英)：am 小写字母的上午或下午标记（中）：上午 相对于GMT的RFC822时区的偏移量:+0800 时区缩写字符串:CST 1970-1-1 00:00:00 到现在所经过的秒数：1347246232 1970-1-1 00:00:00 到现在所经过的毫秒数：1347246232773]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SQL中where子句中不能出现聚合函数的原因]]></title>
    <url>%2F2019%2F08%2F01%2Fsql-where-no-agg%2F</url>
    <content type="text"><![CDATA[本文转载自：SQL中where子句中不能出现聚合函数的原因 首先我们应该熟悉什么聚合函数： 例如SUM(),MIN(),Max()这类的，我们称作是聚合函数。 那么我们不能在where子句中使用这些函数，为什么呢？ 聚集函数也叫列函数，它们都是基于整列数据进行计算的，而where子句则是对数据行进行过滤的，在筛选过程中依赖“基于已经筛选完毕的数据得出的计算结果”是一种悖论，这是行不通的。更简单地说，因为聚集函数要对全列数据进行计算，因而使用它的前提是：结果集已经确定！ 而where子句还处于“确定”结果集的过程中，因而不能使用聚集函数。 与where子句不能出现聚集函数正相反的是，我们几乎看不到不使用聚集函数的having子句（在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与聚合函数一起使用）。为什么？因为在水平方向上根据外部指定条件的筛选（也就是对行的筛选），where子句可以独立完成，剩下的往往都是需要根据结果集自身的统计数据进一步筛选了，这时，几乎都需要通过having子句配合聚集函数来完成。 按照下面这个就是错误的，会报一个错误：Group function is not allowed here 12345select department_id,avg(salary) from employees where avg(salary)&gt;6000 group by department_id --having avg(salary)&gt;6000 原因。 sql语句的执行顺序为 from子句 where 子句 group by 子句 having 子句 order by 子句 select 子句 首先得知道聚合函数是对结果集运算的，当在where子句使用聚合函数时，此时根据group by 分割结果集的子句还没有执行，此时只有from 后的结果集。 所以无法在where子句中使用聚合函数。]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据库两大神器【索引和锁】]]></title>
    <url>%2F2019%2F07%2F27%2Fmysql-index-lock%2F</url>
    <content type="text"><![CDATA[本文转载自：数据库两大神器【索引和锁】 前言 只有光头才能变强 索引和锁在数据库中可以说是非常重要的知识点了，在面试中也会经常会被问到的。 本文力求简单讲清每个知识点，希望大家看完能有所收获 声明：如果没有说明具体的数据库和存储引擎，默认指的是MySQL中的InnoDB存储引擎 索引在之前，我对索引有以下的认知： 索引可以加快数据库的检索速度 表经常进行INSERT/UPDATE/DELETE操作就不要建立索引了，换言之：索引会降低插入、删除、修改等维护任务的速度。 索引需要占物理和数据空间。 了解过索引的最左匹配原则 知道索引的分类：聚集索引和非聚集索引 Mysql支持Hash索引和B+树索引两种 看起来好像啥都知道，但面试让你说的时候可能就GG了： 使用索引为什么可以加快数据库的检索速度啊？ 为什么说索引会降低插入、删除、修改等维护任务的速度。 索引的最左匹配原则指的是什么？ Hash索引和B+树索引有什么区别？主流的使用哪一个比较多？InnoDB存储都支持吗？ 聚集索引和非聚集索引有什么区别？ …….. 聊聊索引的基础知识首先Mysql的基本存储结构是页(记录都存在页里边)： 各个数据页可以组成一个双向链表 而 每个数据页中的记录 又可以组成一个 单向 链表 每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。 所以说，如果我们写select * from user where username = &#39;Java3y&#39;这样没有进行任何优化的sql语句，默认会这样做： 定位到记录所在的页 需要遍历双向链表，找到所在的页 从所在的页内中查找相应的记录 由于不是根据主键查询，只能遍历所在页的单链表了 很明显，在数据量很大的情况下这样查找会很慢！ 索引提高检索速度索引做了些什么可以让我们查询加快速度呢？ 其实就是将无序的数据变成有序(相对)： 要找到id为8的记录简要步骤： 很明显的是：没有用索引我们是需要遍历双向链表来定位对应的页，现在通过“目录”就可以很快地定位到对应的页上了！ 其实底层结构就是B+树，B+树作为树的一种实现，能够让我们很快地查找出对应的记录。 参考资料： Mysql索引 索引降低增删改的速度B+树是平衡树的一种。 平衡树：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 如果一棵普通的树在极端的情况下，是能退化成链表的(树的优点就不复存在了) B+树是平衡树的一种，是不会退化成链表的，树的高度都是相对比较低的(基本符合矮矮胖胖(均衡)的结构)【这样一来我们检索的时间复杂度就是O(logn)】！从上一节的图我们也可以看见，建立索引实际上就是建立一颗B+树。 B+树是一颗平衡树，如果我们对这颗树增删改的话，那肯定会破坏它的原有结构。 要维持平衡树，就必须做额外的工作。正因为这些额外的工作开销，导致索引会降低增删改的速度 B+树删除和修改具体可参考： www.cnblogs.com/wade-luffy/… 哈希索引除了B+树之外，还有一种常见的是哈希索引。 哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。 本质上就是把键值换算成新的哈希值，根据这个哈希值来定位。 看起来哈希索引很牛逼啊，但其实哈希索引有好几个局限(根据他本质的原理可得)： 哈希索引也没办法利用索引完成排序 不支持最左匹配原则 在有大量重复键值情况下，哈希索引的效率也是极低的—-&gt;哈希碰撞问题。 不支持范围查询 参考资料： www.cnblogs.com/zengkefu/p/…—hash索引和b+tree索引 InnoDB支持哈希索引吗？主流的还是使用B+树索引比较多，对于哈希索引，InnoDB是自适应哈希索引的（hash索引的创建由InnoDB存储引擎引擎自动优化创建，我们干预不了）！ 参考资料： blog.csdn.net/doctor_who2… 聚集和非聚集索引简单概括： 聚集索引就是以主键创建的索引 非聚集索引就是以非主键创建的索引 区别： 聚集索引在叶子节点存储的是表中的数据 非聚集索引在叶子节点存储的是主键和索引列 使用非聚集索引查询出数据时，拿到叶子上的主键再去查到想要查找的数据。(拿到主键再查找这个过程叫做回表) 非聚集索引也叫做二级索引，不用纠结那么多名词，将其等价就行了~ 非聚集索引在建立的时候也未必是单列的，可以多个列来创建索引。 此时就涉及到了哪个列会走索引，哪个列不走索引的问题了(最左匹配原则–&gt;后面有说) 创建多个单列(非聚集)索引的时候，会生成多个索引树(所以过多创建索引会占用磁盘空间) 在创建多列索引中也涉及到了一种特殊的索引–&gt;覆盖索引 我们前面知道了，如果不是聚集索引，叶子节点存储的是主键+列值 最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢 覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！ 比如说： 现在我创建了索引(username,age)，在查询数据的时候：select username , age from user where username = &#39;Java3y&#39; and age = 20。 很明显地知道，我们上边的查询是走索引的，并且，要查询出的列在叶子节点都存在！所以，就不用回表了~ 所以，能使用覆盖索引就尽量使用吧~ 索引最左匹配原则最左匹配原则： 索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。 如果是联合索引，那么key也由多个列组成，同时，索引只能用于查找key是否存在（相等），遇到范围查询(&gt;、&lt;、between、like左匹配)等就不能进一步匹配了，后续退化为线性查找。 因此，列的排列顺序决定了可命中索引的列数。 例子： 如有索引(a, b, c, d)，查询条件a = 1 and b = 2 and c &gt; 3 and d = 4，则会在每个节点依次命中a、b、c，无法命中d。(很简单：索引命中只能是相等的情况，不能是范围匹配) =、in自动优化顺序不需要考虑=、in等的顺序，mysql会自动优化这些条件的顺序，以匹配尽可能多的索引列。 例子： 如有索引(a, b, c, d)，查询条件c &gt; 3 and b = 2 and a = 1 and d &lt; 4与a = 1 and c &gt; 3 and b = 2 and d &lt; 4等顺序都是可以的，MySQL会自动优化为a = 1 and b = 2 and c &gt; 3 and d &lt; 4，依次命中a、b、c。 索引总结索引在数据库中是一个非常重要的知识点！上面谈的其实就是索引最基本的东西，要创建出好的索引要顾及到很多的方面： 1，最左前缀匹配原则。这是非常重要、非常重要、非常重要（重要的事情说三遍）的原则，MySQL会一直向右匹配直到遇到范围查询（&gt;,&lt;,BETWEEN,LIKE）就停止匹配。 3，尽量选择区分度高的列作为索引，区分度的公式是 COUNT(DISTINCT col) / COUNT(*)。表示字段不重复的比率，比率越大我们扫描的记录数就越少。 4，索引列不能参与计算，尽量保持列“干净”。比如，FROM_UNIXTIME(create_time) = &#39;2016-06-06&#39; 就不能使用索引，原因很简单，B+树中存储的都是数据表中的字段值，但是进行检索时，需要把所有元素都应用函数才能比较，显然这样的代价太大。所以语句要写成 ： create_time = UNIX_TIMESTAMP(&#39;2016-06-06&#39;)。 5，尽可能的扩展索引，不要新建立索引。比如表中已经有了a的索引，现在要加（a,b）的索引，那么只需要修改原来的索引即可。 6，单个多列组合索引和多个单列索引的检索查询效果不同，因为在执行SQL时，MySQL只能使用一个索引，会从多个单列索引中选择一个限制最为严格的索引。 参考资料： zhuanlan.zhihu.com/p/23624390–简单理解索引 blog.csdn.net/mysteryhaoh…– MySQL学习之——索引(普通索引、唯一索引、全文索引、索引匹配原则、索引命中等) monkeysayhi.github.io/2018/03/06/…—浅谈MySQL的B树索引与索引优化 锁 在mysql中的锁看起来是很复杂的，因为有一大堆的东西和名词：排它锁，共享锁，表锁，页锁，间隙锁，意向排它锁，意向共享锁，行锁，读锁，写锁，乐观锁，悲观锁，死锁。这些名词有的博客又直接写锁的英文的简写—&gt;X锁，S锁，IS锁，IX锁，MMVC… 锁的相关知识又跟存储引擎，索引，事务的隔离级别都是关联的…. 这就给初学数据库锁的人带来不少的麻烦123456## 为什么需要学习数据库锁知识不少人在开发的时候，应该**很少会注意到**这些锁的问题，也很少会给程序加锁(除了**库存**这些对数量准确性要求极高的情况下)一般也就听过常说的乐观锁和悲观锁，了解过基本的含义之后就没了 定心丸：即使我们不会这些锁知识，我们的程序在一般情况下还是可以跑得好好的。因为这些锁数据库隐式帮我们加了 对于UPDATE、DELETE、INSERT语句，InnoDB会自动给涉及数据集加排他锁（X) MyISAM在执行查询语句SELECT前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预 只会在某些特定的场景下才需要手动加锁，学习数据库锁知识就是为了: 能让我们在特定的场景下派得上用场 更好把控自己写的程序 在跟别人聊数据库技术的时候可以搭上几句话 构建自己的知识库体系！在面试的时候不虚 表锁简单介绍首先，从锁的粒度，我们可以分成两大类： 表锁 开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低 行锁 开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高 不同的存储引擎支持的锁粒度是不一样的： InnoDB行锁和表锁都支持！ MyISAM只支持表锁！ InnoDB只有通过索引条件检索数据才使用行级锁，否则，InnoDB将使用表锁 也就是说，InnoDB的行锁是基于索引的！ 表锁下又分为两种模式： 表读锁（Table Read Lock） 表写锁（Table Write Lock） 从下图可以清晰看到，在表读锁和表写锁的环境下： 读读不阻塞，读写阻塞，写写阻塞 ！ 读读不阻塞：当前用户在读数据，其他的用户也在读数据，不会加锁 读写阻塞：当前用户在读数据，其他的用户不能修改当前用户读的数据，会加锁！ 写写阻塞：当前用户在修改数据，其他的用户不能修改当前用户正在修改的数据，会加锁！ 从上面已经看到了：读锁和写锁是互斥的，读写操作是串行。 如果某个进程想要获取读锁，同时另外一个进程想要获取写锁。在mysql里边，写锁是优先于读锁的！ 写锁和读锁优先级的问题是可以通过参数调节的：max_write_lock_count和low-priority-updates 值得注意的是： The LOCAL modifier enables nonconflicting INSERT statements (concurrent inserts) by other sessions to execute while the lock is held. (See Section 8.11.3, “Concurrent Inserts”.) However, READ LOCAL cannot be used if you are going to manipulate the database using processes external to the server while you hold the lock. For InnoDB tables, READ LOCAL is the same as READ MyISAM可以支持查询和插入操作的并发进行。可以通过系统变量concurrent_insert来指定哪种模式，在MyISAM中它默认是：如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个进程读表的同时，另一个进程从表尾插入记录。 但是InnoDB存储引擎是不支持的！ 参考资料： dev.mysql.com/doc/refman/…–官方手册 ourmysql.com/archives/56…—几个参数说明 行锁细讲上边简单讲解了表锁的相关知识，我们使用Mysql一般是使用InnoDB存储引擎的。InnoDB和MyISAM有两个本质的区别： InnoDB支持行锁 InnoDB支持事务 从上面也说了：我们是很少手动加表锁的。表锁对我们程序员来说几乎是透明的，即使InnoDB不走索引，加的表锁也是自动的！ 我们应该更加关注行锁的内容，因为InnoDB一大特性就是支持行锁！ InnoDB实现了以下两种类型的行锁。 共享锁（S锁）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 也叫做读锁：读锁是共享的，多个客户可以同时读取同一个资源，但不允许其他客户修改。 排他锁（X锁)：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 也叫做写锁：写锁是排他的，写锁会阻塞其他的写锁和读锁。 看完上面的有没有发现，在一开始所说的：X锁，S锁，读锁，写锁，共享锁，排它锁其实总共就两个锁，只不过它们有多个名字罢了~ Intention locks do not block anything except full table requests (for example, LOCK TABLES … WRITE). The main purpose of intention locks is to show that someone is locking a row, or going to lock a row in the table. 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁： 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 意向锁也是数据库隐式帮我们做了，不需要程序员操心！ 参考资料： www.zhihu.com/question/51… dev.mysql.com/doc/refman/… MVCC和事务的隔离级别数据库事务有不同的隔离级别，不同的隔离级别对锁的使用是不同的，锁的应用最终导致不同事务的隔离级别 MVCC(Multi-Version Concurrency Control)多版本并发控制，可以简单地认为：MVCC就是行级锁的一个变种(升级版)。 事务的隔离级别就是通过锁的机制来实现，只不过隐藏了加锁细节 在表锁中我们读写是阻塞的，基于提升并发性能的考虑，MVCC一般读写是不阻塞的(所以说MVCC很多情况下避免了加锁的操作) MVCC实现的读写不阻塞正如其名：多版本并发控制—&gt;通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot)，并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度来看，好像是数据库可以提供同一数据的多个版本。 快照有两个级别： 语句级 针对于Read committed隔离级别 事务级别 针对于Repeatable read隔离级别 我们在初学的时候已经知道，事务的隔离级别有4种： Read uncommitted 会出现脏读，不可重复读，幻读 Read committed 会出现不可重复读，幻读 Repeatable read 会出现幻读(但在Mysql实现的Repeatable read配合gap锁不会出现幻读！) Serializable 串行，避免以上的情况！ Read uncommitted会出现的现象—&gt;脏读：一个事务读取到另外一个事务未提交的数据 例子：A向B转账，A执行了转账语句，但A还没有提交事务，B读取数据，发现自己账户钱变多了！B跟A说，我已经收到钱了。A回滚事务【rollback】，等B再查看账户的钱时，发现钱并没有多。 出现脏读的本质就是因为操作(修改)完该数据就立马释放掉锁，导致读的数据就变成了无用的或者是错误的数据。 Read committed避免脏读的做法其实很简单： 就是把释放锁的位置调整到事务提交之后，此时在事务提交前，其他进程是无法对该行数据进行读取的，包括任何操作 但Read committed出现的现象—&gt;不可重复读：一个事务读取到另外一个事务已经提交的数据，也就是说一个事务可以看到其他事务所做的修改 注：A查询数据库得到数据，B去修改数据库的数据，导致A多次查询数据库的结果都不一样【危害：A每次查询的结果都是受B的影响的，那么A查询出来的信息就没有意思了】 上面也说了，Read committed是语句级别的快照！每次读取的都是当前最新的版本！ Repeatable read避免不可重复读是事务级别的快照！每次读取的都是当前事务的版本，即使被修改了，也只会读取当前事务版本的数据。 呃…如果还是不太清楚，我们来看看InnoDB的MVCC是怎么样的吧(摘抄《高性能MySQL》) 至于虚读(幻读)：是指在一个事务内读取到了别的事务插入的数据，导致前后读取不一致。 注：和不可重复读类似，但虚读(幻读)会读到其他事务的插入的数据，导致前后读取不一致 MySQL的Repeatable read隔离级别加上GAP间隙锁已经处理了幻读了。 参考资料： www.jianshu.com/p/cb97f76a9… www.zhihu.com/question/26… 扩展阅读： www.zhihu.com/question/67… 乐观锁和悲观锁无论是Read committed还是Repeatable read隔离级别，都是为了解决读写冲突的问题。 单纯在Repeatable read隔离级别下我们来考虑一个问题： 此时，用户李四的操作就丢失掉了： 丢失更新：一个事务的更新覆盖了其它事务的更新结果。 (ps:暂时没有想到比较好的例子来说明更新丢失的问题，虽然上面的例子也是更新丢失，但一定程度上是可接受的..不知道有没有人能想到不可接受的更新丢失例子呢…) 解决的方法： 使用Serializable隔离级别，事务是串行执行的！ 乐观锁 悲观锁 乐观锁是一种思想，具体实现是，表中有一个版本字段，第一次读的时候，获取到这个字段。处理完业务逻辑开始更新的时候，需要再次查看该字段的值是否和第一次的一样。如果一样更新，反之拒绝。之所以叫乐观，因为这个模式没有从数据库加锁，等到更新的时候再判断是否可以更新。 悲观锁是数据库层面加锁，都会阻塞去等待锁。 悲观锁所以，按照上面的例子。我们使用悲观锁的话其实很简单(手动加行锁就行了)： select * from xxxx for update 在select 语句后边加了 for update相当于加了排它锁(写锁)，加了写锁以后，其他的事务就不能对它修改了！需要等待当前事务修改完之后才可以修改. 也就是说，如果张三使用select ... for update，李四就无法对该条记录修改了~ 乐观锁乐观锁不是数据库层面上的锁，是需要自己手动去加的锁。一般我们添加一个版本字段来实现： 具体过程是这样的： 张三select * from table —&gt;会查询出记录出来，同时会有一个version字段 李四select * from table —&gt;会查询出记录出来，同时会有一个version字段 李四对这条记录做修改：update A set Name=lisi,version=version+1 where ID=#{id} and version=#{version}，判断之前查询到的version与现在的数据的version进行比较，同时会更新version字段 此时数据库记录如下： 张三也对这条记录修改：update A set Name=lisi,version=version+1 where ID=#{id} and version=#{version}，但失败了！因为当前数据库中的版本跟查询出来的版本不一致！ 参考资料： zhuanlan.zhihu.com/p/31537871—什么是悲观锁和乐观锁 www.zhihu.com/question/27…—乐观锁和 MVCC 的区别？ 间隙锁GAP当我们用范围条件检索数据而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合范围条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”。InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。 值得注意的是：间隙锁只会在Repeatable read隔离级别下使用~ 例子：假如emp表中只有101条记录，其empid的值分别是1,2,…,100,101 123Select * from emp where empid &gt; 100 for update;复制代码 上面是一个范围查询，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。 InnoDB使用间隙锁的目的有两个： 为了防止幻读(上面也说了，Repeatable read隔离级别下再通过GAP锁即可避免了幻读) 满足恢复和复制的需要 MySQL的恢复机制要求：在一个事务未提交前，其他并发事务不能插入满足其锁定条件的任何记录，也就是不允许出现幻读 死锁并发的问题就少不了死锁，在MySQL中同样会存在死锁的问题。 但一般来说MySQL通过回滚帮我们解决了不少死锁的问题了，但死锁是无法完全避免的，可以通过以下的经验参考，来尽可能少遇到死锁： 1）以固定的顺序访问表和行。比如对两个job批量更新的情形，简单方法是对id列表先排序，后执行，这样就避免了交叉等待锁的情形；将两个事务的sql顺序调整为一致，也能避免死锁。 2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。 4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。 5）为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。 参考资料： hedengcheng.com/?p=771#_Toc… www.cnblogs.com/LBSer/p/518… 锁总结上面说了一大堆关于MySQL数据库锁的东西，现在来简单总结一下。 表锁其实我们程序员是很少关心它的： 在MyISAM存储引擎中，当执行SQL语句的时候是自动加的。 在InnoDB存储引擎中，如果没有使用索引，表锁也是自动加的。 现在我们大多数使用MySQL都是使用InnoDB，InnoDB支持行锁： 共享锁–读锁–S锁 排它锁–写锁–X锁 在默认的情况下，select是不加任何行锁的~事务可以通过以下语句显示给记录集加共享锁或排他锁。 共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE。 排他锁（X)：SELECT * FROM table_name WHERE ... FOR UPDATE。 InnoDB基于行锁还实现了MVCC多版本并发控制，MVCC在隔离级别下的Read committed和Repeatable read下工作。MVCC能够实现读写不阻塞！ InnoDB实现的Repeatable read隔离级别配合GAP间隙锁已经避免了幻读！ 乐观锁其实是一种思想，正如其名：认为不会锁定的情况下去更新数据，如果发现不对劲，才不更新(回滚)。在数据库中往往添加一个version字段来实现。 悲观锁用的就是数据库的行锁，认为数据库会发生并发冲突，直接上来就把数据锁住，其他事务不能修改，直至提交了当前事务 参考资料： zhuanlan.zhihu.com/p/29150809–Mysql锁总结 blog.csdn.net/mysteryhaoh…–MySQL学习之——锁(行锁、表锁、页锁、乐观锁、悲观锁等) segmentfault.com/a/119000001…–MySQL InnoDB引擎锁的总结 总结本文主要介绍了数据库中的两个比较重要的知识点：索引和锁。他俩可以说息息相关的，锁会涉及到很多关于索引的知识~ 我个人比较重视对整体知识点的把控，一些细节的地方可能就没有去编写了。在每一个知识点下都会有很多的内容，有兴趣的同学可以在我给出的链接中继续阅读学习。当然了，如果有比较好的文章和资料也不妨在评论区分享一下哈~ 我只是在学习的过程中，把自己遇到的问题写出来，整理出来，希望可以对大家有帮助。如果文章有错的地方，希望大家可以在评论区指正，一起学习交流~ 参考资料： 《高性能MySQL 第三版》 作者：Java3y 链接：https://juejin.im/post/5b55b842f265da0f9e589e79 来源：掘金著作权归作者所有。 商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-188:Best Time to Buy and Sell Stock IV]]></title>
    <url>%2F2019%2F07%2F26%2Fleetcode-188%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iv/ 题目描述 题目难度：Hard Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete at most k transactions. Note:You may not engage in multiple transactions at the same time (ie, you must sell the stock before you buy again). Example 1: 123Input: [2,4,1], k = 2Output: 2Explanation: Buy on day 1 (price = 2) and sell on day 2 (price = 4), profit = 4-2 = 2. Example 2: 1234Input: [3,2,6,5,0,3], k = 2Output: 7Explanation: Buy on day 2 (price = 2) and sell on day 3 (price = 6), profit = 6-2 = 4. Then buy on day 5 (price = 0) and sell on day 6 (price = 3), profit = 3-0 = 3. Solutiont[i][j]表示prices中从0到j的股票当中最多交易i次的最大利润 1234567891011121314151617181920212223242526class Solution &#123; public int maxProfit(int k, int[] prices) &#123; int len = prices.length; if (k &gt;= len / 2) return quickSolve(prices); int[][] t = new int[k + 1][len]; for (int i = 1; i &lt;= k; i++) &#123; int tmpMax = -prices[0]; for (int j = 1; j &lt; len; j++) &#123; t[i][j] = Math.max(t[i][j - 1], prices[j] + tmpMax); tmpMax = Math.max(tmpMax, t[i - 1][j - 1] - prices[j]); &#125; &#125; return t[k][len - 1]; &#125; //最大的交易次数为len / 2,所以如果 k 大于等于 len / 2,题目退化为不限交易次数的最大利润 private int quickSolve(int[] prices) &#123; int len = prices.length, profit = 0; for (int i = 1; i &lt; len; i++) // as long as there is a price gap, we gain a profit. if (prices[i] &gt; prices[i - 1]) profit += prices[i] - prices[i - 1]; return profit; &#125;&#125; 代码来自LeetCode]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA 注解的基本原理]]></title>
    <url>%2F2019%2F07%2F26%2Fjava-annotation%2F</url>
    <content type="text"><![CDATA[本文转载自：JAVA 注解的基本原理 以前，『XML』是各大框架的青睐者，它以松耦合的方式完成了框架中几乎所有的配置，但是随着项目越来越庞大，『XML』的内容也越来越复杂，维护成本变高。 于是就有人提出来一种标记式高耦合的配置方式，『注解』。方法上可以进行注解，类上也可以注解，字段属性上也可以注解，反正几乎需要配置的地方都可以进行注解。 关于『注解』和『XML』两种不同的配置模式，争论了好多年了，各有各的优劣，注解可以提供更大的便捷性，易于维护修改，但耦合度高，而 XML 相对于注解则是相反的。 追求低耦合就要抛弃高效率，追求效率必然会遇到耦合。本文意不再辨析两者谁优谁劣，而在于以最简单的语言介绍注解相关的基本内容。 注解的本质「java.lang.annotation.Annotation」接口中有这么一句话，用来描述『注解』。 The common interface extended by all annotation types 所有的注解类型都继承自这个普通的接口（Annotation） 这句话有点抽象，但却说出了注解的本质。我们看一个 JDK 内置注解的定义： 12345@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override &#123;&#125; 这是注解 @Override 的定义，其实它本质上就是： 123public interface Override extends Annotation&#123; &#125; 没错，注解的本质就是一个继承了 Annotation 接口的接口。有关这一点，你可以去反编译任意一个注解类，你会得到结果的。 一个注解准确意义上来说，只不过是一种特殊的注释而已，如果没有解析它的代码，它可能连注释都不如。 而解析一个类或者方法的注解往往有两种形式，一种是编译期直接的扫描，一种是运行期反射。反射的事情我们待会说，而编译器的扫描指的是编译器在对 java 代码编译字节码的过程中会检测到某个类或者方法被一些注解修饰，这时它就会对于这些注解进行某些处理。 典型的就是注解 @Override，一旦编译器检测到某个方法被修饰了 @Override 注解，编译器就会检查当前方法的方法签名是否真正重写了父类的某个方法，也就是比较父类中是否具有一个同样的方法签名。 这一种情况只适用于那些编译器已经熟知的注解类，比如 JDK 内置的几个注解，而你自定义的注解，编译器是不知道你这个注解的作用的，当然也不知道该如何处理，往往只是会根据该注解的作用范围来选择是否编译进字节码文件，仅此而已。 元注解『元注解』是用于修饰注解的注解，通常用在注解的定义上，例如： 12345@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override &#123;&#125; 这是我们 @Override 注解的定义，你可以看到其中的 @Target，@Retention 两个注解就是我们所谓的『元注解』，『元注解』一般用于指定某个注解生命周期以及作用目标等信息。 JAVA 中有以下几个『元注解』： @Target：注解的作用目标 @Retention：注解的生命周期 @Documented：注解是否应当被包含在 JavaDoc 文档中 @Inherited：是否允许子类继承该注解 其中，@Target 用于指明被修饰的注解最终可以作用的目标是谁，也就是指明，你的注解到底是用来修饰方法的？修饰类的？还是用来修饰字段属性的。 @Target 的定义如下： 我们可以通过以下的方式来为这个 value 传值： 1@Target(value = &#123;ElementType.FIELD&#125;) 被这个 @Target 注解修饰的注解将只能作用在成员字段上，不能用于修饰方法或者类。其中，ElementType 是一个枚举类型，有以下一些值： ElementType.TYPE：允许被修饰的注解作用在类、接口和枚举上 ElementType.FIELD：允许作用在属性字段上 ElementType.METHOD：允许作用在方法上 ElementType.PARAMETER：允许作用在方法参数上 ElementType.CONSTRUCTOR：允许作用在构造器上 ElementType.LOCAL_VARIABLE：允许作用在本地局部变量上 ElementType.ANNOTATION_TYPE：允许作用在注解上 ElementType.PACKAGE：允许作用在包上 @Retention 用于指明当前注解的生命周期，它的基本定义如下： 同样的，它也有一个 value 属性： 1@Retention(value = RetentionPolicy.RUNTIME 这里的 RetentionPolicy 依然是一个枚举类型，它有以下几个枚举值可取： RetentionPolicy.SOURCE：当前注解编译期可见，不会写入 class 文件 RetentionPolicy.CLASS：类加载阶段丢弃，会写入 class 文件 RetentionPolicy.RUNTIME：永久保存，可以反射获取 @Retention 注解指定了被修饰的注解的生命周期，一种是只能在编译期可见，编译后会被丢弃，一种会被编译器编译进 class 文件中，无论是类或是方法，乃至字段，他们都是有属性表的，而 JAVA 虚拟机也定义了几种注解属性表用于存储注解信息，但是这种可见性不能带到方法区，类加载时会予以丢弃，最后一种则是永久存在的可见性。 剩下两种类型的注解我们日常用的不多，也比较简单，这里不再详细的进行介绍了，你只需要知道他们各自的作用即可。@Documented 注解修饰的注解，当我们执行 JavaDoc 文档打包时会被保存进 doc 文档，反之将在打包时丢弃。@Inherited 注解修饰的注解是具有可继承性的，也就说我们的注解修饰了一个类，而该类的子类将自动继承父类的该注解。 JAVA 的内置三大注解除了上述四种元注解外，JDK 还为我们预定义了另外三种注解，它们是： @Override @Deprecated @SuppressWarnings @Override 注解想必是大家很熟悉的了，它的定义如下： 1234@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override &#123;&#125; 它没有任何的属性，所以并不能存储任何其他信息。它只能作用于方法之上，编译结束后将被丢弃。 所以你看，它就是一种典型的『标记式注解』，仅被编译器可知，编译器在对 java 文件进行编译成字节码的过程中，一旦检测到某个方法上被修饰了该注解，就会去匹对父类中是否具有一个同样方法签名的函数，如果不是，自然不能通过编译。 @Deprecated 的基本定义如下： 依然是一种『标记式注解』，永久存在，可以修饰所有的类型，作用是，标记当前的类或者方法或者字段等已经不再被推荐使用了，可能下一次的 JDK 版本就会删除。 当然，编译器并不会强制要求你做什么，只是告诉你 JDK 已经不再推荐使用当前的方法或者类了，建议你使用某个替代者。 @SuppressWarnings 主要用来压制 java 的警告，它的基本定义如下： 它有一个 value 属性需要你主动的传值，这个 value 代表一个什么意思呢，这个 value 代表的就是需要被压制的警告类型。例如： 123public static void main(String[] args) &#123; Date date = new Date(2018, 7, 11);&#125; 这么一段代码，程序启动时编译器会报一个警告。 Warning:(8, 21) java: java.util.Date 中的 Date(int,int,int) 已过时 而如果我们不希望程序启动时，编译器检查代码中过时的方法，就可以使用 @SuppressWarnings 注解并给它的 value 属性传入一个参数值来压制编译器的检查。 1234@SuppressWarning(value = "deprecated")public static void main(String[] args) &#123; Date date = new Date(2018, 7, 11);&#125; 这样你就会发现，编译器不再检查 main 方法下是否有过时的方法调用，也就压制了编译器对于这种警告的检查。 当然，JAVA 中还有很多的警告类型，他们都会对应一个字符串，通过设置 value 属性的值即可压制对于这一类警告类型的检查。 自定义注解的相关内容就不再赘述了，比较简单，通过类似以下的语法即可自定义一个注解。 123public @interface InnotationName&#123; &#125; 当然，自定义注解的时候也可以选择性的使用元注解进行修饰，这样你可以更加具体的指定你的注解的生命周期、作用范围等信息。 注解与反射上述内容我们介绍了注解使用上的细节，也简单提到，「注解的本质就是一个继承了 Annotation 接口的接口」，现在我们就来从虚拟机的层面看看，注解的本质到底是什么。 首先，我们自定义一个注解类型： 这里我们指定了 Hello 这个注解只能修饰字段和方法，并且该注解永久存活，以便我们反射获取。 之前我们说过，虚拟机规范定义了一系列和注解相关的属性表，也就是说，无论是字段、方法或是类本身，如果被注解修饰了，就可以被写进字节码文件。属性表有以下几种： RuntimeVisibleAnnotations：运行时可见的注解 RuntimeInVisibleAnnotations：运行时不可见的注解 RuntimeVisibleParameterAnnotations：运行时可见的方法参数注解 RuntimeInVisibleParameterAnnotations：运行时不可见的方法参数注解 AnnotationDefault：注解类元素的默认值 给大家看虚拟机的这几个注解相关的属性表的目的在于，让大家从整体上构建一个基本的印象，注解在字节码文件中是如何存储的。 所以，对于一个类或者接口来说，Class 类中提供了以下一些方法用于反射注解。 getAnnotation：返回指定的注解 isAnnotationPresent：判定当前元素是否被指定注解修饰 getAnnotations：返回所有的注解 getDeclaredAnnotation：返回本元素的指定注解 getDeclaredAnnotations：返回本元素的所有注解，不包含父类继承而来的 方法、字段中相关反射注解的方法基本是类似的，这里不再赘述，我们下面看一个完整的例子。 首先，设置一个虚拟机启动参数，用于捕获 JDK 动态代理类。 -Dsun.misc.ProxyGenerator.saveGeneratedFiles=true 然后 main 函数。 我们说过，注解本质上是继承了 Annotation 接口的接口，而当你通过反射，也就是我们这里的 getAnnotation 方法去获取一个注解类实例的时候，其实 JDK 是通过动态代理机制生成一个实现我们注解（接口）的代理类。 我们运行程序后，会看到输出目录里有这么一个代理类，反编译之后是这样的： 代理类实现接口 Hello 并重写其所有方法，包括 value 方法以及接口 Hello 从 Annotation 接口继承而来的方法。 而这个关键的 InvocationHandler 实例是谁？ AnnotationInvocationHandler 是 JAVA 中专门用于处理注解的 Handler， 这个类的设计也非常有意思。 这里有一个 memberValues，它是一个 Map 键值对，键是我们注解属性名称，值就是该属性当初被赋上的值。 而这个 invoke 方法就很有意思了，大家注意看，我们的代理类代理了 Hello 接口中所有的方法，所以对于代理类中任何方法的调用都会被转到这里来。 var2 指向被调用的方法实例，而这里首先用变量 var4 获取该方法的简明名称，接着 switch 结构判断当前的调用方法是谁，如果是 Annotation 中的四大方法，将 var7 赋上特定的值。 如果当前调用的方法是 toString，equals，hashCode，annotationType 的话，AnnotationInvocationHandler 实例中已经预定义好了这些方法的实现，直接调用即可。 那么假如 var7 没有匹配上这四种方法，说明当前的方法调用的是自定义注解字节声明的方法，例如我们 Hello 注解的 value 方法。这种情况下，将从我们的注解 map 中获取这个注解属性对应的值。 其实，JAVA 中的注解设计个人觉得有点反人类，明明是属性的操作，非要用方法来实现。当然，如果你有不同的见解，欢迎留言探讨。 最后我们再总结一下整个反射注解的工作原理： 首先，我们通过键值对的形式可以为注解属性赋值，像这样：@Hello（value = “hello”）。 接着，你用注解修饰某个元素，编译器将在编译期扫描每个类或者方法上的注解，会做一个基本的检查，你的这个注解是否允许作用在当前位置，最后会将注解信息写入元素的属性表。 然后，当你进行反射的时候，虚拟机将所有生命周期在 RUNTIME 的注解取出来放到一个 map 中，并创建一个 AnnotationInvocationHandler 实例，把这个 map 传递给它。 最后，虚拟机将采用 JDK 动态代理机制生成一个目标注解的代理类，并初始化好处理器。 那么这样，一个注解的实例就创建出来了，它本质上就是一个代理类，你应当去理解好 AnnotationInvocationHandler 中 invoke 方法的实现逻辑，这是核心。一句话概括就是，通过方法名返回注解属性值。 作者：YangAM 链接：https://juejin.im/post/5b45bd715188251b3a1db54f 来源：掘金著作权归作者所有。 商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统之内存管理]]></title>
    <url>%2F2019%2F07%2F25%2Fos-memory-management%2F</url>
    <content type="text"><![CDATA[本文转载自：计算机系统012 - 操作系统之内存管理 内存管理所谓内存管理，其最基本的操作就是由CPU把程序装入到内存中执行。远古批处理时代，进程独占各种计算机资源依次执行，因此只要小于可用内存总大小（主要是除去操作系统本身所占内存）的进程均可加载运行。随着操作系统的发展，提高CPU利用率和降低平均响应时长成了两大主流方向。前面的文章中反复提到提高CPU利用率的最有效方法就是加载多个进程到内存中，通过在一个进程执行I/O操作期间切换至另一进程，避开阻塞等待。也就是说，同一时刻加载入内存空间的进程数越多，可供选择的进程就越多，就越有机会避开I/O操作等待时间，那么，剩下的问题就是，该如何增加加载入内存空间的进程数。 可加载到内存空间的进程总数受到两方面影响： 物理内存总量：硬件上内存空间总量总是有限的，如早期计算机只有256MB内存 单个进程占用内存空间数：为了完成更复杂的功能、呈现更好的视觉效果等，程序日益复杂，运行所需内存空间与日俱增 简而言之，内存与进程间的态势是供不应求。针对问题找方法，可选的解决方法有如下两种： 增大物理内存总量 增大物理内存总量，需考虑硬件技术以及经济能力两方面。硬件技术发展总有其时代局限性，即使到今天，民用计算机中单条内存16GB的容量已经是很高端的了，而这一级别的内存条目前均价为600RMB左右。以台式机为例，插满4根组成64GB需要成本约2400RMB，价格不菲。而且这只是内存而已，毕竟计算机整体性能还和其他如CPU、硬盘、显卡等各大烧钱部件息息相关。因此不管是从硬件技术还是经济效益来考虑，增大物理内存总量这条路潜力很有限。 降低单个进程运行所需内存空间 能用软件实现的功能，只要对效率上要求不十分严格，实现起来的灵活性和性价比要比单纯扩展硬件要高得多，对于内存的使用也同样如此。要想降低进程所需内存空间，只有一种方法，那就是只加载进程中当前可能执行到的部分。 局部性原理从物理学上讲，局部性原理认为一个特定物体只会受到其直接相关环境影响。而对应到计算机系统中，硬件只会与其有直接电气线路相连的其他硬件交互，软件程序中，每一个指令也只会与其附近一段区域内代码相关联。讲句人话就是，硬件交互需要电气接触，软件执行通常只会牵扯到附近代码，毕竟编码过程中本身就会将问题进行模块分解。对比例证，阅读理解文学作品时对于单句的理解需结合上下文，商品使用说明书同样是分模块说明的。 局部性原理为前一小节提出的“只加载进程中当前可能执行到的部分”的解决方案提供了理论支撑。虽然进程中包括了很多代码和数据，但执行过程中CPU是以顺序执行读取到的指令的，假如可以保证每条指令只和附近一些指令相关，那就可以只加载一定量的指令就可以实现进程内部的顺序执行，对于数据部分也是同样的道理。实际应用中，如现在的游戏程序，动辄十几GB、甚至几十GB，其实内部大多是动画CG。如果执行程序的前提是必须全部加载，那么很少有硬件能够成功运行该游戏程序；但基于局部性原理，运行时只加载当前章节或场景必需的代码数据即可，只有当需要切换到其他章节、场景时，再动态加载对应代码数据。 重定位虽然根据局部性原理，使得同时可以加载入内存的进程总数有所提升，但内存总量始终是不能改变的，而进程运行过程中也随时可能增减所需内存。要想保持进程总数甚至加载更多进程，在有增的同时就势必有减。然而增减终归要有源头，而这个源头就是辅助存储设备，也称为二级存储器，典型设备为磁盘Disk。 当内存中没有富余内存空间时，就必须将一部分进程使用到的内存空间减少。这些空间可能包括进程存储原始数据的空间，也有可能包括运行时动态信息的空间，为了再当重新切换回这些进程时可以正确执行，如同进程切换一样，需要将所减少的空间进行存储，而存储的位置只有容量更大的磁盘空间。从内存保存到磁盘、或是从磁盘重新加载回内存的操作就叫做交换。 内存管理中，只保证提供内存空间，并不保证每次提供的内存空间在物理上位置都一样。例如，进程被交换到磁盘后再次加载，所加载到的位置几乎不会与被交换出内存时所处的位置一样。这也是为了保证各进程所需的不同内存空间总数同时，尽可能减小外部碎片（可视为进程间连接处不能使用的空间）。既然位置发生了变化，但进程还要继续执行，那就只能通过重定位技术来Cover掉这个差异。 单纯讲重定位会过于抽象，要理解这个概念，首先要对几种地址类型进行区分： 物理地址/绝对地址，是数据在内存中的实际位置，通常物理地址是一组连续线性递增的地址空间 逻辑地址，指与当前数据在内存中的物理分配地址无关的访问地址，在执行对内存的访问之前必须先把它转换成物理地址 相对地址，相对于某已知点（通常是程序的开始处）的存储单元 由于不同操作系统对于进程的具体定义各有差异，内核所使用的调度策略也有所差别，因此进程所能分配到的地址以及地址空间位置的关系也均有所不同。这样一来，进程运行过程中，自然不能通过物理地址来查找指令、数据等信息，为了避免硬件地址对其影响，进程中使用逻辑地址作为分布依据。要想实现这项功能，操作系统中就必须维护一份逻辑地址与物理地址间的映射表，从而使得操作系统可以将进程运行时的逻辑地址转换为物理地址，进行存取。 典型进程执行过程综合前文所述，如在进程执行中选择只加载（或是尽可能少）必需代码、数据，那么典型进程执行过程如下： 操作系统读取包含程序开始处的一些字节 由于进程刚执行，只有小部分在内存中，因此会因为无法读取到后续指令、数据而触发大量中断 CPU收到中断后移交控制权给操作系统 操作系统确定数据在磁盘上的位置 读取数据至内存中 返回至原进程，继续执行 这样一段时间后，通过预测等CPU技术，将进程可能执行的指令数据提前加载到内存中，中断将很少触发，整体执行过程趋于稳定，直到进程执行结束。 交换单元前一节中整体介绍了内存管理中增加内存中可加载进程总数的有效解决方法，但对于交换的具体细节并没有做过多说明，这里就从交换单元的角度来看看交换过程相关问题。 通过硬件部分的知识我们了解到，物理内存是线性地址空间，而大多数程序是以模块形式组织，因此从逻辑上来讲，以模块形式组织会更有利于组织代码和数据。通常操作系统占据了内存中的某些固定部分，内存的其余部分可供多个用户进程使用，称为用户空间。而根据用户空间从逻辑地址到物理地址的转换单元粒度大小，可分为如下三类。 分区同样是远古时期，主要是使用分区技术进行内存管理。通过将内存空间分区，形成若干边界固定的区域。进程加载时，选择大于或等于所需空间大小的分区进行装载。因此分区技术中，进程加载匹配的单位是分区，进程和分区是是一对一的关系。 分区技术中交换的单位是分区，和其他技术一样，放置时使用到的放置算法主要有三种： 最佳适配，选择与要求大小最接近的块 首次适配，从开始扫描内存，选择大小足够的第一个可用块 下次适配，从上一次放置的位置开始扫描内存，选择下一个大小足够的可用块 相较而言，分区技术存在的最大问题是粒度过大，由于进程所需空间值域较广，很容易引入内部（固定分区特有）、外部碎片，造成内存空间上的浪费。虽然通过压缩等技术可以适当减缓，但效率上仍然不十分可观，因此目前分区技术已基本不再使用。 分页有了分区的前车之鉴，那么分页技术就适当降低了单元粒度，此时一个进程不再对应一个完整分区，而是对应多个页。进一步说明之前，首先了解一下如下概念： 页（page） 一定大小字节数内存单元，属于逻辑单元。进程中所有代码、数据等信息均按页进行存储，属于逻辑组织形式。每个页有页码及其他信息。 页框(page frame) 对应页字节数的物理内存，属于物理单元，是实际存在于物理内存中的可用地址单元。页框相当于页的容器，进程运行过程中，可能会动态加载不同页进入页框，CPU则直接对页框进行存取。 页表（page table） 既然有页和对应页框，那就必须有映射表将两者联系起来，而页表就是页和页框之间的映射表。换言之，知道页就可以查询到页框，知道页框，也可以查询到对应页。 页表存在必要性还有一个角度可以理解，由于单元粒度变小，那么单元总数就上升了。为了便于管理，对于总数众多的事物，通常会采用分级管理的方法。以学校对于学生的管理为例，通常会划分为校长、年级主任、班主任、班长、组长等层级。对于内存也同样如此，通过将每8个bit组织为一个字节，每多少字节组织为一页，划分层级。操作系统为每个进程维护一个页表，页表给出该进程的每一页对应的页框的位置。对于每一页中的位置，则进一步使用偏移量来表示。 当进程尝试引用不在内存中的页时，CPU会判定为页错误，并将控制权暂时从进程移交到操作系统，而操作系统则负责： 确定数据在磁盘上的位置 从内存中获取一个空的页框作为数据容器 加载所需数据至可用页框 更新新页框至页表 返回至原进程，继续执行造成也页错误的指令 第二步中，如所有页框均处于使用中状态，则操作系统必须选择一个页框进行重用。如被交换页框是被其他进程动态加载以存储数据、或是加载进内存后被进程修改过，则必须写回到磁盘中。操作系统选择页框进行重用的方法称为页替换算法，它关系着整体效率。页替换算法有很多侧重性，如最少使用、最早加载等，归根结底，没有适用于所有场景的算法，但通过不断增加有价值的基础信息，是可以使得页替换更加精确地保证某些特性的。 分段分段技术与分页的区别倒不在单元力度上，而是逻辑组织形式。分页的最终单位是页，而分段最终单位是段。对于硬件电路而言，可能区别不是很大，但对于程序开发而言，有更贴近自然语言表达方式的优点。 分段技术将计算机内存分解为各段，每个段有起始地址和长度，以及一系列权限信息（如读、写、执行等）。而同样地，将进程代码数据等为几个段，逻辑地址同样由段号和偏移量组成。 和分页对应，分段时操作系统也需维护一个段表。总的来讲，分页和分段有两个特点： 进程中所有内存访问都是逻辑地址，这些逻辑地址在运行时动态地被转化成物理地址 一个进程可以划分成许多块（页和段），执行过程中，这些块不需要连续地位于内存中。 虚拟内存虚拟内存的原理是使用二级存储器以使得程序可以使用超出物理内存大小的内存资源，撕掉包装，就是说用一部分低速的磁盘空间，充当内存空间，从而假装每个进程都有很多内存空间可以使用。 如果说内存是实实在在存在的内存单元，那么磁盘上的就是虚拟存在的内存单元，因此将这两者合称为虚拟内存。有人可能比较担心，增加了磁盘和内存间数据交换会不会降低了CPU执行效率，比如说如果实现加载了进程全部指令等，那么执行时只需要直接从内存中读取并依次执行即可，但现在，可能需要等待部分不存在于内存中的指令从磁盘读取，势必会增加指令周期数而拖慢整个CPU。那么只能说，影响肯定有，但不大，而且可以通过算法策略进行最小化。回想一下计算机系统整个存储分级机制，同样也是从快到慢，从小到大，但CPU并没有因为内存的低速而变得慢起来。 这些算法策略（以分页为例）主要包括如下几类： 读取策略 确定一个页何时取入内存，常用的方法是请求分页和预先分页。 请求分页，只在访问到某页中单元时才将该页取入内存 预先分页，利用大多数辅存设备特性，一次性读取许多连续页以平均读取时间 放置策略 置换策略 这里不对具体算法策略进行展开，毕竟不是本系列文章的初衷，也容易暴露本人算法渣渣的面貌。 总结行文至此，可以整理出操作系统的内存管理取决于三个基本方面的选择： 是否使用虚存技术 使用分页还是分段，或是二者结合 为各种存储管理特征采用的算法 前两者取决于使用的硬件平台，因此早期系统未提供虚存是因为CPU不支持分页或分段。所以还是那句话，理解一个事物最好追溯其本源，万物存在皆有理，你如果不能理解到其中理，很可能只是因为你没有充分了解其历史局限性。 同时，也希望本篇中讲解到的内存管理方面内容能够给大家形成一个较为清晰的概念，结合前文所述电学、硬件、操作系统等知识，融会贯通，有所感悟，谢谢！ 作者：SniperPan 链接：https://juejin.im/post/5988330c5188256dd1666626 来源：掘金著作权归作者所有。 商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Raft算法原理]]></title>
    <url>%2F2019%2F07%2F25%2Fraft%2F</url>
    <content type="text"><![CDATA[本文转载自：Raft算法原理 简介关于Raft算法，有两篇经典的论文，一篇是《In search of an Understandable Consensus Algorithm》，这是作者最开始讲述Raft算法原理的论文，但是这篇论文太简单了，很多算法的细节没有涉及到。更详细的论文是《CONSENSUS: BRIDGING THEORY AND PRACTICE》，除了包括第一篇论文的内容以外，还加上了很多细节的描述。在我阅读完etcd raft算法库的实现之后，发现这个库的代码基本就是按照后一篇论文来写的，甚至有部分测试用例的注释里也写明了是针对这篇论文的某一个小节的情况做验证。 这篇文章做为我后续分析etcd raft算法的前导文章，将结合后一篇论文加上一些自己的演绎和理解来讲解Raft算法的原理。 算法的基本流程Raft算法概述Raft算法由leader节点来处理一致性问题。leader节点接收来自客户端的请求日志数据，然后同步到集群中其它节点进行复制，当日志已经同步到超过半数以上节点的时候，leader节点再通知集群中其它节点哪些日志已经被复制成功，可以提交到raft状态机中执行。 通过以上方式，Raft算法将要解决的一致性问题分为了以下几个子问题。 leader选举：集群中必须存在一个leader节点。 日志复制：leader节点接收来自客户端的请求然后将这些请求序列化成日志数据再同步到集群中其它节点。 安全性：如果某个节点已经将一条提交过的数据输入raft状态机执行了，那么其它节点不可能再将相同索引 的另一条日志数据输入到raft状态机中执行。 Raft算法需要一直保持的三个属性。 选举安全性（Election Safety）：在一个任期内只能存在最多一个leader节点。 Leader节点上的日志为只添加（Leader Append-Only）：leader节点永远不会删除或者覆盖本节点上面的日志数据。 日志匹配性（Log Matching）：如果两个节点上的日志，在日志的某个索引上的日志数据其对应的任期号相同，那么在两个节点在这条日志之前的日志数据完全匹配。 leader完备性（Leader Completeness）：如果一条日志在某个任期被提交，那么这条日志数据在leader节点上更高任期号的日志数据中都存在。 状态机安全性（State Machine Safety）：如果某个节点已经将一条提交过的数据输入raft状态机执行了，那么其它节点不可能再将相同索引的另一条日志数据输入到raft状态机中执行。 Raft算法基础在Raft算法中，一个集群里面的所有节点有以下三种状态： Leader：领导者，一个集群里只能存在一个Leader。 Follower：跟随者，follower是被动的，一个客户端的修改数据请求如果发送到Follower上面时，会首先由Follower重定向到Leader上， Candidate：参与者，一个节点切换到这个状态时，将开始进行一次新的选举。 每一次开始一次新的选举时，称为一个“任期”。每个任期都有一个对应的整数与之关联，称为“任期号”，任期号用单词“Term”表示，这个值是一个严格递增的整数值。 节点的状态切换状态机如下图所示。 上图中标记了状态切换的6种路径，下面做一个简单介绍，后续都会展开来详细讨论。 start up：起始状态，节点刚启动的时候自动进入的是follower状态。 times out, starts election：follower在启动之后，将开启一个选举超时的定时器，当这个定时器到期时，将切换到candidate状态发起选举。 times out, new election：进入candidate 状态之后就开始进行选举，但是如果在下一次选举超时到来之前，都还没有选出一个新的leade，那么还会保持在candidate状态重新开始一次新的选举。 receives votes from majority of servers：当candidate状态的节点，收到了超过半数的节点选票，那么将切换状态成为新的leader。 discovers current leader or new term：candidate状态的节点，如果收到了来自leader的消息，或者更高任期号的消息，都表示已经有leader了，将切换回到follower状态。 discovers server with higher term：leader状态下如果收到来自更高任期号的消息，将切换到follower状态。这种情况大多数发生在有网络分区的状态下。 如果一个candidate在一次选举中赢得leader，那么这个节点将在这个任期中担任leader的角色。但并不是每个任期号都一定对应有一个leader的，比如上面的情况3中，可能在选举超时到来之前都没有产生一个新的leader，那么此时将递增任期号开始一次新的选举。 从以上的描述可以看出，任期号在raft算法中更像一个“逻辑时钟（logic clock）”的作用，有了这个值，集群可以发现有哪些节点的状态已经过期了。每一个节点状态中都保存一个当前任期号（current term），节点在进行通信时都会带上本节点的当前任期号。如果一个节点的当前任期号小于其他节点的当前任期号，将更新其当前任期号到最新的任期号。如果一个candidate或者leader状态的节点发现自己的当前任期号已经小于其他节点了，那么将切换到follower状态。反之，如果一个节点收到的消息中带上的发送者的任期号已经过期，将拒绝这个请求。 raft节点之间通过RPC请求来互相通信，主要有以下两类RPC请求。RequestVote RPC用于candidate状态的节点进行选举之用，而AppendEntries RPC由leader节点向其他节点复制日志数据以及同步心跳数据的。 leader选举现在来讲解leader选举的流程。 raft算法是使用心跳机制来触发leader选举的。 在节点刚开始启动时，初始状态是follower状态。一个follower状态的节点，只要一直收到来自leader或者candidate的正确RPC消息的话，将一直保持在follower状态。leader节点通过周期性的发送心跳请求（一般使用带有空数据的AppendEntries RPC来进行心跳）来维持着leader节点状态。每个follower同时还有一个选举超时（election timeout）定时器，如果在这个定时器超时之前都没有收到来自leader的心跳请求，那么follower将认为当前集群中没有leader了，将发起一次新的选举。 发起选举时，follower将递增它的任期号然后切换到candidate状态。然后通过向集群中其它节点发送RequestVote RPC请求来发起一次新的选举。一个节点将保持在该任期内的candidate状态下，直到以下情况之一发生。 该candidate节点赢得选举，即收到超过半数以上集群中其它节点的投票。 另一个节点成为了leader。 选举超时到来时没有任何一个节点成为leader。 下面来逐个分析以上几种情况。 第一种情况，如果收到了集群中半数以上节点的投票，那么此时candidate节点将成为新的leader。每个节点在一个任期中只能给一个节点投票，而且遵守“先来后到”的原则。这样就保证了，每个任期最多只有一个节点会赢得选举成为leader。但并不是每个进行选举的candidate节点都会给它投票，在后续的“选举安全性”一节中将展开讨论这个问题。当一个candidate节点赢得选举成为leader后，它将发送心跳消息给其他节点来宣告它的权威性以阻止其它节点再发起新的选举。 第二种情况，当candidate节点等待其他节点时，如果收到了来自其它节点的AppendEntries RPC请求，同时做个请求中带上的任期号不比candidate节点的小，那么说明集群中已经存在leader了，此时candidate节点将切换到follower状态；但是，如果该RPC请求的任期号比candidate节点的小，那么将拒绝该RPC请求继续保持在candidate状态。 第三种情况，一个candidate节点在选举超时到来的时候，既没有赢得也没有输掉这次选举。这种情况发生在集群节点数量为偶数个，同时有两个candidate节点进行选举，而两个节点获得的选票数量都是一样时。当选举超时到来时，如果集群中还没有一个leader存在，那么candidate节点将继续递增任期号再次发起一次新的选举。这种情况理论上可以一直无限发生下去。 为了减少第三种情况发生的概率，每个节点的选举超时时间都是随机决定的，一般在150~300毫秒之间，这样两个节点同时超时的情况就很罕见了。 以上过程用伪代码来表示如下。 1234567891011121314151617181920212223242526节点刚启动，进入follower状态，同时创建一个超时时间在150-300毫秒之间的选举超时定时器。follower状态节点主循环： 如果收到leader节点心跳： 心跳标志位置1 如果选举超时到期： 没有收到leader节点心跳： 任期号term+1，换到candidate状态。 如果收到leader节点心跳： 心跳标志位置空 如果收到选举消息： 如果当前没有给任何节点投票过 或者 消息的任期号大于当前任期号： 投票给该节点 否则： 拒绝投票给该节点candidate状态节点主循环： 向集群中其他节点发送RequestVote请求，请求中带上当前任期号term 收到AppendEntries消息： 如果该消息的任期号 &gt;= 本节点任期号term： 说明已经有leader，切换到follower状态 否则： 拒绝该消息 收到其他节点应答RequestVote消息： 如果数量超过集群半数以上，切换到leader状态 如果选举超时到期： term+1，进行下一次的选举 日志复制日志复制的流程大体如下： 每个客户端的请求都会被重定向发送给leader，这些请求最后都会被输入到raft算法状态机中去执行。 leader在收到这些请求之后，会首先在自己的日志中添加一条新的日志条目。 在本地添加完日志之后，leader将向集群中其他节点发送AppendEntries RPC请求同步这个日志条目，当这个日志条目被成功复制之后（什么是成功复制，下面会谈到），leader节点将会将这条日志输入到raft状态机中，然后应答客户端。 Raft日志的组织形式如下图所示。 每个日志条目包含以下成员。 index：日志索引号，即图中最上方的数字，是严格递增的。 term：日志任期号，就是在每个日志条目中上方的数字，表示这条日志在哪个任期生成的。 command：日志条目中对数据进行修改的操作。 一条日志如果被leader同步到集群中超过半数的节点，那么被称为“成功复制”，这个日志条目就是“已被提交（committed）”。如果一条日志已被提交，那么在这条日志之前的所有日志条目也是被提交的，包括之前其他任期内的leader提交的日志。如上图中索引为7的日志条目之前的所有日志都是已被提交的日志。 以下面的图示来说明日志复制的流程。 客户端发送SET a=1的命令到leader节点上。 leader节点在本地添加一条日志，其对应的命令为SET a=1。这里涉及到两个索引值，committedIndex存储的最后一条提交（commit）日志的索引，appliedIndex存储的是最后一条应用到状态机中的日志索引值，一条日志只有被提交了才能应用到状态机中，因此总有 committedIndex &gt;= appliedIndex不等式成立。在这里只是添加一条日志还并没有提交，两个索引值还指向上一条日志。 leader节点向集群中其他节点广播AppendEntries消息，带上SET a=1命令。 收到AppendEntries请求的follower节点，同样在本地添加了一条新的日志，也还并没有提交。 follower节点向leader节点应答AppendEntries消息。 当leader节点收到集群半数以上节点的AppendEntries请求的应答消息时，认为SET a=1命令成功复制，可以进行提交，于是修改了本地committed日志的索引指向最新的存储SET a=1的日志，而appliedIndex还是保持着上一次的值，因为还没有应用该命令到状态机中。 提交命令完成，给应用层说明这条命令已经提交。此时修改appliedIndex与committedIndex一样了。 leader节点在下一次给follower的AppendEntries请求中，会带上当前最新的committedIndex索引值，follower收到之后同样会修改本地日志的committedIndex索引。 需要说明的是，7和8这两个操作并没有严格的先后顺序，谁在前在后都没关系。 leader上保存着已被提交的最大日志索引信息，在每次向follower节点发送的AppendEntries RPC请求中都会带上这个索引信息，这样follower节点就知道哪个日志已经被提交了，被提交的日志将会输入Raft状态机中执行。 Raft算法保持着以下两个属性，这两个属性共同作用满足前面提到的日志匹配（LogMatch）属性： 如果两个日志条目有相同的索引号和任期号，那么这两条日志存储的是同一个指令。 如果在两个不同的日志数据中，包含有相同索引和任期号的日志条目，那么在这两个不同的日志中，位于这条日志之前的日志数据是相同的。 在正常的情况下，follower节点和leader节点的日志一直保持一致，此时AppendEntries RPC请求将不会失败。但是，当leader节点宕机时日志就可能出现不一致的情况，比如在这个leader节点宕机之前同步的数据并没有得到超过半数以上节点都复制成功了。如下图所示就是一种出现前后日志不一致的情况。 在上图中，最上面的一排数字是日志的索引，盒子中的数据是该日志对应的任期号，左边的字母表示的是a-f这几个不同的节点。图中演示了好几种节点日志与leader节点日志不一致的情况，下面说明中以二元组&lt;任期号，索引号&gt;来说明各个节点的日志数据情况： leader节点：&lt;6, 10&gt;。 a节点：&lt;6,9&gt;，缺少日志。 b节点：&lt;4,4&gt;，任期号比leader小，因此缺少日志。 c节点：&lt;6,11&gt;，任期号与leader相同，但是有比leader日志索引更大的日志，这部分日志是未提交的日志。 d节点：&lt;7,12&gt;，任期号比leader大，这部分日志是未提交的日志。 e节点：&lt;4,7&gt;，任期号与索引都比leader小，因此既缺少日志，也有未提交的日志。 f节点：&lt;3,11&gt;，任期号比leader小，所以缺少日志，而索引比leader大，这部分日志又是未提交的日志。 在Raft算法中，解决日志数据不一致的方式是Leader节点同步日志数据到follower上，覆盖follower上与leader不一致的数据。 为了解决与follower节点同步日志的问题，leader节点中存储着两个与每个follower节点日志相关的数据。 nextIndex存储的是下一次给该节点同步日志时的日志索引。 matchIndex存储的是该节点的最大日志索引。 从以上两个索引的定义可知，在follower与leader节点之间日志复制正常的情况下，nextIndex = matchIndex + 1。但是如果出现不一致的情况，则这个等式可能不成立。每个leader节点被选举出来时，将初始化nextIndex为leader节点最后一条日志，而matchIndex为0，这么做的原因在于：leader节点将从后往前探索follower节点当前存储的日志位置，而在不知道follower节点日志位置的情况下只能置空matchIndex了。 leader节点通过AppendEntries消息来与follower之间进行日志同步的，每次给follower带过去的日志就是以nextIndex来决定，如果follower节点的日志与这个值匹配，将返回成功；否则将返回失败，同时带上本节点当前的最大日志ID，方便leader节点快速定位到follower的日志位置以下一次同步正确的日志数据，而leader节点在收到返回失败的情况下，将置nextIndex = matchIndex + 1。从上面的分析可知，在leader当前之后第一次向follower同步日志失败时，nextIndex = matchIndex + 1 = 1。 以上图的几个节点为例来说明情况。 初始状态下，leader节点将存储每个folower节点的nextIndex为10，matchIndex为0。因此在成为leader节点之后首次向follower节点同步日志数据时，将复制索引位置在10以后的日志数据，同时带上日志二元组&lt;6,10&gt;告知follower节点当前leader保存的follower日志状态。 a节点：由于节点的最大日志数据二元组是&lt;6,9&gt;，正好与leader发过来的日志&lt;6,10&gt;紧挨着，因此返回复制成功。 b节点：由于节点的最大日志数据二元组是&lt;4,4&gt;，与leader发送过来的日志数据&lt;6,10&gt;不匹配，将返回失败同时带上自己最后的日志索引4，leader节点在收到该拒绝消息之后，将修改保存该节点的nextIndex为matchIndex + 1即1，所以下一次leader节点将同步从索引1到10的数据给b节点。 c节点：由于节点的最大日志数据二元组是&lt;6,11&gt;，与leader发送过来的日志数据&lt;6,10&gt;不匹配，将返回失败同时带上自己最后的日志索 引11，leader节点在收到该拒绝消息之后，将修改保存该节点的nextIndex为matchIndex + 1即1，所以下一次leader节点将同步从索引1到10的数据给c节点，由于c节点上有未提交的数据，所以在第二次与leader同步完成之后，这些未提交的数据被清除。 d节点：由于节点的最大日志数据二元组是&lt;7,12&gt;，与leader发送过来的日志数据&lt;6,10&gt;不匹配，将返回失败同时带上自己最后的日志索 引11，leader节点在收到该拒绝消息之后，将修改保存该节点的nextIndex为matchIndex + 1即1，所以下一次leader节点将同步从索引1到10的数据给d节点，由于d节点上有未提交的数据，所以在第二次与leader同步完成之后，这些未提交的数据被清除。 e节点：由于节点的最大日志数据二元组是&lt;4,7&gt;，与leader发送过来的日志数据&lt;6,10&gt;不匹配，将返回失败同时带上自己最后的日志索 引11，leader节点在收到该拒绝消息之后，将修改保存该节点的nextIndex为matchIndex + 1即1，所以下一次leader节点将同步从索引1到10的数据给e节点，由于e节点上缺少的日志数据将被补齐，而多出来的未提交数据将被清除。 f节点：由于节点的最大日志数据二元组是&lt;4,7&gt;，与leader发送过来的日志数据&lt;6,10&gt;不匹配，将返回失败同时带上自己最后的日志索 引11，leader节点在收到该拒绝消息之后，将修改保存该节点的nextIndex为matchIndex + 1即1，所以下一次leader节点将同步从索引1到10的数据给f节点，由于f节点上缺少的日志数据将被补齐，而多出来的未提交数据将被清除。 安全性前面章节已经将leader选举以及日志同步的机制介绍了，这一小节讲解安全性相关的内容。 选举限制raft算法中，并不是所有节点都能成为leader。一个节点要成为leader，需要得到集群中半数以上节点的投票，而一个节点会投票给一个节点，其中一个充分条件是：这个进行选举的节点的日志，比本节点的日志更新。之所以要求这个条件，是为了保证每个当选的节点都有当前最新的数据。为了达到这个检查日志的目的，RequestVote RPC请求中需要带上参加选举节点的日志信息，如果节点发现选举节点的日志信息并不比自己更新，将拒绝给这个节点投票。 如果判断日志的新旧？这通过对比日志的最后一个日志条目数据来决定，首先将对比条目的任期号，任期号更大的日志数据更新；如果任期号相同，那么索引号更大的数据更新。 以上处理RequestVote请求的流程伪代码表示如下。 1follower节点收到RequestVote请求： 对比RequestVote请求中带上的最后一条日志数据： 如果任期号比节点的最后一条数据任期号小： 拒绝投票给该节点 如果索引号比节点的最后一条数据索引小： 拒绝投票给该节点 其他情况： 说明选举节点的日志信息比本节点更新，投票给该节点。 提交前面任期的日志条目如果leader在写入但是还没有提交一条日志之前崩溃，那么这条没有提交的日志是否能提交？有几种情况需要考虑，如下图所示。 在上图中，有以下的场景变更。 情况a：s1是leader，index 2位置写入了数据2，该值只写在了s1，s2上，但是还没有被提交。 情况b: s1崩溃，s5成为新的leader，该节点在index 2上面提交了另外一个值3，但是这个值只写在了s5上面，并没有被提交。 情况c: s5崩溃，s1重新成为leader，这一次，index 2的值2写到了集群的大多数节点上。 此时可能存在以下两种情况： 情况d1: s1崩溃，s5重新成为leader（投票给s5的是s4，s2和s5自身），那么index 2上的值3这一次成功的写入到集群的半数以上节点之上，并成功提交。 情况d2: s1不崩溃，而是将index 2为2的值成功提交。 从情况d的两种场景可以看出，在index 2值为2，且已经被写入到半数以上节点的情况下，同样存在被新的leader覆盖的可能性。 由于以上的原因，对于当前任期之前任期提交的日志，并不通过判断是否已经在半数以上集群节点写入成功来作为能否提交的依据。只有当前leader任期内的日志是通过比较写入数量是否超过半数来决定是否可以提交的。 对于任期之前的日志，Raft采用的方式，是只要提交成功了当前任期的日志，那么在日志之前的日志就认为提交成功了。这也是为什么etcd-Raft代码中，在成为leader之后，需要再提交一条dummy的日志的原因–只要该日志提交成功，leader上该日志之前的日志就可以提交成功。 集群成员变更在上面描述Raft基本算法流程中，都假设集群中的节点是稳定不变的。但是在某些情况下，需要手动改变集群的配置。 安全性安全性是变更集群成员时首先需要考虑到的问题，任何时候都不能出现集群中存在一个以上leader的情况。为了避免出现这种情况，每次变更成员时不能一次添加或者修改超过一个节点，集群不能直接切换到新的状态，如下图所示。 在上图中，server 1、2、3组成的是旧集群，server 4、5是准备新加入集群的节点。注意到如果直接尝试切换到新的状态，在某些时间点里，如图中所示，由于server 1、2上的配置还是旧的集群配置，那么可能这两个节点已经选定了一个leader；而server 3、4、5又是新的配置，它们也可能选定了一个leader，而这两个leader不是同一个，这就出现了集群中存在一个以上leader的情况了。 反之，下图所示是分别往奇数个以及偶数个集群节点中添加删除单个节点的场景。 可以看到，不论旧集群节点数量是奇数还是偶数个，都不会出现同时有两个超过半数以上子集群的存在，也就不可能选出超过一个leader。 raft采用将修改集群配置的命令放在日志条目中来处理，这样做的好处是： 可以继续沿用原来的AppendEntries命令来同步日志数据，只要把修改集群的命令做为一种特殊的命令就可以了。 在这个过程中，可以继续处理客户端请求。 可用性添加新节点到集群中添加一个新的节点到集群时，需要考虑一种情况，即新节点可能落后当前集群日志很多的情况，在这种情况下集群出现故障的概率会大大提高，如下图所示。 上图中的情况a中，s1、s2、s3是原有的集群节点，这时把节点s4添加进来，而s4上又什么数据都没有。如果此时s3发生故障，在集群中原来有三个节点的情况下，本来可以容忍一个节点的失败的；但是当变成四个节点的集群时，s3和s4同时不可用整个集群就不可用了。 因此Raft算法针对这种新添加进来的节点，是如下处理的。 添加进来的新节点首先将不加入到集群中，而是等待数据追上集群的进度。 leader同步数据给新节点的流程是，划分为多个轮次，每一轮同步一部分数据，而在同步的时候，leader仍然可以写入新的数据，只要等新的轮次到来继续同步就好。 以下图来说明同步数据的流程。 如上图中，划分为多个轮次来同步数据。比如，在第一轮同步数据时，leader的最大数据索引为10，那么第一轮就同步10之前的数据。而在同步第一轮数据的同时，leader还能继续接收新的数据，假设当同步第一轮完毕时，最大数据索引变成了20，那么第二轮将继续同步从10到20的数据。以此类推。 这个同步的轮次并不能一直持续下去，一般会有一个限制的轮次数量，比如最多同步10轮。 删除当前集群的leader节点当需要下线当前集群的leader节点时，leader节点将发出一个变更节点配置的命令，只有在该命令被提交之后，原先的leader节点才下线，然后集群会自然有一个节点选举超时而进行新的一轮选举。 处理移出集群的节点如果某个节点在一次配置更新之后，被移出了新的集群，但是这个节点又不知道这个情况，那么按照前面描述的Raft算法流程来说，它应该在选举超时之后，将任期号递增1，发起一次新的选举。虽然最终这个节点不会赢得选举，但是毕竟对集群运行的状态造成了干扰。而且如果这个节点一直不下线，那么上面这个发起新选举的流程就会一直持续下去。 为了解决这个问题，Raft引入了一个成为“PreVote”的流程，在这个流程中，如果一个节点要发起一次新的选举，那么首先会广播给集群中的其它所有节点，询问下当前该节点上的日志是否足以赢下选举。只有在这个PreVote阶段赢得超过半数节点肯定的情况下，才真正发起一次新的选举。 然而，PreVote并不能解决所有的问题，因为很有可能该被移除节点上的日志也是最新的。 由于以上的原因，所以不能完全依靠判断日志的方式来决定是否允许一个节点发起新一轮的选举。 Raft采用了另一种机制。如果leader节点一直保持着与其它节点的心跳消息，那么就认为leader节点是存活的，此时不允许发起一轮新的选举。这样follower节点处理RequestVote请求时，就需要加上判断，除了判断请求进行选举的节点日志是否最新以外，如果当前在一段时间内还收到过来自leader节点的心跳消息，那么也不允许发起新的选举。然而这种情况与前面描述的leader迁移的情况相悖，在leader迁移时是强制要求发起新的选举的，因此RequestVote请求的处理还要加上这种情况的判断。 总结来说，RequestVote请求的处理逻辑大致是这样的。 12345678follower处理RequestVote请求： 如果请求节点的日志不是最新的： 拒绝该请求，返回 如果此时是leader迁移的情况： 接收该请求，返回 如果最近一段时间还有收到来自leader节点的心跳消息： 拒绝该请求，返回 接收该请求 日志压缩日志数据如果不进行压缩处理掉的话，会一直增长下去。为此Raft使用快照数据来进行日志压缩，比如针对键值a的几次操作日志a=1、删除a、a=3最后可以被压缩成为最后的结果数据即a=3。 快照数据和日志数据的组织形式如下图。 在上图中： 未压缩日志前，日志数据保存到了&lt;3,5&gt;的位置，而在&lt;2,3&gt;的位置之前的数据都已经进行提交了，所以可以针对这部分数据进行压缩。 压缩日志之后，快照文件中存放了几个值：压缩时最后一条日志的二元数据是&lt;2,3&gt;，而针对a的几次操作最后的值为a=3，b的值为2。 杂项高效处理只读请求前面已经提到过，处理一个命令时，需要经历以下流程：leader向集群中其它节点广播日志，在日志被超过半数节点应答之后，leader提交该日志，最后才应答客户端。这样的流程对于一个只读请求而言太久了，而且还涉及到日志落盘的操作，对于只读请求而言这些操作是不必要的。 但是如果不经过上面的流程，leader节点在收到一个只读请求时就直接将本节点上保存的数据应答客户端，也是不安全的，因为这可能返回已经过期的数据。一方面leader节点可能已经发生了变化，只是这个节点并不知道；另一方面可能数据也发生了改变。返回过期的数据不符合一致性要求，因此这样的做法也是不允许的。 Raft中针对只读请求是这样做处理的。 leader节点需要有当前已提交日志的信息。在前面提到过不能提交前面任期的日志条目，因此一个新leader产生之后，需要提交一条空日志，这样来确保上一个任期内的日志全部提交。 leader节点保存该只读请求到来时的commit日志索引为readIndex， leader需要确认自己当前还是集群的leader，因为可能会由于有网络分区的原因导致leader已经被隔离出集群而不自知。为了达到这个目的，leader节点将广播一个heartbeat心跳消息给集群中其它节点，当收到半数以上节点的应答时，leader节点知道自己当前还是leader，同时readIndex索引也是当前集群日志提交的最大索引。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Raft</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-215:Kth Largest Element in an Array(数组中第K大的数)]]></title>
    <url>%2F2019%2F07%2F24%2Fleetcode-215%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/kth-largest-element-in-an-array/ 题目描述 题目难度：Medium Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Example 1: 12Input: [3,2,1,5,6,4] and k = 2Output: 5 Example 2: 12Input: [3,2,3,1,2,4,5,5,6] and k = 4Output: 4 Solution1最大堆实现 123456789101112131415161718192021222324252627282930313233class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; if(nums == null || nums.length == 0) return -1; return heapSort(nums, k); &#125; private static int heapSort(int[] array, int k)&#123; for (int i = (array.length) / 2; i &gt;= 0; i--) &#123; heapAdjust(array, i, array.length); &#125; //调整堆,将最大的节点放在堆尾，然后从根节点重新调整 for (int i = array.length - 1; i &gt; array.length - k; i--) &#123; int temp = array[0]; array[0] = array[i]; array[i] = temp; heapAdjust(array, 0, i); &#125; return array[0]; &#125; private static void heapAdjust(int[] array, int s, int len) &#123; int temp, i; temp = array[s]; for(i = 2 * s; i &lt; len; i *= 2)&#123; if(i &lt; len - 1 &amp;&amp; array[i] &lt; array[i + 1]) ++i; if(temp &gt;= array[i]) break; array[s] = array[i]; s = i; &#125; array[s] = temp; &#125;&#125; Solution2快速排序实现 123456789101112131415161718192021class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; if(nums == null || nums.length == 0) return -1; return quickSort(nums, k , 0, nums.length - 1); &#125; private int quickSort(int[] nums, int k, int start, int end)&#123; int tmp = nums[start]; int low = start, high = end; while(low &lt; high)&#123; while(low &lt; high &amp;&amp; nums[high] &gt;= tmp) high--; if(low &lt; high) nums[low] = nums[high]; while(low &lt; high &amp;&amp; nums[low] &lt;= tmp) low++; if(low &lt; high) nums[high] = nums[low]; &#125; nums[low] = tmp; if(low == nums.length - k) return tmp; else if(low &lt; nums.length - k) return quickSort(nums, k, low + 1, end); else return quickSort(nums, k, start, low - 1); &#125;&#125; Solution3用JDK中的优先级队列PriorityQueue实现. 1234567891011121314public class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; PriorityQueue&lt;Integer&gt; largeK = new PriorityQueue&lt;Integer&gt;(k + 1); for(int el : nums) &#123; largeK.add(el); if (largeK.size() &gt; k) &#123; largeK.poll(); &#125; &#125; return largeK.poll(); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-199:Binary Tree Right Side View]]></title>
    <url>%2F2019%2F07%2F23%2Fleetcode-199%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/binary-tree-right-side-view/ 题目描述 题目难度：Medium Given a binary tree, imagine yourself standing on the rightside of it, return the values of the nodes you can see ordered from top to bottom. Example: 123456789Input: [1,2,3,null,5,null,4]Output: [1, 3, 4]Explanation: 1 &lt;--- / \2 3 &lt;--- \ \ 5 4 &lt;--- Solution123456789101112131415161718192021222324252627282930313233343536373839404142/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;Integer&gt; resList = new ArrayList&lt;&gt;(); if(root == null) return resList; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); //当前行结点的个数 int num = 1; //当前行已经遍历了的个数 int cur = 0; int next = 0; queue.offer(root); TreeNode curNode; while(!queue.isEmpty())&#123; curNode = queue.poll(); if(curNode.left != null) &#123; queue.offer(curNode.left); next++; &#125; if(curNode.right != null) &#123; queue.offer(curNode.right); next++; &#125; cur++; if(cur == num)&#123; resList.add(curNode.val); cur = 0; num = next; next = 0; &#125; &#125; return resList; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-114:Flatten Binary Tree to Linked List]]></title>
    <url>%2F2019%2F07%2F23%2Fleetcode-114%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/flatten-binary-tree-to-linked-list/ 题目描述 题目难度: Medium Given a binary tree, flatten it to a linked list in-place. For example, given the following tree: 12345 1 / \ 2 5 / \ \3 4 6 The flattened tree should look like: 12345678910111 \ 2 \ 3 \ 4 \ 5 \ 6 Solution12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; private TreeNode prev = null;public void flatten(TreeNode root) &#123; if (root == null) return; flatten(root.right); flatten(root.left); root.right = prev; root.left = null; prev = root;&#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下各种查找命令（find, grep, which, whereis, locate）]]></title>
    <url>%2F2019%2F07%2F23%2Flinux-find%2F</url>
    <content type="text"><![CDATA[本文转载自：Linux下各种查找命令（find, grep, which, whereis, locate） find命令 find &lt; path &gt; &lt; expression &gt; &lt; cmd &gt; path： 所要搜索的目录及其所有子目录。默认为当前目录。 expression： 所要搜索的文件的特征。 cmd： 对搜索结果进行特定的处理。 如果什么参数也不加，find默认搜索当前目录及其子目录，并且不过滤任何结果（也就是返回所有文件），将它们全都显示在屏幕上。 find命令常用选项及实例 -name 按照文件名查找文件。 find /dir -name filename 在/dir目录及其子目录下面查找名字为filename的文件 find . -name “*.c” 在当前目录及其子目录（用“.”表示）中查找任何扩展名为“c”的文件 -perm 按照文件权限来查找文件。 find . -perm 755 –print 在当前目录下查找文件权限位为755的文件，即文件属主可以读、写、执行，其他用户可以读、执行的文件 -prune 使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略。 find /apps -path “/apps/bin” -prune -o –print 在/apps目录下查找文件，但不希望在/apps/bin目录下查找 find /usr/sam -path “/usr/sam/dir1” -prune -o –print 在/usr/sam目录下查找不在dir1子目录之内的所有文件 -depth：在查找文件时，首先查找当前目录中的文件，然后再在其子目录中查找。 find / -name “CON.FILE” -depth –print 它将首先匹配所有的文件然后再进入子目录中查找 -user 按照文件属主来查找文件。 find ~ -user sam –print 在$HOME目录中查找文件属主为sam的文件 -group 按照文件所属的组来查找文件。 find /apps -group gem –print 在/apps目录下查找属于gem用户组的文件 -mtime -n +n 按照文件的更改时间来查找文件， -n表示文件更改时间距现在n天以内，+n表示文件更改时间距现在n天以前。 find / -mtime -5 –print 在系统根目录下查找更改时间在5日以内的文件 find /var/adm -mtime +3 –print 在/var/adm目录下查找更改时间在3日以前的文件 -nogroup 查找无有效所属组的文件，即该文件所属的组在/etc/groups中不存在。 find / –nogroup -print -nouser 查找无有效属主的文件，即该文件的属主在/etc/passwd中不存在。 find /home -nouser –print -newer file1 ! file2 查找更改时间比文件file1新但比文件file2旧的文件。 -type 查找某一类型的文件，诸如：b - 块设备文件。d - 目录。c - 字符设备文件。p - 管道文件。l - 符号链接文件。f - 普通文件。 find /etc -type d –print 在/etc目录下查找所有的目录 find . ! -type d –print 在当前目录下查找除目录以外的所有类型的文件 find /etc -type l –print 在/etc目录下查找所有的符号链接文件 -size n[c] 查找文件长度为n块的文件，带有c时表示文件长度以字节计。 find . -size +1000000c –print 在当前目录下查找文件长度大于1 M字节的文件 find /home/apache -size 100c –print 在/home/apache目录下查找文件长度恰好为100字节的文件 find . -size +10 –print 在当前目录下查找长度超过10块的文件（一块等于512字节） -mount：在查找文件时不跨越文件系统mount点。 find . -name “*.XC” -mount –print 从当前目录开始查找位于本文件系统中文件名以XC结尾的文件（不进入其他文件系统） -follow：如果find命令遇到符号链接文件，就跟踪至链接所指向的文件 -exec，find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为’command’ {} ;，注意{}和;之间的空格 $ find ./ -size 0 -exec rm {} ; 删除文件大小为零的文件 $ rm -i find ./ -size 0 $ find ./ -size 0 | xargs rm -f &amp; 为了用ls -l命令列出所匹配到的文件，可以把ls -l命令放在find命令的-exec选项中： $ find . -type f -exec ls -l {} ;在/logs目录中查找更改时间在5日以前的文件并删除它们： find /logs -type f -mtime +5 -exec rm {} ; -ok，和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 find . -name “*.conf” -mtime +5 -ok rm { } ; 在当前目录中查找所有文件名以.LOG结尾、更改时间在5日以上的文件，并删除它们，只不过在删除之前先给出提示 说明： 如果你要寻找一个档案的话，那么使用 find 会是一个不错的主意。不过，由于 find 在寻找数据的时候相当的耗硬盘，所以没事情不要使用 find 啦！有更棒的指令可以取代呦，那就是 whereis 与 locate 咯~ 一些常用命令 find . -type f -exec ls -l {} ;查找当前路径下的所有普通文件，并把它们列出来。 find logs -type f -mtime +5 -exec rm {} ;删除logs目录下更新时间为5日以上的文件。 find . -name “*.log” -mtime +5 -ok rm {} ;删除当前路径下以。log结尾的五日以上的文件，删除之前要确认。 find ~ -type f -perm 4755 -print查找$HOME目录下suid位被设置，文件属性为755的文件打印出来。说明： find在有点系统中会一次性得到将匹配到的文件都传给exec，但是有的系统对exec的命令长度做限制，就会报：”参数列太长“，这就需要使用xargs。xargs是部分取传来的文件。 find / -type f -print |xargs filexargs测试文件分类 find . -name “core*” -print|xargs echo “ “&gt;/tmp/core.log将core文件信息查询结果报存到core。log日志。 find / -type f -print | xargs chmod o -w find . -name * -print |xargs grep “DBO” grep命令grep [选项] pattern [文件名] 命令中的选项为： -? 同时显示匹配行上下的？行，如：grep -2 pattern filename 同时显示匹配行的上下2行。 -b，—byte-offset 打印匹配行前面打印该行所在的块号码。 -c,—count 只打印匹配的行数，不显示匹配的内容。 -f File，—file=File 从文件中提取模板。空文件中包含0个模板，所以什么都不匹配。 -h，—no-filename 当搜索多个文件时，不显示匹配文件名前缀。 -i，—ignore-case 忽略大小写差别。 -q，—quiet 取消显示，只返回退出状态。0则表示找到了匹配的行。 -l，—files-with-matches 打印匹配模板的文件清单。 -L，—files-without-match 打印不匹配模板的文件清单。 -n，—line-number 在匹配的行前面打印行号。 -s，—silent 不显示关于不存在或者无法读取文件的错误信息。 -v，—revert-match 反检索，只显示不匹配的行。 -w，—word-regexp 如果被&lt;和&gt;引用，就把表达式做为一个单词搜索。 -V，—version 显示软件版本信息。 ls -l | grep ‘^a’ 通过管道过滤ls -l输出的内容，只显示以a开头的行。grep ‘test’ d* 显示所有以d开头的文件中包含test的行。grep ‘test’ aa bb cc 显示在aa，bb，cc文件中匹配test的行。grep ‘[a-z]’ aa 显示所有包含每个字符串至少有5个连续小写字符的字符串的行。grep ‘w(es)t.‘ aa 如果west被匹配，则es就被存储到内存中，并标记为1，然后搜索任意个字符(.)，这些字符后面紧跟着另外一个es()，找到就显示该行。如果用egrep或grep -E，就不用””号进行转义，直接写成’w(es)t.*’就可以了。grep -i pattern files ：不区分大小写地搜索。默认情况区分大小写grep -l pattern files ：只列出匹配的文件名，grep -L pattern files ：列出不匹配的文件名，grep -w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配‘magic’，而不是‘magical’)，grep -C number pattern files ：匹配的上下文分别显示[number]行，grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的行，grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。 pattern为所要匹配的字符串，可使用下列模式 . 匹配任意一个字符* 匹配0 个或多个*前的字符^ 匹配行开头$ 匹配行结尾[] 匹配[ ]中的任意一个字符，[]中可用 - 表示范围，例如[a-z]表示字母a 至z 中的任意一个\ 转意字符 locate命令locate命令其实是“find -name”的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/locatedb），这个数据库中含有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库。 locate命令的使用实例： $ locate /etc/sh搜索etc目录下所有以sh开头的文件。$ locate -i ~/m搜索用户主目录下，所有以m开头的文件，并且忽略大小写。 whereis命令whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 whereis命令的使用实例： $ whereis grepgrep: /bin/grep /usr/share/man/man1p/grep.1p.gz /usr/share/man/man1/grep.1.gz which命令which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 which命令的使用实例： $ which grep/bin/grep]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-309:Best Time to Buy and Sell Stock with Cooldown]]></title>
    <url>%2F2019%2F07%2F22%2Fleetcode-309%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/ 题目描述 题目难度：Medium Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (ie, buy one and sell one share of the stock multiple times) with the following restrictions: You may not engage in multiple transactions at the same time (ie, you must sell the stock before you buy again). After you sell your stock, you cannot buy stock on next day. (ie, cooldown 1 day) Example: 123Input: [1,2,3,0,2]Output: 3 Explanation: transactions = [buy, sell, cooldown, buy, sell] 解题思路从题目中可以看出，不管哪一天，都只能是 buy 或者 sell 或者 cooldown(rest) 三种状态中的一种，而根据题目的约束条件，我们可以画出下图所示的状态图： 由此图我们可以得到： s0[i] = max(s0[i - 1], s2[i - 1])s1[i] = max(s0[i - 1] - prices[i], s1[i - 1])s2[i] = s1[i - 1] + prices[i] 其中s0，s1，s2分别表示三种状态下的最大利润值。值得注意的是这里的s0，s1和s2不是单纯的buy，sell， rest，而应该是 s0 —— sell后rest或者rest后rests1 —— rest后的buy或者buy后的rests2 —— rest后的sell 同时，可以注意到的是，每次的状态 i 都只与前一次的状态 i - 1有关，也就是说我们可以把空间复杂度从O(n)降到O(1)。 Solution1234567891011121314151617181920class Solution &#123; public int maxProfit(int[] prices) &#123; if (prices.length &lt;= 1) return 0; int s0 = 0; int s1 = -prices[0]; int s2 = Integer.MIN_VALUE; for (int i = 1; i &lt; prices.length; i++)&#123; int pre0 = s0; int pre1 = s1; int pre2 = s2; s0 = Math.max(pre0, pre2); s1 = Math.max(pre0 - prices[i], pre1); //换成 s2 = Math.max(pre2, pre1 + prices[i]); 也是对的 s2 = pre1 + prices[i]; &#125; //最大利润不可能出现在buy而未sell的时候，所以不考虑s1 return Math.max(s0, s2); &#125;&#125; 参考：LeetCode 309: 一个很清晰的DP解题思路]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-152:Maximum Product Subarray]]></title>
    <url>%2F2019%2F07%2F22%2Fleetcode-152%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/maximum-product-subarray/ 题目描述 题目难度：Medium Given an integer array nums, find the contiguous subarray within an array (containing at least one number) which has the largest product. Example 1: 123Input: [2,3,-2,4]Output: 6Explanation: [2,3] has the largest product 6. Example 2: 123Input: [-2,0,-1]Output: 0Explanation: The result cannot be 2, because [-2,-1] is not a subarray. Solution1数组maxArr中第i个元素表示的是从0到i中以i结尾的连续子数组中最大的乘积。 数组minArr中第i个元素表示的是从0到i中以i结尾的连续子数组中最小的乘积。 1234567891011121314151617181920212223class Solution &#123; public int maxProduct(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; int len = nums.length; int[] minArr = new int[len]; int[] maxArr = new int[len]; maxArr[0] = nums[0]; minArr[0] = nums[0]; int max = nums[0]; for(int i = 1;i &lt; nums.length;i++)&#123; if(nums[i] &lt; 0)&#123; maxArr[i] = Math.max(nums[i], minArr[i - 1] * nums[i]); minArr[i] = Math.min(nums[i], maxArr[i - 1] * nums[i]); &#125; else&#123; maxArr[i] = Math.max(nums[i], maxArr[i - 1] * nums[i]); minArr[i] = Math.min(nums[i], minArr[i - 1] * nums[i]); &#125; max = Math.max(max, maxArr[i]); &#125; return max; &#125;&#125; Solution2maxArr[i]的值只和maxArr[i - 1]、minArr[i - 1]和nums[i]有关，minArr[i]类似，所以可以对Solution1的解法进行内存上的优化。 1234567891011121314151617class Solution &#123; public int maxProduct(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; int res = nums[0], max = nums[0], min = nums[0]; for(int i = 1;i &lt; nums.length;i++)&#123; if(nums[i] &lt; 0)&#123; int temp = max; max = min; min = temp; &#125; max = Math.max(nums[i], max * nums[i]); min = Math.min(nums[i], min * nums[i]); res = Math.max(res, max); &#125; return res; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-714:Best Time to Buy and Sell Stock with Transaction Fee]]></title>
    <url>%2F2019%2F07%2F22%2Fleetcode-714%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/ 题目描述 题目难度：Medium Your are given an array of integers prices, for which the i-th element is the price of a given stock on day i; and a non-negative integer fee representing a transaction fee. You may complete as many transactions as you like, but you need to pay the transaction fee for each transaction. You may not buy more than 1 share of a stock at a time (ie. you must sell the stock share before you buy again.) Return the maximum profit you can make. Example 1: 1234Input: prices = [1, 3, 2, 8, 4, 9], fee = 2Output: 8Explanation: The maximum profit can be achieved by:Buying at prices[0] = 1Selling at prices[3] = 8Buying at prices[4] = 4Selling at prices[5] = 9The total profit is ((8 - 1) - 2) + ((9 - 4) - 2) = 8. Note: 0 &lt; prices.length &lt;= 50000. 0 &lt; prices[i] &lt; 50000. 0 &lt;= fee &lt; 50000. 问题解析： 给定数组，其中的元素代表当天的股票价格，不限制股票的购售次数，但要求在购买新股之前要售出之前的股票，求股票交易的最大利润。每次股票交易都要支付固定的交易费。 Solution1可以用DP来做。 与之前的LeetCode-121：Best Time to Buy and Sell Stock (一次股票交易最大利润)和LeetCode-122：Best Time to Buy and Sell Stock II (多次股票交易最大利润)相似，均为股票交易题目，但本题增加了交易费的条件。 分析每天的行为，我们可以知道，主要的行为就是售出和购入。所以我们建立两个动态规划数组：sold和hold，其中sold[i]保存的是第i天出售股票后的最大利润，hold[i]保存的是第i天持有股票的最大利润。 sold[i]有两个状态，一个是在i天售出的利润，或者保持前一天也就是sold[i-1]的售出利润，选择其中的最大利润。 hold[i]也有两个状态，一个是在i天购入的利润，或者保持前一天也就是hold[i-1]的持有利润，选择其中的最大利润。 写为动态规划式为：Soldi=Max(Soldi−1,Holdi−1+Pricesi−fee); Holdi=Max(Holdi,Soldi−1−Pricesi) 1234567891011121314151617class Solution &#123; public int maxProfit(int[] prices, int fee) &#123; int n = prices.length; if (n &lt; 2) return 0; int[] sold = new int[n]; int[] hold = new int[n]; hold[0] = -prices[0]; //拥有的初始金额为0，然后买了第一天的股票，所以拥有的金额为 -prices[0] for (int i = 1; i &lt; n; i++)&#123; sold[i] = Math.max(sold[i-1], hold[i-1] + prices[i] - fee); hold[i] = Math.max(hold[i-1], sold[i-1] - prices[i]); &#125; return sold[n-1]; &#125;&#125; 时间复杂度：O(n)，空间复杂度：O(n) Solution2DP优化。 同样，与LeetCode-152：Maximum Product Subarray (乘积最大连续子数组)相同，Solution 1 的解法可以做空间上的优化。因为动态规划公式中每次计算使用的只有上一次的结果，所以，我们将前一次的结果做临时的保存即可实现空间上的优化。 1234567891011121314151617class Solution &#123; public int maxProfit(int[] prices, int fee) &#123; int n = prices.length; if (n &lt; 2) return 0; int sold = 0; int hold = -prices[0]; for (int i = 1; i &lt; n; i++)&#123; int soldpre = sold; sold = Math.max(soldpre, hold + prices[i] - fee); hold = Math.max(hold, soldpre - prices[i]); &#125; return sold; &#125;&#125; 时间复杂度：O(n)，空间复杂度：O(1) 参考： LeetCode-714：Best Time to Buy and Sell Stock with Transaction Fee (带有抛售费用的股票最大利润) – medium]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-123:Best Time to Buy and Sell Stock III]]></title>
    <url>%2F2019%2F07%2F21%2Fleetcode-123%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iii/ 题目描述Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete at most two transactions. Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). 测试用例Example 1: 1234Input: [3,3,5,0,0,3,1,4]Output: 6Explanation: Buy on day 4 (price = 0) and sell on day 6 (price = 3), profit = 3-0 = 3. Then buy on day 7 (price = 1) and sell on day 8 (price = 4), profit = 4-1 = 3. Example 2: 12345Input: [1,2,3,4,5]Output: 4Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4. Note that you cannot buy on day 1, buy on day 2 and sell them later, as you are engaging multiple transactions at the same time. You must sell before buying again. Example 3: 123Input: [7,6,4,3,1]Output: 0Explanation: In this case, no transaction is done, i.e. max profit = 0. 解题思路把价钱数组prices一分为二，分别计算[0, i]和(i, prices.length - 1]的最大利润，然后加起来，在所有和中求最大值，即为最多交易两次的最大利润。 AC代码array1数组中下标为i元素表示从i到结尾的最大利润。 array2数组中下标为i元素表示从0到1的最大利润。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; //3,3,5,0,0,3,1,4 //2 2 0 4 4 1 3 0 //0 0 2 2 2 3 1 4 public int maxProfit(int[] prices) &#123; if(prices == null || prices.length == 0) return 0; int[] array1 = getMaxPrices1(prices); int[] array2 = getMaxPrices2(prices); int max = 0, sum = 0; for(int i = 0;i &lt; prices.length;i++)&#123; sum = array1[i] + array2[i]; max = Math.max(max, sum); &#125; return max; &#125; public int[] getMaxPrices1(int[] prices)&#123; if(prices == null || prices.length == 0) return null; int len = prices.length; int max = 0; int maxPrice = prices[len - 1]; int[] maxPrices = new int[len]; int i = len - 2; while(i &gt;= 0)&#123; max = Math.max(max, maxPrice - prices[i]); maxPrices[i] = max; maxPrice = Math.max(maxPrice, prices[i]); i--; &#125; return maxPrices; &#125; public int[] getMaxPrices2(int[] prices)&#123; if(prices == null || prices.length == 0) return null; int len = prices.length; int max = 0; int minPrice = prices[0]; int[] maxPrices = new int[len]; int i = 1; while(i &lt; len)&#123; max = Math.max(max, prices[i] - minPrice); minPrice = Math.min(minPrice, prices[i]); maxPrices[i] = max; i++; &#125; return maxPrices; &#125; &#125; 最优方法代码来自Leetcode 123456789101112class Solution &#123; public int maxProfit(int[] prices) &#123; int sell1 = 0, sell2 = 0, buy1 = Integer.MIN_VALUE, buy2 = Integer.MIN_VALUE; for (int i : prices) &#123; sell2 = Math.max(sell2, i + buy2); buy2 = Math.max(buy2, sell1 - i); sell1 = Math.max(sell1, i + buy1); buy1 = Math.max(buy1, -i); &#125; return sell2; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-122:Best Time to Buy and Sell Stock II]]></title>
    <url>%2F2019%2F07%2F21%2Fleetcode-122%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/ 题目描述 题目难度：Easy Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times). Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Example 1: 1234Input: [7,1,5,3,6,4]Output: 7Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4. Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3. Example 2: 12345Input: [1,2,3,4,5]Output: 4Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4. Note that you cannot buy on day 1, buy on day 2 and sell them later, as you are engaging multiple transactions at the same time. You must sell before buying again. Example 3: 123Input: [7,6,4,3,1]Output: 0Explanation: In this case, no transaction is done, i.e. max profit = 0. AC代码12345678910public class Solution &#123; public int maxProfit(int[] prices) &#123; int total = 0; for (int i=0; i&lt; prices.length-1; i++) &#123; if (prices[i+1]&gt;prices[i]) total += prices[i+1]-prices[i]; &#125; return total;&#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-138:Copy List with Random Pointer]]></title>
    <url>%2F2019%2F07%2F21%2Fleetcode-138%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/copy-list-with-random-pointer/ 题目描述A linked list is given such that each node contains an additional random pointer which could point to any node in the list or null. Return a deep copy of the list. 123456Input:&#123;&quot;$id&quot;:&quot;1&quot;,&quot;next&quot;:&#123;&quot;$id&quot;:&quot;2&quot;,&quot;next&quot;:null,&quot;random&quot;:&#123;&quot;$ref&quot;:&quot;2&quot;&#125;,&quot;val&quot;:2&#125;,&quot;random&quot;:&#123;&quot;$ref&quot;:&quot;2&quot;&#125;,&quot;val&quot;:1&#125;Explanation:Node 1&apos;s value is 1, both of its next and random pointer points to Node 2.Node 2&apos;s value is 2, its next pointer points to null and its random pointer points to itself. Note: You must return the copy of the given head as a reference to the cloned list.AC AC代码1用一个map保存原结点和新节点的对应关系，做法好理解。 1234567891011121314151617181920212223242526272829303132333435363738394041/*// Definition for a Node.class Node &#123; public int val; public Node next; public Node random; public Node() &#123;&#125; public Node(int _val,Node _next,Node _random) &#123; val = _val; next = _next; random = _random; &#125;&#125;;*/class Solution &#123; public Node copyRandomList(Node head) &#123; if (head == null) return null; Map&lt;Node, Node&gt; map = new HashMap&lt;Node, Node&gt;(); // loop 1. copy all the nodes Node node = head; while (node != null) &#123; map.put(node, new Node(node.val, null, null)); node = node.next; &#125; // loop 2. assign next and random pointers node = head; while (node != null) &#123; map.get(node).next = map.get(node.next); map.get(node).random = map.get(node.random); node = node.next; &#125; return map.get(head); &#125;&#125; AC代码2把原结点和新节点交错链接，如： 原结点1 -&gt; 新节点1 -&gt; 原结点2 -&gt; 新节点2 -原结点3 -&gt; 新节点3 123456789101112131415161718192021222324252627282930313233343536373839404142/*// Definition for a Node.class Node &#123; public int val; public Node next; public Node random; public Node() &#123;&#125; public Node(int _val,Node _next,Node _random) &#123; val = _val; next = _next; random = _random; &#125;&#125;;*/class Solution &#123; public Node copyRandomList(Node head) &#123; if(head==null) return null; Node h = head, next = null; while(h!=null)&#123; next = h.next; h.next = new Node(h.val, next, null); h = next; &#125; h = head; while(h!=null)&#123; if(h.random!=null) h.next.random = h.random.next; h = h.next.next; &#125; h=head; Node newHead = head.next, copy = null; while(h!=null)&#123; copy = h.next; h.next = h.next.next; copy.next = copy.next==null ? null : copy.next.next; h = h.next; &#125; return newHead; &#125;&#125; 以上代码均来自Leetcode]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP算法]]></title>
    <url>%2F2019%2F07%2F21%2FKMP%2F</url>
    <content type="text"><![CDATA[本文转载自：KMP算法（1）：如何理解KMP 背景给定一个主串（以 S 代替）和模式串（以 P 代替），要求找出 P 在 S 中出现的位置，此即串的模式匹配问题。 Knuth-Morris-Pratt 算法（简称 KMP）是解决这一问题的常用算法之一，这个算法是由高德纳（Donald Ervin Knuth）和沃恩·普拉特在1974年构思，同年詹姆斯·H·莫里斯也独立地设计出该算法，最终三人于1977年联合发表。 在继续下面的内容之前，有必要在这里介绍下两个概念：真前缀 和 真后缀。 由上图所得， “真前缀”指除了自身以外，一个字符串的全部头部组合；”真后缀”指除了自身以外，一个字符串的全部尾部组合。（网上很多博客，应该说是几乎所有的博客，也包括我以前写的，都是“前缀”。严格来说，“真前缀”和“前缀”是不同的，既然不同，还是不要混为一谈的好！） 朴素字符串匹配算法初遇串的模式匹配问题，我们脑海中的第一反应，就是朴素字符串匹配（即所谓的暴力匹配），代码如下： 123456789101112131415161718192021222324252627/* 字符串下标始于 0 */int NaiveStringSearch(string S, string P)&#123; int i = 0; // S 的下标 int j = 0; // P 的下标 int s_len = S.size(); int p_len = P.size(); while (i &lt; s_len &amp;&amp; j &lt; p_len) &#123; if (S[i] == P[j]) // 若相等，都前进一步 &#123; i++; j++; &#125; else // 不相等 &#123; i = i - j + 1; j = 0; &#125; &#125; if (j == p_len) // 匹配成功 return i - j; return -1;&#125; 暴力匹配的时间复杂度为 $O(nm)$，其中 $n$ 为 S 的长度，$m$ 为 P 的长度。很明显，这样的时间复杂度很难满足我们的需求。 接下来进入正题：时间复杂度为 $Θ(n+m)$ 的 KMP 算法。 KMP字符串匹配算法算法流程以下摘自阮一峰的字符串匹配的KMP算法，并作稍微修改。 （1） 首先，主串”BBC ABCDAB ABCDABCDABDE”的第一个字符与模式串”ABCDABD”的第一个字符，进行比较。因为B与A不匹配，所以模式串后移一位。 （2） 因为B与A又不匹配，模式串再往后移。 （3） 就这样，直到主串有一个字符，与模式串的第一个字符相同为止。 （4） 接着比较主串和模式串的下一个字符，还是相同。 （5） 直到主串有一个字符，与模式串对应的字符不相同为止。 （6） 这时，最自然的反应是，将模式串整个后移一位，再从头逐个比较。这样做虽然可行，但是效率很差，因为你要把”搜索位置”移到已经比较过的位置，重比一遍。 （7） 一个基本事实是，当空格与D不匹配时，你其实是已经知道前面六个字符是”ABCDAB”。KMP算法的想法是，设法利用这个已知信息，不要把”搜索位置”移回已经比较过的位置，而是继续把它向后移，这样就提高了效率。 （8） i 0 1 2 3 4 5 6 7 模式串 A B C D A B D ‘\0’ next[i] -1 0 0 0 0 1 2 0 怎么做到这一点呢？可以针对模式串，设置一个跳转数组int next[]，这个数组是怎么计算出来的，后面再介绍，这里只要会用就可以了。 （9） 已知空格与D不匹配时，前面六个字符”ABCDAB”是匹配的。根据跳转数组可知，不匹配处D的next值为2，因此接下来从模式串下标为2的位置开始匹配。 （10） 因为空格与Ｃ不匹配，C处的next值为0，因此接下来模式串从下标为0处开始匹配。 （11） 因为空格与A不匹配，此处next值为-1，表示模式串的第一个字符就不匹配，那么直接往后移一位。 （12） 逐位比较，直到发现C与D不匹配。于是，下一步从下标为2的地方开始匹配。 （13） 逐位比较，直到模式串的最后一位，发现完全匹配，于是搜索完成。 next数组是如何求出的next数组的求解基于“真前缀”和“真后缀”，即next[i]等于P[0]...P[i - 1]最长的相同真前后缀的长度（请暂时忽视i等于0时的情况，下面会有解释）。我们依旧以上述的表格为例，为了方便阅读，我复制在下方了。 i 0 1 2 3 4 5 6 7 模式串 A B C D A B D ‘\0’ next[ i ] -1 0 0 0 0 1 2 0 i = 0，对于模式串的首字符，我们统一为next[0] = -1； i = 1，前面的字符串为A，其最长相同真前后缀长度为0，即next[1] = 0； i = 2，前面的字符串为AB，其最长相同真前后缀长度为0，即next[2] = 0； i = 3，前面的字符串为ABC，其最长相同真前后缀长度为0，即next[3] = 0； i = 4，前面的字符串为ABCD，其最长相同真前后缀长度为0，即next[4] = 0； i = 5，前面的字符串为ABCDA，其最长相同真前后缀为A，即next[5] = 1； i = 6，前面的字符串为ABCDAB，其最长相同真前后缀为AB，即next[6] = 2； i = 7，前面的字符串为ABCDABD，其最长相同真前后缀长度为0，即next[7] = 0。 那么，为什么根据最长相同真前后缀的长度就可以实现在不匹配情况下的跳转呢？举个代表性的例子：假如i = 6时不匹配，此时我们是知道其位置前的字符串为ABCDAB，仔细观察这个字符串，首尾都有一个AB，既然在i = 6处的D不匹配，我们为何不直接把i = 2处的C拿过来继续比较呢，因为都有一个AB啊，而这个AB就是ABCDAB的最长相同真前后缀，其长度2正好是跳转的下标位置。 有的读者可能存在疑问，若在i = 5时匹配失败，按照我讲解的思路，此时应该把i = 1处的字符拿过来继续比较，但是这两个位置的字符是一样的啊，都是B，既然一样，拿过来比较不就是无用功了么？其实不是我讲解的有问题，也不是这个算法有问题，而是这个算法还未优化，关于这个问题在下面会详细说明，不过建议读者不要在这里纠结，跳过这个，下面你自然会恍然大悟。 思路如此简单，接下来就是代码实现了，如下： 1234567891011121314151617181920/* P 为模式串，下标从 0 开始 */void GetNext(string P, int next[])&#123; int p_len = P.size(); int i = 0; // P 的下标 int j = -1; next[0] = -1; while (i &lt; p_len - 1) &#123; if (j == -1 || P[i] == P[j]) &#123; i++; j++; next[i] = j; &#125; else j = next[j]; &#125;&#125; 一脸懵逼，是不是。。。上述代码就是用来求解模式串中每个位置的next[]值。 下面具体分析，我把代码分为两部分来讲： （1）：i和j的作用是什么？ i和j就像是两个”指针“，一前一后，通过移动它们来找到最长的相同真前后缀。 （2）：if…else…语句里做了什么？ 假设i和j的位置如上图，由next[i] = j得，也就是对于位置i来说，区段[0, i - 1]的最长相同真前后缀分别是[0, j - 1]和[i - j, i - 1]，即这两区段内容相同。 按照算法流程，if (P[i] == P[j])，则i++; j++; next[i] = j;；若不等，则j = next[j]，见下图： next[j]代表[0, j - 1]区段中最长相同真前后缀的长度。如图，用左侧两个椭圆来表示这个最长相同真前后缀，即这两个椭圆代表的区段内容相同；同理，右侧也有相同的两个椭圆。所以else语句就是利用第一个椭圆和第四个椭圆内容相同来加快得到[0, i - 1]区段的相同真前后缀的长度。 细心的朋友会问if语句中j == -1存在的意义是何？第一，程序刚运行时，j是被初始为-1，直接进行P[i] == P[j]判断无疑会边界溢出；第二，else语句中j = next[j]，j是不断后退的，若j在后退中被赋值为-1（也就是j = next[0]），在P[i] == P[j]判断也会边界溢出。综上两点，其意义就是为了特殊边界判断。 完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;/* P 为模式串，下标从 0 开始 */void GetNext(string P, int next[])&#123; int p_len = P.size(); int i = 0; // P 的下标 int j = -1; next[0] = -1; while (i &lt; p_len - 1) &#123; if (j == -1 || P[i] == P[j]) &#123; i++; j++; next[i] = j; &#125; else j = next[j]; &#125;&#125;/* 在 S 中找到 P 第一次出现的位置 */int KMP(string S, string P, int next[])&#123; GetNext(P, next); int i = 0; // S 的下标 int j = 0; // P 的下标 int s_len = S.size(); int p_len = P.size(); while (i &lt; s_len &amp;&amp; j &lt; p_len) &#123; if (j == -1 || S[i] == P[j]) // P 的第一个字符不匹配或 S[i] == P[j] &#123; i++; j++; &#125; else j = next[j]; // 当前字符匹配失败，进行跳转 &#125; if (j == p_len) // 匹配成功 return i - j; return -1;&#125;int main()&#123; int next[100] = &#123; 0 &#125;; cout &lt;&lt; KMP("bbc abcdab abcdabcdabde", "abcdabd", next) &lt;&lt; endl; // 15 return 0;&#125; KMP优化 i 0 1 2 3 4 5 6 7 模式串 A B C D A B D ‘\0’ next[ i ] -1 0 0 0 0 1 2 0 以3.2的表格为例（已复制在上方），若在i = 5时匹配失败，按照3.2的代码，此时应该把i = 1处的字符拿过来继续比较，但是这两个位置的字符是一样的，都是B，既然一样，拿过来比较不就是无用功了么？这我在3.2已经解释过，之所以会这样是因为KMP不够完美。那怎么改写代码就可以解决这个问题呢？很简单。 123456789101112131415161718192021222324/* P 为模式串，下标从 0 开始 */void GetNextval(string P, int nextval[])&#123; int p_len = P.size(); int i = 0; // P 的下标 int j = -1; nextval[0] = -1; while (i &lt; p_len - 1) &#123; if (j == -1 || P[i] == P[j]) &#123; i++; j++; if (P[i] != P[j]) nextval[i] = j; else nextval[i] = nextval[j]; // 既然相同就继续往前找真前缀 &#125; else j = nextval[j]; &#125;&#125; 在此也给各位读者提个醒，KMP算法严格来说分为KMP算法（未优化版）和KMP算法（优化版），所以建议读者在表述KMP算法时，最好告知你的版本，因为两者在某些情况下区别很大，这里简单说下。 KMP算法（未优化版）： next数组表示最长的相同真前后缀的长度，我们不仅可以利用next来解决模式串的匹配问题，也可以用来解决类似字符串重复问题等等，这类问题大家可以在各大OJ找到，这里不作过多表述。 KMP算法（优化版）： 根据代码很容易知道（名称也改为了nextval），优化后的next仅仅表示相同真前后缀的长度，但不一定是最长（称其为“最优相同真前后缀”更为恰当）。此时我们利用优化后的next可以在模式串匹配问题中以更快的速度得到我们的答案（相较于未优化版），但是上述所说的字符串重复问题，优化版本则束手无策。 所以，该采用哪个版本，取决于你在现实中遇到的实际问题。 参考文献 严蔚敏. 数据结构（C语言版） 阮一峰. 字符串匹配的KMP算法]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>KMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-85:Maximal Rectangle(最大矩阵面积)]]></title>
    <url>%2F2019%2F07%2F20%2Fleetcode-85%2F</url>
    <content type="text"><![CDATA[题目链接：https://blog.csdn.net/tkzc_csk/article/details/88096438 题目描述 题目难度:Hard Given a 2D binary matrix filled with 0’s and 1’s, find the largest rectangle containing only 1’s and return its area. Example: Input: [[“1”,“0”,“1”,“0”,“0”],[“1”,“0”,“1”,“1”,“1”],[“1”,“1”,“1”,“1”,“1”],[“1”,“0”,“0”,“1”,“0”]] Output: 6 AC代码参考自:https://leetcode.com/problems/maximal-rectangle/discuss/231921/Simple-Java-Solution-based-on-84.-Largest-Rectangle-in-Histogram 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Solution &#123; // 首先，也是用dp做：heights保存某一列从上数第i行的连续的'1'的个数； // 然后使用84的逻辑，取连续区间最大的矩形大小就可以了； public int maximalRectangle(char[][] matrix) &#123; if (matrix == null || matrix.length == 0 || matrix[0] == null || matrix[0].length == 0) return 0; int width = matrix[0].length, res = 0; int[] heights = new int[width]; for (char[] row : matrix) &#123; for (int c = 0; c &lt; width; c++) &#123; if (row[c] == '1') heights[c]++; else heights[c] = 0; &#125; res = Math.max(res, largestRectangleArea(heights)); &#125; return res; &#125; // 以下部分是第84题的解法：分治求最大矩形（不需要修改） // 思路：对于每一段区间，都存在一个最小值 // 对于最小值，无非就是三种可能： // 1：要么整段面积最大，2、3：要么最小值左边或者最小值右边（均不包含最小值）的面积最大，采用分治法递归解决； // 遇到有序排列的区间，采用递归会降低效率，于是只要单独计算并且比较就可以 public int largestRectangleArea(int[] heights) &#123; return largestRect(heights, 0, heights.length - 1); &#125; private int largestRect(int[] heights, int start, int end) &#123; if (start &gt; end) return 0; if (start == end) return heights[start]; int minIndex = start; // 使用是否有序排列的变量可以显著提高效率 // 这里可以检测双向（变大或者变小的顺序） int inc = 0, dec = 0; for (int i = start + 1; i &lt;= end; i++) &#123; if (heights[i] &lt; heights[minIndex]) minIndex = i; if (heights[i] &gt; heights[i - 1]) inc++; // 升序 else if (heights[i] &lt; heights[i - 1]) dec--; // 降序 &#125; int res = 0; // 升序 if (dec == 0) &#123; for (int i = start; i &lt;= end; i++) res = Math.max(res, heights[i] * (end - i + 1)); &#125; // 降序 else if (inc == 0) &#123; for (int i = start; i &lt;= end; i++) res = Math.max(res, heights[i] * (i - start + 1)); &#125; // 无序 else &#123; res = Math.max(Math.max(largestRect(heights, minIndex + 1, end), largestRect(heights, start, minIndex - 1)), heights[minIndex] * (end - start + 1)); &#125; return res; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-84:Largest Rectangle in Histogram]]></title>
    <url>%2F2019%2F07%2F20%2Fleetcode-84%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/largest-rectangle-in-histogram/ 题目描述 题目难度：Hard Given n non-negative integers representing the histogram’s bar height where the width of each bar is 1, find the area of largest rectangle in the histogram. Above is a histogram where width of each bar is 1, given height = [2,1,5,6,2,3]. Example: Input: [2,1,5,6,2,3]Output: 10 AC代码参考自:https://leetcode.com/problems/largest-rectangle-in-histogram/discuss/225556/Java-solution-with-explanations-in-Chinese S1:双重遍历法本题要求的是一段连续的矩形，能够组成的面积最大的矩形的面积，所以，只要能够求出这一段矩形的位置的就可以了。最简单的想法，将所有的可能的组合都求解一遍，比较之后保留最大的那一个就好了，如下： 12345678910111213public static int largestRectangleArea(int[] heights) &#123; int max = 0; for (int i = 0; i &lt; heights.length; i++) &#123; int min = heights[i]; for (int j = i; j &gt;= 0; j--) &#123; if (heights[j] &lt; min) &#123; min = heights[j]; &#125; max = Math.max(max, min * (i-j+1)); &#125; &#125; return max;&#125; 通过一个双重遍历，外层遍历矩形的结束地址，内层遍历矩形起始地址，通过求算每一个组合的最大组成面积求解。时间复杂度为O(n^2)。 S2:分治法上面的方法我们是依据矩形的起始、结束位置进行分类、求算，那么我们还可以使用另一种方法求算：按照组合矩阵的高度从低到高求解，然后取最大。我们知道，在给定的一组矩阵中，如果取高度为最低的那一个，那么宽度必然可取数组的长度。如，给定的一个数组[2,1,2]，那么当我们取高度为 1 ，那么宽度必然可取 3 。此时组合矩阵的面积是 3 。这时，组合矩阵取得是最低高度 1 。那么接下来，如果取一个更高的高度，那么这个高度为 1 的小矩阵必然不会包含在内，所以下一步要组成的矩阵必然在这个 1 的左边或者右边，这样一来，我们就可以看作，这个高度为 1 的小矩阵将整个矩阵序列分割成了两部分，我们下一步要判断的组合矩阵就在左部或者右部，如此一下，我们只需要求左右两部中能够组成的最大面积就好了，于是，一个数组[2,1,2]被最小值1分割成了两个数组[2]和[2]，如此一来，求[2,1,2]的解的问题变成了求[2]的解，而[2]的解只有一个 2 ，以此类推，我们就可以将任意一个大的问题分解成若干个小的问题，然后在这几个小问题中求最大值即可。代码如下： 12345678910111213141516171819202122public static int largestRectangleArea2(int[] heights) &#123; return largest(heights, 0, heights.length-1);&#125;public static int largest(int[] heights, int start, int end) &#123; if (start &gt; end) return 0; if (start == end) return heights[start]; boolean sorted = true; int min = start; for (int i = start+1; i &lt;= end; i++) &#123; if (heights[i] &lt; heights[i-1]) sorted = false; if (heights[i] &lt; heights[min]) min = i; &#125; if (sorted) &#123; int max = heights[start] * (end - start + 1); for (int i = start+1; i &lt;= end; i++) &#123; max = Math.max(max, heights[i] * (end - i + 1)); &#125; return max; &#125; return Math.max(Math.max(largest(heights, start, min-1), largest(heights, min+1, end)), heights[min] * (end - start + 1));&#125; 对于一个数组而言，可能成为最大值的解有三个： 最小值左部的某个解 最小值右部的某个解 包含当前最小值的解 另外，上面的方法中，通过判断当前的子数组是否是一个有序数组，来简化这个数组的判断，上面仅判断了由小到大的顺序，还可以通过判断是否是一个由大到小的顺序进一步提高判断效率。此算法的时间复杂度为O(n*log n)。 S3:利用栈​ 第一种方法的外层遍历，是组合矩阵的结束位置，然后在内层逐个遍历组合矩阵的开始位置。通过对这样一个模型的分析，对于以位置 i 结束的组合矩阵来说，它与当前位置之前的矩阵的高度有一定关系：如果 i 位置的高度大于 i-1 位置的高度，则在与这个位置之前的矩阵的组合中，不能以当前位置的高度作为组合矩阵的高度。如数组[1,2]，那么对于位置 2 来说，这两个位置的组合不能以 2 作为组合矩阵高度。如果 i 位置的高度不大于 i-1 位置的高度，则存在高度为当前高度的组合矩阵。如数组[2，1]，对于位置 1 ，有一种组合方法[1,1]，所以在这种情况下应该判断一下。​ 另外，我们知道，对于一个高度递增的数组来说，很容易求出其组合矩阵的最大面积，那么是否可以将任意一个矩阵转化为一个高度递增的矩阵序列？如将一个数组[2,3,1,3,5]变成[1,1,1,3,5]，但是这个过程中对 2 作了改变，我们要通过一定办法弥补这里的改变，一种方法是，在作改变前，将其能够组成的组合矩阵的最大面积记录下来，完了之后便可以对其做出改变了。对于上面那个例子，实际上我们是对两个递增的数组求解：[2,3]和[1,1,1,3,5]，而它们之间的最大解必然与[2,3,1,3,5]的解相同。于是，将一个不规则数组转化为若干有序数组，再进行求解，便是本方法的思想。下面是利用栈的一种实现，利用栈的话不需要对原数据进行修改，而是直接将原数组截取成多个有序数组，通过保存索引值确定当前的组合矩阵的宽度： 123456789101112131415public static int largestRectangleArea3(int[] heights) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int n = heights.length; int max = 0; for (int i = 0; i &lt;= n; i++) &#123; while (!stack.empty() &amp;&amp; (i &gt;= n || heights[stack.peek()] &gt; heights[i])) &#123; int top = stack.pop(); int h = heights[top]; int w = stack.empty() ? i : (i-stack.peek()-1); max = Math.max(max, h*w); &#125; stack.push(i); &#125; return max;&#125; S4:最长宽度法我们知道，由多个矩阵组成的组合矩阵，其宽度为矩阵的数量，高度为矩阵中高度最低的那一个。所以，对于任意一个位置的矩阵，假设此时的组合矩阵高度为当前位置矩阵的高度，我们只需要求出这时的组合矩阵的最大宽度，便可以求出以当前矩阵的高度为高度的组合矩阵的最大面积，那么我们只需要求出每一个位置能够形成的最大解，便能够得到本题的解。要求出某个位置的最大解，如 i ，可以这样理解，从数组[0,…,n]中找到最大的一段[start,…,i,…end]，并且在这段中，位置 i 处的高度最低，那么此时这段矩阵组成的组合矩阵的面积必然是(end-start) * heights[i]。所以我们要在位置 i 的左边找到一段[start,…,i]，是的这里面每一个矩阵的高度都大于等于 i 处的高度，同理，还要在 i 的右边找到一段[i,…end]，找到这些之后，位置 i 处的解就可以求得了。所以，对于每一个位置，只要找到在 i 的左边第一个小于 i 处高度的位置 start ，再找到 i 的右边第一个小于 i 处高度的位置 end 便可。而这个数据可以通过两个遍历求解。 123456789101112131415161718192021222324252627282930313233343536public static int largestRectangleArea4(int[] heights) &#123; int n = heights.length; if (n == 0) return 0; int[] leftLess = new int[n], rightLess = new int[n]; // find left leftLess[0] = 0; for (int i = 1; i &lt; n; i++) &#123; int p = i-1; while (true) &#123; if (p &gt;= 0 &amp;&amp; heights[i] &lt;= heights[p]) &#123; p -= leftLess[p] + 1; &#125; else &#123; leftLess[i] = i - p - 1; break; &#125; &#125; &#125; // find right rightLess[n-1] = 0; for (int i = n-2; i &gt;= 0; i--) &#123; int p = i+1; while (true) &#123; if (p &lt; n &amp;&amp; heights[i] &lt;= heights[p]) &#123; p += rightLess[p] + 1; &#125; else &#123; rightLess[i] = p - 1 - i; break; &#125; &#125; &#125; int max = 0; for (int i = 0; i &lt; n; i++) &#123; max = Math.max(max, heights[i] * (leftLess[i] + rightLess[i] + 1)); &#125; return max;&#125; 使用一个数组 leftLess 保存每个位置的左边高度大于当前位置高度的矩阵数量，rightLess 同理。具体的求解方法为：对于位置 i ，如果 i-1 处的高度大于 i 处的高度，那么因为leftLess[i-1]保存的是大于位置 i-1 处高度的数量，那么这些位置的高度必然也大于位置 i 处的高度，所以，我们可以直接跳过1 + leftLess[i-1]个位置，判断下一个位置 p 处的高度是否大于 i 处的高度，如果还是大于，那么还要继续跳过leftLess[p] + 1个位置，再判断…直到 p 处的高度小于 i 处的高度，此时在 i 的左边比 i 处高度大的矩阵的数量就是i - p - 1个。对于 rightLess 的求解同理。]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆排序Java实现]]></title>
    <url>%2F2019%2F07%2F20%2FheapSort%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425private static void heapSort(int[] array)&#123; for (int i = (array.length) / 2; i &gt;= 0; i--) &#123; heapAdjust(array, i, array.length); &#125; //调整堆,将最大的节点放在堆尾，然后从根节点重新调整 for (int i = array.length - 1; i &gt;= 0; i--) &#123; int temp = array[0]; array[0] = array[i]; array[i] = temp; heapAdjust(array, 0, i); &#125; &#125; private static void heapAdjust(int[] array, int s, int len) &#123; int temp, i; temp = array[s]; for(i = 2 * s; i &lt; len; i *= 2)&#123; if(i &lt; len - 1 &amp;&amp; array[i] &lt; array[i + 1]) ++i; if(temp &gt;= array[i]) break; array[s] = array[i]; s = i; &#125; array[s] = temp; &#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-机器人的运动范围]]></title>
    <url>%2F2019%2F07%2F19%2Fjianzhioffer-robot-motion-range%2F</url>
    <content type="text"><![CDATA[题目描述地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ AC代码典型的BFS就可以解决 1234567891011121314151617181920212223242526272829303132public class Solution &#123; int count = 0; //保存最终的结果，即可以到达多少个格子 public int movingCount(int threshold, int rows, int cols) &#123; boolean[][] seen = new boolean[rows][cols]; //判断是否被遍历过 backtrack(threshold, rows, cols, seen, 0, 0); //从 [0][0] 开始遍历 return count; &#125; //direction int[] dirx = &#123;0, 0, -1, 1&#125;; int[] diry = &#123;1, -1, 0, 0&#125;; private void backtrack(int threshold, int rows, int cols, boolean[][] seen, int i, int j)&#123; if(i &lt; 0 || i &gt;= rows || j &lt; 0 || j &gt;= cols || seen[i][j] || (sumBit(i) + sumBit(j)) &gt; threshold) return; seen[i][j] = true; count++; //从当前节点开始，依次遍历上下左右四个方向（方向的顺序对结果没有影响） for(int k = 0;k &lt; 4;k++) backtrack(threshold, rows, cols, seen, i + dirx[k], j + diry[k]); &#125; private int sumBit(int i)&#123; int sum = 0; while(i != 0)&#123; sum += i % 10; i /= 10; &#125; return sum; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-矩阵中的路径]]></title>
    <url>%2F2019%2F07%2F19%2Fjianzhioffer-path-in-the-matrix%2F</url>
    <content type="text"><![CDATA[题目描述请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 解题思路其实这个题不难的，典型的回溯法就可以解决：从数组某个点出发，判断是否和路径首字母相同。如果相同的话，则依次遍历这个点的上下左右四个方向是否和路径的第二个字母相同，相同的话继续。不同的话，回溯到上个节点。 AC代码1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Solution &#123; //matrix是用一维数组表示的二维数组，没有看清楚 public boolean hasPath(char[] matrix, int rows, int cols, char[] str) &#123; boolean[] seen = new boolean[matrix.length]; //记录数组各元素有没有被遍历，题目要求每个元素最多出现一次。 for(int i = 0;i &lt; rows;i++)&#123; for(int j = 0;j &lt; cols;j++)&#123; //每个元素都可能符合要求 if(backtrack(matrix, rows, cols, str, seen, i, j, 0) == true) return true; &#125; &#125; return false; &#125; //方向：下上左右 int[] dirx = &#123;0, 0, -1, 1&#125;; int[] diry = &#123;1, -1, 0, 0&#125;; /** 判断从 i 行 j 列元素开始是否存在一条路径和题目给出的路径中的第 curr 位开始到结束相同 matrix:原始一维数组 rows:一维数字的行 cols:一维数字的列 str:题目给出的路径 seen:matrix数组中每个元素是否被遍历过 i:当前元素的行 j:当前元素的列 curr:题目给出路径的下标 */ private boolean backtrack(char[] matrix, int rows, int cols, char[] str, boolean[] seen, int i, int j, int curr)&#123; int idx = i * cols + j; //i行j列元素在数组中的下标 if(curr == str.length) return true; //判断完毕，说明存在一条路径 if(i &lt; 0 || i &gt;= rows || j &lt; 0 || j &gt;= cols || seen[idx] || matrix[idx] != str[curr]) return false; boolean res = false; seen[idx] = true; curr++; for(int k = 0;k &lt; 4;k++)&#123; //判断当前元素上下左右四个方向是否和题目给出路径的curr + 1 为相同 res |= backtrack(matrix, rows, cols, str, seen, i + dirx[k], j + diry[k], curr); //注意，这里不能用++curr ***** &#125; seen[idx] = false; //注意:这个代码不可以放在上一个for里面 return res; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-滑动窗口的最大值]]></title>
    <url>%2F2019%2F07%2F19%2Fjianzhioffer-sliding-window-max%2F</url>
    <content type="text"><![CDATA[题目描述给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个：{[2,3,4],2,6,2,5,1}，{2,[3,4,2],6,2,5,1}，{2,3,[4,2,6],2,5,1}，{2,3,4,[2,6,2],5,1}，{2,3,4,2,[6,2,5],1}，{2,3,4,2,6,[2,5,1]} AC代码1234567891011121314151617181920212223242526272829303132import java.util.ArrayList;import java.util.LinkedList;//ArrayDeque, LinkedBlockingDeque, LinkedList 均继承自 Deque （double ended queue）， 均实现了双端队列 public class Solution &#123;/**用一个双端队列，队列第一个位置保存当前窗口的最大值元素对应的下标，当窗口滑动一次1.判断当前最大值是否过期,过期的话直接去掉最大值元素对应的下标2.新增加的值从队尾开始比较，把所有比他小的值丢掉*/public ArrayList&lt;Integer&gt; maxInWindows(int [] num, int size) &#123; ArrayList&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(size == 0) return res; int begin; LinkedList&lt;Integer&gt; q = new LinkedList&lt;&gt;(); for(int i = 0; i &lt; num.length; i++)&#123; begin = i - size + 1; if(q.isEmpty()) q.add(i); else if(begin &gt; q.peekFirst()) //不好理解的话可以翻译过来： i - size + 1 &gt; q.peekFirst() ,即 i - q.peekFirst() + 1&gt; size，因为队首保存的是最大元素的下标， i 与队首最大元素的下标差值大于 size，说明包含下标 i 的数字所在的窗口中没有目前队首保存的最大元素。所以要把当前最大元素删除。 q.pollFirst(); while((!q.isEmpty()) &amp;&amp; num[q.peekLast()] &lt;= num[i]) q.pollLast(); q.add(i); if(begin &gt;= 0) //满足一个窗口的大小 res.add(num[q.peekFirst()]); &#125; return res; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-460:LFU Cache]]></title>
    <url>%2F2019%2F07%2F19%2Fleetcode-460%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/lfu-cache/ 题目描述 题目难度：Hard Design and implement a data structure for Least Frequently Used (LFU) cache. It should support the following operations: get and put. get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.put(key, value) - Set or insert the value if the key is not already present. When the cache reaches its capacity, it should invalidate the least frequently used item before inserting a new item. For the purpose of this problem, when there is a tie (i.e., two or more keys that have the same frequency), the least recently used key would be evicted. Follow up:Could you do both operations in O(1) time complexity? Example: 123456789101112LFUCache cache = new LFUCache( 2 /* capacity */ );cache.put(1, 1);cache.put(2, 2);cache.get(1); // returns 1cache.put(3, 3); // evicts key 2cache.get(2); // returns -1 (not found)cache.get(3); // returns 3.cache.put(4, 4); // evicts key 1.cache.get(1); // returns -1 (not found)cache.get(3); // returns 3cache.get(4); // returns 4 AC代码代码来自leetcode 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class LFUCache &#123; //存储cache key-value对 HashMap&lt;Integer, Integer&gt; vals; //存储每个 key 的访问频率 HashMap&lt;Integer, Integer&gt; counts; //存储访问频率对应的 key 集合(保持 key 的插入顺序) HashMap&lt;Integer, LinkedHashSet&lt;Integer&gt;&gt; lists; int cap; int min = -1; public LFUCache(int capacity) &#123; cap = capacity; vals = new HashMap&lt;&gt;(); counts = new HashMap&lt;&gt;(); lists = new HashMap&lt;&gt;(); lists.put(1, new LinkedHashSet&lt;&gt;()); &#125; public int get(int key) &#123; if(!vals.containsKey(key)) return -1; int count = counts.get(key); counts.put(key, count+1); lists.get(count).remove(key); if(count==min &amp;&amp; lists.get(count).size()==0) min++; if(!lists.containsKey(count+1)) lists.put(count+1, new LinkedHashSet&lt;&gt;()); lists.get(count+1).add(key); return vals.get(key); &#125; public void put(int key, int value) &#123; if(cap&lt;=0) return; if(vals.containsKey(key)) &#123; vals.put(key, value); get(key); return; &#125; if(vals.size() &gt;= cap) &#123; int evit = lists.get(min).iterator().next(); lists.get(min).remove(evit); vals.remove(evit); &#125; vals.put(key, value); counts.put(key, 1); min = 1; lists.get(1).add(key); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-栈的压入、弹出序列]]></title>
    <url>%2F2019%2F07%2F19%2Fjianzhioffer-push-pop-sequence%2F</url>
    <content type="text"><![CDATA[题目描述输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的。) 解题思路借用一个辅助的栈，遍历压栈顺序，先将第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。 举例： 入栈1,2,3,4,5 出栈4,5,3,2,1 首先1入辅助栈，此时栈顶1≠4，继续入栈2 此时栈顶2≠4，继续入栈3 此时栈顶3≠4，继续入栈4 此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3 此时栈顶3≠5，继续入栈5 此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3 …. 依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 AC代码123456789101112131415161718import java.util.Stack;public class Solution &#123; public boolean IsPopOrder(int [] pushA,int [] popA) &#123; if(pushA == null || popA == null || pushA.length != popA.length) return false; int popAIdx = 0; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); for(int i = 0;i &lt; pushA.length;i++)&#123; stack.push(pushA[i]); while(!stack.empty() &amp;&amp; stack.peek() == popA[popAIdx])&#123; stack.pop(); popAIdx++; &#125; &#125; if(stack.empty()) return true; return false; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-包含min函数的栈]]></title>
    <url>%2F2019%2F07%2F19%2Fjianzhioffer-contain-min-stack%2F</url>
    <content type="text"><![CDATA[题目描述定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 AC代码1234567891011121314151617181920212223242526272829303132import java.util.Stack;public class Solution &#123; private Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); private Stack&lt;Integer&gt; minStack = new Stack&lt;&gt;(); public void push(int node) &#123; stack.push(node); if(minStack.isEmpty()) &#123; minStack.push(node); return; &#125; if(node &lt;= min())&#123; minStack.push(node); &#125; &#125; public void pop() &#123; int node = stack.pop(); if(node == min()) minStack.pop(); &#125; public int top() &#123; return stack.peek(); &#125; public int min() &#123; return minStack.peek(); &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树前序中序后序遍历的非递归写法]]></title>
    <url>%2F2019%2F07%2F19%2Fbinary-tree-traversal-non-recursive%2F</url>
    <content type="text"><![CDATA[二叉树前序中序后序遍历的递归遍历非常简单，这里就写一下非递归的方法。核心思路是把每一个结点看成父节点，叶子结点是左右孩子是null的父结点。 前序遍历(中左右)思路使用一个栈来存储结点，以便回到之前的父结点。 不断往左子树深入并不断先打印值再入栈直到左叶子的空左孩子 弹出栈顶，将指针指向它的右孩子 循环1,2步骤直至栈为空且指针也为空（意思是当栈为空但指针不为空时，继续。比如当栈顶元素是二叉树的根节点时，就会出现这种情况，此时指针指向根结点的右孩子，但是栈为空） 代码123456789101112131415void preOrder(TreeNode root)&#123; if(root == null) return; Stack&lt;TreeNode&gt; s = new Stack&lt;&gt;(); while (!s.isEmpty() || root != null)&#123; while(root != null)&#123; System.out.print(root.val + " "); s.push(root); root = root.left; &#125; if(!s.isEmpty())&#123; TreeNode t = s.pop(); root = t.right; &#125; &#125;&#125; 中序遍历(左中右)思路 不断往左子树深入并不断入栈直到左叶子的空左孩子 弹出栈顶，打印值，并将指针指向它的右孩子 循环1,2步骤直至栈为空且指针也为空 代码123456789101112131415 void inOrder(TreeNode root)&#123; if(root == null) return; Stack&lt;TreeNode&gt; s = new Stack&lt;&gt;(); while (!s.isEmpty() || root != null)&#123; while(root != null)&#123; s.push(root); root = root.left; &#125; if(!s.isEmpty())&#123; TreeNode t = s.pop(); System.out.print(t.val + " "); root = t.right; &#125; &#125;&#125; 后续遍历(左右中)思路后续遍历稍微复杂一些，因为当左孩子完成打印并从栈中弹出父结点的时候，此时需要判断右孩子需不需要打印，有两种情况： 如果右孩子为空，或者右孩子已经完成了打印，则打印当前的结点 如果右孩子未打印过，则需要将右孩子入栈 这里设置一个指针last来标记上一次打印的结点，这样只要判断last是不是右孩子就知道右孩子打印过没有了 具体步骤为： 不断往左子树深入并不断入栈直到左叶子的空左孩子 弹出栈顶，如果右孩子为null或者last是右孩子，则打印当前值；如果不是，则将指针指向右孩子 循环1,2步骤直至栈为空且指针也为空 代码123456789101112131415161718192021void postOrder(TreeNode root)&#123; if(root == null) return; Stack&lt;TreeNode&gt; s = new Stack&lt;&gt;(); TreeNode last = null; while (!s.isEmpty() || root!=null)&#123; while (root != null)&#123; s.push(root); root = root.left; &#125; if(!s.isEmpty())&#123; TreeNode t = s.pop(); if(t.right == null || last == t.right)&#123;//在这里面打印t并处理last之后，并不用处理root，因为之所以能进入这里，是因为root一定等于null，所以下一轮循环一定还能进入这里，然后弹出t的父结点做处理 System.out.print(t.val + " "); last = t; &#125;else&#123;//右孩子还没有打印过 s.push(t);//因为当前结点未打印，所以要重新放回去，等右孩子打印完之后回来打印 root = t.right; &#125; &#125; &#125;&#125; 本文转载自：二叉树前序中序后序遍历的非递归写法（Java）]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HTTP之长连接和短连接]]></title>
    <url>%2F2019%2F07%2F18%2Fhttp-long-connection-short-connection%2F</url>
    <content type="text"><![CDATA[本文转载自：面试-长链接与短链接 前言 长链接和短链接又分别称为持续链接和非持续链接 Web页面上通常有很多对象。有些对象较小，比如HTML文本或图标；有些对象却很大，如视频文件。对于众多小的文件，如果它们的所有请求以及响应都经过相同的TCP连接来发送，其工作效率就会提高。这种将多个请求/响应对经同一个TCP链接进行传送的方式被称为持续连接。此时，又可以分为两种情况：无流水线的持续链接和有流水线的持续链接。对于前者，仅当前面的响应已经收到时，客户机才会发出新的请求，这使每个引用对象都会引入一个RTT时延。对于后者，只要客户机遇到一个引用对象，他就发送请求，对于所有的引用的对象，只引入一个RTT时延。这种流水线的持续连接是HTTP/1.1的默认状况。 而对于很大的对象，如果每个请求/响应都单独经过一条TCP连接发送，用户的感受将会更好。这种将每个请求/响应都经过一个单独的TCP连接进行发送的方式被称为非持续连接。尽管HTTP客户机和服务器在其默认方式下均使用持续连接方式，但也能将他们的配置成使用非持续连接方式。 表层-HTTP长链接与锻炼的区别是头域：Connection。 在http1.0及之前都是默认方式为非持久性链接（关键词未知查不到抱歉！）。在HTTP1.1之后就将链接默认为Keep-Alive表示是持久性链接。当链接想要关闭的话就将状态字设置为close。 题外话，这个头域还有一个作用是可以控制不再转发给代理。 他的根源是控制tcp的链接状态。 一个详细的HTTP请求： 123456789101112浏览器与服务器之间使用持续链接方式，将出现下列情况：（1）HTTP客户机进程发起一个到服务器URL的TCP连接，连接服务器端口是80，这使客户机和服务器上分别有一个套接字与该链接相关联。（2）HTTP客户机经过它的套接字接受该请求报文，请求报文中包含了路径名/home.index。（3）HTTP服务器进程经过他的套接字接受该请求报文，从其存储器（RAM或磁盘）中检索出对象/home.index；在一个HTTP响应报文中封装该对象，并通过其套接字向客户姐发送响应报文。（4）HTTP客户机接收响应报文，分析该报文并指出封装的对象是一个HTML文件，并得到对8个JPEG图形和5个视频的共13个引用。（5）先传送基本的HTML文件，然后这13个引用对象通过相同的TCP连接一个接一个地传送（流水线技术），直至该页面上的对象全部传输完毕。其中若传输每个JPEG图形文件用时tj,传输每个视频用时tv，并且忽略了HTML文件的传输时间，则共用时8tj+5tv, 且tv&gt;&gt;tj。（6）若一条连接经过一定时间间隔（一个可配置的超时间隔）仍未被使用，则HTTP服务器进程通知TCP断开。 在上述例子中，如果使用了非持续性连接方式，那么每个TCP连接在服务器发送一个对象后关闭，即该连接并不为其他的对象持续下来。这样一共产生了14个TCP连接。 持久化的链接的好处在于减少了tcp链接的重复建立和断开所造成的额外开销，减轻了服务器的负载。另外，减少开销的那部分时间，使http请求和响应能够更早的结束，这样web的显示速度也就相应的提高了。 采用持久化的原因是现在的页面越来越大！html中的内容需要发送多次resquest来进行接收，每次都断开tcp链接都会产生明显的开销。 持久化链接固然好，但是我们要知道一个系统的链接数目是有限的。我们可以通过ulimit -n可以查看最大链接数。 本质-TCP短连接我们模拟一下TCP短连接的情况，client向server发起连接请求，server接到请求，然后双方建立连接。client向server 发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起 close操作。为什么呢，一般的server不会回复完client后立即关闭连接的，当然不排除有特殊的情况。从上面的描述看，短连接一般只会在 client/server间传递一次读写操作。 短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段 长连接接下来我们再模拟一下长连接的情况，client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 先说一下TCP/IP详解上讲到的TCP保活功能，保活功能主要为服务器应用提供，服务器应用希望知道客户主机是否崩溃，从而可以代表客户使用资源。如果客户已经消失，使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，则服务器将应远等待客户端的数据，保活功能就是试图在服务器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一： 1.客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。 2.客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。 3.客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 4.客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。 链接的实现就是通过socket来进行通讯的 长连接短连接操作过程短连接的操作步骤是：建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接长连接的操作步骤是：建立连接——数据传输…（保持连接）…数据传输——关闭连接]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis主从复制原理]]></title>
    <url>%2F2019%2F07%2F18%2Fredis-master-slave-replication%2F</url>
    <content type="text"><![CDATA[本文转载自：深入Redis：详解 Redis主从复制的原理! 复制过程复制的过程步骤如下： 从节点执行 slaveof 命令 从节点只是保存了 slaveof 命令中主节点的信息，并没有立即发起复制 从节点内部的定时任务发现有主节点的信息，开始使用 socket 连接主节点 连接建立成功后，发送 ping 命令，希望得到 pong 命令响应，否则会进行重连 如果主节点设置了权限，那么就需要进行权限验证；如果验证失败，复制终止。 权限验证通过后，进行数据同步，这是耗时最长的操作，主节点将把所有的数据全部发送给从节点。 当主节点把当前的数据同步给从节点后，便完成了复制的建立流程。接下来，主节点就会持续的把写命令发送给从节点，保证主从数据一致性。 数据间的同步上面说的复制过程，其中有一个步骤是“同步数据集”，这个就是现在讲的‘数据间的同步’。 redis 同步有 2 个命令：sync 和 psync，前者是 redis 2.8 之前的同步命令，后者是 redis 2.8 为了优化 sync 新设计的命令。我们会重点关注 2.8 的 psync 命令。 psync 命令需要 3 个组件支持：1.主从节点各自复制偏移量2.主节点复制积压缓冲区3.主节点运行 ID 主从节点各自复制偏移量 12345678910参与复制的主从节点都会维护自身的复制偏移量。主节点在处理完写入命令后，会把命令的字节长度做累加记录，统计信息在 info replication 中的 master_repl_offset 指标中。从节点每秒钟上报自身的的复制偏移量给主节点，因此主节点也会保存从节点的复制偏移量。从节点在接收到主节点发送的命令后，也会累加自身的偏移量，统计信息在 info replication 中。通过对比主从节点的复制偏移量，可以判断主从节点数据是否一致。 主节点复制积压缓冲区 1234567复制积压缓冲区是一个保存在主节点的一个固定长度的先进先出的队列。默认大小 1MB。这个队列在 slave 连接是创建。这时主节点响应写命令时，不但会把命令发送给从节点，也会写入复制缓冲区。他的作用就是用于部分复制和复制命令丢失的数据补救。通过 info replication 可以看到相关信息。 主节点运行 ID 123456789101112每个 redis 启动的时候，都会生成一个 40 位的运行 ID。运行 ID 的主要作用是用来识别 Redis 节点。如果使用 ip+port 的方式，那么如果主节点重启修改了 RDB/AOF 数据，从节点再基于偏移量进行复制将是不安全的。所以，当运行 id 变化后，从节点将进行全量复制。也就是说，redis 重启后，默认从节点会进行全量复制。如果在重启时不改变运行 ID 呢？可以通过 debug reload 命令重新加载 RDB 并保持运行 ID 不变。从而有效的避免不必要的全量复制。他的缺点则是：debug reload 命令会阻塞当前 Redis 节点主线程，因此对于大数据量的主节点或者无法容忍阻塞的节点，需要谨慎使用。一般通过故障转移机制可以解决这个问题。 psync 命令的使用方式 1234命令格式为 `psync &#123;runId&#125; &#123;offset&#125;`runId : 从节点所复制主节点的运行 idoffset：当前从节点已复制的数据偏移量 psync 执行流程： 流程说明：从节点发送 psync 命令给主节点，runId 就是目标主节点的 ID，如果没有默认为 -1，offset 是从节点保存的复制偏移量，如果是第一次复制则为 -1. 主节点会根据 runid 和 offset 决定返回结果： 如果回复 +FULLRESYNC {runId} {offset} ，那么从节点将触发全量复制流程。 如果回复 +CONTINUE，从节点将触发部分复制。 如果回复 +ERR，说明主节点不支持 2.8 的 psync 命令，将使用 sync 执行全量复制。 到这里，数据之间的同步就讲的差不多了，篇幅还是比较长的。主要是针对 psync 命令相关之间的介绍。 全量复制全量复制是 Redis 最早支持的复制方式，也是主从第一次建立复制时必须经历的的阶段。触发全量复制的命令是 sync 和 psync。之前说过，这两个命令的分水岭版本是 2.8，redis 2.8 之前使用 sync 只能执行全量不同，2.8 之后同时支持全量同步和部分同步。 流程如下： 介绍一下上图步骤： 发送 psync 命令（sync ？ -1） 主节点根据命令返回 FULLRESYNC 从节点记录主节点 ID 和 offset 主节点 bgsave 并保存 RDB 到本地 主节点发送 RBD 文件到从节点 从节点收到 RDB 文件并加载到内存中 主节点在从节点接受数据的期间，将新数据保存到“复制客户端缓冲区”，当从节点加载 RDB 完毕，再发送过去。（如果从节点花费时间过长，将导致缓冲区溢出，最后全量同步失败） 从节点清空数据后加载 RDB 文件，如果 RDB 文件很大，这一步操作仍然耗时，如果此时客户端访问，将导致数据不一致，可以使用配置slave-server-stale-data 关闭. 从节点成功加载完 RBD 后，如果开启了 AOF，会立刻做 bgrewriteaof。 以上加粗的部分是整个全量同步耗时的地方。 注意： 如过 RDB 文件大于 6GB，并且是千兆网卡，Redis 的默认超时机制（60 秒），会导致全量复制失败。可以通过调大 repl-timeout 参数来解决此问题。 Redis 虽然支持无盘复制，即直接通过网络发送给从节点，但功能不是很完善，生产环境慎用。 部分复制当从节点正在复制主节点时，如果出现网络闪断和其他异常，从节点会让主节点补发丢失的命令数据，主节点只需要将复制缓冲区的数据发送到从节点就能够保证数据的一致性，相比较全量复制，成本小很多。 步骤如下： 当从节点出现网络中断，超过了 repl-timeout 时间，主节点就会中断复制连接。 主节点会将请求的数据写入到“复制积压缓冲区”，默认 1MB。 当从节点恢复，重新连接上主节点，从节点会将 offset 和主节点 id 发送到主节点 主节点校验后，如果偏移量的数后的数据在缓冲区中，就发送 cuntinue 响应 —— 表示可以进行部分复制 主节点将缓冲区的数据发送到从节点，保证主从复制进行正常状态。 心跳主从节点在建立复制后，他们之间维护着长连接并彼此发送心跳命令。 心跳的关键机制如下： 主从都有心跳检测机制，各自模拟成对方的客户端进行通信，通过 client list 命令查看复制相关客户端信息，主节点的连接状态为 flags = M，从节点的连接状态是 flags = S。 主节点默认每隔 10 秒对从节点发送 ping 命令，可修改配置 repl-ping-slave-period 控制发送频率。 从节点在主线程每隔一秒发送 replconf ack{offset} 命令，给主节点上报自身当前的复制偏移量。 主节点收到 replconf 信息后，判断从节点超时时间，如果超过 repl-timeout 60 秒，则判断节点下线。 注意：为了降低主从延迟，一般把 redis 主从节点部署在相同的机房/同城机房，避免网络延迟带来的网络分区造成的心跳中断等情况。 异步复制主节点不但负责数据读写，还负责把写命令同步给从节点，写命令的发送过程是异步完成，也就是说主节点处理完写命令后立即返回客户度，并不等待从节点复制完成。 异步复制的步骤很简单，如下： 主节点接受处理命令 主节点处理完后返回响应结果 对于修改命令，异步发送给从节点，从节点在主线程中执行复制的命令。 总结本文主要分析了 Redis 的复制原理，包括复制过程，数据之间的同步，全量复制的流程，部分复制的流程，心跳设计，异步复制流程。 其中，可以看出，RDB 数据之间的同步非常耗时。所以，Redis 在 2.8 版本退出了类似增量复制的 psync 命令，当 Redis 主从直接发生了网络中断，不会进行全量复制，而是将数据放到缓冲区（默认 1MB）里，在通过主从之间各自维护复制 offset 来判断缓存区的数据是否溢出，如果没有溢出，只需要发送缓冲区数据即可，成本很小，反之，则要进行全量复制，因此，控制缓冲区大小非常的重要。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux IO模式及 select、poll、epoll详解]]></title>
    <url>%2F2019%2F07%2F17%2Flinux-io-model%2F</url>
    <content type="text"><![CDATA[本文转载自：Linux IO模式及 select、poll、epoll详解 同步IO和异步IO，阻塞IO和非阻塞IO分别是什么，到底有什么区别？不同的人在不同的上下文下给出的答案是不同的。所以先限定一下本文的上下文。 1本文讨论的背景是Linux环境下的network IO。 概念说明在进行解释之前，首先要说明几个概念： 用户空间和内核空间 进程切换 进程的阻塞 文件描述符 缓存 I/O 用户空间与内核空间现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。**操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 进程切换为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。 注：总而言之就是很耗资源，具体的可以参考这篇文章：进程切换 进程的阻塞正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 文件描述符fd文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 缓存 I/O缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 IO模式刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。 阻塞 I/O（blocking IO）在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样： 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞 I/O（nonblocking IO）linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子： 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 I/O 多路复用（ IO multiplexing）IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。 所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 异步 I/O（asynchronous IO）inux下的asynchronous IO其实用得很少。先看一下它的流程： 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 总结blocking和non-blocking的区别调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 synchronous IO和asynchronous IO的区别在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。 有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。 而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 各个IO Model的比较如图所示： 通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。 I/O 多路复用之select、poll、epoll详解select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。（这里啰嗦下） select1int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用select后函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。 select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。 poll1int poll (struct pollfd *fds, unsigned int nfds, int timeout); 不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。 12345struct pollfd &#123; int fd; /* file descriptor */ short events; /* requested events to watch */ short revents; /* returned events witnessed */&#125;; pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 epollepoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 epoll操作过程epoll操作过程需要三个接口，分别如下： 123int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 1. int epoll_create(int size);创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。 当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；函数是对指定描述符fd执行op操作。 epfd：是epoll_create()的返回值。 op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。 fd：是需要监听的fd（文件描述符） epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下： 12345678910111213struct epoll_event &#123; __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */&#125;;//events可以是以下几个宏的集合：EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 3. int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);等待epfd上的io事件，最多返回maxevents个事件。参数events用来表示从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。 工作模式 epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下： LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 1. LT模式LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。 2. ET模式ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once) ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 3. 总结假如有这样一个例子： 我们已经把一个用来从管道中读取数据的文件句柄(RFD)添加到epoll描述符 这个时候从管道的另一端被写入了2KB的数据 调用epoll_wait(2)，并且它会返回RFD，说明它已经准备好读取操作 然后我们读取了1KB的数据 调用epoll_wait(2)…… LT模式：如果是LT模式，那么在第5步调用epoll_wait(2)之后，仍然能受到通知。 ET模式：如果我们在第1步将RFD添加到epoll描述符的时候使用了EPOLLET标志，那么在第5步调用epoll_wait(2)之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候 ET 工作模式才会汇报事件。因此在第5步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。 当使用epoll的ET模型来工作时，当产生了一个EPOLLIN事件后，读数据的时候需要考虑的是当recv()返回的大小如果等于请求的大小，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取： 1234567891011121314151617181920212223while(rs)&#123; buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0); if(buflen &lt; 0)&#123; // 由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读 // 在这里就当作是该次事件已处理处. if(errno == EAGAIN)&#123; break; &#125; else&#123; return; &#125; &#125; else if(buflen == 0)&#123; // 这里表示对端的socket已正常关闭. &#125; if(buflen == sizeof(buf)&#123; rs = 1; // 需要再次读取 &#125; else&#123; rs = 0; &#125;&#125; Linux中的EAGAIN含义 Linux环境下开发经常会碰到很多错误(设置errno)，其中EAGAIN是其中比较常见的一个错误(比如用在非阻塞操作中)。从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞(non-blocking)操作(对文件或socket)的时候。 例如，以 O_NONBLOCK的标志打开文件/socket/FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。又例如，当一个系统调用(比如fork)因为没有足够的资源(比如虚拟内存)而执行失败，返回EAGAIN提示其再调用一次(也许下次就能成功)。 代码演示下面是一段不完整的代码且格式不对，意在表述上面的过程，去掉了一些模板代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#define IPADDRESS "127.0.0.1"#define PORT 8787#define MAXSIZE 1024#define LISTENQ 5#define FDSIZE 1000#define EPOLLEVENTS 100listenfd = socket_bind(IPADDRESS,PORT);struct epoll_event events[EPOLLEVENTS];//创建一个描述符epollfd = epoll_create(FDSIZE);//添加监听描述符事件add_event(epollfd,listenfd,EPOLLIN);//循环等待for ( ; ; )&#123; //该函数返回已经准备好的描述符事件数目 ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1); //处理接收到的连接 handle_events(epollfd,events,ret,listenfd,buf);&#125;//事件处理函数static void handle_events(int epollfd,struct epoll_event *events,int num,int listenfd,char *buf)&#123; int i; int fd; //进行遍历;这里只要遍历已经准备好的io事件。num并不是当初epoll_create时的FDSIZE。 for (i = 0;i &lt; num;i++) &#123; fd = events[i].data.fd; //根据描述符的类型和事件类型进行处理 if ((fd == listenfd) &amp;&amp;(events[i].events &amp; EPOLLIN)) handle_accpet(epollfd,listenfd); else if (events[i].events &amp; EPOLLIN) do_read(epollfd,fd,buf); else if (events[i].events &amp; EPOLLOUT) do_write(epollfd,fd,buf); &#125;&#125;//添加事件static void add_event(int epollfd,int fd,int state)&#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&amp;ev);&#125;//处理接收到的连接static void handle_accpet(int epollfd,int listenfd)&#123; int clifd; struct sockaddr_in cliaddr; socklen_t cliaddrlen; clifd = accept(listenfd,(struct sockaddr*)&amp;cliaddr,&amp;cliaddrlen); if (clifd == -1) perror("accpet error:"); else &#123; printf("accept a new client: %s:%d\n",inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port); //添加一个客户描述符和事件 add_event(epollfd,clifd,EPOLLIN); &#125; &#125;//读处理static void do_read(int epollfd,int fd,char *buf)&#123; int nread; nread = read(fd,buf,MAXSIZE); if (nread == -1) &#123; perror("read error:"); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLIN); //删除监听 &#125; else if (nread == 0) &#123; fprintf(stderr,"client close.\n"); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLIN); //删除监听 &#125; else &#123; printf("read message is : %s",buf); //修改描述符对应的事件，由读改为写 modify_event(epollfd,fd,EPOLLOUT); &#125; &#125;//写处理static void do_write(int epollfd,int fd,char *buf) &#123; int nwrite; nwrite = write(fd,buf,strlen(buf)); if (nwrite == -1)&#123; perror("write error:"); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLOUT); //删除监听 &#125;else&#123; modify_event(epollfd,fd,EPOLLIN); &#125; memset(buf,0,MAXSIZE); &#125;//删除事件static void delete_event(int epollfd,int fd,int state) &#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_DEL,fd,&amp;ev);&#125;//修改事件static void modify_event(int epollfd,int fd,int state)&#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&amp;ev);&#125;//注：另外一端我就省了 epoll总结在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) epoll的优点主要是一下几个方面： 监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max查看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。 IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。 如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。 参考用户空间与内核空间，进程上下文与中断上下文[总结]进程切换维基百科-文件描述符Linux 中直接 I/O 机制的介绍IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇）Linux中select poll和epoll的区别IO多路复用之select总结IO多路复用之poll总结IO多路复用之epoll总结]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>select</tag>
        <tag>poll</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java CAS 原理剖析]]></title>
    <url>%2F2019%2F07%2F17%2Fjava-cas%2F</url>
    <content type="text"><![CDATA[在Java并发中，我们最初接触的应该就是synchronized关键字了，但是synchronized属于重量级锁，很多时候会引起性能问题，volatile也是个不错的选择，但是volatile不能保证原子性，只能在某些场合下使用。 像synchronized这种独占锁属于悲观锁，它是在假设一定会发生冲突的，那么加锁恰好有用，除此之外，还有乐观锁，乐观锁的含义就是假设没有发生冲突，那么我正好可以进行某项操作，如果要是发生冲突呢，那我就重试直到成功，乐观锁最常见的就是CAS。 我们在读Concurrent包下的类的源码时，发现无论是ReenterLock内部的AQS，还是各种Atomic开头的原子类，内部都应用到了CAS，最常见的就是我们在并发编程时遇到的i++这种情况。传统的方法肯定是在方法上加上synchronized关键字: 12345678public class Test &#123; public volatile int i; public synchronized void add() &#123; i++; &#125;&#125; 但是这种方法在性能上可能会差一点，我们还可以使用AtomicInteger，就可以保证i原子的++了。 12345678public class Test &#123; public AtomicInteger i; public void add() &#123; i.getAndIncrement(); &#125;&#125; 我们来看getAndIncrement的内部： 123public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125; 再深入到getAndAddInt(): 12345678public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; 这里我们见到compareAndSwapInt这个函数，它也是CAS缩写的由来。那么仔细分析下这个函数做了什么呢？ 首先我们发现compareAndSwapInt前面的this，那么它属于哪个类呢，我们看上一步getAndAddInt，前面是unsafe。这里我们进入的Unsafe类。这里要对Unsafe类做个说明。结合AtomicInteger的定义来说： 12345678910111213141516public class AtomicInteger extends Number implements java.io.Serializable &#123; private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125; private volatile int value; ... 在AtomicInteger数据定义的部分，我们可以看到，其实实际存储的值是放在value中的，除此之外我们还获取了unsafe实例，并且定义了valueOffset。再看到static块，懂类加载过程的都知道，static块的加载发生于类加载的时候，是最先初始化的，这时候我们调用unsafe的objectFieldOffset从Atomic类文件中获取value的偏移量，那么valueOffset其实就是记录value的偏移量的。 再回到上面一个函数getAndAddInt，我们看var5获取的是什么，通过调用unsafe的getIntVolatile(var1, var2)，这是个native方法，具体实现到JDK源码里去看了，其实就是获取var1中，var2偏移量处的值。var1就是AtomicInteger，var2就是我们前面提到的valueOffset,这样我们就从内存里获取到现在valueOffset处的值了。 现在重点来了，compareAndSwapInt（var1, var2, var5, var5 + var4）其实换成compareAndSwapInt（obj, offset, expect, update）比较清楚，意思就是如果obj内的value和expect相等，就证明没有其他线程改变过这个变量，那么就更新它为update，如果这一步的CAS没有成功，那就采用自旋的方式继续进行CAS操作，取出乍一看这也是两个步骤了啊，其实在JNI里是借助于一个CPU指令完成的。所以还是原子操作。 CAS底层原理CAS底层使用JNI调用C代码实现的，如果你有Hotspot源码，那么在Unsafe.cpp里可以找到它的实现： 123456static JNINativeMethod methods_15[] = &#123; //省略一堆代码... &#123;CC"compareAndSwapInt", CC"("OBJ"J""I""I"")Z", FN_PTR(Unsafe_CompareAndSwapInt)&#125;, &#123;CC"compareAndSwapLong", CC"("OBJ"J""J""J"")Z", FN_PTR(Unsafe_CompareAndSwapLong)&#125;, //省略一堆代码...&#125;; 我们可以看到compareAndSwapInt实现是在Unsafe_CompareAndSwapInt里面，再深入到Unsafe_CompareAndSwapInt: 123456UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper("Unsafe_CompareAndSwapInt"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;UNSAFE_END p是取出的对象，addr是p中offset处的地址，最后调用了Atomic::cmpxchg(x, addr, e), 其中参数x是即将更新的值，参数e是原内存的值。代码中能看到cmpxchg有基于各个平台的实现，这里我选择Linux X86平台下的源码分析： 12345678inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; int mp = os::is_MP(); __asm__ volatile (LOCK_IF_MP(%4) "cmpxchgl %1,(%3)" : "=a" (exchange_value) : "r" (exchange_value), "a" (compare_value), "r" (dest), "r" (mp) : "cc", "memory"); return exchange_value;&#125; 这是一段小汇编，__asm__说明是ASM汇编，__volatile__禁止编译器优化 12// Adding a lock prefix to an instruction on MP machine#define LOCK_IF_MP(mp) "cmp $0, " #mp "; je 1f; lock; 1: " os::is_MP判断当前系统是否为多核系统，如果是就给总线加锁，所以同一芯片上的其他处理器就暂时不能通过总线访问内存，保证了该指令在多处理器环境下的原子性。 在正式解读这段汇编前，我们来了解下嵌入汇编的基本格式： 12345asm ( assembler template : output operands /* optional */ : input operands /* optional */ : list of clobbered registers /* optional */ ); template就是cmpxchgl %1,(%3)表示汇编模板 output operands表示输出操作数,=a对应eax寄存器 input operand 表示输入参数，%1 就是exchange_value, %3是dest, %4就是mp， r表示任意寄存器，a还是eax寄存器 list of clobbered registers就是些额外参数，cc表示编译器cmpxchgl的执行将影响到标志寄存器, memory告诉编译器要重新从内存中读取变量的最新值，这点实现了volatile的感觉。 那么表达式其实就是cmpxchgl exchange_value ,dest，我们会发现%2也就是compare_value没有用上，这里就要分析cmpxchgl的语义了。cmpxchgl末尾l表示操作数长度为4，上面已经知道了。cmpxchgl会默认比较eax寄存器的值即compare_value和exchange_value的值，如果相等，就把dest的值赋值给exchange_value,否则，将exchange_value赋值给eax。具体汇编指令可以查看Intel手册CMPXCHG 最终，JDK通过CPU的cmpxchgl指令的支持，实现AtomicInteger的CAS操作的原子性。 CAS 的问题 ABA问题 CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。这就是CAS的ABA问题。 常见的解决思路是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A-B-A 就会变成1A-2B-3A。 目前在JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 循环时间长开销大 上面我们说过如果CAS不成功，则会原地自旋，如果长时间自旋会给CPU带来非常大的执行开销。 作者：卡巴拉的树 链接：https://juejin.im/post/5a73cbbff265da4e807783f5 来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构造回文]]></title>
    <url>%2F2019%2F07%2F17%2Flongest-common-subsequence%2F</url>
    <content type="text"><![CDATA[本文转载自：腾讯2017暑期实习生笔试题、 题目描述 解题思路看到这个图的时候相信大家明白了吧，就是这个题，我一直没有思路，今天突然想起来了，所以就准备解决它。其实这个题主要是运用一个算法思路来解决，最长公共子序列。 仔细想一想，将字符串逆序后与原来的字符串求最长公共子序列不就是这个构造回文吗？这应该很好理解吧，下面简单科普一下最长公共子序列：这中序列不是连续的，意思是可以有间隔，去掉那些干扰项以后，两个序列完全相同，而且要求这个子序列最长。 这类问题和之前 leetcode 上机器人跳到最右下角那个题一样，是一种动态规划的题。而且这种问题的当前位置的解受前面位置的解的影响，假设 s1，s2为两个字符串，i表示s1中第i个字符，j表示s2中第j个字符，那么: 再一次说明一下解题思路：输入字符串S，将字符串S逆序，逆序后的字符串为tmp，然后求S与tmp的最长公共子序列，最后用S的长度减去最长公共子序列的长度，就是需要删除的元素的个数。 相信思路大家明白思路了，下面给出代码。 AC代码1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std; int LongestStrHui(string &amp;s)&#123; string tmp = s; //逆序 reverse(tmp.begin(), tmp.end()); int len = s.size(); //初始化二维数组,数组多开辟了一些空间，是为了优化 vector&lt;vector&lt;int&gt;&gt; V(len + 1, vector&lt;int&gt;(len + 1, 0)); //根据"公式"开始去找各个位置的公共子序列长度 for (int i = 0; i &lt; len; ++i) &#123; for (int j = 0; j &lt; len; ++j) &#123; if (s[i] == tmp[j]) V[i + 1][j + 1] = 1 + V[i][j]; else V[i + 1][j + 1] = max(V[i][j + 1], V[i + 1][j]); &#125; &#125; //整个序列的最长公共子序列 return len - V[len][len];&#125; int main()&#123; string s; while (cin &gt;&gt; s) &#123; cout &lt;&lt; LongestStrHui(s)&lt;&lt;endl; &#125; return 0;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux之top]]></title>
    <url>%2F2019%2F07%2F17%2Flinux-top%2F</url>
    <content type="text"><![CDATA[本文转载自：每天一个linux命令（44）：top命令 top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。下面详细介绍它的使用方法。top是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止.比较准确的说,top命令提供了实时的对系统处理器的状态监视.它将显示系统中CPU最“敏感”的任务列表.该命令可以按CPU使用.内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定. 1．命令格式： top [参数] 2．命令功能： 显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 3．命令参数： -b 批处理 -c 显示完整的治命令 -I 忽略失效过程 -s 保密模式 -S 累积模式 -i&lt;时间&gt; 设置间隔时间 -u&lt;用户名&gt; 指定用户名 -p&lt;进程号&gt; 指定进程 -n&lt;次数&gt; 循环显示的次数 4．使用实例： 实例1：显示进程信息 命令： top 输出： [root@TG1704 log]# top top - 14:06:23 up 70 days, 16:44, 2 users, load average: 1.25, 1.32, 1.35 Tasks: 206 total, 1 running, 205 sleeping, 0 stopped, 0 zombie Cpu(s): 5.9%us, 3.4%sy, 0.0%ni, 90.4%id, 0.0%wa, 0.0%hi, 0.2%si, 0.0%st Mem: 32949016k total, 14411180k used, 18537836k free, 169884k buffers Swap: 32764556k total, 0k used, 32764556k free, 3612636k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 28894 root 22 0 1501m 405m 10m S 52.2 1.3 2534:16 java 18249 root 18 0 3201m 1.9g 11m S 35.9 6.0 569:39.41 java 2808 root 25 0 3333m 1.0g 11m S 24.3 3.1 526:51.85 java 25668 root 23 0 3180m 704m 11m S 14.0 2.2 360:44.53 java 574 root 25 0 3168m 611m 10m S 12.6 1.9 556:59.63 java 1599 root 20 0 3237m 1.9g 11m S 12.3 6.2 262:01.14 java 1008 root 21 0 3147m 842m 10m S 0.3 2.6 4:31.08 java 13823 root 23 0 3031m 2.1g 10m S 0.3 6.8 176:57.34 java 28218 root 15 0 12760 1168 808 R 0.3 0.0 0:01.43 top 29062 root 20 0 1241m 227m 10m S 0.3 0.7 2:07.32 java ​ 1 root 15 0 10368 684 572 S 0.0 0.0 1:30.85 init ​ 2 root RT -5 0 0 0 S 0.0 0.0 0:01.01 migration/0 ​ 3 root 34 19 0 0 0 S 0.0 0.0 0:00.00 ksoftirqd/0 ​ 4 root RT -5 0 0 0 S 0.0 0.0 0:00.00 watchdog/0 ​ 5 root RT -5 0 0 0 S 0.0 0.0 0:00.80 migration/1 ​ 6 root 34 19 0 0 0 S 0.0 0.0 0:00.00 ksoftirqd/1 ​ 7 root RT -5 0 0 0 S 0.0 0.0 0:00.00 watchdog/1 ​ 8 root RT -5 0 0 0 S 0.0 0.0 0:20.59 migration/2 ​ 9 root 34 19 0 0 0 S 0.0 0.0 0:00.09 ksoftirqd/2 10 root RT -5 0 0 0 S 0.0 0.0 0:00.00 watchdog/2 11 root RT -5 0 0 0 S 0.0 0.0 0:23.66 migration/3 12 root 34 19 0 0 0 S 0.0 0.0 0:00.03 ksoftirqd/3 13 root RT -5 0 0 0 S 0.0 0.0 0:00.00 watchdog/3 14 root RT -5 0 0 0 S 0.0 0.0 0:20.29 migration/4 15 root 34 19 0 0 0 S 0.0 0.0 0:00.07 ksoftirqd/4 16 root RT -5 0 0 0 S 0.0 0.0 0:00.00 watchdog/4 17 root RT -5 0 0 0 S 0.0 0.0 0:23.07 migration/5 18 root 34 19 0 0 0 S 0.0 0.0 0:00.07 ksoftirqd/5 19 root RT -5 0 0 0 S 0.0 0.0 0:00.00 watchdog/5 20 root RT -5 0 0 0 S 0.0 0.0 0:17.16 migration/6 21 root 34 19 0 0 0 S 0.0 0.0 0:00.05 ksoftirqd/6 22 root RT -5 0 0 0 S 0.0 0.0 0:00.00 watchdog/6 23 root RT -5 0 0 0 S 0.0 0.0 0:58.28 migration/7 说明： 统计信息区： 前五行是当前系统情况整体的统计信息区。下面我们看每一行信息的具体意义。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下： 14:06:23 — 当前系统时间 up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！） 2 users — 当前有2个用户登录系统 load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。 load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下： 系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行，cpu状态信息，具体属性说明如下： 5.9%us — 用户空间占用CPU的百分比。 3.4% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 90.4% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.2% si — 软中断（Software Interrupts）占用CPU的百分比 备注：在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！ 第四行,内存状态，具体信息如下： 32949016k total — 物理内存总量（32GB） 14411180k used — 使用中的内存总量（14GB） 18537836k free — 空闲内存总量（18GB） 169884k buffers — 缓存的内存量 （169M） 第五行，swap交换分区信息，具体信息说明如下： 32764556k total — 交换区总量（32GB） 0k used — 使用的交换区总量（0K） 32764556k free — 空闲交换区总量（32GB） 3612636k cached — 缓冲的交换区总量（3.6GB） 备注： 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 如果出于习惯去计算可用内存数，这里有个近似的计算公式：第四行的free + 第四行的buffers + 第五行的cached，按这个公式此台服务器的可用内存：18537836k +169884k +3612636k = 22GB左右。 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。 第六行，空行。 第七行以下：各进程（任务）的状态监控，项目列信息说明如下： PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） 其他使用技巧： 1.多U多核CPU监控 在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况： ​ 观察上图，服务器有16个逻辑CPU，实际上是4个物理CPU。再按数字键1，就会返回到top基本视图界面。 2.高亮显示当前运行进程 ​ 敲击键盘“b”（打开/关闭加亮效果），top的视图变化如下： ​ 我们发现进程id为2570的“top”进程被加亮了，top进程就是视图第二行显示的唯一的运行态（running）的那个进程，可以通过敲击“y”键关闭或打开运行态进程的加亮效果。 3.进程字段排序 默认进入top时，各进程是按照CPU的占用量来排序的，在下图中进程ID为28894的java进程排在第一（cpu占用142%），进程ID为574的java进程排在第二（cpu占用16%）。 ​ ​ 敲击键盘“x”（打开/关闭排序列的加亮效果），top的视图变化如下： ​ 可以看到，top默认的排序列是“%CPU”。 *4. *通过”shift + &gt;”或”shift + &lt;”可以向右或左改变排序列 ​ 下图是按一次”shift + &gt;”的效果图,视图现在已经按照%MEM来排序。 ​ 5.top交互命令 在top 命令执行过程中可以使用的一些交互命令。这些命令都是单字母的，如果在命令行中使用了s 选项， 其中一些命令可能会被屏蔽。 h 显示帮助画面，给出一些简短的命令总结说明 k 终止一个进程。 i 忽略闲置和僵死进程。这是一个开关式命令。 q 退出程序 r 重新安排一个进程的优先级别 S 切换到累计模式 s 改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 s f或者F 从当前显示中添加或者删除项目 o或者O 改变显示项目的顺序 l 切换显示平均负载和启动时间信息 m 切换显示内存信息 t 切换显示进程和CPU状态信息 c 切换显示命令名称和完整命令行 M 根据驻留内存大小进行排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[distributed-lock]]></title>
    <url>%2F2019%2F07%2F16%2Fdistributed-lock%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[详解CMS垃圾回收机制]]></title>
    <url>%2F2019%2F07%2F15%2Fjvm-cms%2F</url>
    <content type="text"><![CDATA[什么是CMS？Concurrent Mark Sweep。 看名字就知道，CMS是一款并发、使用标记-清除算法的gc。 CMS是针对老年代进行回收的GC。 CMS有什么用？CMS以获取最小停顿时间为目的。 在一些对响应时间有很高要求的应用或网站中，用户程序不能有长时间的停顿，CMS 可以用于此场景。 CMS收集器的主要设计目标是：低应用停顿时间。它通过两种方式实现这一目标： 不压缩老年代，而是使用空闲列表来管理回收空间。 大部分标记清理工作与应用程序并发执行。 主要问题：由于不压缩带来的老年代堆碎片，或者在对象分配率高的情况下，都可能导致Full GC。 CMS如何执行？CMS收集器的GC周期主要由7个阶段组成，其中有两个阶段会发生stop-the-world，其他阶段都是并发执行的。（亦有4个阶段、6个阶段等说法） Phase 1: Initial Mark（初始化标记）初始化标记阶段，是CMS GC的第一个阶段，也是标记阶段的开始。主要工作是标记可直达的存活对象。 主要标记过程 从GC Roots遍历可直达的老年代对象； 遍历被新生代存活对象所引用的老年代对象。 程序执行情况 支持单线程或并行标记。 发生stop-the-world，暂停所有应用线程。 （Marked obj：老年代绿色圆点表示被初始化标记的对象。） Phase 2: Concurrent Mark（并发标记）并发标记阶段，是CMS GC的第二个阶段。 在该阶段，GC线程和应用线程将并发执行。也就是说，在第一个阶段（Initial Mark）被暂停的应用线程将恢复运行。 并发标记阶段的主要工作是，通过遍历第一个阶段（Initial Mark）标记出来的存活对象，继续递归遍历老年代，并标记可直接或间接到达的所有老年代存活对象。 （Current obj：该对象的引用关系发生变化，对下一个对象的引用被删除。） 由于在并发标记阶段，应用线程和GC线程是并发执行的，因此可能产生新的对象或对象关系发生变化，例如： 新生代的对象晋升到老年代； 直接在老年代分配对象； 老年代对象的引用关系发生变更； 等等。 对于这些对象，需要重新标记以防止被遗漏。为了提高重新标记的效率，本阶段会把这些发生变化的对象所在的Card标识为Dirty，这样后续就只需要扫描这些Dirty Card的对象，从而避免扫描整个老年代。 Phase 3: Concurrent Preclean（并发预清理）在并发预清洗阶段，将会重新扫描前一个阶段标记的Dirty对象，并标记被Dirty对象直接或间接引用的对象，然后清除Card标识。 标记被Dirty对象直接或间接引用的对象： 清除Dirty对象的Card标识： Phase 4: Concurrent Abortable Preclean（可中止的并发预清理）本阶段尽可能承担更多的并发预处理工作，从而减轻在Final Remark阶段的stop-the-world。 在该阶段，主要循环的做两件事： 处理 From 和 To 区的对象，标记可达的老年代对象； 和上一个阶段一样，扫描处理Dirty Card中的对象。 具体执行多久，取决于许多因素，满足其中一个条件将会中止运行： 执行循环次数达到了阈值； 执行时间达到了阈值； 新生代Eden区的内存使用率达到了阈值。 Phase 5: Final Remark（重新标记）预清理阶段也是并发执行的，并不一定是所有存活对象都会被标记，因为在并发标记的过程中对象及其引用关系还在不断变化中。 因此，需要有一个stop-the-world的阶段来完成最后的标记工作，这就是重新标记阶段（CMS标记阶段的最后一个阶段）。主要目的是重新扫描之前并发处理阶段的所有残留更新对象。 主要工作： 遍历新生代对象，重新标记；（新生代会被分块，多线程扫描） 根据GC Roots，重新标记； 遍历老年代的Dirty Card，重新标记。这里的Dirty Card，大部分已经在Preclean阶段被处理过了。 Phase 6: Concurrent Sweep（并发清理）并发清理阶段，主要工作是清理所有未被标记的死亡对象，回收被占用的空间。 Phase 7: Concurrent Reset（并发重置）并发重置阶段，将清理并恢复在CMS GC过程中的各种状态，重新初始化CMS相关数据结构，为下一个垃圾收集周期做好准备。 CMS有什么问题？CMS这三个字母就隐含了问题所在。并发+标记-清除算法 是问题的来源。 并发并发意味着多线程抢占CPU资源，即GC线程与用户线程抢占CPU。这可能会造成用户线程执行效率下降。 CMS默认的回收线程数是(CPU个数+3)/4。这个公式的意思是当CPU大于4个时,保证回收线程占用至少25%的CPU资源，这样用户线程占用75%的CPU，这是可以接受的。 但是，如果CPU资源很少，比如只有两个的时候怎么办？按照上面的公式，CMS会启动1个GC线程。相当于GC线程占用了50%的CPU资源，这就可能导致用户程序的执行速度忽然降低了50%，50%已经是很明显的降低了。 这种场景怎么处理呢？ 我给的答案是可以不用考虑这种场景。现在的PC机中都至少有双核处理器，更别说大型的服务器了。 CMS的解决方案是提供了一个 incremental mode（增量模式）。 在这种模式下，进行并发标记、清理时让GC线程、用户线程交替运行，尽量减少GC线程独占CPU资源的时间。 这会造成GC时间更长，但对用户线程造成的影响就会少一些。 但实践证明，这种模式下CMS的表现很一般，并没有什么大的优化。 i-CMS已经被声明为“deprecated”，不再提倡使用。 (https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/cms.html#concurrent_mark_sweep_cms_collector) 浮动垃圾并发清理阶段用户线程还在运行，这段时间就可能产生新的垃圾，新的垃圾在此次GC无法清除，只能等到下次清理。这些垃圾有个专业名词：浮动垃圾。 由于垃圾回收阶段用户线程仍在执行，必需预留出内存空间给用户线程使用。因此不能像其他回收器那样，等到老年代满了再进行GC。 CMS 提供了CMSInitiatingOccupancyFraction参数来设置老年代空间使用百分比,达到百分比就进行垃圾回收。 这个参数默认是92%，参数选择需要看具体的应用场景。 设置的太小会导致频繁的CMS GC，产生大量的停顿；反过来想，设置的太高会发生什么？ 假设现在设置为99%，还剩1%的空间可以使用。 在并发清理阶段，若用户线程需要使用的空间大于1%，就会产生Concurrent Mode Failure错误，意思就是说并发模式失败。 这时，虚拟机就会启动备案：使用Serial Old收集器重新对老年代进行垃圾回收.如此一来，停顿时间变得更长。 所以CMSInitiatingOccupancyFraction的设置要具体问题具体分析。 网上有一些设置此参数的公式，个人认为不是很严谨(原因就是CMS另外一个问题导致的),因此不写出来以免大家疑惑。 其实CMS有动态检查机制。 CMS会根据历史记录，预测老年代还需要多久填满及进行一次回收所需要的时间。 在老年代空间用完之前，CMS可以根据自己的预测自动执行垃圾回收。 这个特性可以使用参数UseCMSInitiatingOccupancyOnly来关闭。 这里提个问题给读者思考，如果让你设计，如何预测什么时候开始自动执行？ 内存碎片前两个问题是由并发引起的，接下来要说的问题就是由标记-清除算法引起的。 使用标记-清除算法可能造成大量的空间碎片。空间碎片过多，就会给大对象分配带来麻烦。 往往老年代还有很大剩余空间，但无法找到足够大的连续空间来分配当前对象,不得不触发一次Full GC。 CMS的解决方案是使用UseCMSCompactAtFullCollection参数(默认开启)，在顶不住要进行Full GC时开启内存碎片整理。 这个过程需要STW，碎片问题解决了,但停顿时间又变长了。 虚拟机还提供了另外一个参数CMSFullGCsBeforeCompaction，用于设置执行多少次不压缩的Full GC后，跟着来一次带压缩的（默认为0，每次进入Full GC时都进行碎片整理）。 延伸一个“foreground collector”的东西给大家，这个玩意在Java8中也声明为deprecated。(https://bugs.openjdk.java.net/browse/JDK-8027132) 参考：Java之CMS GC的7个阶段详解CMS垃圾回收机制]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP1.0、HTTP1.1 和 HTTP2.0 的区别]]></title>
    <url>%2F2019%2F07%2F14%2Fhttp1-2%2F</url>
    <content type="text"><![CDATA[本文转载自：HTTP1.0、HTTP1.1 和 HTTP2.0 的区别 HTTP的历史早在 HTTP 建立之初，主要就是为了将超文本标记语言(HTML)文档从Web服务器传送到客户端的浏览器。也是说对于前端来说，我们所写的HTML页面将要放在我们的 web 服务器上，用户端通过浏览器访问url地址来获取网页的显示内容，但是到了 WEB2.0 以来，我们的页面变得复杂，不仅仅单纯的是一些简单的文字和图片，同时我们的 HTML 页面有了 CSS，Javascript，来丰富我们的页面展示，当 ajax 的出现，我们又多了一种向服务器端获取数据的方法，这些其实都是基于 HTTP 协议的。同样到了移动互联网时代，我们页面可以跑在手机端浏览器里面，但是和 PC 相比，手机端的网络情况更加复杂，这使得我们开始了不得不对 HTTP 进行深入理解并不断优化过程中。 HTTP的基本优化影响一个 HTTP 网络请求的因素主要有两个：带宽和延迟。 带宽：如果说我们还停留在拨号上网的阶段，带宽可能会成为一个比较严重影响请求的问题，但是现在网络基础建设已经使得带宽得到极大的提升，我们不再会担心由带宽而影响网速，那么就只剩下延迟了。 延迟： 浏览器阻塞（HOL blocking）：浏览器会因为一些原因阻塞请求。浏览器对于同一个域名，同时只能有 4 个连接（这个根据浏览器内核不同可能会有所差异），超过浏览器最大连接数限制，后续请求就会被阻塞。 DNS 查询（DNS Lookup）：浏览器需要知道目标服务器的 IP 才能建立连接。将域名解析为 IP 的这个系统就是 DNS。这个通常可以利用DNS缓存结果来达到减少这个时间的目的。 建立连接（Initial connection）：HTTP 是基于 TCP 协议的，浏览器最快也要在第三次握手时才能捎带 HTTP 请求报文，达到真正的建立连接，但是这些连接无法复用会导致每次请求都经历三次握手和慢启动。三次握手在高延迟的场景下影响较明显，慢启动则对文件类大请求影响较大。 HTTP1.0和HTTP1.1的一些区别HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在： 缓存处理，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 If-Modified-Since是标准的HTTP请求头标签，在发送HTTP请求时，把浏览器端缓存页面的最后修改时间一起发到服务器去，服务器会把这个时间与服务器上实际文件的最后修改时间进行比较。 如果时间一致，那么返回HTTP状态码304（不返回文件内容），客户端接到之后，就直接把本地缓存文件显示到浏览器中。 如果时间不一致，就返回HTTP状态码200和新的文件内容，客户端接到之后，会丢弃旧文件，把新文件缓存起来，并显示到浏览器中。 带宽优化及网络连接的使用，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。 长连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。 HTTPS与HTTP的一些区别 HTTPS协议需要到CA申请证书，一般免费证书很少，需要交费。 HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。 HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 HTTPS可以有效的防止运营商劫持，解决了防劫持的一个大问题。 SPDY：HTTP1.x的优化2012年google如一声惊雷提出了SPDY的方案，优化了HTTP1.X的请求延迟，解决了HTTP1.X的安全性，具体如下： 降低延迟，针对HTTP高延迟的问题，SPDY优雅的采取了多路复用（multiplexing）。多路复用通过多个请求stream共享一个tcp连接的方式，解决了HOL blocking的问题，降低了延迟同时提高了带宽的利用率。 请求优先级（request prioritization）。多路复用带来一个新的问题是，在连接共享的基础之上有可能会导致关键请求被阻塞。SPDY允许给每个request设置优先级，这样重要的请求就会优先得到响应。比如浏览器加载首页，首页的html内容应该优先展示，之后才是各种静态资源文件，脚本文件等加载，这样可以保证用户能第一时间看到网页内容。 header压缩。前面提到HTTP1.x的header很多时候都是重复多余的。选择合适的压缩算法可以减小包的大小和数量。 基于HTTPS的加密协议传输，大大提高了传输数据的可靠性。 服务端推送（server push），采用了SPDY的网页，例如我的网页有一个sytle.css的请求，在客户端收到sytle.css数据的同时，服务端会将sytle.js的文件推送给客户端，当客户端再次尝试获取sytle.js时就可以直接从缓存中获取到，不用再发请求了。SPDY构成图： SPDY位于HTTP之下，TCP和SSL之上，这样可以轻松兼容老版本的HTTP协议(将HTTP1.x的内容封装成一种新的frame格式)，同时可以使用已有的SSL功能。 HTTP2.0性能惊人HTTP/2: the Future of the Internet https://link.zhihu.com/?target=https://http2.akamai.com/demo 是 Akamai 公司建立的一个官方的演示，用以说明 HTTP/2 相比于之前的 HTTP/1.1 在性能上的大幅度提升。 同时请求 379 张图片，从Load time 的对比可以看出 HTTP/2 在速度上的优势。 HTTP2.0：SPDY的升级版HTTP2.0可以说是SPDY的升级版（其实原本也是基于SPDY设计的），但是，HTTP2.0 跟 SPDY 仍有不同的地方，如下： HTTP2.0和SPDY的区别： HTTP2.0 支持明文 HTTP 传输，而 SPDY 强制使用 HTTPS HTTP2.0 消息头的压缩算法采用 HPACK http://http2.github.io/http2-spec/compression.html，而非 SPDY 采用的 DEFLATE http://zh.wikipedia.org/wiki/DEFLATE HTTP2.0和HTTP1.X相比的新特性 新的二进制格式（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（MultiPlexing），即连接共享，即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。 header压缩，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。 服务端推送（server push），同SPDY一样，HTTP2.0也具有server push功能。 HTTP2.0的升级改造 前文说了HTTP2.0其实可以支持非HTTPS的，但是现在主流的浏览器像chrome，firefox表示还是只支持基于 TLS 部署的HTTP2.0协议，所以要想升级成HTTP2.0还是先升级HTTPS为好。 当你的网站已经升级HTTPS之后，那么升级HTTP2.0就简单很多，如果你使用NGINX，只要在配置文件中启动相应的协议就可以了，可以参考NGINX白皮书，NGINX配置HTTP2.0官方指南 https://www.nginx.com/blog/nginx-1-9-5/。 使用了HTTP2.0那么，原本的HTTP1.x怎么办，这个问题其实不用担心，HTTP2.0完全兼容HTTP1.x的语义，对于不支持HTTP2.0的浏览器，NGINX会自动向下兼容的。 附注HTTP2.0的多路复用和HTTP1.X中的长连接复用有什么区别？ HTTP/1.* 一次请求-响应，建立一个连接，用完关闭；每一个请求都要建立一个连接； HTTP/1.1 Pipeling解决方式为，若干个请求排队串行化单线程处理，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时等，后续请求只能被阻塞，毫无办法，也就是人们常说的线头阻塞； HTTP/2多个请求可同时在一个连接上并行执行。某个请求任务耗时严重，不会影响到其它连接的正常执行；具体如图： 服务器推送到底是什么？服务端推送能把客户端所需要的资源伴随着index.html一起发送到客户端，省去了客户端重复请求的步骤。正因为没有发起请求，建立连接等操作，所以静态资源通过服务端推送的方式可以极大地提升速度。具体如下： 普通的客户端请求过程： 服务端推送的过程： 为什么需要头部压缩？假定一个页面有100个资源需要加载（这个数量对于今天的Web而言还是挺保守的）, 而每一次请求都有1kb的消息头（这同样也并不少见，因为Cookie和引用等东西的存在）, 则至少需要多消耗100kb来获取这些消息头。HTTP2.0可以维护一个字典，差量更新HTTP头部，大大降低因头部传输产生的流量。具体参考：HTTP/2 头部压缩技术介绍 HTTP2.0多路复用有多好？HTTP 性能优化的关键并不在于高带宽，而是低延迟。TCP 连接会随着时间进行自我「调谐」，起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输的速度。这种调谐则被称为 TCP 慢启动。由于这种原因，让原本就具有突发性和短时性的 HTTP 连接变的十分低效。HTTP/2 通过让所有数据流共用同一个连接，可以更有效地使用 TCP 连接，让高带宽也能真正的服务于 HTTP 的性能提升。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议]]></title>
    <url>%2F2019%2F07%2F14%2Fhttp-introduction%2F</url>
    <content type="text"><![CDATA[本文转载自：5分钟让你明白HTTP HTTP简介http协议介绍 HTTP协议（HyperText Transfer Protocol，超文本传输协议）是因特网上应用最为广泛的一种网络传输协议，所有的WWW文件都必须遵守这个标准。 HTTP是基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等） HTTP协议通常承载于TCP协议之上，有时也承载于TLS或SSL协议层之上，这个时候，就成了我们常说的HTTPS。如下图 HTTP是一个应用层协议，由请求和响应构成，是一个标准的客户端服务器模型。HTTP是一个无状态的协议。 HTTP默认的端口号为80，HTTPS的端口号为443。 http协议工作流程一次HTTP操作称为一个事务，其工作过程大概如下： 用户在浏览器中键入需要访问网页的URL或者点击某个网页中链接； 浏览器根据URL中的域名，通过DNS解析出目标网页的IP地址； 123浏览器请求这个页面：http://hackr.ip/index.html在这一步，需要域名系统DNS解析域名hackr.ip,得主机的IP地址 20X.189.105.112。然后将上面结合本机自己的信息，封装成一个http请求数据包 在HTTP开始工作前，客户端首先会通过TCP/IP协议来和服务端建立链接（TCP三次握手） 建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和其他内容。 服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。 一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码： 1Connection:keep-alive ，TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。 短连接短连接的操作步骤是： 建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接 如果客户请求频繁，将在TCP的建立和关闭操作上浪费较多时间和带宽。 长链接 长链接,指在一个连接上可以连续发送多个数据包，在连接保持期间，如果没有数据包发送，需要双方发链路检测包。 长链接操作步骤： 建立连接——数据传输…（保持连接）…数据传输——关闭连接 长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间 长链接分为 without pipelining 和 with pipelining，下图中是without pipelining，客户端只在收到前一个请求的响应后，才发出新的请求。 管线化下图是with pipelining，每次建立链接后无需等待请求回来就可以发送下一个请求 Http请求报文客户端发送一个HTTP请求到服务器的请求消息包括以下格式： 请求行（request line）、请求头部（header）、请求体组成，下图给出了请求报文的一般格式。 12345678910111213141516171819请求行: 方法: GET 获取资源 POST 向服务器端发送数据，传输实体主体 PUT 传输文件 HEAD 获取报文首部 DELETE 删除文件 OPTIONS 询问支持的方法 TRACE 追踪路径 URL 协议/版本号 请求头: 通用首部(General Header) 请求首部(Request Header) 响应首部(Response Header) 实体首部(Entity Header Fields) 请求体 请求报文拆解： get请求 post请求 Http响应报文HTTP响应组成：响应行、响应头、响应体。 1234567响应行 （HTTP/1.1）表明HTTP版本为1.1版本，状态码为200，状态消息为（ok）响应头 Date:生成响应的日期和时间； Content-Type:指定了MIME类型的HTML(text/html),编码类型是ISO-8859-1响应体复制代码 响应报文拆解： Http状态码 类别 原因 1XX Informational(信息性状态码) 2XX Success(成功状态码) 3XX Redirection(重定向) 4XX Client Error(客户端错误状态码) 5XX Server Error(服务器错误状态吗) 2XX 成功123200(OK 客户端发过来的数据被正常处理204(Not Content 正常响应，没有实体206(Partial Content 范围请求，返回部分数据，响应报文中由Content-Range指定实体内容 3XX 重定向12345301(Moved Permanently) 永久重定向302(Found) 临时重定向，规范要求，方法名不变，但是都会改变303(See Other) 和302类似，但必须用GET方法304(Not Modified) 状态未改变， 配合(If-Match、If-Modified-Since、If-None_Match、If-Range、If-Unmodified-Since)307(Temporary Redirect) 临时重定向，不该改变请求方法 4XX 客户端错误1234400(Bad Request) 请求报文语法错误401 (unauthorized) 需要认证403(Forbidden) 服务器拒绝访问对应的资源404(Not Found) 服务器上无法找到资源 5XX 服务器端错误12500(Internal Server Error)服务器故障503(Service Unavailable) 服务器处于超负载或正在停机维护 首部首部字段 首部字段名 说明 Cache-Control 控制缓存行为 Connection 链接的管理 Date 报文日期 Pragma 报文指令 Trailer 报文尾部的首部 Trasfer-Encoding 指定报文主体的传输编码方式 Upgrade 升级为其他协议 Via 代理服务器信息 Warning 错误通知 请求首部字段 首部字段名 说明 Accept 用户代理可处理的媒体类型 Accept-Charset 优先的字符集 Accept-Encoding 优先的编码 Accept-Langulage 优先的语言 Authorization Web认证信息 Expect 期待服务器的特定行为 From 用户的电子邮箱地址 Host 请求资源所在的服务器 If-Match 比较实体标记 If-Modified-Since 比较资源的更新时间 If-None-Match 比较实体标记 If-Range 资源未更新时发送实体Byte的范围请求 If-Unmodified-Since 比较资源的更新时间(和If-Modified-Since相反) Max-Forwards 最大传输跳数 Proxy-Authorization 代理服务器需要客户端认证 Range 实体字节范围请求 Referer 请求中的URI的原始获取方 TE 传输编码的优先级 User-Agent HTTP客户端程序的信息 响应首部字段 首部字段名 说明 Accept-Ranges 是否接受字节范围 Age 资源的创建时间 ETag 资源的匹配信息 Location 客户端重定向至指定的URI Proxy-Authenticate 代理服务器对客户端的认证信息 Retry-After 再次发送请求的时机 Server 服务器的信息 Vary 代理服务器缓存的管理信息 www-Authenticate 服务器对客户端的认证 实体首部字段 首部字段名 说明 Allow 资源可支持的HTTP方法 Content-Encoding 实体的编码方式 Content-Language 实体的自然语言 Content-Length 实体的内容大小(字节为单位) Content-Location 替代对应资源的URI Content-MD5 实体的报文摘要 Content-Range 实体的位置范围 Content-Type 实体主体的媒体类型 Expires 实体过期时间 Last-Modified 资源的最后修改时间 参考： HTTP简介 HTTP协议详解 HTTP HTTP工作过程]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-146:LRU Cache]]></title>
    <url>%2F2019%2F07%2F14%2Fleetcode-146%2F</url>
    <content type="text"><![CDATA[原题链接：https://leetcode.com/problems/lru-cache/ 题目描述 题目难度：Medium Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put. get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item. The cache is initialized with a positive capacity. Follow up:Could you do both operations in O(1) time complexity? Example: 1234567891011LRUCache cache = new LRUCache( 2 /* capacity */ );cache.put(1, 1);cache.put(2, 2);cache.get(1); // returns 1cache.put(3, 3); // evicts key 2cache.get(2); // returns -1 (not found)cache.put(4, 4); // evicts key 1cache.get(1); // returns -1 (not found)cache.get(3); // returns 3cache.get(4); // returns 4 AC代码11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374class LRUCache &#123; int cap; HashMap&lt;Integer, Node&gt; node_map; class Node &#123; Node prev; Node next; int key; int val; Node (Node p, Node n, int k, int v) &#123; this.prev = p; this.next = n; this.val = v; this.key = k; &#125; Node() &#123;&#125; &#125; Node head, tail; private void removeNode(Node node) &#123; node.prev.next = node.next; node.next.prev = node.prev; &#125; private void addToHead(Node node) &#123; Node next = head.next; node.next = next; next.prev = node; node.prev = head; head.next = node; &#125; public LRUCache(int capacity) &#123; this.cap = capacity; this.node_map = new HashMap(); this.head = new Node(); this.tail = new Node(); head.next = tail; tail.prev = head; &#125; public int get(int key) &#123; if(node_map.containsKey(key) == false) return -1; Node node = node_map.get(key); removeNode(node); addToHead(node); return node.val; &#125; public void put(int key, int value) &#123; if(node_map.containsKey(key)) &#123; Node node = node_map.get(key); node.val = value; removeNode(node); addToHead(node); &#125; else &#123; if(node_map.size() == this.cap) &#123; // System.out.println(tail.prev.key); node_map.remove(tail.prev.key); removeNode(tail.prev); &#125; Node node = new Node(null, null, key, value); node_map.put(key, node); addToHead(node); &#125; &#125;&#125;/** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */ AC代码2利用jdk中的LinkedHashMap实现 123456789101112131415161718192021222324252627282930313233class LRUCache &#123; private LinkedHashMap&lt;Integer, Integer&gt; map; private int capacity; public LRUCache(int capacity) &#123; this.capacity = capacity; map = new LinkedHashMap&lt;Integer, Integer&gt;(capacity, .75f, true) &#123; @Override protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) &#123; // Remove the eldest element whenever size of cache exceeds the capacity return (size() &gt; capacity); &#125; &#125;; &#125; public int get(int key) &#123; return map.getOrDefault(key, -1); &#125; public void put(int key, int value) &#123; map.put(key, value); &#125;&#125;/** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */ 注：以上代码均来自于leetcode]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>LRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop和Spark的异同]]></title>
    <url>%2F2019%2F07%2F14%2Fhadoop-spark%2F</url>
    <content type="text"><![CDATA[本文转载自：Hadoop和Spark的异同 解决问题的层面不一样Hadoop和Spark两者都是大数据框架，但是各自存在的目的不尽相同。 Hadoop实质上是解决大数据大到无法在一台计算机上进行存储、无法在要求的时间内进行处理的问题，是一个分布式数据基础设施。 HDFS，它将巨大的数据集分派到一个由普通计算机组成的集群中的多个节点进行存储，通过将块保存到多个副本上，提供高可靠的文件存储。 MapReduce，通过简单的Mapper和Reducer的抽象提供一个编程模型，可以在一个由几十台上百台的机器上并发地分布式处理大量数据集，而把并发、分布式和故障恢复等细节隐藏。 Hadoop复杂的数据处理需要分解为多个Job（包含一个Mapper和一个Reducer）组成的有向无环图。 Spark则允许程序开发者使用有向无环图（DAG）开发复杂的多步数据管道。而且还支持跨有向无环图的内存数据共享，以便不同的作业可以共同处理同一个数据。是一个专门用来对那些分布式存储的大数据进行处理的工具，它并不会进行分布式数据的存储。 可将Spark看作是Hadoop MapReduce的一个替代品而不是Hadoop的替代品。 Hadoop的局限和不足 一个Job只有Map和Reduce两个阶段，复杂的计算需要大量的Job完成，Job间的依赖关系由开发人员进行管理。 中间结果也放到HDFS文件系统中。对于迭代式数据处理性能比较差。 Reduce Task需要等待所有的Map Task都完成后才开始计算。 时延高，只适用批量数据处理，对于交互式数据处理，实时数据处理的支持不够。 两者可合可分 Hadoop除了提供HDFS分布式数据存储功能之外，还提供了MapReduce的数据处理功能。所以我们完全可以抛开Spark，仅使用Hadoop自身的MapReduce来完成数据的处理。 相反，Spark也不是非要依附在Hadoop身上才能生存。但它没有提供文件管理系统，所以，它必须和其他的分布式文件系统进行集成才能运作。我们可以选择Hadoop的HDFS，也可以选择其他的基于云的数据系统平台。但Spark默认来说还是被用在Hadoop上面的，被认为它们的结合是最好的选择。 Spark数据处理速度秒杀MapReduce Spark因为处理数据的方式不一样，会比MapReduce快上很多。MapReduce是分步对数据进行处理的: “从集群中读取数据，进行一次处理，将结果写到集群，从集群中读取更新后的数据，进行下一次的处理，将结果写到集群，等等…” Spark会在内存中以接近“实时”的时间完成所有的数据分析：“从集群中读取数据，完成所有必须的分析处理（依赖多个算子），将结果写回集群，完成，” Spark的批处理速度比MapReduce快近10倍，内存中的数据分析速度则快近100倍。 如果需要处理的数据和结果需求大部分情况下是静态的，且有充足的时间等待批处理的完成，MapReduce的处理方式也是完全可以接受的。 但如果你需要对时实流数据进行分析，比如来自工厂的传感器收集回来的数据，又或者用户访问网站的日志信息，那么更应该使用Spark进行处理。 灾难恢复机制 两者的灾难恢复方式不同，因为Hadoop将每次处理后的数据都写入到磁盘上，所以其天生就能很有弹性的对系统错误进行处理。 Spark的数据对象存储在分布于数据集群中的叫做弹性分布式数据集(RDD: Resilient Distributed Dataset)中。这些数据对象既可以放在内存，也可以放在磁盘，所以RDD同样也可以提供完成的灾难恢复功能。 Spark优势 Spark的优势不仅体现在性能提升上，Spark框架为批处理（Spark Core），交互式（Spark SQL），流式（Spark Streaming），机器学习（MLlib），图计算（GraphX）提供了一个统一的数据处理平台。 Spark通过在数据处理过程中成本更低的Shuffle方式，将MapReduce提升到一个更高的层次。利用内存数据存储和接近实时的处理能力，Spark比其他的大数据处理技术的性能要快很多倍。 Spark将中间结果保存在内存中而不是写入磁盘，当需要多次处理同一数据集时，这一点特别实用。 支持比Map和Reduce更多的函数。 Spark的RDD是分布式大数据处理的高层次抽象的数据集合，对这个集合的任何操作都可以像函数式编程中操作内存中的集合一样直观、简便，但集合操作的实现确是在后台分解成一系列Task发送到集群上完成。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解spark之架构与原理]]></title>
    <url>%2F2019%2F07%2F14%2Fspark-Architecture%2F</url>
    <content type="text"><![CDATA[Spark提供了一个全面、统一的框架用于管理各种有着不同性质（文本数据、图表数据等）的数据集和数据源（批量数据或实时的流数据）的大数据处理的需求。官方资料介绍Spark可以将Hadoop集群中的应用在内存中的运行速度提升100倍，甚至能够将应用在磁盘上的运行速度提升10倍。 基本概念 Application：用户编写的Spark应用程序。 Driver：Spark中的Driver即运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭。 Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task。 RDD：弹性分布式数据集，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。 DAG：有向无环图，反映RDD之间的依赖关系。 Stage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表一组关联的，相互之间没有Shuffle依赖关系的任务组成的任务集。 RDD（resillient distributed dataset）：弹性分布式数据集。 Task：具体执行任务。Task分为ShuffleMapTask和ResultTask两种。ShuffleMapTask和ResultTask分别类似于Hadoop中的Map，Reduce。 job：用户提交的作业。一个Job可能由一到多个Task组成。一个Job包含多个RDD及作用于相应RDD上的各种操作。 Stage：Job分成的阶段。一个Job可能被划分为一到多个Stage。 Partition：数据分区。即一个RDD的数据可以划分为多少个分区。 NarrowDependency：窄依赖。即子RDD依赖于父RDD中固定的Partition。NarrowDependency分为OneToOneDependency和RangeDependency两种。 ShuffleDependency：shuffle依赖，也称为宽依赖。即子RDD对父RDD中的所有Partition都有依赖。 DAG（Directed Acycle graph）：有向无环图。用于反映各RDD之间的依赖关系。 Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型1) Standalon : spark原生的资源管理，由Master负责资源的分配2) Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架3) Hadoop Yarn: 主要是指Yarn中的ResourceManager 一个Application由一个Driver和若干个Job构成，一个Job由多个Stage构成，一个Stage由多个没有Shuffle关系的Task组成。 当执行一个Application时，Driver会向集群管理器申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果会返回给Driver，或者写到HDFS或者其它数据库中。 使用场景Spark适用场景： Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。 数据量不是特别大，但是要求近实时统计分析需求 spark不适用场景： 内存hold不住的场景，在内存不足的情况下，Spark会下放到磁盘，会降低应有的性能 有高实时性要求的流式计算业务，例如实时性要求毫秒级 由于RDD设计上的只读特点，所以Spark对于待分析数据频繁变动的情景很难做（并不是不可以），比如数据集在频繁变化（不停增删改），而且又需要结果具有很强的一致性（不一致时间窗口很小），那么就不合适了。 流线长或文件流量非常大的数据集不适合。你会发现你的内存不够用，集群压力大时一旦一个task失败会导致他前面一条线所有的前置任务全部重跑，然后恶性循环会导致更多的task失败，整个sparkapp效率极低。就不如MapReduce啦！ 由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如web服务的存储或者是增量的web爬虫和索引。就是对于那种增量修改的应用模型不适合。 架构及生态Apache Spark是一个正在快速成长的开源集群计算系统。Apache Spark生态系统中的包和框架日益丰富，使得Spark能够进行高级数据分析。Apache Spark的快速成功得益于它的强大功能和易于使用性。相比于传统的MapReduce大数据分析，Spark效率更高、运行时速度更快。Apache Spark 提供了内存中的分布式计算能力，具有Java、 Scala、Python、R四种编程语言的API编程接口。Spark生态系统如下图所示： Spark Core：包含Spark的基本功能，包含任务调度，内存管理，容错机制等，内部定义了RDDs(弹性分布式数据集)，提供了很多APIs来创建和操作这些RDDs。为其他组件提供底层的服务。其他Spark的库都是构建在RDD和Spark Core之上的 Spark SQL：Spark处理结构化数据的库，就像Hive SQL,Mysql一样，企业中用来做报表统计。提供通过Apache Hive的SQL变体Hive查询语言（HiveQL）与Spark进行交互的API。每个数据库表被当做一个RDD，Spark SQL查询被转换为Spark操作。 Spark Streaming：对实时数据流进行处理和控制。Spark Streaming允许程序能够像普通RDD一样处理实时数据。实时数据流处理组件，类似Storm。Spark Streaming提供了API来操作实时流数据。企业中用来从Kafka接收数据做实时统计。 MLlib：一个常用机器学习算法库，算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作。 GraphX：控制图、并行图操作和计算的一组算法和工具的集合。GraphX扩展了RDD API，包含控制图、创建子图、访问路径上所有顶点的操作 Spark架构的组成图如下： Cluster Manager：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器 Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。 Driver： 运行Application 的main()函数 Executor：执行器，是为某个Application运行在worker node上的一个进程 与Hadoop比较Hadoop MapReduce缺点： 表达能力有限 磁盘IO开销大，任务之间的衔接涉及IO开销 延迟高，Map任务要全部结束，reduce任务才能开始。 Spark借鉴Hadoop MapReduce优点的同时，解决了MapReuce所面临的问题，有如下优点： Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供多种数据集操作类型，编程模型比Hadoop MapReduce更灵活。 Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高 Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制。 Spark 的Executor利用多线程来执行具体的任务减少任务的启动开销； Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，有效减少IO开销； Spark编程模型Spark 应用程序从编写到提交、执行、输出的整个过程如图所示，图中描述的步骤如下： 1) 用户使用SparkContext提供的API（常用的有textFile、sequenceFile、runJob、stop等）编写Driver application程序。此外SQLContext、HiveContext及StreamingContext对SparkContext进行封装，并提供了SQL、Hive及流式计算相关的API。 2) 使用SparkContext提交的用户应用程序，首先会使用BlockManager和BroadcastManager将任务的Hadoop配置进行广播。然后由DAGScheduler将任务转换为RDD并组织成DAG，DAG还将被划分为不同的Stage。最后由TaskScheduler借助ActorSystem将任务提交给集群管理器（Cluster Manager）。 3) 集群管理器（ClusterManager）给任务分配资源，即将具体任务分配到Worker上，Worker创建Executor来处理任务的运行。Standalone、YARN、Mesos、EC2等都可以作为Spark的集群管理器。 spark计算模型RDD可以看做是对各种数据计算模型的统一抽象，Spark的计算过程主要是RDD的迭代计算过程。RDD的迭代计算过程非常类似于管道。分区数量取决于partition数量的设定，每个分区的数据只会在一个Task中计算。所有分区可以在多个机器节点的Executor上并行执行。 集群架构设计 整个集群分为 Master 节点和Worker 节点，相当于 Hadoop 的 Master 和 Slave 节点。 Master 节点上常驻 Master 守护进程，负责管理全部的 Worker 节点。 Worker 节点上常驻 Worker 守护进程，负责与 Master 节点通信并管理 executors。 Driver 官方解释是 “The process running the main() function of the application and creating the SparkContext”。Application 就是用户自己写的 Spark 程序（driver program)。 spark 运行流程与特点 构建Spark Application的运行环境，启动SparkContext SparkContext向资源管理器（可以是Standalone，Mesos，Yarn）申请运行Executor资源，并启动StandaloneExecutorbackend Executor向SparkContext申请Task SparkContext将应用程序分发给Executor SparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给Task Scheduler，最后由Task Scheduler将Task发送给Executor运行 Task在Executor上运行，运行完释放所有资源 特点： 每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行Task。这种Application隔离机制是有优势的，无论是从调度角度看（每个Driver调度他自己的任务），还是从运行角度看（来自不同Application的Task运行在不同JVM中），当然这样意味着Spark Application不能跨应用程序共享数据，除非将数据写入外部存储系统 Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了 提交SparkContext的Client应该靠近Worker节点（运行Executor的节点），最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换 Task采用了数据本地性和推测执行的优化机制 RDD特性及运行流程RDD特性 高效的容错性，根据DAG图恢复分区，数据复制或者记录日志 RDD血缘关系、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作 中间结果持久化到内存，数据在内存中的多个RDD操作之间进行传递，避免了不必要的读写磁盘开销存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化 RDD在Spark中运行大概分为以下三步： 创建RDD对象 DAGScheduler模块介入运算，计算RDD之间的依赖关系，RDD之间的依赖关系就形成了DAG 每一个Job被分为多个Stage。划分Stage的一个主要依据是当前计算因子的输入是否是确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销 创建 RDD 上面的例子除去最后一个 collect 是个动作，不会创建 RDD 之外，前面四个转换都会创建出新的 RDD 。因此第一步就是创建好所有 RDD( 内部的五项信息 )？创建执行计划 Spark 会尽可能地管道化，并基于是否要重新组织数据来划分阶段 (stage) ，例如本例中的 groupBy() 转换就会将整个执行计划划分成两阶段执行。最终会产生一个 DAG(directed acyclic graph ，有向无环图 ) 作为逻辑执行计划 调度任务 将各阶段划分成不同的 任务 (task) ，每个任务都是数据和计算的合体。在进行下一阶段前，当前阶段的所有任务都要执行完成。因为下一阶段的第一个转换一定是重新组织数据的，所以必须等当前阶段所有结果数据都计算出来了才能继续 spark运行模式standalone: 独立集群运行模式 Standalone模式使用Spark自带的资源调度框架 采用Master/Slaves的典型架构，选用ZooKeeper来实现Master的HA yarnSpark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式） Yarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://hadoop1:4040访问，而YARN通过http:// hadoop1:8088访问 YARN-client的工作流程步骤为： Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派 Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container） 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己 Spark Cluster模式: 在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动； 第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成 Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等 ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化 ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等 ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己 参考： 深入理解spark之架构与原理 Spark基本架构及运行原理]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程间通信方式]]></title>
    <url>%2F2019%2F07%2F13%2Fprocess-communication%2F</url>
    <content type="text"><![CDATA[本文转载自：进程间通信IPC (InterProcess Communication) 进程间通信的概念每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，在内核中开辟一块缓冲区，进程1把数据从用户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为进程间通信（IPC，InterProcess Communication） 进程间通信模型 进程间通信的7种方式第一类：传统的Unix通信机制 管道/匿名管道(pipe) 管道是半双工的，数据只能向一个方向流动；需要双方通信时，需要建立起两个管道。 只能用于父子进程或者兄弟进程之间(具有亲缘关系的进程); 单独构成一种独立的文件系统：管道对于管道两端的进程而言，就是一个文件，但它不是普通的文件，它不属于某种文件系统，而是自立门户，单独构成一种文件系统，并且只存在与内存中。 数据的读出和写入：一个进程向管道中写的内容被管道另一端的进程读出。写入的内容每次都添加在管道缓冲区的末尾，并且每次都是从缓冲区的头部读出数据。 进程间管道通信模型 管道的实质： 管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据，管道一端的进程顺序的将数据写入缓冲区，另一端的进程则顺序的读出数据。 该缓冲区可以看做是一个循环队列，读和写的位置都是自动增长的，不能随意改变，一个数据只能被读一次，读出来以后在缓冲区就不复存在了。 当缓冲区读空或者写满时，有一定的规则控制相应的读进程或者写进程进入等待队列，当空的缓冲区有新数据写入或者满的缓冲区有数据读出来时，就唤醒等待队列中的进程继续读写。 管道的局限： 管道的主要局限性正体现在它的特点上： 只支持单向数据流； 只能用于具有亲缘关系的进程之间； 没有名字； 管道的缓冲区是有限的（管道制存在于内存中，在管道创建时，为缓冲区分配一个页面大小）； 管道所传送的是无格式字节流，这就要求管道的读出方和写入方必须事先约定好数据的格式，比如多少字节算作一个消息（或命令、或记录）等等； 有名管道(FIFO) 匿名管道，由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道(FIFO)。 有名管道不同于匿名管道之处在于它提供了一个路径名与之关联，以有名管道的文件形式存在于文件系统中，这样，即使与有名管道的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过有名管道相互通信，因此，通过有名管道不相关的进程也能交换数据。值的注意的是，有名管道严格遵循先进先出(first in first out),对匿名管道及有名管道的读总是从开始处返回数据，对它们的写则把数据添加到末尾。它们不支持诸如lseek()等文件定位操作。有名管道的名字存在于文件系统中，内容存放在内存中。 匿名管道和有名管道总结： （1）管道是特殊类型的文件，在满足先入先出的原则条件下可以进行读写，但不能进行定位读写。 （2）匿名管道是单向的，只能在有亲缘关系的进程间通信；有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 （3）无名管道阻塞问题：无名管道无需显示打开，创建时直接返回文件描述符，在读写时需要确定对方的存在，否则将退出。如果当前进程向无名管道的一端写数据，必须确定另一端有某一进程。如果写入无名管道的数据超过其最大值，写操作将阻塞，如果管道中没有数据，读操作将阻塞，如果管道发现另一端断开，将自动退出。 （4）有名管道阻塞问题：有名管道在打开时需要确实对方的存在，否则将阻塞。即以读方式打开某管道，在此之前必须一个进程以写方式打开管道，否则阻塞。此外，可以以读写（O_RDWR）模式打开有名管道，即当前进程读，当前进程写，不会阻塞。 延伸阅读：该博客有匿名管道和有名管道的C语言实践 信号(Signal) 信号是Linux系统中用于进程间互相通信或者操作的一种机制，信号可以在任何时候发给某一进程，而无需知道该进程的状态。 如果该进程当前并未处于执行状态，则该信号就由内核保存起来，直到该进程恢复执行并传递给它为止。 如果一个信号被进程设置为阻塞，则该信号的传递被延迟，直到其阻塞被取消是才被传递给进程。 Linux系统中常用信号： （1）SIGHUP：用户从终端注销，所有已启动进程都将收到该进程。系统缺省状态下对该信号的处理是终止进程。 （2）SIGINT：程序终止信号。程序运行过程中，按Ctrl+C键将产生该信号。 （3）SIGQUIT：程序退出信号。程序运行过程中，按Ctrl+\\键将产生该信号。 （4）SIGBUS和SIGSEGV：进程访问非法地址。 （5）SIGFPE：运算中出现致命错误，如除零操作、数据溢出等。 （6）SIGKILL：用户终止进程执行信号。shell下执行kill -9发送该信号。 （7）SIGTERM：结束进程信号。shell下执行kill 进程pid发送该信号。 （8）SIGALRM：定时器信号。 （9）SIGCLD：子进程退出信号。如果其父进程没有忽略该信号也没有处理该信号，则子进程退出后将形成僵尸进程。 信号来源 信号是软件层次上对中断机制的一种模拟，是一种异步通信方式，信号可以在用户空间进程和内核之间直接交互，内核可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件主要有两个来源： 硬件来源：用户按键输入Ctrl+C退出、硬件异常如无效的存储访问等。 软件终止：终止进程信号、其他进程调用kill函数、软件异常产生信号。 信号生命周期和处理流程 （1）信号被某个进程产生，并设置此信号传递的对象（一般为对应进程的pid），然后传递给操作系统； （2）操作系统根据接收进程的设置（是否阻塞）而选择性的发送给接收者，如果接收者阻塞该信号（且该信号是可以阻塞的），操作系统将暂时保留该信号，而不传递，直到该进程解除了对此信号的阻塞（如果对应进程已经退出，则丢弃此信号），如果对应进程没有阻塞，操作系统将传递此信号。 （3）目的进程接收到此信号后，将根据当前进程对此信号设置的预处理方式，暂时终止当前代码的执行，保护上下文（主要包括临时寄存器数据，当前程序位置以及当前CPU的状态）、转而执行中断服务程序，执行完成后在回复到中断的位置。当然，对于抢占式内核，在中断返回时还将引发新的调度。 信号的生命周期 消息(Message)队列 消息队列是存放在内核中的消息链表，每个消息队列由消息队列标识符表示。 与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。 另外与管道不同的是，消息队列在某个进程往一个队列写入消息之前，并不需要另外某个进程在该队列上等待消息的到达。延伸阅读：消息队列C语言的实践 消息队列特点总结： （1）消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识. （2）消息队列允许一个或多个进程向它写入与读取消息. （3）管道和消息队列的通信数据都是先进先出的原则。 （4）消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比FIFO更有优势。 （5）消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。 （6）目前主要有两种类型的消息队列：POSIX消息队列以及System V消息队列，系统V消息队列目前被大量使用。系统V消息队列是随内核持续的，只有在内核重启或者人工删除时，该消息队列才会被删除。 共享内存(share memory) 使得多个进程可以直接读写同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。 为了在多个进程间交换信息，内核专门留出了一块内存区，可以由需要访问的进程将其映射到自己的私有地址空间。进程就可以直接读写这一块内存而不需要进行数据的拷贝，从而大大提高效率。 由于多个进程共享一段内存，因此需要依靠某种同步机制（如信号量）来达到进程间的同步及互斥。 延伸阅读：Linux支持的主要三种共享内存方式：mmap()系统调用、Posix共享内存，以及System V共享内存实践 共享内存原理图 信号量(semaphore) 信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。 为了获得共享资源，进程需要执行下列操作： （1）创建一个信号量：这要求调用者指定初始值，对于二值信号量来说，它通常是1，也可是0。 （2）等待一个信号量：该操作会测试这个信号量的值，如果小于0，就阻塞。也称为P操作。 （3）挂出一个信号量：该操作将信号量的值加1，也称为V操作。 为了正确地实现信号量，信号量值的测试及减1操作应当是原子操作（即P、V操作都是原子操作）。为此，信号量通常是在内核中实现的。Linux环境中，有三种类型：Posix（可移植性操作系统接口）有名信号量（使用Posix IPC名字标识）、Posix基于内存的信号量（存放在共享内存区中）、System V信号量（在内核中维护）。这三种信号量都可用于进程间或线程间的同步。 两个进程使用一个二值信号量 两个进程所以用一个Posix有名二值信号量 一个进程两个线程共享基于内存的信号量 信号量与普通整型变量的区别： （1）信号量是非负整型变量，除了初始化之外，它只能通过两个标准原子操作：wait(semap) , signal(semap) ; 来进行访问； （2）操作也被成为PV原语（P来源于荷兰语proberen”测试”，V来源于荷兰语verhogen”增加”，P表示通过的意思，V表示释放的意思），而普通整型变量则可以在任何语句块中被访问； 信号量与互斥量之间的区别： （1）互斥量用于线程的互斥，信号量用于线程的同步。这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。 互斥：**是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。** 同步：**是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。** 在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源 （2）互斥量值只能为0/1，信号量值可以为非负整数。 也就是说，一个互斥量只能用于一个资源的互斥访问，它不能实现多个资源的多线程互斥问题。信号量可以实现多个同类资源的多线程互斥和同步。当信号量为单值信号量是，也可以完成一个资源的互斥访问。 （3）互斥量的加锁和解锁必须由同一线程分别对应使用，信号量可以由一个线程释放，另一个线程得到。 套接字(socket) 套接字是一种通信机制，凭借这种机制，客户/服务器（即要进行通信的进程）系统的开发工作既可以在本地单机上进行，也可以跨网络进行。也就是说它可以让不在同一台计算机但通过网络连接计算机上的进程进行通信。 Socket是应用层和传输层之间的桥梁 套接字是支持TCP/IP的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。 套接字特性 套接字的特性由3个属性确定，它们分别是：域、端口号、协议类型。 （1）套接字的域 它指定套接字通信中使用的网络介质，最常见的套接字域有两种： 一是AF_INET，它指的是Internet网络。当客户使用套接字进行跨网络的连接时，它就需要用到服务器计算机的IP地址和端口来指定一台联网机器上的某个特定服务，所以在使用socket作为通信的终点，服务器应用程序必须在开始通信之前绑定一个端口，服务器在指定的端口等待客户的连接。 另一个域AF_UNIX，表示UNIX文件系统，它就是文件输入/输出，而它的地址就是文件名。 （2）套接字的端口号 每一个基于TCP/IP网络通讯的程序(进程)都被赋予了唯一的端口和端口号，端口是一个信息缓冲区，用于保留Socket中的输入/输出信息，端口号是一个16位无符号整数，范围是0-65535，以区别主机上的每一个程序（端口号就像房屋中的房间号），低于256的端口号保留给标准应用程序，比如pop3的端口号就是110，每一个套接字都组合进了IP地址、端口，这样形成的整体就可以区别每一个套接字。 （3）套接字协议类型 因特网提供三种通信机制， 一是流套接字，流套接字在域中通过TCP/IP连接实现，同时也是AF_UNIX中常用的套接字类型。流套接字提供的是一个有序、可靠、双向字节流的连接，因此发送的数据可以确保不会丢失、重复或乱序到达，而且它还有一定的出错后重新发送的机制。 二个是数据报套接字，它不需要建立连接和维持一个连接，它们在域中通常是通过UDP/IP协议实现的。它对可以发送的数据的长度有限制，数据报作为一个单独的网络消息被传输,它可能会丢失、复制或错乱到达，UDP不是一个可靠的协议，但是它的速度比较高，因为它并一需要总是要建立和维持一个连接。 三是原始套接字，原始套接字允许对较低层次的协议直接访问，比如IP、 ICMP协议，它常用于检验新的协议实现，或者访问现有服务中配置的新设备，因为RAW SOCKET可以自如地控制Windows下的多种协议，能够对网络底层的传输机制进行控制，所以可以应用原始套接字来操纵网络层和传输层应用。比如，我们可以通过RAW SOCKET来接收发向本机的ICMP、IGMP协议包，或者接收TCP/IP栈不能够处理的IP包，也可以用来发送一些自定包头或自定协议的IP包。网络监听技术很大程度上依赖于SOCKET_RAW。 原始套接字与标准套接字的区别在于： 原始套接字可以读写内核没有处理的IP数据包，而流套接字只能读取TCP协议的数据，数据报套接字只能读取UDP协议的数据。因此，如果要访问其他协议发送数据必须使用原始套接字。 套接字通信的建立 Socket通信基本流程 服务器端 （1）首先服务器应用程序用系统调用socket来创建一个套接字，它是系统分配给该服务器进程的类似文件描述符的资源，它不能与其他的进程共享。 （2）然后，服务器进程会给套接字起个名字，我们使用系统调用bind来给套接字命名。然后服务器进程就开始等待客户连接到这个套接字。 （3）接下来，系统调用listen来创建一个队列并将其用于存放来自客户的进入连接。 （4）最后，服务器通过系统调用accept来接受客户的连接。它会创建一个与原有的命名套接不同的新套接字，这个套接字只用于与这个特定客户端进行通信，而命名套接字（即原先的套接字）则被保留下来继续处理来自其他客户的连接（建立客户端和服务端的用于通信的流，进行通信）。 客户端 （1）客户应用程序首先调用socket来创建一个未命名的套接字，然后将服务器的命名套接字作为一个地址来调用connect与服务器建立连接。 （2）一旦连接建立，我们就可以像使用底层的文件描述符那样用套接字来实现双向数据的通信（通过流进行数据传输）。 延伸阅读 ：Java socket编程 引用 进程间通信–管道 Linux进程间通信——使用共享内存 进程间通信—共享内存 信号量与互斥锁 信号量]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程和线程的区别]]></title>
    <url>%2F2019%2F07%2F13%2Fprocess-thread%2F</url>
    <content type="text"><![CDATA[根本区别：进程是操作系统资源分配的基本单位，而线程是任务调度和执行的基本单位。 进程进程是资源（CPU、内存等）分配的基本单位，它是程序执行时的一个实例。程序运行时系统就会创建一个进程，并为它分配资源，然后把该进程放入进程就绪队列，进程调度器选中它的时候就会为它分配CPU时间，程序开始真正运行。 线程线程是程序执行时的最小单位，它是进程的一个执行流，是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。同样多线程也可以实现并发操作，每个请求分配一个线程来处理。 进程和线程的区别 根本区别：进程是资源分配的最小单位，线程是程序执行的最小单位。 开销方面：每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。 进程有自己的独立地址空间，每启动一个进程，系统就会为它分配地址空间，建立数据表来维护代码段、堆栈段和数据段，这种操作非常昂贵。而线程是共享进程中的数据的，使用相同的地址空间，因此CPU切换一个线程的花费远比进程要小很多，同时创建一个线程的开销也比进程要小很多。 通信方面：线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。 所处环境：在操作系统中能同时运行多个进程（程序）；而在同一个进程（程序）中有多个线程同时执行（通过CPU调度，在每个时间片中只有一个线程执行） 内存分配方面：系统在运行的时候会为每个进程分配不同的内存空间；而对线程而言，除了CPU外，系统不会为线程分配内存（线程所使用的资源来自其所属进程的资源），线程组之间只能共享资源。 包含关系：一个程序至少有一个进程,一个进程至少有一个线程. 参考：一道面试题：说说进程和线程的区别进程和线程的主要区别（总结）]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>进程</tag>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-变态青蛙跳台阶]]></title>
    <url>%2F2019%2F07%2F13%2Fjianzhioffer-jumping-stairs%2F</url>
    <content type="text"><![CDATA[青蛙跳台阶题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 解法1123456789101112131415161718public class Solution &#123; int res = 0; public int JumpFloor(int target) &#123; int count = 0; helper(count, target); return res; &#125; void helper(int count, int target)&#123; if(count == target)&#123; res++; return; &#125; if(count &gt; target) return; helper(count + 1,target); // helper 的第一个参数不能是 ++count,因为这样的话 count 会自增1，影响后面的 helper 调用 helper(count + 2,target); &#125;&#125; 解法2123456public class Solution &#123; public int JumpFloor(int target) &#123; if(target &lt; 0) return 0; return target &lt; 3 ? target : JumpFloor(target - 1) + JumpFloor(target - 2); &#125;&#125; 解法3类似于求斐波那契数列。 12345678910111213public class Solution &#123; public int JumpFloor(int target) &#123; int[] steps = new int[target]; if(target == 0) return 0; if(target == 1) return 1; if(target == 2) return 2; steps[0] = 1; steps[1] = 2; for(int i = 2;i &lt; target;i++) steps[i] = steps[i - 1] + steps[i - 2]; return steps[target - 1]; &#125;&#125; 解法4比解法3空间效率更高，常数级别。 1234567891011121314public class Solution &#123; public int JumpFloor(int target) &#123; if(target == 0) return 0; if(target == 1) return 1; if(target == 2) return 2; int a = 1, b = 2, c = 0; for(int i = 2;i &lt; target;i++)&#123; c = a + b; a = b; b = c; &#125; return c; &#125;&#125; 变态蛙跳台阶题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 解法分析：因为n级台阶，第一步有n种跳法：跳1级、跳2级、到跳n级跳1级，剩下n-1级，则剩下跳法是f(n-1)跳2级，剩下n-2级，则剩下跳法是f(n-2)所以f(n)=f(n-1)+f(n-2)+…+f(1)因为f(n-1)=f(n-2)+f(n-3)+…+f(1)所以f(n)=2*f(n-1) 1234public class Solution &#123; public int JumpFloorII(int target) return 1&lt;&lt;--target;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer:矩阵覆盖]]></title>
    <url>%2F2019%2F07%2F12%2Fjianzhioffer-matrix-coverage%2F</url>
    <content type="text"><![CDATA[题目描述我们可以用2 * 1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2 * 1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ AC代码1递归的方式。 设2 * n的大矩阵共有f(n)种覆盖方法，那么 f(n) = f(n - 1) + f(n - 2)，即 f(n) 可以由f(n - 1) 加上一个竖着的2 * 1的小矩阵拼接， 也可以由f(n - 2) 加上两个横着的2 * 1的小矩阵拼接。 12345678public class Solution &#123; public int RectCover(int target) &#123; if(target &lt;= 0) return 0; if(target == 1) return 1; if(target == 2) return 2; return RectCover(target - 1) + RectCover(target - 2); &#125;&#125; AC代码2123456789101112131415public class Solution &#123; public int RectCover(int target) &#123; if(target &lt;= 0) return 0; if(target == 1) return 1; if(target == 2) return 2; int[] nums = new int[target]; nums[0] = 1; nums[1] = 2; for(int i = 2;i &lt; target;i++)&#123; nums[i] = nums[i - 1] + nums[i - 2]; &#125; return nums[target - 1]; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-121:Best Time to Buy and Sell Stock]]></title>
    <url>%2F2019%2F07%2F12%2Fleetcode-121%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/best-time-to-buy-and-sell-stock/ 题目描述Say you have an array for which the ith element is the price of a given stock on day i. If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit. Note that you cannot sell a stock before you buy one. 测试用例Example 1: 1234Input: [7,1,5,3,6,4]Output: 5Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5. Not 7-1 = 6, as selling price needs to be larger than buying price. Example 2: 123Input: [7,6,4,3,1]Output: 0Explanation: In this case, no transaction is done, i.e. max profit = 0. AC代码1从后往前遍历，记录当前所有元素中的最大价格maxPrice，然后计算从当前元素prices[i] 开始到结尾的最大利润prices[i] - max，更新最大利润max，最大价格maxPrice。 123456789101112131415class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices == null || prices.length == 0) return 0; int max = 0; int len = prices.length; int maxPrice = prices[len - 1]; int i = len - 2; while(i &gt;= 0)&#123; max = Math.max(max, maxPrice - prices[i]); maxPrice = Math.max(maxPrice, prices[i]); i--; &#125; return max; &#125;&#125; AC代码2引入数组maxPrices，maxPrices[i]表示从0到i为止的最大利润。 1234567891011121314151617class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices == null || prices.length == 0) return 0; int len = prices.length; int max = 0; int minPrice = prices[0]; int[] maxPrices = new int[len]; int i = 1; while(i &lt; len)&#123; max = Math.max(max, prices[i] - minPrice); minPrice = Math.min(minPrice, prices[i]); maxPrices[i] = max; i++; &#125; return maxPrices[len - 1]; &#125;&#125; AC代码3引入数组maxPrices，maxPrices[i]表示从i到结尾的最大利润。 1234567891011121314151617class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices == null || prices.length == 0) return 0; int len = prices.length; int max = 0; int maxPrice = prices[len - 1]; int[] maxPrices = new int[len]; int i = len - 2; while(i &gt;= 0)&#123; max = Math.max(max, maxPrice - prices[i]); maxPrices[i] = max; maxPrice = Math.max(maxPrice, prices[i]); i--; &#125; return maxPrices[0]; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer:二维数组中的查找]]></title>
    <url>%2F2019%2F07%2F12%2Fjianzhioffer-find-two-dimensional-array%2F</url>
    <content type="text"><![CDATA[题目描述在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 AC代码从二维数组对应的矩阵的右上角或者左下角开始查找。 123456789101112131415161718public class Solution &#123; public boolean Find(int target, int [][] array) &#123; if(array == null || array.length == 0 || array[0].length == 0) return false; int rows = array.length; int clos = array[0].length; int curX = 0; int curY = clos - 1; while(array[curX][curY] != target) &#123; if(array[curX][curY] &gt; target) if(curY - 1 &gt;= 0) curY--; else break; else if(curX + 1 &lt; rows) curX++; else break; &#125; if(array[curX][curY] == target) return true; return false; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-21:Merge Two Sorted Lists(合并两个有序链表)]]></title>
    <url>%2F2019%2F07%2F11%2Fleetcode-21%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/merge-two-sorted-lists/ 题目描述 题目难度：Easy Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. Example: 12Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 AC代码1普通的解法 1234567891011121314151617181920212223class Solution &#123; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if(l1 == null) return l2; if(l2 == null) return l1; ListNode head = new ListNode(0), cur = head; while(l1 != null &amp;&amp; l2 != null)&#123; if(l1.val &gt; l2.val)&#123; cur.next = l2; l2 = l2.next; cur = cur.next; &#125; else&#123; cur.next = l1; l1 = l1.next; cur = cur.next; &#125; &#125; if(l1 != null) cur.next = l1; if(l2 != null) cur.next = l2; return head.next; &#125;&#125; AC代码递归解法 12345678910111213141516171819202122class Solution &#123; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; return mergeCore(l1,l2); &#125; private ListNode mergeCore(ListNode l1, ListNode l2)&#123; if(l1 != null &amp;&amp; l2 != null)&#123; if(l1.val &lt; l2.val)&#123; l1.next = mergeCore(l1.next, l2); return l1; &#125; else&#123; l2.next = mergeCore(l1, l2.next); return l2; &#125; &#125; if(l1 != null) return l1; else return l2; &#125; &#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-221:Maximal Square(最大正方形)]]></title>
    <url>%2F2019%2F07%2F11%2Fleetcode-221%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/maximal-square/ 题目描述 题目难度：Medium Given a 2D binary matrix filled with 0’s and 1’s, find the largest square containing only 1’s and return its area. Example: 12345678Input: 1 0 1 0 01 0 1 1 11 1 1 1 11 0 0 1 0Output: 4 解题思路动态规划，构造一个等大小的dp[n][m]矩阵，dp[i][j]表示以matrix[i][j]为右下角的全1正方形的边长。初始化dp[i][0]和dp[0][j]分别为matrix[i][0]和matrix[0][j]，因为以边缘这一行一列中的点为右下角的全1正方形边长只有0和1两种情况，分别由他们本身是0还是1决定。初始化之后开始遍历这个dp矩阵，如果matrix[i][j]为0，则dp[i][j]为1，如果matrix[i][j]为1，则dp[i][j]=min{dp[i-1][j],dp[i][j-1],dp[i-1][j-1]}+1，这一步可以这样理解： 只有满足matrix[i][j]左方上方和左上方三个地方的正方形边长为2的时候，它的边长才能为3，任意一个为1，则它的边长为2，所以取三者最小值+1就是以该点为右下角的全1正方形边长。最后只用遍历dp矩阵找出最大值即可。 作者：I讨厌鬼I 链接：https://www.jianshu.com/p/100a5937869d 来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 AC代码12345678910111213141516class Solution &#123; public int maximalSquare(char[][] a) &#123; if(a.length == 0) return 0; int m = a.length, n = a[0].length, result = 0; int[][] b = new int[m+1][n+1]; for (int i = 1 ; i &lt;= m; i++) &#123; for (int j = 1; j &lt;= n; j++) &#123; if(a[i-1][j-1] == '1') &#123; b[i][j] = Math.min(Math.min(b[i][j-1] , b[i-1][j-1]), b[i-1][j]) + 1; result = Math.max(b[i][j], result); // update result &#125; &#125; &#125; return result*result;&#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-53:Maximum Subarray]]></title>
    <url>%2F2019%2F07%2F11%2Fleetcode-53%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/maximum-subarray/ 题目描述 题目难度：Easy Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Example: 123Input: [-2,1,-3,4,-1,2,1,-5,4],Output: 6Explanation: [4,-1,2,1] has the largest sum = 6. Follow up: If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle. AC代码12345678910111213class Solution &#123; public int maxSubArray(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; int max = nums[0]; int sum = max; for(int i = 1;i &lt; nums.length;i++)&#123; if(nums[i] &gt;= sum + nums[i]) sum = nums[i]; else sum += nums[i]; max = Math.max(max, sum); &#125; return max; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-142:Linked List Cycle II]]></title>
    <url>%2F2019%2F07%2F10%2Fleetcode-142%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/linked-list-cycle-ii/ 题目描述 题目难度：Medium Given a linked list, return the node where the cycle begins. If there is no cycle, return null. To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list. Note: Do not modify the linked list. 测试用例Example 1: 123Input: head = [3,2,0,-4], pos = 1Output: tail connects to node index 1Explanation: There is a cycle in the linked list, where tail connects to the second node. Example 2: 123Input: head = [1,2], pos = 0Output: tail connects to node index 0Explanation: There is a cycle in the linked list, where tail connects to the first node. Example 3: 123Input: head = [1], pos = -1Output: no cycleExplanation: There is no cycle in the linked list. Follow-up: Can you solve it without using extra space? AC代码12345678910111213141516171819202122232425262728293031/** * Definition for singly-linked list. * class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode detectCycle(ListNode head) &#123; if(head == null || head.next == null) return null; ListNode slowNode = head; ListNode quickNode = head; while(quickNode != null &amp;&amp; quickNode.next != null)&#123; slowNode = slowNode.next; quickNode = quickNode.next.next; if(slowNode == quickNode)&#123; quickNode = head; while(slowNode != quickNode)&#123; slowNode = slowNode.next; quickNode = quickNode.next; &#125; return quickNode; &#125; &#125; return null; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-92:Reverse Linked List II]]></title>
    <url>%2F2019%2F07%2F10%2Fleetcode-92%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/reverse-linked-list-ii/ 题目描述Reverse a linked list from position m to n. Do it in one-pass. Note: 1 ≤ m ≤ n ≤ length of list. Example: 12Input: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL, m = 2, n = 4Output: 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;NULL AC代码1朴素的解法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseBetween(ListNode head, int m, int n) &#123; if(m == n) return head; ListNode newHead = null; ListNode curNode = null; int i = 0; if(m == 1) &#123; //从头节点开始 newHead = new ListNode(0); curNode = head; ListNode end = head; //m - n之间的元素翻转完成之后的尾节点 while(i &lt; n - m + 1)&#123; curNode = head; head = head.next; curNode.next = newHead.next; newHead.next = curNode; i++; &#125; end.next = head; return newHead.next; &#125; else&#123; i = 0; newHead = head; while(i &lt; m - 2) &#123; head = head.next; i++; &#125; ListNode start = head; // m 的前一个节点 ListNode end = head.next; //m - n之间的元素翻转完成之后的尾节点 head = head.next; ListNode fakeNode = new ListNode(0); curNode = head; i = 0; while(i &lt; n - m + 1)&#123; curNode = head; head = head.next; curNode.next = fakeNode.next; fakeNode.next = curNode; i++; &#125; if(head != null) end.next = head; start.next = fakeNode.next; return newHead; &#125; &#125;&#125; AC代码2优雅的解法，借用leetcode上面的代码 举例： 有一个链表， 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null, m = 2，n = 4，那么翻转后的链表为1 -&gt; 4 -&gt; 3 -&gt; 2 -&gt; 5 -&gt; 6 -&gt; null。 首先创建一个假的头结点 dummy 0，然后把它放到原链表的最前面，现在的链表是 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null。 然后创建两指针，并初始化 12ListNode pre = dummy;ListNode cur = dummy.next; 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; nullpre cur（pre 指向0，cur指向1） 然后使cur指向要翻转的第一个节点2，pre指向cur的前一个节点1 123456int i = m;while (i &gt; 1) &#123; pre = pre.next; cur = cur.next; i--;&#125; 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null pre cur（pre 指向1，cur指向2） 接下来开始进行翻转操作：现在的链表: 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null 翻转一次后的链表 0 -&gt; 1 -&gt; 3 -&gt; 2 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null 1234567while (n - m &gt; 0) &#123; ListNode temp = cur.next; cur.next = temp.next; temp.next = pre.next; pre.next = temp; n--;&#125; 1ListNode temp = cur.next; 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null pre cur temp（pre 指向1，cur指向2,temp 指向3） 1cur.next = temp.next; 0 -&gt; 1 -&gt; 2 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null2 和 3 同时指向 4， 2 到 3的指针断开，（pre 指向1，cur指向2,temp 指向3） 1temp.next = pre.next; 3 指向 23-&gt; 2 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null0 -&gt; 1 -&gt; 2 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null 1pre.next = temp; 1 指向 30 -&gt; 1 -&gt; 3 -&gt; 2 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; null pre cur (pre 指向1，cur指向2) 上述是翻转第一次的过程，共需要翻转n - m = 2 次。可以看到 pre 和 cur 指针不会变。 完整代码： 123456789101112131415161718192021222324252627282930313233/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseBetween(ListNode head, int m, int n) &#123; if (head == null || m == n) &#123; return head; &#125; ListNode dummy = new ListNode(0); dummy.next = head; ListNode pre = dummy; ListNode cur = dummy.next; int i = m; while (i &gt; 1) &#123; pre = pre.next; cur = cur.next; i--; &#125; while (n - m &gt; 0) &#123; ListNode temp = cur.next; cur.next = temp.next; temp.next = pre.next; pre.next = temp; n--; &#125; return dummy.next; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[https的交互过程]]></title>
    <url>%2F2019%2F07%2F10%2Fhttps-interaction-process%2F</url>
    <content type="text"><![CDATA[本文转载自：https的交互过程 HTTPS其实是由两部分组成：HTTP + SSL / TLS，也就是在HTTP上又加了一层处理加密信息的模块。服务端和客户端的信息传输都会被加密，所以传输的数据都是加密后的数据。 下面介绍https具体是如何进行加密，解密，验证的，也就是https的交互过程。 https的交互过程客户端发起HTTPS请求比如在浏览器里输入https://blog.csdn.net/yjclsx，然后请求到server的**443**端口。浏览器会把自身支持的一系列Cipher Suite（密钥算法套件，简称Cipher）发送给服务端。 服务端返回服务端接收到客户端所有的Cipher后与自身支持的对比，如果不支持则连接断开，反之则会从中选出一种加密算法和HASH算法以证书的形式返回给客户端，证书中还包含了公钥、颁证机构、网址、失效日期等等。 采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请。区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面。这套证书其实就是一对公钥和私钥，用于非对称加密。 客户端验证证书并响应客户端收到服务端响应后会做以下几件事： 验证证书的合法性颁发证书的机构是否合法与是否过期，证书中包含的网站地址是否与正在访问的地址一致等。证书验证通过后，在浏览器的地址栏提示网站安全。 生成随机密码如果证书验证通过，或者用户接受了不授信的证书，此时客户端会生成一串随机数，用于后续进行对称加密，以后称之为密钥，然后用证书中的公钥加密这个密钥。 HASH握手信息用最开始约定好的HASH方式，把握手消息取HASH值，然后用密钥加密 “握手消息+握手消息的HASH值(即签名)” 。在这里之所以要加上握手消息的HASH值，主要是把握手消息做一个签名，用于验证握手消息在传输过程中没有被篡改过。 最后将之前生成的所有信息发送给服务端。 服务端解析和验证服务端接收客户端发来的数据之后要做以下的操作： 解析和验证使用自己的私钥将客户端使用公钥加密的信息解密得到之前客户端生成的密钥。然后使用密钥解密客户端发来的加密后的握手消息得到 “握手消息+握手消息的HASH值(即签名)” ，最后同样对握手信息进行HASH签名，比对和客户端传来的HASH值是否一致，如果一致则HASH验证通过，即表明握手消息在传输过程中没有被篡改过。 返回同样使用密钥加密一段握手消息，发送给客户端。 客户端解析和验证客户端用密钥解密并计算握手消息的HASH，如果与服务端发来的HASH一致，此时握手过程结束。 后续通信由于RSA等非对称加密算法的性能是非常低的，原因在于寻找大素数、大数计算、数据分割需要耗费很多的CPU周期，所以一般的HTTPS连接只在第一次握手时使用非对称加密，通过握手交换对称加密的密钥，之后所有的通信数据将由之前客户端生成的密钥进行对称加密。 因为这串密钥只有客户端和服务端知道，所以即使中间请求被拦截也是没法解密数据的，以此保证了通信的安全。 结语这里客户端与服务端互相发送加密的握手消息并验证，目的是为了保证双方都安全地获得了一致的密钥，并且可以正常的加密解密数据，为后续真正数据的传输做一次测试。另外，HTTPS一般使用的加密与HASH算法如下：非对称加密算法：RSA，DSA/DSS对称加密算法：AES，RC4，DES, 3DESHASH算法：MD5，SHA1，SHA256 参考文献1、http://www.cnblogs.com/zhuqil/archive/2012/07/23/2604572.html 2、https://www.cnblogs.com/zery/p/5164795.html]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux修改权限命令--chmod]]></title>
    <url>%2F2019%2F07%2F10%2FLinux-chmod%2F</url>
    <content type="text"><![CDATA[本文转载自：Linux修改权限功能——chmod chmod命令概况chmod是Linux/Unix中修改文件或者目录权限的命令，通过修改权限可以让指定的人对文件可读、可写、可运行，极大地保证了数据的安全性。 chmod命令的语法 命令名称： chmod 执行权限： 所有用户 功能描述： 改变文件或目录权限 语法： 第一种方法 chmod [{ugoa}{+-=}{rwx}] [文件或目录] 备注： u：所有者 g：所属组 o：其他人 a：所有人 +：为用户增加权限 -：为用户减少权限 =：为用户赋予权限 r：读权限 w：写权限 x：执行权限 第二种方法 chmod -R [mode=421] [文件或目录] ←（这种方法用的比较多） 备注： r：4 w：2 x：1 r为读权限，可以用4来表示， w为写权限，可以用2来表示， x为执行权限，可以用1来表示。 -R 递归修改（就是将嵌套在很多文件夹中的文件权限修改了，如果没有这个，只能到指定的文 件夹下进行修改） 范例： chmod 777 /etc/hurenxiang 将hurenxiang这个文件夹权限改为对所有用户可读，可写，可执行 chmod 775 /etc/caiyao 将caiyao这个文件夹权限改为其他用户不可写 chmod命令结构详解在Linux中输入ll，会出现指定目录下文件/目录的详细信息，本文重点讲解权限部分，也就是下图中红色框中的部分。 详解下图：权限一共是10个字符，第1个字符分为1组，后面9个字符分为3组。 第1个字符有两种情况：如果是“d”则代表这是一个文件夹，如果是“—”代表这是一个文件； 第2、3、4个字符：这3个字符共同代表的是文件的拥有者，可以用u来表示，拥有四个权限：r：读权限 ， w：写权限，x：执行权限，—：无权限； 第5、6、7个字符：这3个字符代表的是文件所属群组，可以用g来表示，拥有四个权限：r：读权限 ， w：写权限，x：执行权限，—：无权限； 第8、9、10个字符：这3个字符代表的是除拥有者和所属群组之外的其他所有的用户，可以用o来表示，拥有四个权限：r：读权限 ， w：写权限，x：执行权限，—：无权限。 注意：后面9个字符除了u，g，o外，还有一个a，代表了所有用户。 chmod命令的使用chmod命令都详细讲解完了，下面用几个例子巩固一下： 这是通过两个实例，详细回顾了一波上面的知识点。 -rw–wx–x //该文件的拥有者有读取和写入的权限，但没有执行权限； 所在群组没有读取的权限，但有写入和执行的权限； 其他用户没有读取和写入的权限，但有执行的权限 d–xr-xrw- //该文件夹的拥有者没有读取和写入的权限，但有执行的权限； 所在群组没有写入的权限，但有读取和执行的权限； 其他用户没有执行权限，但有读取和写入的权限 这是修改权限的一种方法实例。 chmod u-w , g+r hurenxiang //对hurenxiang文件设置本人没有写入的权限，设置所在群组有读取的权限chmod g+x , o+w hurenxiang //对hurenxiang文件设置群组有执行的权限，设置其他用户有写入的权限 这是修改权限的另一种方法实例（建议）。 chmod 777 hurenxiang //对hurenxiang文件所有的用户都有读取，写入，执行的权限chmod 751 hurenxiang //对hurenxiang文件群组中的人没有写入的权限，其他的用户没有读取和写入的权限]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>chmod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-206:Reverse Linked List(翻转链表)]]></title>
    <url>%2F2019%2F07%2F10%2Fleetcode-206%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/reverse-linked-list/ 题目描述 题目难度：Easy Reverse a singly linked list. Example: 12Input: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULLOutput: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL Follow up: A linked list can be reversed either iteratively or recursively. Could you implement both? Solution1需要创建一个新节点 此方法相当于头插法 1234567891011121314151617181920212223/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseList(ListNode head) &#123; if(head == null || head.next == null) return head; ListNode dummyHead = new ListNode(0); ListNode cur = head; ListNode next; while(cur != null)&#123; next = cur.next; cur.next = dummyHead.next; dummyHead.next = cur; cur = next; &#125; return dummyHead.next; &#125;&#125; Solution2不需要创建新节点 代码来自LeetCode 12345678910111213class Solution &#123; public ListNode reverseList(ListNode head) &#123; /* iterative solution */ ListNode newHead = null; while (head != null) &#123; ListNode next = head.next; head.next = newHead; newHead = head; head = next; &#125; return newHead; &#125;&#125; Solution3递归解法 代码来自LeetCode 1234567891011121314class Solution &#123;public ListNode reverseList(ListNode head) &#123; /* recursive solution */ return reverseListInt(head, null);&#125;private ListNode reverseListInt(ListNode head, ListNode newHead) &#123; if (head == null) return newHead; ListNode next = head.next; head.next = newHead; return reverseListInt(next, head);&#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
        <tag>翻转链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-160:Intersection of Two Linked Lists(两个链表的交点)]]></title>
    <url>%2F2019%2F07%2F10%2Fleetcode-160%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/intersection-of-two-linked-lists/ 题目描述 题目难度：Easy Write a program to find the node at which the intersection of two singly linked lists begins. For example, the following two linked lists: begin to intersect at node c1. 测试用例Example 1: 123Input: intersectVal = 8, listA = [4,1,8,4,5], listB = [5,0,1,8,4,5], skipA = 2, skipB = 3Output: Reference of the node with value = 8Input Explanation: The intersected node&apos;s value is 8 (note that this must not be 0 if the two lists intersect). From the head of A, it reads as [4,1,8,4,5]. From the head of B, it reads as [5,0,1,8,4,5]. There are 2 nodes before the intersected node in A; There are 3 nodes before the intersected node in B. Example 2: 123Input: intersectVal = 2, listA = [0,9,1,2,4], listB = [3,2,4], skipA = 3, skipB = 1Output: Reference of the node with value = 2Input Explanation: The intersected node&apos;s value is 2 (note that this must not be 0 if the two lists intersect). From the head of A, it reads as [0,9,1,2,4]. From the head of B, it reads as [3,2,4]. There are 3 nodes before the intersected node in A; There are 1 node before the intersected node in B. Example 3: 1234Input: intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2Output: nullInput Explanation: From the head of A, it reads as [2,6,4]. From the head of B, it reads as [1,5]. Since the two lists do not intersect, intersectVal must be 0, while skipA and skipB can be arbitrary values.Explanation: The two lists do not intersect, so return null. Notes: If the two linked lists have no intersection at all, return null. The linked lists must retain their original structure after the function returns. You may assume there are no cycles anywhere in the entire linked structure. Your code should preferably run in O(n) time and use only O(1) memory. AC代码12345678910111213141516171819202122/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; ListNode p1 = headA; ListNode p2 = headB; while(p1 != p2)&#123; p1 = (p1 == null ? headB : p1.next); p2 = (p2 == null ? headA : p2.next); &#125; return p1; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP中的粘包现象产生的原因以及解决办法]]></title>
    <url>%2F2019%2F07%2F10%2Ftcp-sticky-bag%2F</url>
    <content type="text"><![CDATA[本文转载自：TCP粘包，拆包及解决方法 在进行Java NIO学习时，发现，如果客户端连续不断的向服务端发送数据包时，服务端接收的数据会出现两个数据包粘在一起的情况，这就是TCP协议中经常会遇到的粘包以及拆包的问题。 我们都知道TCP属于传输层的协议，传输层除了有TCP协议外还有UDP协议。那么UDP是否会发生粘包或拆包的现象呢？答案是不会。UDP是基于报文发送的，从UDP的帧结构可以看出，在UDP首部采用了16bit来指示UDP数据报文的长度，因此在应用层能很好的将不同的数据报文区分开，从而避免粘包和拆包的问题。而TCP是基于字节流的，虽然应用层和TCP传输层之间的数据交互是大小不等的数据块，但是TCP把这些数据块仅仅看成一连串无结构的字节流，没有边界；另外从TCP的帧结构也可以看出，在TCP的首部没有表示数据长度的字段，基于上面两点，在使用TCP传输数据时，才有粘包或者拆包现象发生的可能。 粘包、拆包表现形式现在假设客户端向服务端连续发送了两个数据包，用packet1和packet2来表示，那么服务端收到的数据可以分为三种，现列举如下： 第一种情况，接收端正常收到两个数据包，即没有发生拆包和粘包的现象，此种情况不在本文的讨论范围内。 第二种情况，接收端只收到一个数据包，由于TCP是不会出现丢包的，所以这一个数据包中包含了发送端发送的两个数据包的信息，这种现象即为粘包。这种情况由于接收端不知道这两个数据包的界限，所以对于接收端来说很难处理。 第三种情况，这种情况有两种表现形式，如下图。接收端收到了两个数据包，但是这两个数据包要么是不完整的，要么就是多出来一块，这种情况即发生了拆包和粘包。这两种情况如果不加特殊处理，对于接收端同样是不好处理的。 粘包、拆包发生原因发生TCP粘包或拆包有很多原因，现列出常见的几点，可能不全面，欢迎补充， 1、要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包。 2、待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。 3、要发送的数据小于TCP发送缓冲区的大小，会启用Nagle算法（可配置是否启用），TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包。 5、接收数据端的应用层没有及时读取接收缓冲区中的数据，导致接收缓冲区有多次收到的数据，将发生粘包。 等等。 粘包、拆包解决办法通过以上分析，我们清楚了粘包或拆包发生的原因，那么如何解决这个问题呢？解决问题的关键在于如何给每个数据包添加边界信息，常用的方法有如下几个： 1、发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了。 2、发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。 3、可以在数据包之间设置边界，如添加特殊符号，这样，接收端通过这个边界就可以将不同的数据包拆分开。 等等。 样例程序我将在程序中使用两种方法来解决粘包和拆包问题，固定数据包长度和添加长度首部，这两种方法各有优劣。固定数据包长度传输效率一般，尤其是在要发送的数据长度长短差别很大的时候效率会比较低，但是编程实现比较简单；添加长度首部虽然可以获得较高的传输效率，冗余信息少且固定，但是编程实现较为复杂。下面给出的样例程序是基于之前的文章《Java中BIO，NIO和AIO使用样例》中提到的NIO实例的，如果对NIO的使用还不是很熟悉，可以先了解一下Java中NIO编程。 固定数据包长度这种处理方式的思路很简单，发送端在发送实际数据前先把数据封装为固定长度，然后在发送出去，接收端接收到数据后按照这个固定长度进行拆分即可。发送端程序如下： 1234567891011121314151617181920212223242526272829303132333435363738// 发送端String msg = "hello world " + number++; socketChannel.write(ByteBuffer.wrap(new FixLengthWrapper(msg).getBytes())); // 封装固定长度的工具类public class FixLengthWrapper &#123; public static final int MAX_LENGTH = 32; private byte[] data; public FixLengthWrapper(String msg) &#123; ByteBuffer byteBuffer = ByteBuffer.allocate(MAX_LENGTH); byteBuffer.put(msg.getBytes()); byte[] fillData = new byte[MAX_LENGTH - msg.length()]; byteBuffer.put(fillData); data = byteBuffer.array(); &#125; public FixLengthWrapper(byte[] msg) &#123; ByteBuffer byteBuffer = ByteBuffer.allocate(MAX_LENGTH); byteBuffer.put(msg); byte[] fillData = new byte[MAX_LENGTH - msg.length]; byteBuffer.put(fillData); data = byteBuffer.array(); &#125; public byte[] getBytes() &#123; return data; &#125; public String toString() &#123; StringBuilder sb = new StringBuilder(); for (byte b : getBytes()) &#123; sb.append(String.format("0x%02X ", b)); &#125; return sb.toString(); &#125;&#125; 可以看到客户端在发送数据前首先把数据封装为长度为32bytes的数据包，这个长度是根据目前实际数据包长度来规定的，这个长度必须要大于所有可能出现的数据包的长度，这样才不会出现把数据“截断”的情况。接收端程序如下： 123456789101112private static void processByFixLength(SocketChannel socketChannel) throws IOException &#123; while (socketChannel.read(byteBuffer) &gt; 0) &#123; byteBuffer.flip(); while (byteBuffer.remaining() &gt;= FixLengthWrapper.MAX_LENGTH) &#123; byte[] data = new byte[FixLengthWrapper.MAX_LENGTH]; byteBuffer.get(data, 0, FixLengthWrapper.MAX_LENGTH); System.out.println(new String(data) + " &lt;---&gt; " + number++); &#125; byteBuffer.compact(); &#125;&#125; 可以看出接收端的处理很简单，只需要每次读取固定的长度即可区分出来不同的数据包。 添加长度首部这种方式的处理较上面提到的方式稍微复杂一点。在发送端需要给待发送的数据添加固定的首部，然后再发送出去，然后在接收端需要根据这个首部的长度信息进行数据包的组合或拆分，发送端程序如下： 123456789101112131415161718192021222324252627282930313233343536// 发送端String msg = "hello world " + number++; // add the head represent the data lengthsocketChannel.write(ByteBuffer.wrap(new PacketWrapper(msg).getBytes())); // 添加长度首部的工具类public class PacketWrapper &#123; private int length; private byte[] payload; public PacketWrapper(String payload) &#123; this.payload = payload.getBytes(); this.length = this.payload.length; &#125; public PacketWrapper(byte[] payload) &#123; this.payload = payload; this.length = this.payload.length; &#125; public byte[] getBytes() &#123; ByteBuffer byteBuffer = ByteBuffer.allocate(this.length + 4); byteBuffer.putInt(this.length); byteBuffer.put(payload); return byteBuffer.array(); &#125; public String toString() &#123; StringBuilder sb = new StringBuilder(); for (byte b : getBytes()) &#123; sb.append(String.format("0x%02X ", b)); &#125; return sb.toString(); &#125;&#125; 从程序可以看到，发送端在发送数据前首先给待发送数据添加了代表长度的首部，首部长为4bytes（即int型长度），这样接收端在收到这个数据之后，首先需要读取首部，拿到实际数据长度，然后再继续读取实际长度的数据，即实现了组包和拆包的操作。程序如下： 12345678910111213141516171819202122232425262728private static void processByHead(SocketChannel socketChannel) throws IOException &#123; while (socketChannel.read(byteBuffer) &gt; 0) &#123; // 保存bytebuffer状态 int position = byteBuffer.position(); int limit = byteBuffer.limit(); byteBuffer.flip(); // 判断数据长度是否够首部长度 if (byteBuffer.remaining() &lt; 4) &#123; byteBuffer.position(position); byteBuffer.limit(limit); continue; &#125; // 判断bytebuffer中剩余数据是否足够一个包 int length = byteBuffer.getInt(); if (byteBuffer.remaining() &lt; length) &#123; byteBuffer.position(position); byteBuffer.limit(limit); continue; &#125; // 拿到实际数据包 byte[] data = new byte[length]; byteBuffer.get(data, 0, length); System.out.println(new String(data) + " &lt;---&gt; " + number++); byteBuffer.compact(); &#125;&#125; 关键信息已经在程序中做了注释，可以很明显的感觉到这种方法的处理难度相对于固定长度要大一些，不过这种方式可以获取更大的传输效率。 这里需要提醒各位同学一个问题，由于我在测试的时候采用的是一台机器连续发送数据来模拟高并发的场景，所以在测试的时候会发现服务器端收到的数据包的个数经常会小于包的序号，好像发生了丢包。但经过仔细分析可以发现，这种情况是因为TCP发送缓存溢出导致的丢包，也就是这个数据包根本没有发出来。也就是说，发送端发送数据过快，导致接收端缓存很快被填满，这个时候接收端会把通知窗口设置为0从而控制发送端的流量，这样新到的数据只能暂存在发送端的发送缓存中，当发送缓存溢出后，就出现了我上面提到的丢包，这个问题可以通过增大发送端缓存来缓解这个问题， 1socketChannel.socket().setSendBufferSize(102400); 当然这个话题不在本文的讨论范围，如果有兴趣的同学可以参阅《TCP/IP详解卷一》中的拥塞窗口一章。 关于源码说明，源码默认是把粘包和拆包处理这一部分注释掉了，分别位于NIOTcpServer和NIOTcpClient文件中，需要测试粘包和拆包处理程序的同学需要把这一段注释给去掉。 详见：https://blog.csdn.net/scythe666/article/details/51996268（含源码下载） tcp粘包udp不粘包的原因：https://blog.csdn.net/hik_zxw/article/details/48398935]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>粘包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis分布式锁之redisson]]></title>
    <url>%2F2019%2F07%2F10%2Fredis-distributed-lock-redisson%2F</url>
    <content type="text"><![CDATA[原文链接：拜托，面试请不要再问我Redis分布式锁的实现原理【石杉的架构笔记】 写在前面现在面试，一般都会聊聊分布式系统这块的东西。通常面试官都会从服务框架（Spring Cloud、Dubbo）聊起，一路聊到分布式事务、分布式锁、ZooKeeper等知识。 所以咱们这篇文章就来聊聊分布式锁这块知识，具体的来看看Redis分布式锁的实现原理。 说实话，如果在公司里落地生产环境用分布式锁的时候，一定是会用开源类库的，比如Redis分布式锁，一般就是用Redisson框架就好了，非常的简便易用。 大家如果有兴趣，可以去看看Redisson的官网，看看如何在项目中引入Redisson的依赖，然后基于Redis实现分布式锁的加锁与释放锁。 下面给大家看一段简单的使用代码片段，先直观的感受一下： 怎么样，上面那段代码，是不是感觉简单的不行！ 此外，人家还支持redis单实例、redis哨兵、redis cluster、redis master-slave等各种部署架构，都可以给你完美实现。 Redisson实现Redis分布式锁的底层原理好的，接下来就通过一张手绘图，给大家说说Redisson这个开源框架对Redis分布式锁的实现原理。 加锁机制咱们来看上面那张图，现在某个客户端要加锁。如果该客户端面对的是一个redis cluster集群，他首先会根据hash节点选择一台机器。 这里注意，仅仅只是选择一台机器！这点很关键！ 紧接着，就会发送一段lua脚本到redis上，那段lua脚本如下所示： 为啥要用lua脚本呢？ 因为一大坨复杂的业务逻辑，可以通过封装在lua脚本中发送给redis，保证这段复杂业务逻辑执行的原子性。 那么，这段lua脚本是什么意思呢？ KEYS[1]代表的是你加锁的那个key，比如说： RLock lock = redisson.getLock(“myLock”); 这里你自己设置了加锁的那个锁key就是“myLock”。 ARGV[1]代表的就是锁key的默认生存时间，默认30秒。 ARGV[2]代表的是加锁的客户端的ID，类似于下面这样： 8743c9c0-0795-4907-87fd-6c719a6b4586:1 给大家解释一下，第一段if判断语句，就是用“exists myLock”命令判断一下，如果你要加锁的那个锁key不存在的话，你就进行加锁。 如何加锁呢？很简单，用下面的命令： hset myLock 8743c9c0-0795-4907-87fd-6c719a6b4586:1 1 通过这个命令设置一个hash数据结构，这行命令执行后，会出现一个类似下面的数据结构： 上述就代表“8743c9c0-0795-4907-87fd-6c719a6b4586:1”这个客户端对“myLock”这个锁key完成了加锁。 接着会执行“pexpire myLock 30000”命令，设置myLock这个锁key的生存时间是30秒。 好了，到此为止，ok，加锁完成了。 锁互斥机制那么在这个时候，如果客户端2来尝试加锁，执行了同样的一段lua脚本，会咋样呢？ 很简单，第一个if判断会执行“exists myLock”，发现myLock这个锁key已经存在了。 接着第二个if判断，判断一下，myLock锁key的hash数据结构中，是否包含客户端2的ID，但是明显不是的，因为那里包含的是客户端1的ID。 所以，客户端2会获取到pttl myLock返回的一个数字，这个数字代表了myLock这个锁key的剩余生存时间。比如还剩15000毫秒的生存时间。 此时客户端2会进入一个while循环，不停的尝试加锁。 watch dog自动延期机客户端1加锁的锁key默认生存时间才30秒，如果超过了30秒，客户端1还想一直持有这把锁，怎么办呢？ 简单！只要客户端1一旦加锁成功，就会启动一个watch dog看门狗，他是一个后台线程，会每隔10秒检查一下，如果客户端1还持有锁key，那么就会不断的延长锁key的生存时间。 可重入加锁机制那如果客户端1都已经持有了这把锁了，结果可重入的加锁会怎么样呢？ 比如下面这种代码： 这时我们来分析一下上面那段lua脚本。 第一个if判断肯定不成立，“exists myLock”会显示锁key已经存在了。 第二个if判断会成立，因为myLock的hash数据结构中包含的那个ID，就是客户端1的那个ID，也就是“8743c9c0-0795-4907-87fd-6c719a6b4586:1” 此时就会执行可重入加锁的逻辑，他会用： incrby myLock 8743c9c0-0795-4907-87fd-6c71a6b4586:1 1 通过这个命令，对客户端1的加锁次数，累加1。 此时myLock数据结构变为下面这样： 大家看到了吧，那个myLock的hash数据结构中的那个客户端ID，就对应着加锁的次数 释放锁机制如果执行lock.unlock()，就可以释放分布式锁，此时的业务逻辑也是非常简单的。 其实说白了，就是每次都对myLock数据结构中的那个加锁次数减1。 如果发现加锁次数是0了，说明这个客户端已经不再持有锁了，此时就会用： “del myLock”命令，从redis里删除这个key。 然后呢，另外的客户端2就可以尝试完成加锁了。 这就是所谓的分布式锁的开源Redisson框架的实现机制。 一般我们在生产系统中，可以用Redisson框架提供的这个类库来基于redis进行分布式锁的加锁与释放锁。 上述Redis分布式锁的缺点其实上面那种方案最大的问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的redis slave实例。 但是这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。(但是这个加锁的key还没有来得及同步到slave节点)。 接着就会导致，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，而客户端1也以为自己成功加了锁。 此时就会导致多个客户端对一个分布式锁完成了加锁。 这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。 所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：在redis master实例宕机的时候，可能导致多个客户端同时完成加锁。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redisson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-用两个栈实现队列]]></title>
    <url>%2F2019%2F07%2F10%2Fjianzhioffer-two-stack-queue%2F</url>
    <content type="text"><![CDATA[题目描述用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 AC代码12345678910111213141516171819import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; stack1.push(node); &#125; public int pop() &#123; if(stack2.isEmpty())&#123; while(!stack1.isEmpty())&#123; stack2.push(stack1.pop()); &#125; &#125; return stack2.pop(); &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中Bean的生命周期]]></title>
    <url>%2F2019%2F07%2F09%2Fspring-bean-life-cycle%2F</url>
    <content type="text"><![CDATA[本文转载自：Bean的生命周期 正确理解spring容器中bean的生命周期很重要，有利于对bean的装配过程中进行一些自定义内容。如下图所示的生命周期： 如上图所见，在bean准备就绪之前，bean工厂执行了若干启动步骤，详细描述如下： spring对bean实例化； spring将值和bean的引用注入到bean对应的属性中； 如果bean实现了BeanNameAware接口，spring将bean的id传递给setBeanName()方法，该方法的作用就是设置bean的id到BeanFactory中方便创建该bean，同时让bean知道自己在BeanFactory配置中的名字； 如果bean实现了BeanFactoryAware接口，spring将调用setBeanFactroy()方法，将BeanFactory容器实例传入，便于bean够获取配置他们的BeanFactory的引用； 如果bean实现了ApplicationContextAware接口，spring将调用setApplicationContext()方法，将bean所在应用上下文的引用传进来，便于bean获取它所在的Spring容器(ApplicationContext是BeanFactory的子接口，有更多的实现方法)； 如果bean实现了BeanPostProcessor接口，spring将调用它的postProcessBeforeInitialization()方法，该方法在bean初始化之前调用。BeanPostProcessor经常被用作对Bean内容的更改，并且由于这个是在Bean初始化结束时调用那个的方法，也可以被用于内存或者缓存技术； 如果bean实现了InitializingBean接口，spring将调用它的afterPropertiesSet()方法，当bean的所用属性被设置完成之后调用该方法； 如果bean实现了BeanPostProcessor接口，spring将调用它的postProcessAfterInitialization()方法，在bean初始化完成之后调用；完成以上工作之后就可以应用这个Bean了，默认的Bean是SingleTon。 如果bean实现了DisposableBean接口，spring将调用它的destory()方法；]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC执行流程及源码解析]]></title>
    <url>%2F2019%2F07%2F09%2Fspring-mvc%2F</url>
    <content type="text"><![CDATA[本文转载自：SpringMVC执行流程及源码解析 在SpringMVC中主要是围绕着DispatcherServlet来设计，可以把它当做指挥中心。这里先说明一下SpringMVC文档给出的执行流程，然后是我们稍微具体的执行流程，最后是流程大致的源码跟踪。关于很很很详细的源码解析，这里暂先不做。 官方文档中的流程首先看下SpringMVC文档上给的流程图： 这张图片给了我们大概的执行流程： 用户请求首先发送到前端控制器DispatcherServlet，DispatcherServlet根据请求的信息来决定使用哪个页面控制器Controller（也就是我们通常编写的Controller）来处理该请求。找到控制器之后，DispatcherServlet将请求委托给控制器去处理。 接下来页面控制器开始处理用户请求，页面控制器会根据请求信息进行处理，调用业务层等等，处理完成之后，会把结果封装成一个ModelAndView返回给DispatcherServlet。 前端控制器DispatcherServlet接到页面控制器的返回结果后，根据返回的视图名选择相应的试图模板，并根据返回的数据进行渲染。 最后前端控制器DispatcherServlet将结果返回给用户。 更具体的流程上面只是总体流程，接下来我们稍微深入一点，看下更具体的流程，这里没有图，只有步骤解析： 用户请求发送到前端控制器DispatcherServlet。 前端控制器DispatcherServlet接收到请求后，DispatcherServlet会使用HandlerMapping来处理，HandlerMapping会查找到具体进行处理请求的Handler对象。 HandlerMapping找到对应的Handler之后，并不是返回一个Handler原始对象，而是一个Handler执行链，在这个执行链中包括了拦截器和处理请求的Handler。HandlerMapping返回一个执行链给DispatcherServlet。 DispatcherServlet接收到执行链之后，会调用Handler适配器去执行Handler。 Handler适配器执行完成Handler（也就是我们写的Controller）之后会得到一个ModelAndView，并返回给DispatcherServlet。 DispatcherServlet接收到Handler适配器返回的ModelAndView之后，会根据其中的视图名调用视图解析器。 视图解析器根据逻辑视图名解析成一个真正的View视图，并返回给DispatcherServlet。 DispatcherServlet接收到视图之后，会根据上面的ModelAndView中的model来进行视图中数据的填充，也就是所谓的视图渲染。 渲染完成之后，DispatcherServlet就可以将结果返回给用户了。 源码DispatcherServlet是一个Servlet，我们知道在Servlet在处理一个请求的时候会交给service方法进行处理，这里也不例外，DispatcherServlet继承了FrameworkServlet，首先进入FrameworkServlet的service方法： 123456789101112protected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //请求方法 String method = request.getMethod(); //PATCH方法单独处理 if (method.equalsIgnoreCase(RequestMethod.PATCH.name())) &#123; processRequest(request, response); &#125; else &#123;//其他的请求类型的方法经由父类，也就是HttpServlet处理 super.service(request, response); &#125;&#125; HttpServlet中会根据请求类型的不同分别调用doGet或者doPost等方法，FrameworkServlet中已经重写了这些方法，在这些方法中会调用processRequest进行处理，在processRequest中会调用doService方法，这个doService方法就是在DispatcherServlet中实现的。下面就看下DispatcherServlet中的doService方法的实现。 请求到达DispatcherServletdoService方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; //给request中的属性做一份快照 Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; logger.debug("Taking snapshot of request attributes before include"); attributesSnapshot = new HashMap&lt;String, Object&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith("org.springframework.web.servlet")) &#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; //如果我们没有配置类似本地化或者主题的处理器之类的 //SpringMVC会使用默认的值 //默认配置文件是DispatcherServlet.properties request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); try &#123; //开始处理 doDispatch(request, response); &#125; finally &#123; if (WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; return; &#125; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125;&#125; DispatcherServlet开始真正的处理，doDispatch方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; //SpringMVC中异步请求的相关知识，暂先不解释 WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; //先检查是不是Multipart类型的，比如上传等 //如果是Multipart类型的，则转换为MultipartHttpServletRequest类型 processedRequest = checkMultipart(request); multipartRequestParsed = processedRequest != request; //获取当前请求的Handler mappedHandler = getHandler(processedRequest, false); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; //获取当前请求的Handler适配器 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 对于header中last-modified的处理 String method = request.getMethod(); boolean isGet = "GET".equals(method); if (isGet || "HEAD".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; //拦截器的preHandle方法进行处理 if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; try &#123; //真正调用Handler的地方 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; &#125; //处理成默认视图名，就是添加前缀和后缀等 applyDefaultViewName(request, mv); //拦截器postHandle方法进行处理 mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; //处理最后的结果，渲染之类的都在这里 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Error err) &#123; triggerAfterCompletionWithError(processedRequest, response, mappedHandler, err); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); return; &#125; // Clean up any resources used by a multipart request. if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125;&#125; 可以看到大概的步骤还是按照我们上面分析的走的。 查找请求对应的Handler对象对应着这句代码mappedHandler = getHandler(processedRequest, false);，看下具体的getHandler方法： 123protected HandlerExecutionChain getHandler(HttpServletRequest request, boolean cache) throws Exception &#123; return getHandler(request);&#125; 继续往下看getHandler： 1234567891011protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; //遍历所有的handlerMappings进行处理 //handlerMappings是在启动的时候预先注册好的 for (HandlerMapping hm : this.handlerMappings) &#123; HandlerExecutionChain handler = hm.getHandler(request); if (handler != null) &#123; return handler; &#125; &#125; return null;&#125; 继续往下看getHandler，在AbstractHandlerMapping类中： 12345678910111213141516171819public final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; //根据request获取handler Object handler = getHandlerInternal(request); if (handler == null) &#123; //如果没有找到就使用默认的handler handler = getDefaultHandler(); &#125; if (handler == null) &#123; return null; &#125; //如果Handler是String，表明是一个bean名称 //需要超照对应bean if (handler instanceof String) &#123; String handlerName = (String) handler; handler = getApplicationContext().getBean(handlerName); &#125; //封装Handler执行链 return getHandlerExecutionChain(handler, request);&#125; 根据requrst获取handler首先看下根据requrst获取handler步骤getHandlerInternal方法，在AbstractHandlerMethodMapping中： 12345678protected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception &#123; //获取request中的url，用来匹配handler String lookupPath = getUrlPathHelper().getLookupPathForRequest(request); //根据路径寻找Handler HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request); //根据handlerMethod中的bean来实例化Handler并添加进HandlerMethod return (handlerMethod != null) ? handlerMethod.createWithResolvedBean() : null;&#125; 看下根据路径寻找handler的方法lookupHandlerMethod： 123456789101112131415161718192021222324252627282930313233343536373839protected HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request) throws Exception &#123; List&lt;Match&gt; matches = new ArrayList&lt;Match&gt;(); //直接匹配 List&lt;T&gt; directPathMatches = this.urlMap.get(lookupPath); //如果有匹配的，就添加进匹配列表中 if (directPathMatches != null) &#123; addMatchingMappings(directPathMatches, matches, request); &#125; //还没有匹配的，就遍历所有的处理方法查找 if (matches.isEmpty()) &#123; // No choice but to go through all mappings addMatchingMappings(this.handlerMethods.keySet(), matches, request); &#125; //找到了匹配的 if (!matches.isEmpty()) &#123; Comparator&lt;Match&gt; comparator = new MatchComparator(getMappingComparator(request)); Collections.sort(matches, comparator); //排序之后，获取第一个 Match bestMatch = matches.get(0); //如果有多个匹配的，会找到第二个最合适的进行比较一下 if (matches.size() &gt; 1) &#123; Match secondBestMatch = matches.get(1); if (comparator.compare(bestMatch, secondBestMatch) == 0) &#123; Method m1 = bestMatch.handlerMethod.getMethod(); Method m2 = secondBestMatch.handlerMethod.getMethod(); throw new IllegalStateException( "Ambiguous handler methods mapped for HTTP path '" + request.getRequestURL() + "': &#123;" + m1 + ", " + m2 + "&#125;"); &#125; &#125; //设置request参数 handleMatch(bestMatch.mapping, lookupPath, request); //返回匹配的url的处理的方法 return bestMatch.handlerMethod; &#125; else &#123;//最后还没有找到，返回null return handleNoMatch(handlerMethods.keySet(), lookupPath, request); &#125;&#125; 获取默认Handler如果上面没有获取到Handler，就会获取默认的Handler。如果还获取不到就返回null。 处理String类型的Handler如果上面处理完的Handler是String类型的，就会根据这个handlerName获取bean。 封装Handler执行链上面获取完Handler，就开始封装执行链了，就是将我们配置的拦截器加入到执行链中去，getHandlerExecutionChain： 123456789101112131415161718protected HandlerExecutionChain getHandlerExecutionChain(Object handler, HttpServletRequest request) &#123; //如果当前Handler不是执行链类型，就使用一个新的执行链实例封装起来 HandlerExecutionChain chain = (handler instanceof HandlerExecutionChain) ? (HandlerExecutionChain) handler : new HandlerExecutionChain(handler); //先获取适配类型的拦截器添加进去拦截器链 chain.addInterceptors(getAdaptedInterceptors()); //当前的url String lookupPath = urlPathHelper.getLookupPathForRequest(request); //遍历拦截器，找到跟当前url对应的，添加进执行链中去 for (MappedInterceptor mappedInterceptor : mappedInterceptors) &#123; if (mappedInterceptor.matches(lookupPath, pathMatcher)) &#123; chain.addInterceptor(mappedInterceptor.getInterceptor()); &#125; &#125; return chain;&#125; 获取对应请求的Handler适配器getHandlerAdapter： 123456789protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException &#123; //遍历所有的HandlerAdapter，找到和当前Handler匹配的就返回 //我们这里会匹配到RequestMappingHandlerAdapter for (HandlerAdapter ha : this.handlerAdapters) &#123; if (ha.supports(handler)) &#123; return ha; &#125; &#125;&#125; 缓存的处理也就是对last-modified的处理 执行拦截器的preHandle方法就是遍历所有的我们定义的interceptor，执行preHandle方法 使用Handler适配器执行当前的Handlerha.handle执行当前Handler，我们这里使用的是RequestMappingHandlerAdapter，首先会进入AbstractHandlerMethodAdapter的handle方法： 1234public final ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; return handleInternal(request, response, (HandlerMethod) handler);&#125; handleInternal方法，在RequestMappingHandlerAdapter中： 12345678910111213141516171819202122232425protected final ModelAndView handleInternal(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception &#123; if (getSessionAttributesHandler(handlerMethod).hasSessionAttributes()) &#123; // Always prevent caching in case of session attribute management. checkAndPrepare(request, response, this.cacheSecondsForSessionAttributeHandlers, true); &#125; else &#123; // Uses configured default cacheSeconds setting. checkAndPrepare(request, response, true); &#125; // Execute invokeHandlerMethod in synchronized block if required. if (this.synchronizeOnSession) &#123; HttpSession session = request.getSession(false); if (session != null) &#123; Object mutex = WebUtils.getSessionMutex(session); synchronized (mutex) &#123; return invokeHandleMethod(request, response, handlerMethod); &#125; &#125; &#125; //执行方法，封装ModelAndView return invokeHandleMethod(request, response, handlerMethod);&#125; 组装默认视图名称前缀和后缀名都加上 执行拦截器的postHandle方法遍历intercepter的postHandle方法。 处理最后的结果，渲染之类的processDispatchResult方法： 123456789101112131415161718192021222324252627282930313233343536private void processDispatchResult(HttpServletRequest request, HttpServletResponse response, HandlerExecutionChain mappedHandler, ModelAndView mv, Exception exception) throws Exception &#123; boolean errorView = false; if (exception != null) &#123; if (exception instanceof ModelAndViewDefiningException) &#123; mv = ((ModelAndViewDefiningException) exception).getModelAndView(); &#125; else &#123; Object handler = (mappedHandler != null ? mappedHandler.getHandler() : null); mv = processHandlerException(request, response, handler, exception); errorView = (mv != null); &#125; &#125; // Did the handler return a view to render? if (mv != null &amp;&amp; !mv.wasCleared()) &#123; //渲染 render(mv, request, response); if (errorView) &#123; WebUtils.clearErrorRequestAttributes(request); &#125; &#125; else &#123; &#125; if (WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Concurrent handling started during a forward return; &#125; if (mappedHandler != null) &#123; mappedHandler.triggerAfterCompletion(request, response, null); &#125;&#125; 重点看下render方法，进行渲染： 12345678910111213141516171819202122protected void render(ModelAndView mv, HttpServletRequest request, HttpServletResponse response) throws Exception &#123; //设置本地化 Locale locale = this.localeResolver.resolveLocale(request); response.setLocale(locale); View view; if (mv.isReference()) &#123; //解析视图名，得到视图 view = resolveViewName(mv.getViewName(), mv.getModelInternal(), locale, request); &#125; else &#123; // No need to lookup: the ModelAndView object contains the actual View object. view = mv.getView(); if (view == null) &#123; throw new ServletException("ModelAndView [" + mv + "] neither contains a view name nor a " + "View object in servlet with name '" + getServletName() + "'"); &#125; &#125; //委托给视图进行渲染 view.render(mv.getModelInternal(), request, response);&#125; view.render就是进行视图的渲染，然后跳转页面等处理。 到这里大概的流程就走完了。其中涉及到的东西还有很多，暂先不做详细处理。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见加密算法原理及概念]]></title>
    <url>%2F2019%2F07%2F09%2Fcommon-encryption-algorithm%2F</url>
    <content type="text"><![CDATA[本文转载自：常见加密算法原理及概念 概述在安全领域，利用密钥加密算法来对通信的过程进行加密是一种常见的安全手段。利用该手段能够保障数据安全通信的三个目标： 1、数据的保密性，防止用户的数据被窃取或泄露； 2、保证数据的完整性，防止用户传输的数据被篡改； 3、通信双方的身份确认，确保数据来源与合法的用户； 而常见的密钥加密算法类型大体可以分为三类：对称加密、非对称加密、单向加密。下面我们来了解下相关的算法原理及其常见的算法。 对称加密算法对称加密算法采用单密钥加密，在通信过程中，数据发送方将原始数据分割成固定大小的块，经过密钥和加密算法逐个加密后，发送给接收方；接收方收到加密后的报文后，结合密钥和解密算法解密组合后得出原始数据。由于加解密算法是公开的，因此在这过程中，密钥的安全传递就成为了至关重要的事了。而密钥通常来说是通过双方协商，以物理的方式传递给对方，或者利用第三方平台传递给对方，一旦这过程出现了密钥泄露，不怀好意的人就能结合相应的算法拦截解密出其加密传输的内容。 对称加密算法原理 对称加密算法拥有着算法公开、计算量小、加密速度和效率高的特点，但是也有着密钥单一、密钥管理困难等缺点。 常见的对称加密算法有： DES：分组式加密算法，以64位为分组对数据加密，加解密使用同一个算法。 3DES：三重数据加密算法，对每个数据块应用三次DES加密算法。 AES：高级加密标准算法，是美国联邦政府采用的一种区块加密标准，用于替代原先的DES，目前已被广泛应用。 Blowfish：Blowfish算法是一个64位分组及可变密钥长度的对称密钥分组密码算法，可用来加密64比特长度的字符串。 非对称加密算法非对称加密算法采用公钥和私钥两种不同的密码来进行加解密。公钥和私钥是成对存在，公钥是从私钥中提取产生公开给所有人的，如果使用公钥对数据进行加密，那么只有对应的私钥才能解密，反之亦然。 下图为简单非对称加密算法的常见流程： 非对称加密流程 发送方Bob从接收方Alice获取其对应的公钥，并结合相应的非对称算法将明文加密后发送给Alice；Alice接收到加密的密文后，结合自己的私钥和非对称算法解密得到明文。这种简单的非对称加密算法的应用其安全性比对称加密算法来说要高，但是其不足之处在于无法确认公钥的来源合法性以及数据的完整性。 非对称加密算法具有安全性高、算法强度负复杂的优点，其缺点为加解密耗时长、速度慢，只适合对少量数据进行加密，其常见算法包括： RSA：RSA算法基于一个十分简单的数论事实：将两个大素数相乘十分容易，但那时想要对其乘积进行因式分解却极其困难，因此可以将乘积公开作为加密密钥，可用于加密，也能用于签名。 DSA：数字签名算法，仅能用于签名，不能用于加解密。 DSS：数字签名标准，技能用于签名，也可以用于加解密。 ELGamal：利用离散对数的原理对数据进行加解密或数据签名，其速度是最慢的。 单向加密单向加密算法常用于提取数据指纹，验证数据的完整性。发送者将明文通过单向加密算法加密生成定长的密文串，然后传递给接收方。接收方在收到加密的报文后进行解密，将解密获取到的明文使用相同的单向加密算法进行加密，得出加密后的密文串。随后将之与发送者发送过来的密文串进行对比，若发送前和发送后的密文串相一致，则说明传输过程中数据没有损坏；若不一致，说明传输过程中数据丢失了。单向加密算法只能用于对数据的加密，无法被解密，其特点为定长输出、雪崩效应。常见的算法包括：MD5、sha1、sha224等等，其常见用途包括：数字摘要、数字签名等等。 单向加密校验过程 密钥交换密钥交换IKE（Internet Key Exchange）通常是指双方通过交换密钥来实现数据加密和解密，常见的密钥交换方式有下面两种： 1、公钥加密，将公钥加密后通过网络传输到对方进行解密，这种方式缺点在于具有很大的可能性被拦截破解，因此不常用； 2、Diffie-Hellman，DH算法是一种密钥交换算法，其既不用于加密，也不产生数字签名。DH算法的巧妙在于需要安全通信的双方可以用这个方法确定对称密钥。然后可以用这个密钥进行加密和解密。但是注意，这个密钥交换协议/算法只能用于密钥的交换，而不能进行消息的加密和解密。双方确定要用的密钥后，要使用其他对称密钥操作加密算法实际加密和解密消息。DH算法通过双方共有的参数、私有参数和算法信息来进行加密，然后双方将计算后的结果进行交换，交换完成后再和属于自己私有的参数进行特殊算法，经过双方计算后的结果是相同的，此结果即为密钥。 如： A 有p和g两个参数，A还有一个属于自己的私有参数x； B 有p和g两个参数，B还有一个属于自己的私有参数y； A和B均使用相同的加密算法计算其对应的值：value_A=px%g，value_B=py%g 随后双方交换计算后的值，然后再分别使用自己的私有参数对去求次方，如： A拿到value_B值后，对其求x次方得(py%g)x=p^xy%g； B拿到value_A值后，对其求y次方得(px%g)y=p^xy%g； 最终得到的结果是一致的。 在整个过程中，第三方人员只能获取p、g两个值，AB双方交换的是计算后的结果，因此这种方式是很安全的。 公钥基础设施（PKI）公钥基础设施是一个包括硬件、软件、人员、策略和规程的集合，用于实现基于公钥密码机制的密钥和证书的生成、管理、存储、分发和撤销的功能，其组成包括：签证机构CA、注册机构RA、证书吊销列表CRL和证书存取库CB。 PKI采用证书管理公钥，通过第三方可信任CA中心，把用户的公钥和其他用户信息组生成证书，用于验证用户的身份。 公钥证书是以数字签名的方式声明，它将公钥的值绑定到持有对应私钥的个人、设备或服务身份。公钥证书的生成遵循X.509协议的规定，其内容包括：证书名称、证书版本、序列号、算法标识、颁发者、有效期、有效起始日期、有效终止日期、公钥 、证书签名等等的内容。 CA证书认证的流程如下图，Bob为了向Alice证明自己是Bob和某个公钥是自己的，她便向一个Bob和Alice都信任的CA机构申请证书，Bob先自己生成了一对密钥对（私钥和公钥），把自己的私钥保存在自己电脑上，然后把公钥给CA申请证书，CA接受申请于是给Bob颁发了一个数字证书，证书中包含了Bob的那个公钥以及其它身份信息，当然，CA会计算这些信息的消息摘要并用自己的私钥加密消息摘要（数字签名）一并附在Bob的证书上，以此来证明这个证书就是CA自己颁发的。Alice得到Bob的证书后用CA的证书（自签署的）中的公钥来解密消息摘要，随后将摘要和Bob的公钥发送到CA服务器上进行核对。CA在接收到Alice的核对请求后，会根据Alice提供的信息核对Bob的证书是否合法，如果确认合法则回复Alice证书合法。Alice收到CA的确认回复后，再去使用从证书中获取的Bob的公钥加密邮件然后发送给Bob，Bob接收后再以自己的私钥进行解密。 ​ CA证书认证流程]]></content>
      <categories>
        <category>密码学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL存储引擎-InnoDB与MyISAM的区别]]></title>
    <url>%2F2019%2F07%2F09%2Fmysql-innodb-myisam%2F</url>
    <content type="text"><![CDATA[本文转载自：MySQL存储引擎－－MyISAM与InnoDB区别 MyISAM是MySQL的默认数据库引擎（5.5版之前），由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然性能极佳，但却有一个缺点：不支持事务处理（transaction）。不过，在这几年的发展下，MySQL也导入了InnoDB（另一种数据库引擎），以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM。 InnoDB，是MySQL的数据库引擎之一，为MySQL AB发布binary的标准之一。InnoDB由Innobase Oy公司所开发，2006年五月时由甲骨文公司并购。与传统的ISAM与MyISAM相比，InnoDB的最大特色就是支持了ACID兼容的事务（Transaction）功能，类似于PostgreSQL。目前InnoDB采用双轨制授权，一是GPL授权，另一是专有软件授权。 MyISAM与InnoDB的区别是什么？存储结构MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。 存储空间MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。 InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 可移植性、备份及恢复MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。 事务支持MyISAM：强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。InnoDB：提供事务，支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 AUTO_INCREMENTMyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。InnoDB：InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 表锁差异MyISAM：只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 全文索引MyISAM：支持 FULLTEXT类型的全文索引InnoDB：不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。 表主键MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。 表的具体行数MyISAM：保存有表的总行数，如果select count() from table;会直接取出出该值。InnoDB：没有保存表的总行数，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。 CURD操作MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。 外键MyISAM：不支持InnoDB：支持 通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储过程、视图、行级锁定等等，在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。另外，任何一种表都不是万能的，只用恰当的针对业务类型来选择合适的表类型，才能最大的发挥MySQL的性能优势。如果不是很复杂的Web应用，非关键应用，还是可以继续考虑MyISAM的，这个具体情况可以自己斟酌。 存储引擎选择的基本原则 采用MyISAM引擎 R/W &gt; 100:1 且update相对较少(读多写少) 并发不高 表数据量小 硬件资源有限 采用InnoDB引擎 R/W比较小，频繁更新大字段 表数据量超过1000万，并发高 安全性和可用性要求高 采用Memory引擎 有足够的内存 对数据一致性要求不高，如在线人数和session等应用 需要定期归档数据 1摘自：《PHP核心技术与最佳实践》]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>InnoDB</tag>
        <tag>MyISAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条SQL语句执行得很慢的原因有哪些？]]></title>
    <url>%2F2019%2F07%2F09%2Fmysql-sql-slow%2F</url>
    <content type="text"><![CDATA[本文转载自公众号:苦逼的码农 作者：帅地 说实话，这个问题可以涉及到 MySQL 的很多核心知识，可以扯出一大堆，就像要考你计算机网络的知识时，问你“输入URL回车之后，究竟发生了什么”一样，看看你能说出多少了。 之前腾讯面试的实话，也问到这个问题了，不过答的很不好，之前没去想过相关原因，导致一时之间扯不出来。所以今天，我带大家来详细扯一下有哪些原因，相信你看完之后一定会有所收获，不然你打我。 开始装逼：分类讨论一条 SQL 语句执行的很慢，那是每次执行都很慢呢？还是大多数情况下是正常的，偶尔出现很慢呢？所以我觉得，我们还得分以下两种情况来讨论。 1、大多数情况是正常的，只是偶尔会出现很慢的情况。 2、在数据量不变的情况下，这条SQL语句一直以来都执行的很慢。 针对这两种情况，我们来分析下可能是哪些原因导致的。 针对偶尔很慢的情况一条 SQL 大多数情况正常，偶尔才能出现很慢的情况，针对这种情况，我觉得这条SQL语句的书写本身是没什么问题的，而是其他原因导致的，那会是什么原因呢？ 数据库在刷新脏页（flush）我也无奈啊当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在内存中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到磁盘中去，而是把这些更新的记录写入到 redo log日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 刷脏页有下面4种场景（后两种不用太关注“性能”问题）： redolog写满了：redo log 里的容量是有限的，如果数据库一直很忙，更新又很频繁，这个时候 redo log 很快就会被写满了，这个时候就没办法等到空闲的时候再把数据同步到磁盘的，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，就会导致我们平时正常的SQL语句突然执行的很慢，所以说，数据库在在同步数据到磁盘的时候，就有可能导致我们的SQL语句执行的很慢了。 内存不够用了：如果一次查询较多的数据，恰好碰到所查数据页不在内存中时，需要申请内存，而此时恰好内存不足的时候就需要淘汰一部分内存数据页，如果是干净页，就直接释放，如果恰好是脏页就需要刷脏页。 MySQL 认为系统“空闲”的时候：这时系统没什么压力。 MySQL 正常关闭的时候：这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 拿不到锁我能怎么办这个就比较容易想到了，我们要执行的这条语句，刚好这条语句涉及到的表，别人在用，并且加锁了，我们拿不到锁，只能慢慢等待别人释放锁了。或者，表没有加锁，但要使用到的某个一行被加锁了，这个时候，我也没办法啊。 如果要判断是否真的在等待锁，我们可以用 show processlist这个命令来查看当前的状态哦，这里我要提醒一下，有些命令最好记录一下，反正，我被问了好几个命令，都不知道怎么写，呵呵。 下来我们来访分析下第二种情况，我觉得第二种情况的分析才是最重要的 针对一直都这么慢的情况如果在数据量一样大的情况下，这条 SQL 语句每次都执行的这么慢，那就要好好考虑下你的 SQL 书写了，下面我们来分析下哪些原因会导致我们的 SQL 语句执行的很不理想。 我们先来假设我们有一个表，表里有下面两个字段,分别是主键 id，和两个普通字段 c 和 d。 123456mysql&gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB; 扎心了，没用到索引没有用上索引，我觉得这个原因是很多人都能想到的，例如你要查询这条语句 1select * from t where 100 &lt;c and c &lt; 100000; （1）、字段没有索引 刚好你的 c 字段上没有索引，那么抱歉，只能走全表扫描了，你就体验不会索引带来的乐趣了，所以，这会导致这条查询语句很慢。 （2）、字段有索引，但却没有用索引 好吧，这个时候你给 c 这个字段加上了索引，然后又查询了一条语句 1select * from t where c - 1 = 1000; 我想问大家一个问题，这样子在查询的时候会用索引查询吗？ 答是不会，如果我们在字段的左边做了运算，那么很抱歉，在查询的时候，就不会用上索引了，所以呢，大家要注意这种字段上有索引，但由于自己的疏忽，导致系统没有使用索引的情况了。 正确的查询应该如下 1select * from t where c = 1000 + 1; 有人可能会说，右边有运算就能用上索引？难道数据库就不会自动帮我们优化一下，自动把 c - 1=1000 自动转换为 c = 1000+1。 不好意思，确实不会帮你，所以，你要注意了。 （3）、函数操作导致没有用上索引 如果我们在查询的时候，对字段进行了函数操作，也是会导致没有用上索引的，例如 1select * from t where pow(c,2) = 1000; 这里我只是做一个例子，假设函数 pow 是求 c 的 n 次方，实际上可能并没有 pow(c,2)这个函数。其实这个和上面在左边做运算也是很类似的。 所以呢，一条语句执行都很慢的时候，可能是该语句没有用上索引了，不过具体是啥原因导致没有用上索引的呢，你就要会分析了，我上面列举的三个原因，应该是出现的比较多的吧。 呵呵，数据库自己选错索引了我们在进行查询操作的时候，例如 1select * from t where 100 &lt; c and c &lt; 100000; 我们知道，主键索引和非主键索引是有区别的，主键索引存放的值是整行字段的数据，而非主键索引上存放的值不是整行字段的数据，而且存放主键字段的值。不大懂的可以看这篇文章： 【思维导图-索引篇】搞定数据库索引就是这么简单 里面有说到主键索引和非主键索引的区别。 也就是说，我们如果走 c 这个字段的索引的话，最后会查询到对应主键的值，然后，再根据主键的值走主键索引，查询到整行数据返回。 好吧扯了这么多，其实我就是想告诉你，就算你在 c 字段上有索引，系统也并不一定会走 c 这个字段上的索引，而是有可能会直接扫描扫描全表，找出所有符合 100 &lt; c and c &lt; 100000 的数据。 为什么会这样呢？ 其实是这样的，系统在执行这条语句的时候，会进行预测：究竟是走 c 索引扫描的行数少，还是直接扫描全表扫描的行数少呢？显然，扫描行数越少当然越好了，因为扫描行数越少，意味着I/O操作的次数越少。 如果是扫描全表的话，那么扫描的次数就是这个表的总行数了，假设为 n；而如果走索引 c 的话，我们通过索引 c 找到主键之后，还得再通过主键索引来找我们整行的数据，也就是说，需要走两次索引。而且，我们也不知道符合 100 c &lt; and c &lt; 10000 这个条件的数据有多少行，万一这个表是全部数据都符合呢？这个时候意味着，走 c 索引不仅扫描的行数是 n，同时还得每行数据走两次索引。 所以呢，系统是有可能走全表扫描而不走索引的。那系统是怎么判断呢？ 判断来源于系统的预测，也就是说，如果要走 c 字段索引的话，系统会预测走 c 字段索引大概需要扫描多少行。如果预测到要扫描的行数很多，它可能就不走索引而直接扫描全表了。 那么问题来了，系统是怎么预测判断的呢？这里我给你讲下系统是怎么判断的吧，虽然这个时候我已经写到脖子有点酸了。 系统是通过索引的区分度来判断的，一个索引上不同的值越多，意味着出现相同数值的索引越少，意味着索引的区分度越高。我们也把区分度称之为基数，即区分度越高，基数越大。所以呢，基数越大，意味着符合 100 &lt; c and c &lt; 10000 这个条件的行数越少。 所以呢，一个索引的基数越大，意味着走索引查询越有优势。 那么问题来了，怎么知道这个索引的基数呢？ 系统当然是不会遍历全部来获得一个索引的基数的，代价太大了，索引系统是通过遍历部分数据，也就是通过采样的方式，来预测索引的基数的。 扯了这么多，重点的来了，居然是采样，那就有可能出现失误的情况，也就是说，c 这个索引的基数实际上是很大的，但是采样的时候，却很不幸，把这个索引的基数预测成很小。例如你采样的那一部分数据刚好基数很小，然后就误以为索引的基数很小。然后就呵呵，系统就不走 c 索引了，直接走全部扫描了。 所以呢，说了这么多，得出结论：由于统计的失误，导致系统没有走索引，而是走了全表扫描，而这，也是导致我们 SQL 语句执行的很慢的原因。 这里我声明一下，系统判断是否走索引，扫描行数的预测其实只是原因之一，这条查询语句是否需要使用使用临时表、是否需要排序等也是会影响系统的选择的。 不过呢，我们有时候也可以通过强制走索引的方式来查询，例如 1select * from t force index(a) where c &lt; 100 and c &lt; 100000; 我们也可以通过 1show index from t; 来查询索引的基数和实际是否符合，如果和实际很不符合的话，我们可以重新来统计索引的基数，可以用这条命令 1analyze table t; 来重新统计分析。 既然会预测错索引的基数，这也意味着，当我们的查询语句有多个索引的时候，系统有可能也会选错索引哦，这也可能是 SQL 执行的很慢的一个原因。 好吧，就先扯这么多了，你到时候能扯出这么多，我觉得已经很棒了，下面做一个总结。 总结以上是我的总结与理解，最后一个部分，我怕很多人不大懂数据库居然会选错索引，所以我详细解释了一下，下面我对以上做一个总结。 一个 SQL 执行的很慢，我们要分两种情况讨论： 1、大多数情况下很正常，偶尔很慢，则有如下原因 (1)、数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。 (2)、执行的时候，遇到锁，如表锁、行锁。 2、这条 SQL 语句一直执行的很慢，则有如下原因。 (1)、没有用上索引：例如该字段没有索引；该字段有索引，但是索引失效，由于对字段进行运算、函数操作导致无法用索引。 (2)、数据库选错了索引。 大家如果有补充的，也是可以留言区补充一波哦。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java8之LinkedHashMap]]></title>
    <url>%2F2019%2F07%2F09%2Fjava8-LinkedHashMap%2F</url>
    <content type="text"><![CDATA[本文转载自：LinkedHashMap 源码详细分析（JDK1.8） 概述LinkedHashMap 继承自 HashMap，在 HashMap 基础上，通过维护一条双向链表，解决了 HashMap 不能随时保持遍历顺序和插入顺序一致的问题。除此之外，LinkedHashMap 对访问顺序也提供了相关支持。在一些场景下，该特性很有用，比如缓存。在实现上，LinkedHashMap 很多方法直接继承自 HashMap，仅为维护双向链表覆写了部分方法。所以，要看懂 LinkedHashMap 的源码，需要先看懂 HashMap 的源码。关于 HashMap 的源码分析，本文并不打算展开讲了。大家可以参考我之前的一篇文章“HashMap 源码详细分析(JDK1.8)”。在那篇文章中，我配了十多张图帮助大家学习 HashMap 源码。 本篇文章的结构与我之前两篇关于 Java 集合类（集合框架）的源码分析文章不同，本文将不再分析集合类的基本操作（查找、遍历、插入、删除），而是把重点放在双向链表的维护上。包括链表的建立过程，删除节点的过程，以及访问顺序维护的过程等。好了，接下里开始分析吧。 原理上一章说了 LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构。该结构由数组和链表或红黑树组成，结构示意图大致如下： ] LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。其结构可能如下图： ] 上图中，淡蓝色的箭头表示前驱引用，红色箭头表示后继引用。每当有新键值对节点插入，新节点最终会接在 tail 引用指向的节点后面。而 tail 引用则会移动到新的节点上，这样一个双向链表就建立起来了。 上面的结构并不是很难理解，虽然引入了红黑树，导致结构看起来略为复杂了一些。但大家完全可以忽略红黑树，而只关注链表结构本身。好了，接下来进入细节分析吧。 源码分析Entry 的继承体系在对核心内容展开分析之前，这里先插队分析一下键值对节点的继承体系。先来看看继承体系结构图： 上面的继承体系乍一看还是有点复杂的，同时也有点让人迷惑。HashMap 的内部类 TreeNode 不继承它的了一个内部类 Node，却继承自 Node 的子类 LinkedHashMap 内部类 Entry。这里这样做是有一定原因的，这里先不说。先来简单说明一下上面的继承体系。LinkedHashMap 内部类 Entry 继承自 HashMap 内部类 Node，并新增了两个引用，分别是 before 和 after。这两个引用的用途不难理解，也就是用于维护双向链表。同时，TreeNode 继承 LinkedHashMap 的内部类 Entry 后，就具备了和其他 Entry 一起组成链表的能力。但是这里需要大家考虑一个问题。当我们使用 HashMap 时，TreeNode 并不需要具备组成链表能力。如果继承 LinkedHashMap 内部类 Entry ，TreeNode 就多了两个用不到的引用，这样做不是会浪费空间吗？简单说明一下这个问题（水平有限，不保证完全正确），这里这么做确实会浪费空间，但与 TreeNode 通过继承获取的组成链表的能力相比，这点浪费是值得的。在 HashMap 的设计思路注释中，有这样一段话： Because TreeNodes are about twice the size of regular nodes, weuse them only when bins contain enough nodes to warrant use(see TREEIFY_THRESHOLD). And when they become too small (due toremoval or resizing) they are converted back to plain bins. Inusages with well-distributed user hashCodes, tree bins arerarely used. 大致的意思是 TreeNode 对象的大小约是普通 Node 对象的2倍，我们仅在桶（bin）中包含足够多的节点时再使用。当桶中的节点数量变少时（取决于删除和扩容），TreeNode 会被转成 Node。当用户实现的 hashCode 方法具有良好分布性时，树类型的桶将会很少被使用。 通过上面的注释，我们可以了解到。一般情况下，只要 hashCode 的实现不糟糕，Node 组成的链表很少会被转成由 TreeNode 组成的红黑树。也就是说 TreeNode 使用的并不多，浪费那点空间是可接受的。假如 TreeNode 机制继承自 Node 类，那么它要想具备组成链表的能力，就需要 Node 去继承 LinkedHashMap 的内部类 Entry。这个时候就得不偿失了，浪费很多空间去获取不一定用得到的能力。 说到这里，大家应该能明白节点类型的继承体系了。这里单独拿出来说一下，为下面的分析做铺垫。叙述略为啰嗦，见谅。 链表的建立过程链表的建立过程是在插入键值对节点时开始的，初始情况下，让 LinkedHashMap 的 head 和 tail 引用同时指向新节点，链表就算建立起来了。随后不断有新节点插入，通过将新节点接在 tail 引用指向节点的后面，即可实现链表的更新。 Map 类型的集合类是通过 put(K,V) 方法插入键值对，LinkedHashMap 本身并没有覆写父类的 put 方法，而是直接使用了父类的实现。但在 HashMap 中，put 方法插入的是 HashMap 内部类 Node 类型的节点，该类型的节点并不具备与 LinkedHashMap 内部类 Entry 及其子类型节点组成链表的能力。那么，LinkedHashMap 是怎样建立链表的呢？在展开说明之前，我们先看一下 LinkedHashMap 插入操作相关的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// HashMap 中实现public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;// HashMap 中实现final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) &#123;...&#125; // 通过节点 hash 定位节点所在的桶位置，并检测桶中是否包含节点引用 if ((p = tab[i = (n - 1) &amp; hash]) == null) &#123;...&#125; else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) &#123;...&#125; else &#123; // 遍历链表，并统计链表长度 for (int binCount = 0; ; ++binCount) &#123; // 未在单链表中找到要插入的节点，将新节点接在单链表的后面 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) &#123;...&#125; break; &#125; // 插入的节点已经存在于单链表中 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) &#123;...&#125; afterNodeAccess(e); // 回调方法，后续说明 return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) &#123;...&#125; afterNodeInsertion(evict); // 回调方法，后续说明 return null;&#125;// HashMap 中实现Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(hash, key, value, next);&#125;// LinkedHashMap 中覆写Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); // 将 Entry 接在双向链表的尾部 linkNodeLast(p); return p;&#125;// LinkedHashMap 中实现private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; // last 为 null，表明链表还未建立 if (last == null) head = p; else &#123; // 将新节点 p 接在链表尾部 p.before = last; last.after = p; &#125;&#125; 上面就是 LinkedHashMap 插入相关的源码，这里省略了部分非关键的代码。我根据上面的代码，可以知道 LinkedHashMap 插入操作的调用过程。如下： 我把 newNode 方法红色背景标注了出来，这一步比较关键。LinkedHashMap 覆写了该方法。在这个方法中，LinkedHashMap 创建了 Entry，并通过 linkNodeLast 方法将 Entry 接在双向链表的尾部，实现了双向链表的建立。双向链表建立之后，我们就可以按照插入顺序去遍历 LinkedHashMap，大家可以自己写点测试代码验证一下插入顺序。 以上就是 LinkedHashMap 维护插入顺序的相关分析。本节的最后，再额外补充一些东西。大家如果仔细看上面的代码的话，会发现有两个以after开头方法，在上文中没有被提及。在 JDK 1.8 HashMap 的源码中，相关的方法有3个： 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; 根据这三个方法的注释可以看出，这些方法的用途是在增删查等操作后，通过回调的方式，让 LinkedHashMap 有机会做一些后置操作。上述三个方法的具体实现在 LinkedHashMap 中，本节先不分析这些实现，相关分析会在后续章节中进行。 链表节点的删除过程与插入操作一样，LinkedHashMap 删除操作相关的代码也是直接用父类的实现。在删除节点时，父类的删除逻辑并不会修复 LinkedHashMap 所维护的双向链表，这不是它的职责。那么删除及节点后，被删除的节点该如何从双链表中移除呢？当然，办法还算是有的。上一节最后提到 HashMap 中三个回调方法运行 LinkedHashMap 对一些操作做出响应。所以，在删除节点后，回调方法 afterNodeRemoval 会被调用。LinkedHashMap 覆写该方法，并在该方法中完成了移除被删除节点的操作。相关源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// HashMap 中实现public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;// HashMap 中实现final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) &#123;...&#125; else &#123; // 遍历单链表，寻找要删除的节点，并赋值给 node 变量 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) &#123;...&#125; // 将要删除的节点从单链表中移除 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); // 调用删除回调方法进行后续操作 return node; &#125; &#125; return null;&#125;// LinkedHashMap 中覆写void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 将 p 节点的前驱后后继引用置空 p.before = p.after = null; // b 为 null，表明 p 是头节点 if (b == null) head = a; else b.after = a; // a 为 null，表明 p 是尾节点 if (a == null) tail = b; else a.before = b;&#125; 删除的过程并不复杂，上面这么多代码其实就做了三件事： 根据 hash 定位到桶位置 遍历链表或调用红黑树相关的删除方法 从 LinkedHashMap 维护的双链表中移除要删除的节点 举个例子说明一下，假如我们要删除下图键值为 3 的节点。 根据 hash 定位到该节点属于3号桶，然后在对3号桶保存的单链表进行遍历。找到要删除的节点后，先从单链表中移除该节点。如下： 然后再双向链表中移除该节点： 删除及相关修复过程并不复杂，结合上面的图片，大家应该很容易就能理解，这里就不多说了。 访问顺序的维护过程前面说了插入顺序的实现，本节来讲讲访问顺序。默认情况下，LinkedHashMap 是按插入顺序维护链表。不过我们可以在初始化 LinkedHashMap，指定 accessOrder 参数为 true，即可让它按访问顺序维护链表。访问顺序的原理上并不复杂，当我们调用get/getOrDefault/replace等方法时，只需要将这些方法访问的节点移动到链表的尾部即可。相应的源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// LinkedHashMap 中覆写public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; // 如果 accessOrder 为 true，则调用 afterNodeAccess 将被访问节点移动到链表最后 if (accessOrder) afterNodeAccess(e); return e.value;&#125;// LinkedHashMap 中覆写void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; // 如果 b 为 null，表明 p 为头节点 if (b == null) head = a; else b.after = a; if (a != null) a.before = b; /* * 这里存疑，父条件分支已经确保节点 e 不会是尾节点， * 那么 e.after 必然不会为 null，不知道 else 分支有什么作用 */ else last = b; if (last == null) head = p; else &#123; // 将 p 接在链表的最后 p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; 上面就是访问顺序的实现代码，并不复杂。下面举例演示一下，帮助大家理解。假设我们访问下图键值为3的节点，访问前结构为： 访问后，键值为3的节点将会被移动到双向链表的最后位置，其前驱和后继也会跟着更新。访问后的结构如下： 基于 LinkedHashMap 实现缓存前面介绍了 LinkedHashMap 是如何维护插入和访问顺序的，大家对 LinkedHashMap 的原理应该有了一定的认识。本节我们来写一些代码实践一下，这里通过继承 LinkedHashMap 实现了一个简单的 LRU 策略的缓存。在写代码之前，先介绍一下前置知识。 在3.1节分析链表建立过程时，我故意忽略了部分源码分析。本节就把忽略的部分补上，先看源码吧： 12345678910111213void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // 根据条件判断是否移除最近最少被访问的节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;// 移除最近最少被访问条件之一，通过覆盖此方法可实现不同策略的缓存protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; 上面的源码的核心逻辑在一般情况下都不会被执行，所以之前并没有进行分析。上面的代码做的事情比较简单，就是通过一些条件，判断是否移除最近最少被访问的节点。看到这里，大家应该知道上面两个方法的用途了。当我们基于 LinkedHashMap 实现缓存时，通过覆写removeEldestEntry方法可以实现自定义策略的 LRU 缓存。比如我们可以根据节点数量判断是否移除最近最少被访问的节点，或者根据节点的存活时间判断是否移除该节点等。本节所实现的缓存是基于判断节点数量是否超限的策略。在构造缓存对象时，传入最大节点数。当插入的节点数超过最大节点数时，移除最近最少被访问的节点。实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637public class SimpleCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private static final int MAX_NODE_NUM = 100; private int limit; public SimpleCache() &#123; this(MAX_NODE_NUM); &#125; public SimpleCache(int limit) &#123; super(limit, 0.75f, true); this.limit = limit; &#125; public V save(K key, V val) &#123; return put(key, val); &#125; public V getOne(K key) &#123; return get(key); &#125; public boolean exists(K key) &#123; return containsKey(key); &#125; /** * 判断节点数是否超限 * @param eldest * @return 超限返回 true，否则返回 false */ @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; return size() &gt; limit; &#125;&#125; 测试代码如下： 12345678910111213141516171819202122public class SimpleCacheTest &#123; @Test public void test() throws Exception &#123; SimpleCache&lt;Integer, Integer&gt; cache = new SimpleCache&lt;&gt;(3); for (int i = 0; i &lt; 10; i++) &#123; cache.save(i, i * i); &#125; System.out.println("插入10个键值对后，缓存内容："); System.out.println(cache + "\n"); System.out.println("访问键值为7的节点后，缓存内容："); cache.getOne(7); System.out.println(cache + "\n"); System.out.println("插入键值为1的键值对后，缓存内容："); cache.save(1, 1); System.out.println(cache); &#125;&#125; 测试结果如下： 在测试代码中，设定缓存大小为3。在向缓存中插入10个键值对后，只有最后3个被保存下来了，其他的都被移除了。然后通过访问键值为7的节点，使得该节点被移到双向链表的最后位置。当我们再次插入一个键值对时，键值为7的节点就不会被移除。 本节作为对前面内的补充，简单介绍了 LinkedHashMap 在其他方面的应用。本节内容及相关代码并不难理解，这里就不在赘述了。 总结本文从 LinkedHashMap 维护双向链表的角度对 LinkedHashMap 的源码进行了分析，并在文章的结尾基于 LinkedHashMap 实现了一个简单的 Cache。在日常开发中，LinkedHashMap 的使用频率虽不及 HashMap，但它也个重要的实现。在 Java 集合框架中，HashMap、LinkedHashMap 和 TreeMap 三个映射类基于不同的数据结构，并实现了不同的功能。HashMap 底层基于拉链式的散列结构，并在 JDK 1.8 中引入红黑树优化过长链表的问题。基于这样结构，HashMap 可提供高效的增删改查操作。LinkedHashMap 在其之上，通过维护一条双向链表，实现了散列数据结构的有序遍历。TreeMap 底层基于红黑树实现，利用红黑树的性质，实现了键值对排序功能。 到此，本篇文章就写完了，感谢大家的阅读！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>LinkedHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-数组中只出现一次的数字]]></title>
    <url>%2F2019%2F07%2F09%2Fjianzhioffer-appear-once%2F</url>
    <content type="text"><![CDATA[题目描述一个整型数组里除了两个数字之外，其他的数字都出现了偶数次。请写程序找出这两个只出现一次的数字。 解题思路先来个简单题，如果题目是这样子的：一个整型数组里除了一个数字之外，其他的数字都出现了偶数次。请写程序找出这一个只出现一次的数字。这个题就so easy了。xor（异或）有个重要的性质：任何数字异或它自己都等于0（异或，一种位运算，两个比特相同结果则为0，否则为1）用0依次异或数组的元素，最后的结果就是只出现一次或者只出现奇数次的数字（因为偶数次的数字在异或运算中都抵消了）。 现在的问题是有两个出现次数为一次的数字。那么只要把数组分成两部分，每一部分分别含有一个出现次数为1（或者奇数次也可以的）的数字就可以了。 先用0对整个数字进行异或运算，结果为xor，是两个只出现一次的数字的异或的结果。出现次数为1的两个数字肯定不相同，那么xor中至少有一位为1。我们在xor中找到第一位为1的位置，记为n。根据第n为是不是1可以把整个数组分为两部分。每一部分都只含有一个出现次数为1的数字，然后剩余的数字都出现偶数次。 AC代码1234567891011121314151617//num1,num2分别为长度为1的数组。传出参数//将num1[0],num2[0]设置为返回结果public class Solution &#123; public void FindNumsAppearOnce(int [] array,int num1[] , int num2[]) &#123; if(array.length &lt; 2) return; int xor = 0; int partion = 1; for(int i = 0;i &lt; array.length;i++) xor ^= array[i]; // xor 中从右往左第一次出现 1 的位置 // 如: 10 &amp; -10 = 2 (10的二进制为 1010) partion = xor &amp; -xor; for(int i = 0;i &lt; array.length;i++)&#123; if((partion &amp; array[i]) == 0) num1[0] ^= array[i]; else num2[0] ^= array[i]; &#125; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中集合类的继承体系]]></title>
    <url>%2F2019%2F07%2F08%2Fjava-collection-map%2F</url>
    <content type="text"><![CDATA[Java中集合类存放在java.util包中，主要有三种：set、list、map。 List继承体系如下： Set继承体系如下： Map继承体系如下：]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>集合类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java7新特性-catch捕获多个异常]]></title>
    <url>%2F2019%2F07%2F08%2Fjava7-catch-more-exception%2F</url>
    <content type="text"><![CDATA[java 7使得我们能够在同一个catch语句块中捕获多种不同的异常，这也叫做多重异常捕获。 在java7以前，我们可能要这样做： 12345678910111213try &#123; // execute code that may throw 1 of the 3 exceptions below.&#125; catch(SQLException e) &#123; logger.log(e);&#125; catch(IOException e) &#123; logger.log(e);&#125; catch(Exception e) &#123; logger.severe(e);&#125; 正如上面看到的，SQLException 和IOException 这两个异常都是以相同的方式来处理的，但是你仍然要为这两个异常写两个独立的catch语句块。 java 7中你可以像下面这样捕获多个异常： 12345678910try &#123; // execute code that may throw 1 of the 3 exceptions below.&#125; catch(&lt;strong&gt;SQLException | IOException e&lt;/strong&gt;) &#123; logger.log(e);&#125; catch(Exception e) &#123; logger.severe(e);&#125; 注意，第一个catch语句块中的两个异常名字是被管道字符|分割的。两个异常类名之间的管道字符就是在同一个catch语句块中声明多个异常的方法。 参考：在java 7中捕获多个异常]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java7新特性-增强try(Try-with-resources)]]></title>
    <url>%2F2019%2F07%2F08%2Fjava7-Try-with-resources%2F</url>
    <content type="text"><![CDATA[本文转载自：增强 try （Try-with-resources Try-with-resourcesTry-with-resources是Java7出现的一个新的异常处理机制，它能够很容易地关闭在try-catch语句块中使用的资源。 传统的关闭资源方式是利用Try-Catch-Finally管理资源（旧的代码风格） 即在Java7以前程序中使用的资源需要被明确地关闭。 123456789101112131415private static void printFile() throws IOException &#123; InputStream input = null; try&#123; input = new FileInputStream("d:\\hello.txt"); int data = input.read(); while(data != -1)&#123; System.out.print((char) data); data = input.read(); &#125; &#125; finally &#123; if(input != null)&#123; input.close(); &#125; &#125; &#125; 以上程序 try语句块中有3处能抛出异常，finally语句块中有一处会抛出异常。 不论try语句块中是否有异常抛出，finally语句块始终会被执行。这意味着，不论try语句块中发生什么，InputStream 都会被关闭，或者说都会试图被关闭。如果关闭失败，close()方法也可能会抛出异常。 假设try语句块抛出一个异常，然后finally语句块被执行。同样假设finally语句块也抛出了一个异常。那么哪个异常会根据调用栈往外传播？ 即使try语句块中抛出的异常与异常传播更相关，最终还是finally语句块中抛出的异常会根据调用栈向外传播。 在Java7以后，对于上面的例子可以用try-with-resource 结构这样写： 123456789private static void printFileJava7() throws IOException &#123; try(FileInputStream input = new FileInputStream("d:\\hello.txt")) &#123; int data = input.read(); while(data != -1)&#123; System.out.print((char) data); data = input.read(); &#125; &#125; &#125; 注意方法中的第一行： 1try(FileInputStream input = new FileInputStream(&quot;d:\\hello.txt&quot;)) 这就是try-with-resource 结构的用法。FileInputStream 类型变量就在try关键字后面的括号中声明。而且一个FileInputStream 类型被实例化并被赋给了这个变量。 当try语句块运行结束时，FileInputStream 会被自动关闭。这是因为FileInputStream 实现了java中的 java.lang.AutoCloseable接口。所有实现了这个接口的类都可以在try-with-resources结构中使用。 当try-with-resources结构中抛出一个异常，同时FileInputStream被关闭时（调用了其close方法）也抛出一个异常，try-with-resources结构中抛出的异常会向外传播，而FileInputStream被关闭时抛出的异常被抑制了。这与文章开始处利用旧风格代码的例子（在finally语句块中关闭资源）相反。 使用多个资源你也可以在块中使用多个资源而且这些资源都能被自动地关闭。下面是例子： 12345678910private static void printFileJava7() throws IOException &#123; try(FileInputStream input = new FileInputStream("d:\\hello.txt"); BufferedInputStream bufferedInput = new BufferedInputStream(input)) &#123; int data = buffereInput.read(); while(data != -1)&#123; System.out.println( (char) data); data = bufferedInput.read(); &#125; &#125;&#125; 上面的例子在try关键字后的括号里创建了两个资源——FileInputStream 和BufferedInputStream。当程序运行离开try语句块时，这两个资源都会被自动关闭。 这些资源将按照他们被创建顺序的逆序来关闭。首先BufferedInputStream 会被关闭，然后FileInputStream会被关闭。 自定义AutoCloseable 实现这个try-with-resources结构里不仅能够操作java内置的类。你也可以在自己的类中实现java.lang.AutoCloseable接口，然后在try-with-resources结构里使用这个类。 AutoClosable 接口仅仅有一个方法，接口定义如下： 123public interface AutoClosable&#123; public void close() throws Exception;&#125; 任何实现了这个接口的方法都可以在try-with-resources结构中使用。下面是一个简单的例子： 12345678910public class MyAutoClosable implements AutoClosable&#123; public void doIt()&#123; System.out.println("MyAutoClosable doing it!"); &#125; @Override public void close() throws Exception&#123; System.out.println("MyAutoCloseable Closed!"); &#125;&#125; 下面是MyAutoClosable 在try-with-resources结构中使用的例子： 12345private static void myAutoClosable() throws Exception&#123; try(MyAutoClosable myAutoClosable = new MyAutoClosable())&#123; myAutoClosable.doIt(); &#125;&#125; 当方法myAutoClosable.doIt()被调用时，下面是打印到System.out的输出： 12MyAutoClosable doing it!MyAutoClosable closed! 通过上面这些你可以看到，不论try-catch中使用的资源是自己创造的还是java内置的类型，try-with-resources都是一个能够确保资源能被正确地关闭的强大方法。 小结： 自动关闭资源的try语句相当于包含了隐式的finally块（用于关闭资源），因此这个try语句可以既没有catch块，也没有finally块。 被自动关闭的资源必须实现Closeable或AutoCloseable接口。（Closeable是AutoCloseable的子接口，Closeeable接口里的close()方法声明抛出了IOException,;AutoCloseable接口里的close()方法声明抛出了Exception） Java7几乎把所有的“资源类”（包括文件IO的各种类，JDBC编程的Connection、Statement等接口……）进行了改写，改写后的资源类都实现了AutoCloseable或Closeable接口 Java7新增的自动关闭资源的try语句允许在try关键字后紧跟一对圆括号，里面可以声明、初始化一个或多个资源，此处的资源指的是那些必须在程序结束时显示关闭的资源（数据库连接、网络连接等），try语句会在该语句结束时自动关闭这些资源。 被关闭的资源必须放在try语句后的圆括号中声明、初始化。如果程序有需要自动关闭资源的try语句后可以带多个catch块和一个finally块。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java8的新特性]]></title>
    <url>%2F2019%2F07%2F08%2Fjava8-new-feature%2F</url>
    <content type="text"><![CDATA[本文转载自：Java8 新特性 前言北京时间2018年9月26日，Oracle官方发布Java 11。既然版本都更新到11了，现在才来学8是不是太晚了？其实不是的，目前应该大部分都还是使用的Java 7和Java 8，这两个应该还是主流。而Java 8 又有一些激动人心的新特性，所以还是值得学习的。Java 8 新特性主要有以下几点： Lambda表达式(重点)； 函数式接口； 方法引用与构造器引用； Stream API(重点)； 接口中的默认方法与静态方法； 新时间日期API； 其他新特性。 有了以上新特性，Java 8就可以做到： 速度更快； 代码更少(增加了新的语法 Lambda 表达式)； 方便操作集合(Stream API) 便于并行； 最大化减少空指针异常 Optional。 接下来一起来了解一下Java 8的这些新特性。 Lambada表达式1、什么是lambda？ Lambda 是一个匿名函数，我们可以把 Lambda 表达式理解为是一段可以传递的代码（将代码像数据一样进行传递）。可以写出更简洁、更灵活的代码。 2、了解新操作符： Java 8引入了新的操作符，-&gt;，叫箭头操作符或者叫lambda操作符。当使用lambda表达式时就需要使用这个操作符。 3、lambda表达式语法： 箭头操作符将lambda表达式分成了两部分： 左侧：lambda表达式的参数列表(接口中抽象方法的参数列表) 右侧：lambda表达式中所需执行的功能(lambda体，对抽象方法的实现) 语法有如下几种格式： 语法格式一(无参数无返回值)： () -&gt; 具体实现 语法格式二(有一个参数无返回值)： (x) -&gt; 具体实现 或 x -&gt; 具体实现 语法格式三(有多个参数，有返回值，并且lambda体中有多条语句)：(x,y) -&gt; {具体实现} 语法格式四：若方法体只有一条语句，那么大括号和return都可以省略 注：lambda表达式的参数列表的参数类型可以省略不写，可以进行类型推断。 看几个例子： 例一： 123456789101112@Testpublic void test1()&#123; // 实现一个线程 int num = 0;//jdk1.8以前，这个必须定义为final，下面才能用，1.8后默认就为final Runnable runnable = new Runnable() &#123; @Override public void run() &#123; System.out.println("hello world"+ num); &#125; &#125;; runnable.run();&#125; 创建一个线程，重写run方法，在run方法里面打印一句话。我们想要的就是System.out.println(&quot;hello world&quot;+ num);这行代码，但是为了实现这行代码，不得不多写了好多行。lambda就可以解决这一点，看看用lambda如何实现: 12Runnable runnable1 = () -&gt; System.out.println("hello world"+num);runnable1.run(); 用lambda这样就搞定了。首先还是Runnable runnable1 =，但是不用new了，右边就用lambda实现。我们要使用的是该接口的run方法，run方法不需要参数，所以lambda表达式左边就是()，lambda表达式右边是抽象方法的实现，也就是第一种方式中run方法的方法体写到lambda表达式右边就可以了。 例二： 123456Comparator&lt;Integer&gt; comparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return Integer.compare(o1,o2);//就这一行关键代码 &#125;&#125;; 以前写一个比较器就要像上面那样写，先new比较器类，然后在其compare方法里写核心代码。用lambda实现： 1Comparator&lt;Integer&gt; comparator = (x,y) -&gt; Integer.compare(x,y); compare方法需要两个参数，所以箭头操作符左边写(x,y)，右边是compare方法的实现，所以应该写return Integer.compare(o1,o2);，但是根据上面的语法格式四可知，return可以省略，因此就写成了上面那样。 通过这两个例子可以感受到lambda表达式的简洁，但是问题来了：我们说lambda表达式就是一个匿名函数，我们只需要指定参数和lambda体即可，那么它是如何判断重写的是哪个方法呢？比如一个接口中有多个方法，如果使用lambda表达式来写，那么如何判断我们使用的是该接口的哪个方法？其实是不能判断的！通过上面两个例子可以发现，Runnable接口和Comparator接口都是只有一个方法的接口，所以可以使用lambda。 函数式接口1、什么是函数式接口？ 像Runnable和Comparator这样只有一个方法的接口，称为函数式接口。也可以在接口上加上@FunctionalInterface注解，如果编译通过，则该接口就是函数式接口。lambda表达式就需要函数式接口的支持。 2、看一个需求： 需求：需要对两个数进行加减乘除等运算，怎么实现？ 传统做法：传统做法中，需要进行几种运算，我们就要写几个方法。一种运算对应一个方法。 lambda做法：首先要定义一个函数式接口，接口中只有一个方法，接收两个参数。 1234@FunctionalInterfacepublic interface MyInterface &#123; public Integer getValue(Integer num1,Integer num2);&#125; 然后就可以使用了： 123456789@Testpublic void test5()&#123; MyInterface myInterface = (x,y) -&gt; x*y;//乘法运算 MyInterface myInterface1 = (x,y) -&gt; x+y;//加法运算 Integer result1 = myInterface.getValue(100,200); Integer result2 = myInterface1.getValue(1024,2048); System.out.println(result1); System.out.println(result2);&#125; 所以用lambda的话，只需要定义一个函数式接口，不管进行什么操作，都可以用lambda解决，不用再一种运算对应一个方法。但是，还需要自己定义函数式接口，好像也没简单很多。Java考虑到这点了，所以内置了函数式接口。 3、四大内置函数式接口： 为了不需要我们自己定义函数式接口，Java内置了四大函数式接口，这四大接口加上它们的子类，完全满足我们的使用了。四大函数式接口是： Consumer：消费型接口(void accept(T t))，接收一个参数，无返回值。 Supplier：供给型接口(T get())，无参数，有返回值。 Function&lt;T,R&gt;：函数型接口(R apply(T t))，接收一个参数，有返回值。 Predicate：断言型接口(boolean test(T t))，接收一个参数，返回Boolean值。 4、四大函数式接口的使用： 接下来看看具体如何使用这四大函数式接口。 消费型接口的使用： 12Consumer consumer = (x) -&gt; System.out.println("消费了"+x+"元");consumer.accept(100); 供给型接口的使用： 12Supplier&lt;Integer&gt; supplier = () -&gt; (int)(Math.random() * 100);//生成随机数System.out.println(supplier.get()); 函数型接口的使用： 123Function&lt;String,String&gt; function = str -&gt; str.toUpperCase();//将传入的字符串转成大写String s = function.apply("adcdefggffs");System.out.println(s); 断言型接口的使用： 1234567891011121314151617 //需求：将满足条件的字符串添加到集合中去 public List&lt;String&gt; filterString(List&lt;String&gt; strings, Predicate&lt;String&gt; predicate)&#123; List&lt;String&gt; stringList = new ArrayList&lt;&gt;(); for (String string : strings) &#123; if (predicate.test(string))&#123; stringList.add(string); &#125; &#125; return stringList; &#125;//测试@Test public void test4()&#123; List&lt;String&gt; list = Arrays.asList("hello","world","niu","bi"); List&lt;String&gt; newList = filterString(list,str -&gt; str.length() &gt; 3);//选出长度大于3的字符串 newList.forEach(System.out::println); &#125; 方法引用与构造器引用当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用。不过实现抽象方法的参数列表，必须与引用方法的参数列表保持一致。 1、方法引用语法： 对象::实例方法 类::静态方法 类::实例方法 2、方法引用具体用法： 说了那么多可能还不清楚到底什么意思，一起来看几个例子。 语法一例子： 12Consumer&lt;String&gt; consumer = x -&gt; System.out.println(x);//传统写法Consumer&lt;String&gt; consumer = System.out::println;//使用方法引用 println方法和Consumer的accept方法都是无返回值，接收一个参数，所以可以这样写。 语法二例子： 123Comparator&lt;Integer&gt; comparator = (x,y) -&gt; Integer.compare(x,y);//因为compare方法已经被Integer实现了，且是静态的，所以这样用就行。Comparator&lt;Integer&gt; comparator1 = Integer::compare; 语法三例子： 1234BiPredicate&lt;String,String&gt; biPredicate = (x,y) -&gt; x.equals(y);//可以改成如下写法//不过要满足：第一个参数是实例方法的调用者，第二个参数是实例方法的参数时，就可以这样用BiPredicate&lt;String,String&gt; biPredicate1 = String::equals; 3、构造器引用： 12345Supplier&lt;Employee&gt; supplier = () -&gt; new Employee();//可以改写成这样//注意：需要调用的构造器的参数列表要与函数接口中抽象方法的参数列表一致Supplier&lt;Employee&gt; supplier1 = Employee::new;Employee employee = supplier.get(); Stream APIStream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。使用Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简而言之，Stream API 提供了一种高效且易于使用的处理数据的方式。 1、理解Stream: Stream被称作流，是用来处理集合以及数组的数据的。它有如下特点： Stream 自己不会存储元素。 Stream 不会改变源对象。相反，他们会返回一个持有结果的新Stream。 Stream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行。 2、使用Stream的三个步骤： 创建Stream：一个数据源（如：集合、数组），获取一个流 中间操作：一个中间操作链，对数据源的数据进行处理 终止操作：一个终止操作，执行中间操作链，并产生结果 3、创建Stream: 直接看代码： 12345678910111213141516171819202122//1、通过集合提供的stream方法或parallelStream()方法创建List&lt;String&gt; list = new ArrayList&lt;&gt;();Stream&lt;String&gt; stringStream = list.stream();//2、通过Arrays中的静态方法stream获取数组流Employee[] employees = new Employee[10];Stream&lt;Employee&gt; stream = Arrays.stream(employees);//3、通过Stream类的静态方法of()创建流Stream&lt;String&gt; stream1 = Stream.of("aa","bb","cc");//4、创建无限流//迭代方式创建无限流//从0开始，每次加2，生成无限个Stream&lt;Integer&gt; stream2 = Stream.iterate(0,(x) -&gt; x+2);//生成10个stream2.limit(10).forEach(System.out::println);//生成方式创建无限流Stream.generate(() -&gt; Math.random()) .limit(5) .forEach(System.out::println); 上面介绍了集合、数组创建流的几种方式，都有对应的注解。 4、中间操作： 筛选与切片： filter – 接收lambda，从流中排除某些数据。 limit – 截断流，使其元素不超过给定数量。 skip(n) – 跳过元素，返回一个扔掉了前n个元素的流，若不足n个元素，则返回空流。 distinct – 筛选，通过流所生成元素的hashCode()和equals()去除重复元素,所以对象必须重新hashCode方法和equals方法。 看代码： 1234employees.stream()//已有employees集合 .filter((e) -&gt; e.getAge() &gt; 18)//中间操作(选出年龄大于18的) .limit(1)//中间操作(只返回一个) .forEach(System.out::println);//终止操作 映射： map – 接收lambda，将元素转换成其他形式或提取信息。接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。 flatMap – 接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所以流连接成一个流。 看例子： 1234List&lt;String&gt; list = Arrays.asList("aa","bb","cc","dd");list.stream() .map(str -&gt; str.toUpperCase())//将所有的转成大写 .forEach(System.out::println); 排序： sorted() – 自然排序(按照Comparable来排序)。 sorted(Comparator com) – 定制排序(按照Comparator来排序)。 看例子： 1234567891011121314List&lt;String&gt; list = Arrays.asList("ccc","bbb","aaa","ddd");list.stream() .sorted()//自然排序 .forEach(System.out::print);//aaa,bbb,ccc,ddd//定制排序employees.stream()//employees是一个存有多名员工的集合 .sorted((e1, e2) -&gt; &#123; if (e1.getAge().equals(e2.getAge()))&#123; //如果年龄一样 return e1.getName().compareTo(e2.getName());//就比较姓名 &#125;else &#123; return e1.getAge().compareTo(e2.getAge());//年龄不一样就比较年龄 &#125; &#125;).forEach(System.out::println); 5、终止操作： 查找与匹配： allMatch – 检查是否匹配所有元素。 anyMatch – 检查是否至少匹配一个元素。 noneMatch – 检查是否没有匹配所有元素。 findFirst – 返回第一个元素。 findAny – 返回当前流中任意元素。 count – 返回流中元素总个数。 max – 返回流中最大值。 min – 返回流中最小值。 1234//看看employee集合中是不是所有都是男的boolean b = employees.stream() .allMatch(e -&gt; e.getGender().equals("男"));System.out.println(b); 规约： reduce(T identity,BinaryOperator) – 可以将流中元素反复结合起来，得到一个值。 1234567891011//规约求和List&lt;Integer&gt; list = Arrays.asList(1,3,5,4,4,3);Integer sum = list.stream() .reduce(0,(x,y) -&gt; x+y);//首先把0作为x，把1作为y，进行加法运算得到1，把1再作为x，把3作为y，以此类推System.out.println(sum);//获取工资总和Optional&lt;Double&gt; optional = employees.stream() .map(Employee::getSalary)//提取工资 .reduce(Double::sum);//求工资总和System.out.println(optional2.get()); 收集： collect – 将流转换为其他形式。接收一个Collector接口的实现，用于给Stream中元素做汇总的方法。 12345678910111213141516171819202122232425262728293031323334//把公司中所有员工的姓名提取出来并收集到一个集合中去List&lt;String&gt; stringList = employees.stream() .map(Employee::getName)//提取员工姓名 //.collect(Collectors.toList());//收集到list集合 //.collect(Collectors.toSet());//收集到set集合 .collect(Collectors.toCollection(LinkedList::new));//这种方式可收集到任意集合stringList.forEach(System.out::println);//遍历集合//计算工资平均值Double avgSalary = employees.stream() .collect(Collectors.averagingDouble(Employee::getSalary));System.out.println(avgSalary);//根据年龄分组Map&lt;Integer,List&lt;Employee&gt;&gt; map = employees.stream() .collect(Collectors.groupingBy(Employee::getAge));System.out.println(map);//先按性别分组，性别一样时按年龄分组Map&lt;String,Map&lt;Integer,List&lt;Employee&gt;&gt;&gt; map1 = employees.stream() .collect(Collectors.groupingBy(Employee::getGender,Collectors.groupingBy(Employee::getAge)));System.out.println(map1);//分区，满足条件的一个区，不满足的另一个区Map&lt;Boolean,List&lt;Employee&gt;&gt; map2 = employees.stream() .collect(Collectors.partitioningBy(e -&gt; e.getSalary() &gt; 6000));//工资大于6000的为true区，否则为false区System.out.println(map2);//获取工资的总额、平均值等DoubleSummaryStatistics dss = employees.stream() .collect(Collectors.summarizingDouble(Employee::getSalary));System.out.println(dss.getSum());System.out.println(dss.getAverage());System.out.println(dss.getMax()); 并行流与串行流1、fork/join框架： 此框架就是在必要的情况下，将一个大任务，进行拆分(fork)成若干个小任务（拆到不可再拆时），再将一个个的小任务运算的结果进行 join 汇总。 fork/join 2、并行流与串行流： 通过上面的图可以知道，使用fork/join框架可以提高效率(运算量越大越明显，运算量可能反而更慢，因为拆分也需要时间)，但是在Java 8之前需要自己实现fork/join，还是挺麻烦的，Java 8就方便多了，因为提供了并行流，底层就是使用了fork/join。Stream API 可以声明性地通过parallel()与 sequential() 在并行流与顺序流之间进行切换。 1234567891011121314151617181920@Testpublic void test()&#123; Instant start = Instant.now(); //普通做法求0加到10000000000的和 LongStream.rangeClosed(0,100000000000L) .reduce(0,Long::sum); Instant end = Instant.now(); System.out.println("耗费"+ Duration.between(end ,start) + "秒");//55秒&#125;@Testpublic void test2()&#123; Instant start = Instant.now(); //并行流求0加到10000000000的和 LongStream.rangeClosed(0,100000000000L) .parallel()//使用并行流 .reduce(0,Long::sum); Instant end = Instant.now(); System.out.println("耗费"+ Duration.between(end ,start) + "秒");//30秒&#125; 通过运行上面的程序可以明显感受到并行流的高效。 新时间日期APIJava 8之前的Date和Calendar都是线程不安全的，而且使用起来比较麻烦，Java 8提供了全新的时间日期API，LocalDate(日期)、LocalTime(时间)、LocalDateTime(时间和日期) 、Instant (时间戳)、Duration(用于计算两个“时间”间隔)、Period(用于计算两个“日期”间隔)等。 1、LocalDate、LocalTime、LocalDateTime： 这三个用法一样。 123456789//获取当前系统时间LocalDateTime localDateTime = LocalDateTime.now();//当前时间日期LocalDateTime localDateTime2 = localDateTime.plusYears(2);//加两年System.out.println(localDateTime.getMonth());System.out.println(localDateTime);System.out.println(localDateTime2);//指定时间LocalDateTime localDateTime1 = LocalDateTime.of(2018,12,13,21,8);System.out.println(localDateTime1); 2、Instant 时间戳： 时间戳就是计算机读的时间，它是以Unix元年(传统的设定为UTC时区1970年1月1日午夜时分)开始算起的。 123456 //计算机读的时间：时间戳(Instant)，1970年1月1日0时0分0秒到此时的毫秒值Instant instant = Instant.now();System.out.println(instant);//默认是美国时区，8个时差OffsetDateTime offsetDateTime = instant.atOffset(ZoneOffset.ofHours(8));//加上时差System.out.println(offsetDateTime);System.out.println(instant.toEpochMilli());//显示毫秒值 3、Duration 和 Period： 123456789101112131415LocalTime localTime = LocalTime.now();try &#123; Thread.sleep(1000);&#125;catch (Exception e)&#123; e.printStackTrace();&#125;LocalTime localTime1 = LocalTime.now();System.out.println(Duration.between(localTime,localTime1).toMillis());//获取两个日期之间的间隔LocalDate localDate = LocalDate.of(2012,1,1);LocalDate localDate1 = LocalDate.now();Period period = Period.between(localDate,localDate1);System.out.println(period);System.out.println(period.getYears()+"年"+period.getMonths()+"月"+period.getDays()+"日"); 4、时间校正器(TemporalAdjuster)： 1234567LocalDateTime localDateTime = LocalDateTime.now();System.out.println(localDateTime);LocalDateTime localDateTime1 = localDateTime.withDayOfMonth(1);//localDate日期中月份的1号System.out.println(localDateTime1);localDateTime1.with(TemporalAdjusters.firstDayOfNextMonth());//下一个月的第一天localDateTime.with(TemporalAdjusters.next(DayOfWeek.SUNDAY));//下周日 5、格式化日期(.DateTimeFormatter )： 1234567891011121314151617181920@Testpublic void test6()&#123; //DateTimeFormatter:格式化 //使用预设格式 DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ISO_DATE_TIME; LocalDateTime localDateTime = LocalDateTime.now(); String str = localDateTime.format(dateTimeFormatter); System.out.println(str); System.out.println("=========================="); //自定义格式 DateTimeFormatter dateTimeFormatter1 = DateTimeFormatter.ofPattern("yyyy年MM月dd日 HH:mm:ss"); String str2 = localDateTime.format(dateTimeFormatter1); //这样格式化也可以 String str3 = dateTimeFormatter1.format(localDateTime); System.out.println(str2); System.out.println(str3); //退回到解析前的格式 LocalDateTime newDate = localDateTime.parse(str,dateTimeFormatter); System.out.println(newDate);&#125; 6、时区的处理： Java8 中加入了对时区的支持，带时区的时间为分别为：ZonedDate、ZonedTime、ZonedDateTime。 123456 @Test public void test7()&#123; //ZonedDate ZonedTime ZonedDateTime LocalDateTime dateTime = LocalDateTime.now(ZoneId.of("Europe/Tallinn")); System.out.println(dateTime);&#125; 接口中的默认方法和静态方法123456789public interface MyInterface &#123; default String test()&#123; return "允许存在有具体实现的方法"; &#125; public static String test2()&#123; return "接口中还可以有静态方法"; &#125;&#125; 如上所示，Java 8的接口中允许有默认方法和静态方法。如果一个类继承了一个类还实现了一个接口，而且接口中的默认方法和父类中的方法同名，这时采用类优先原则。也就是说，子类使用的是父类的方法，而不是接口中的同名方法。 其他新特性1、Optional类： 这个类是为了尽可能减少空指针异常的。就是把普通对象用Optional包起来，做了一些封装。看看其用法： 123456789101112131415161718192021222324252627@Datapublic class Man &#123; //男人类 private Godness godness;//女神&#125;@Datapublic class Godness &#123; private String name; public Godness(String name)&#123; this.name = name; &#125; public Godness()&#123; &#125;&#125;//获取男人心中的女神的名字(有的人不一定有女神，也就是说女神可能为空) //常规做法要加很多判断 public String getGodnessName(Man man)&#123; if (man != null)&#123; Godness godness = man.getGodness(); if (godness != null)&#123; return godness.getName(); &#125;else&#123; return "我心中没有女神"; &#125; &#125;else &#123; return "男人为空"; &#125; &#125; 一个man类，有一个成员变量女神，女神也是一个类，有一个成员变量，名字。要获取man心中的女神，为了防止控制针异常，要做很多的判断。如果使用Optional呢？做法如下： 123456789101112//新男人类@Datapublic class NewMan &#123; private Optional&lt;Godness&gt; godness = Optional.empty();&#125;//使用optional后的方法public String getGodnessName2(Optional&lt;NewMan&gt; man)&#123; return man.orElse(new NewMan()) .getGodness() .orElse(new Godness("我没有女神")) .getName();&#125; 这样就简单多了。 2、重复注解与类型注解： Java 8 可以使用重复注解和类型注解，如下图： 重复注解&amp;类型注解 总结：本文说了一些Java 8 的新特性，重点就是lambda表达式和Stream API，可以简化很多操作。肯可能还有些文中未涉及的，在此抛砖引玉，望各位大佬指点！]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java8中foreach原理]]></title>
    <url>%2F2019%2F07%2F08%2Fjava8-foreach%2F</url>
    <content type="text"><![CDATA[本文转载自：java foreach原理 foreach语法糖foreach是java的语法糖，所谓语法糖就是通过编译器或者其它手段优化了代码，给使用带来了遍历。比如，没有forach之前，我们需要这样遍历一个集合 123for(int i=0; i&lt;list.size; i++)&#123;//.....&#125; 是不是很麻烦？ 如果用foreach只需要这样 1234List&lt;String&gt; list = new ArrayList&lt;String&gt;();for(String e : list)&#123;//&#125; 是不是省事多了，不用索引值，不用判断是否越界。 foreach集合原理但是，今天探讨的是foreach是什么，我们还是通过反编译代码来看吧。源代码 12345678List&lt;String&gt; a = new ArrayList&lt;String&gt;(); a.add("1"); a.add("2"); a.add("3"); for(String temp : a)&#123; System.out.print(temp); &#125; 反编译后的代码 12345678List a = new ArrayList();a.add("1"); a.add("2"); a.add("3"); String temp; for(Iterator i$ = a.iterator(); i$.hasNext(); System.out.print(temp))&#123; temp = (String)i$.next();&#125; foreach集合相当于获取迭代器（a.iterator），通过判断是否有下一个元素(i.hasNext())，，然后移动光标(i.hasNext())，执行操作(System.out.print(temp)) 我们知道集合都实现了iterator接口 1234567public interface Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); void remove();&#125; 1public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; foreach数组好了，foreach集合的真面目，我们看懂了。因为集合实现了Iterator接口，所以遍历时走的Iterator的方法，但是foreach不只可以遍历集合，还可以遍历数组，难道数组也实现了Iterator接口？ 同样，看源代码 1234String[] arr = &#123;"1","2"&#125;;for(String e : arr)&#123; System.out.println(e); &#125; 反编译后代码 12345678String arr[] = &#123; "1", "2" &#125;; String arr$[] = arr; int len$ = arr$.length; for(int i$ = 0; i$ &lt; len$; i$++) &#123; String e = arr$[i$]; System.out.println(e); &#125; 可以看到foreach数组，走的是for(int i=0; i&lt; len; i++)经典模式。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JDK1.8中jvm的变化以及理由]]></title>
    <url>%2F2019%2F07%2F08%2Fjava8-jvm%2F</url>
    <content type="text"><![CDATA[本文转载自：选择JDK1.8的理由之JVM内存变化 jvm运行时内存区域主要分为：程序计数器，jvm栈，本地方法栈，堆，方法区。 JVM中内存JVM中内存通常划分为两个部分，分别为堆内存与栈内存， 栈内存主要用执行线程方法,存放本地临时变量与线程中方法执行时候需要的引用对象地址。 JVM所有的对象数据都存放在堆内存中，相比栈内存，堆内存可以大的多，所以JVM一直通过对堆内存划分不同的功能区块实现对堆内存中对象管理。 堆内存不够最常见的错误就是OOM(OutOfMemoryError) 栈内存溢出最常见的错误就是StackOverflowError，程序有递归调用时候最容易发生 堆内存划分在JDK7以及其前期的JDK版本中，堆内存通常被分为三块区域Nursery内存(young generation)、长时内存(old generation)、永久内存(Permanent Generation for VM Matedata)，显示如下图： 其中最上一层是Nursery内存，一个对象被创建以后首先被放到Nursery中的Eden内存中，如果存活期超两个Survivor之后就会被转移到长时内存中(Old Generation)中。 永久内存中存放着类的方法、变量等元数据信息。如果永久内存不够，我们就会得到如下错误： java.lang.OutOfMemoryError: PermGen 而在JDK8中情况发生了明显的变化，就是一般情况下你都不会得到这个错误，原因在于JDK8中把存放元数据中的永久内存从堆内存中移到了本地内存(native memory)中，JDK8中JVM堆内存结构就变成了如下： 这样永久内存就不再占用堆内存，它可以通过自动增长来避免JDK7以及前期版本中常见的永久内存错误(java.lang.OutOfMemoryError: PermGen)，也许这个就是你的JDK升级到JDK8的理由之一吧。当然JDK8也提供了一个新的设置Matespace内存大小的参数，通过这个参数可以设置Matespace内存大小，这样我们可以根据自己项目的实际情况，避免过度浪费本地内存，达到有效利用。 相关参数-XX:MaxMetaspaceSize=128m设置最大的元内存空间128兆 注意：如果不设置JVM将会根据一定的策略自动增加本地元内存空间。 如果你设置的元内存空间过小，你的应用程序可能得到以下错误： java.lang.OutOfMemoryError: Metadata space jdk8移除了PermGen，取而代之的是MetaSpace元空间（Metaspace）： 一种新的内存空间的诞生。JDK8 HotSpot JVM 使用本地内存来存储类元数据信息并称之为：元空间（Metaspace）；这与Oracle JRockit 和IBM JVM’s很相似。这将是一个好消息：意味着不会再有java.lang.OutOfMemoryError:PermGen问题，也不再需要你进行调优及监控内存空间的使用，但是新特性不能消除类和类加载器导致的内存泄漏。你需要使用不同的方法以及遵守新的命名约定来追踪这些问题。 PermGen 空间的状况 这部分内存空间将全部移除。 JVM的参数：PermSize 和 MaxPermSize 会被忽略并给出警告（如果在启用时设置了这两个参数）。 Metaspace 内存分配模型 （最大区别）大部分类元数据都在本地内存中分配。 Metaspace 容量 默认情况下，类元数据只受可用的本地内存限制（容量取决于是32位或是64位操作系统的可用虚拟内存大小）。 新参数（MaxMetaspaceSize） 用于限制本地内存分配给类元数据的大小。如果没有指定这个参数，元空间会在运行时根据需要动态调整。 Metaspace 垃圾回收 对于僵死的类及类加载器的垃圾回收将在元数据使用达到“MaxMetaspaceSize”参数的设定值时进行。 适时地监控和调整元空间对于减小垃圾回收频率和减少延时是很有必要的。持续的元空间垃圾回收说明，可能存在类、类加载器导致的内存泄漏或是大小设置不合适。 Java 堆内存的影响一些杂项数据已经移到Java堆空间中。升级到JDK8之后，会发现Java堆 空间有所增长。 Metaspace 监控 元空间的使用情况可以从HotSpot1.8的详细GC日志输出中得到。 参数设置默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数来指定元空间的大小： -XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 -XX:MaxMetaspaceSize，最大空间，默认是没有限制的。 除了上面两个指定大小的选项以外，还有两个与 GC 相关的属性： -XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集 -XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集 更新原因 1、字符串存在永久代中，容易出现性能问题和内存溢出。 2、类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代(就小了）溢出。 3、永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。 4、Oracle 可能会将HotSpot 与 JRockit 合二为一。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[死锁]]></title>
    <url>%2F2019%2F07%2F08%2Fdeadlock%2F</url>
    <content type="text"><![CDATA[本文转载自：死锁，死锁的四个必要条件以及处理策略 什么是死锁多线程以及多进程改善了系统资源的利用率并提高了系统的处理能力。然而，并发执行也带来了新的问题——死锁。 死锁是指两个或两个以上的进程（线程）在运行过程中因争夺资源而造成的一种僵局（Deadly-Embrace) ) ，若无外力作用，这些进程（线程）都将无法向前推进。 下面我们通过一些实例来说明死锁现象。 先看生活中的一个实例，2个人一起吃饭但是只有一双筷子，2人轮流吃（同时拥有2只筷子才能吃）。某一个时候，一个拿了左筷子，一人拿了右筷子，2个人都同时占用一个资源，等待另一个资源，这个时候甲在等待乙吃完并释放它占有的筷子，同理，乙也在等待甲吃完并释放它占有的筷子，这样就陷入了一个死循环，谁也无法继续吃饭。。。在计算机系统中也存在类似的情况。例如，某计算机系统中只有一台打印机和一台输入设备，进程P1正占用输入设备，同时又提出使用打印机的请求，但此时打印机正被进程P2 所占用，而P2在未释放打印机之前，又提出请求使用正被P1占用着的输入设备。这样两个进程相互无休止地等待下去，均无法继续执行，此时两个进程陷入死锁状态。 关于死锁的一些结论： 参与死锁的进程数至少为两个 参与死锁的所有进程均等待资源 参与死锁的进程至少有两个已经占有资源 死锁进程是系统中当前进程集合的一个子集 死锁会浪费大量系统资源，甚至导致系统崩溃。死锁与饥饿 饥饿（Starvation）指一个进程一直得不到资源。 死锁和饥饿都是由于进程竞争资源而引起的。饥饿一般不占有资源，死锁进程一定占有资源。 资源的类型可重用资源和消耗性资源可重用资源（永久性资源）可被多个进程多次使用，如所有硬件。 只能分配给一个进程使用，不允许多个进程共享。进程在对可重用资源的使用时，须按照请求资源、使用资源、释放资源这样的顺序。系统中每一类可重用资源中的单元数目是相对固定的，进程在运行期间，既不能创建，也不能删除。 消耗性资源（临时性资源）又称临时性资源，是由进程在运行期间动态的创建和消耗的。 消耗性资源在进程运行期间是可以不断变化的，有时可能为0。进程在运行过程中，可以不断地创造可消耗性资源的单元，将它们放入该资源类的缓冲区中，以增加该资源类的单元数目。进程在运行过程中，可以请求若干个可消耗性资源单元，用于进程自己消耗，不再将它们返回给该资源类中。可消耗资源通常是由生产者进程创建，由消费者进程消耗。最典型的可消耗资源是用于进程间通信的消息。 可抢占资源和不可抢占资源可抢占资源可抢占资源指某进程在获得这类资源后，该资源可以再被其他进程或系统抢占。对于这类资源是不会引起死锁的。 CPU 和主存均属于可抢占性资源。 不可抢占资源一旦系统把某资源分配给该进程后，就不能将它强行收回，只能在进程用完后自行释放。 磁带机、打印机等属于不可抢占性资源。 死锁产生的原因 竞争不可抢占资源引起死锁通常系统中拥有的不可抢占资源，其数量不足以满足多个进程运行的需要，使得进程在运行过程中，会因争夺资源而陷入僵局，如磁带机、打印机等。只有对不可抢占资源的竞争才可能产生死锁，对可抢占资源的竞争是不会引起死锁的。 竞争可消耗资源引起死锁 进程推进顺序不当引起死锁进程在运行过程中，请求和释放资源的顺序不当，也同样会导致死锁。例如，并发进程 P1、P2分别保持了资源R1、R2，而进程P1申请资源R2，进程P2申请资源R1时，两者都会因为所需资源被占用而阻塞。信号量使用不当也会造成死锁。进程间彼此相互等待对方发来的消息，结果也会使得这 些进程间无法继续向前推进。例如，进程A等待进程B发的消息，进程B又在等待进程A 发的消息，可以看出进程A和B不是因为竞争同一资源，而是在等待对方的资源导致死锁。竞争不可抢占资源引起死锁如：共享文件时引起死锁系统中拥有两个进程P1和P2，它们都准备写两个文件F1和F2。而这两者都属于可重用和不可抢占性资源。如果进程P1在打开F1的同时，P2进程打开F2文件，当P1想打开F2时由于F2已结被占用而阻塞，当P2想打开1时由于F1已结被占用而阻塞，此时就会无线等待下去，形成死锁。 竞争可消耗资源引起死锁如：进程通信时引起死锁系统中拥有三个进程P1、P2和P3，m1、m2、m3是3可消耗资源。进程P1一方面产生消息m1，将其发送给P2，另一方面要从P3接收消息m3。而进程P2一方面产生消息m2，将其发送给P3，另一方面要从P1接收消息m1。类似的，进程P3一方面产生消息m3，将其发送给P1，另一方面要从P2接收消息m2。如果三个进程都先发送自己产生的消息后接收别人发来的消息，则可以顺利的运行下去不会产生死锁，但要是三个进程都先接收别人的消息而不产生消息则会永远等待下去，产生死锁。 进程推进顺序不当引起死锁上图中，如果按曲线1的顺序推进，两个进程可顺利完成；如果按曲线2的顺序推进，两个进程可顺利完成；如果按曲线3的顺序推进，两个进程可顺利完成；如果按曲线4的顺序推进，两个进程将进入不安全区D中，此时P1保持了资源R1，P2保持了资源R2，系统处于不安全状态，如果继续向前推进，则可能产生死锁。 产生死锁的四个必要条件互斥条件进程要求对所分配的资源（如打印机）进行排他性控制，即在一段时间内某资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待。 不可剥夺条件进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能由获得该资源的进程自己来释放（只能是主动释放)。 请求与保持条件进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。 循环等待条件存在一种进程资源的循环等待链，链中每一个进程已获得的资源同时被链中下一个进程所请求。即存在一个处于等待状态的进程集合{Pl, P2, …, pn}，其中Pi等待的资源被P(i+1)占有（i=0, 1, …, n-1)，Pn等待的资源被P0占有，如图2-15所示。 直观上看，循环等待条件似乎和死锁的定义一样，其实不然。按死锁定义构成等待环所要求的条件更严，它要求Pi等待的资源必须由P(i+1)来满足，而循环等待条件则无此限制。 例如，系统中有两台输出设备，P0占有一台，PK占有另一台，且K不属于集合{0, 1, …, n}。 Pn等待一台输出设备，它可以从P0获得，也可能从PK获得。因此，虽然Pn、P0和其他 一些进程形成了循环等待圈，但PK不在圈内，若PK释放了输出设备，则可打破循环等待, 如图2-16所示。因此循环等待只是死锁的必要条件。 资源分配图含圈而系统又不一定有死锁的原因是同类资源数大于1。但若系统中每类资 源都只有一个资源，则资源分配图含圈就变成了系统出现死锁的充分必要条件。 以上这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之一不满足，就不会发生死锁。 产生死锁的一个例子: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 一个简单的死锁类 * 当DeadLock类的对象flag==1时（td1），先锁定o1,睡眠500毫秒 * 而td1在睡眠的时候另一个flag==0的对象（td2）线程启动，先锁定o2,睡眠500毫秒 * td1睡眠结束后需要锁定o2才能继续执行，而此时o2已被td2锁定； * td2睡眠结束后需要锁定o1才能继续执行，而此时o1已被td1锁定； * td1、td2相互等待，都需要得到对方锁定的资源才能继续执行，从而死锁。 */public class DeadLock implements Runnable &#123; public int flag = 1; //静态对象是类的所有对象共享的 private static Object o1 = new Object(), o2 = new Object(); @Override public void run() &#123; System.out.println("flag=" + flag); if (flag == 1) &#123; synchronized (o1) &#123; try &#123; Thread.sleep(500); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; synchronized (o2) &#123; System.out.println("1"); &#125; &#125; &#125; if (flag == 0) &#123; synchronized (o2) &#123; try &#123; Thread.sleep(500); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; synchronized (o1) &#123; System.out.println("0"); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; DeadLock td1 = new DeadLock(); DeadLock td2 = new DeadLock(); td1.flag = 1; td2.flag = 0; //td1,td2都处于可执行状态，但JVM线程调度先执行哪个线程是不确定的。 //td2的run()可能在td1的run()之前运行 new Thread(td1).start(); new Thread(td2).start(); &#125; &#125; 处理死锁的方法 预防死锁：通过设置某些限制条件，去破坏产生死锁的四个必要条件中的一个或几个条件，来防止死锁的发生。 避免死锁：在资源的动态分配过程中，用某种方法去防止系统进入不安全状态，从而避免死锁的发生。 检测死锁：允许系统在运行过程中发生死锁，但可设置检测机构及时检测死锁的发生，并采取适当措施加以清除。 解除死锁：当检测出死锁后，便采取适当措施将进程从死锁状态中解脱出来。 预防死锁破坏“互斥”条件:就是在系统里取消互斥。若资源不被一个进程独占使用，那么死锁是肯定不会发生的。但一般来说在所列的四个条件中，“互斥”条件是无法破坏的。因此，在死锁预防里主要是破坏其他几个必要条件，而不去涉及破坏“互斥”条件。 注意：互斥条件不能被破坏，否则会造成结果的不可再现性。 破坏“占有并等待”条件:破坏“占有并等待”条件，就是在系统中不允许进程在已获得某种资源的情况下，申请其他资源。即要想出一个办法，阻止进程在持有资源的同时申请其他资源。方法一：创建进程时，要求它申请所需的全部资源，系统或满足其所有要求，或什么也不给它。这是所谓的 “ 一次性分配”方案。方法二：要求每个进程提出新的资源申请前，释放它所占有的资源。这样，一个进程在需要资源S时，须先把它先前占有的资源R释放掉，然后才能提出对S的申请，即使它可能很快又要用到资源R。 破坏“不可抢占”条件：破坏“不可抢占”条件就是允许对资源实行抢夺。方法一：如果占有某些资源的一个进程进行进一步资源请求被拒绝，则该进程必须释放它最初占有的资源，如果有必要，可再次请求这些资源和另外的资源。方法二：如果一个进程请求当前被另一个进程占有的一个资源，则操作系统可以抢占另一个进程，要求它释放资源。只有在任意两个进程的优先级都不相同的条件下，方法二才能预防死锁。 破坏“循环等待”条件：破坏“循环等待”条件的一种方法，是将系统中的所有资源统一编号，进程可在任何时刻提出资源申请，但所有申请必须按照资源的编号顺序（升序）提出。这样做就能保证系统不出现死锁。 避免死锁理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和解除死锁。所以，在系统设计、进程调度等方面注意如何让这四个必要条件不成立，如何确定资源的合理分配算法，避免进程永久占据系统资源。此外，也要防止进程在处于等待状态的情况下占用资源。因此，对资源的分配要给予合理的规划。 预防死锁和避免死锁的区别：预防死锁是设法至少破坏产生死锁的四个必要条件之一,严格的防止死锁的出现,而避免死锁则不那么严格的限制产生死锁的必要条件的存在,因为即使死锁的必要条件存在,也不一定发生死锁。避免死锁是在系统运行过程中注意避免死锁的最终发生。 常用避免死锁的方法有序资源分配法这种算法资源按某种规则系统中的所有资源统一编号（例如打印机为1、磁带机为2、磁盘为3、等等），申请时必须以上升的次序。系统要求申请进程： 1、对它所必须使用的而且属于同一类的所有资源，必须一次申请完； 2、在申请不同类资源时，必须按各类设备的编号依次申请。例如：进程PA，使用资源的顺序是R1，R2； 进程PB，使用资源的顺序是R2，R1；若采用动态分配有可能形成环路条件，造成死锁。 采用有序资源分配法：R1的编号为1，R2的编号为2； PA：申请次序应是：R1，R2 PB：申请次序应是：R1，R2 这样就破坏了环路条件，避免了死锁的发生。 银行家算法详见银行家算法. 常用避免死锁的技术 加锁顺序（线程按照一定的顺序加锁） 加锁时限（线程尝试获取锁的时候加上一定的时限，超过时限则放弃对该锁的请求，并释放自己占有的锁） 死锁检测 加锁顺序当多个线程需要相同的一些锁，但是按照不同的顺序加锁，死锁就很容易发生。 如果能确保所有的线程都是按照相同的顺序获得锁，那么死锁就不会发生。看下面这个例子： 12345678910Thread 1: lock A lock B Thread 2: wait for A lock C (when A locked) Thread 3: wait for A wait for B wait for C 如果一个线程（比如线程3）需要一些锁，那么它必须按照确定的顺序获取锁。它只有获得了从顺序上排在前面的锁之后，才能获取后面的锁。 例如，线程2和线程3只有在获取了锁A之后才能尝试获取锁C(译者注：获取锁A是获取锁C的必要条件)。因为线程1已经拥有了锁A，所以线程2和3需要一直等到锁A被释放。然后在它们尝试对B或C加锁之前，必须成功地对A加了锁。 按照顺序加锁是一种有效的死锁预防机制。但是，这种方式需要你事先知道所有可能会用到的锁(译者注：并对这些锁做适当的排序)，但总有些时候是无法预知的。 加锁时限另外一个可以避免死锁的方法是在尝试获取锁的时候加一个超时时间，这也就意味着在尝试获取锁的过程中若超过了这个时限该线程则放弃对该锁请求。若一个线程没有在给定的时限内成功获得所有需要的锁，则会进行回退并释放所有已经获得的锁，然后等待一段随机的时间再重试。这段随机的等待时间让其它线程有机会尝试获取相同的这些锁，并且让该应用在没有获得锁的时候可以继续运行(译者注：加锁超时后可以先继续运行干点其它事情，再回头来重复之前加锁的逻辑)。 以下是一个例子，展示了两个线程以不同的顺序尝试获取相同的两个锁，在发生超时后回退并重试的场景： 12345678910Thread 1 locks A Thread 2 locks B Thread 1 attempts to lock B but is blocked Thread 2 attempts to lock A but is blocked Thread 1’s lock attempt on B times out Thread 1 backs up and releases A as well Thread 1 waits randomly (e.g. 257 millis) before retrying. Thread 2’s lock attempt on A times out Thread 2 backs up and releases B as well Thread 2 waits randomly (e.g. 43 millis) before retrying. 在上面的例子中，线程2比线程1早200毫秒进行重试加锁，因此它可以先成功地获取到两个锁。这时，线程1尝试获取锁A并且处于等待状态。当线程2结束时，线程1也可以顺利的获得这两个锁（除非线程2或者其它线程在线程1成功获得两个锁之前又获得其中的一些锁）。 需要注意的是，由于存在锁的超时，所以我们不能认为这种场景就一定是出现了死锁。也可能是因为获得了锁的线程（导致其它线程超时）需要很长的时间去完成它的任务。 此外，如果有非常多的线程同一时间去竞争同一批资源，就算有超时和回退机制，还是可能会导致这些线程重复地尝试但却始终得不到锁。如果只有两个线程，并且重试的超时时间设定为0到500毫秒之间，这种现象可能不会发生，但是如果是10个或20个线程情况就不同了。因为这些线程等待相等的重试时间的概率就高的多（或者非常接近以至于会出现问题）。(译者注：超时和重试机制是为了避免在同一时间出现的竞争，但是当线程很多时，其中两个或多个线程的超时时间一样或者接近的可能性就会很大，因此就算出现竞争而导致超时后，由于超时时间一样，它们又会同时开始重试，导致新一轮的竞争，带来了新的问题。) 这种机制存在一个问题，在Java中不能对synchronized同步块设置超时时间。你需要创建一个自定义锁，或使用Java5中java.util.concurrent包下的工具。写一个自定义锁类不复杂，但超出了本文的内容。后续的Java并发系列会涵盖自定义锁的内容。 检测死锁一般来说，由于操作系统有并发，共享以及随机性等特点，通过预防和避免的手段达到排除死锁的目的是很困难的。这需要较大的系统开销，而且不能充分利用资源。为此，一种简便的方法是系统为进程分配资源时，不采取任何限制性措施，但是提供了检测和解脱死锁的手段：能发现死锁并从死锁状态中恢复出来。因此，在实际的操作系统中往往采用死锁的检测与恢复方法来排除死锁。*死锁检测与恢复是指系统设有专门的机构，当死锁发生时，该机构能够检测到死锁发生的位置和原因，并能通过外力破坏死锁发生的必要条件，从而使得并发进程从死锁状态中恢复出来。 * 死锁检测是一个更好的死锁预防机制，它主要是针对那些不可能实现按序加锁并且锁超时也不可行的场景。 每当一个线程获得了锁，会在线程和锁相关的数据结构中（map、graph等等）将其记下。除此之外，每当有线程请求锁，也需要记录在这个数据结构中。 当一个线程请求锁失败时，这个线程可以遍历锁的关系图看看是否有死锁发生。例如，线程A请求锁7，但是锁7这个时候被线程B持有，这时线程A就可以检查一下线程B是否已经请求了线程A当前所持有的锁。如果线程B确实有这样的请求，那么就是发生了死锁（线程A拥有锁1，请求锁7；线程B拥有锁7，请求锁1）。 当然，死锁一般要比两个线程互相持有对方的锁这种情况要复杂的多。线程A等待线程B，线程B等待线程C，线程C等待线程D，线程D又在等待线程A。线程A为了检测死锁，它需要递进地检测所有被B请求的锁。从线程B所请求的锁开始，线程A找到了线程C，然后又找到了线程D，发现线程D请求的锁被线程A自己持有着。这是它就知道发生了死锁。 下面是一幅关于四个线程（A,B,C和D）之间锁占有和请求的关系图。像这样的数据结构就可以被用来检测死锁。 解除死锁一旦检测出死锁，就应立即釆取相应的措施，以解除死锁。死锁解除的主要方法有：1) 资源剥夺法。挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但应防止被挂起的进程长时间得不到资源，而处于资源匮乏的状态。2) 撤销进程法。强制撤销部分、甚至全部死锁进程并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。3) 进程回退法。让一（多）个进程回退到足以回避死锁的地步，进程回退时自愿释放资源而不是被剥夺。要求系统保持进程的历史信息，设置还原点。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>DeadLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[免密登陆原理]]></title>
    <url>%2F2019%2F07%2F08%2Flogin-without-password%2F</url>
    <content type="text"><![CDATA[本文转载自：[靠谱原创！] SSH免密登录设置—-原理详解+具体操作(全国人民看完都懂了！) 首先介绍一下SSH： 当我们用一台服务器登录另一台服务器可直接使用SSH协议进行登陆： 1234//具体格式：//ssh [用户名]@[IP]ssh wdy@192.168.33.12 也可以直接远程传送文件到另一台服务器，具体格式如下： 1234//具体格式：//scp [文件名] [目标服务器用户名]@[目标服务器IP] : [目标复制位置]scp test.txt root@192.168.33.12:/home 注意：以上操作方法存在弊端，每次操作都需要输入目标服务器的密码，不适合集群服务器的批量操作。所以一般我们会用SSH的第二种身份验证机制：密钥验证。验证流程如下图： 即在源服务器上先生成一份公钥和一份密钥，将公钥复制到目标服务器，利用命令将公钥添加至目标服务器的授权列表(authorized_keys)。当有服务器带着公钥申请连接服务器时，目标服务器首先在authorized_keys中查找是否存在该公钥，如果存在则开始进行验证。首先生成一个随机字符串，利用对应公钥进行加密，然后返回给申请连接的服务器，申请连接服务器利用私钥进行解密，再将字符串返回给目标服务器完成验证，进行后续操作。 首先在源服务器上生成公钥和密钥： 1234//具体格式：//其中 -t [加密方式] ssh-keygen -t rsa//默认会在 /root/.ssh/ 重生成公钥和密钥 id_rsa id_rsa.pub 如下图： 接下来我们需要将源服务器生成的公钥拷贝到目标服务器中并添加至authorized_keys列表中，这两步可以使用一个简便命令进行执行： 1234//将公钥添加至目标服务器的authorized_keys列表中//具体格式：//ssh-copy-id [目标服务器IP] ssh-copy-id 192.168.33.12 截图如下： 我们在目标服务器的 /root/.ssh 文件夹中查看授权列表authorized_keys发现原服务器的公钥已经添加进去： 至此我们已经完成了SSH免密登录的设置，可以再源服务器上进行登录验证，已经不需要再输入密码： 同样我们可以在目标服务器上进行同样的设置使两台服务器相互免密登录。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leetcode-3:Longest Substring Without Repeating Characters(最长无重复子串、最大不重复子序列)]]></title>
    <url>%2F2019%2F07%2F08%2Fleetcode3%2F</url>
    <content type="text"><![CDATA[题目描述 题目难度:Medium Given a string, find the length of the longest substring without repeating characters. Example 1: 123Input: &quot;abcabcbb&quot;Output: 3 Explanation: The answer is &quot;abc&quot;, with the length of 3. Example 2: 123Input: &quot;bbbbb&quot;Output: 1Explanation: The answer is &quot;b&quot;, with the length of 1. Example 3: 1234Input: &quot;pwwkew&quot;Output: 3Explanation: The answer is &quot;wke&quot;, with the length of 3. Note that the answer must be a substring, &quot;pwke&quot; is a subsequence and not a substring. AC代码1234567891011121314151617class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; if(s == null || s.length() == 0) return 0; int[] charHashMap = new int[256]; for(int i = 0;i &lt; 256;i++) charHashMap[i] = -1; int len = s.length(); int start = 0; int max = 0; for(int i = 0;i &lt; len;i++)&#123; int index = s.charAt(i); start = Math.max(start, charHashMap[index] + 1); charHashMap[index] = i; max = Math.max(max, i - start + 1); &#125; return max; &#125;&#125; 更简洁的解法 123456789101112class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int max = 0; int[] index = new int[128]; for (int i = 0, j = 0; j &lt; s.length(); j++) &#123; i = Math.max(index[s.charAt(j)], i); max = Math.max(max, j - i + 1); index[s.charAt(j)] = j + 1; &#125; return max; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux之inode]]></title>
    <url>%2F2019%2F07%2F08%2Flinux-inode%2F</url>
    <content type="text"><![CDATA[本文转载自:理解inode inode是什么理解inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。“块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。 文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”。 每一个文件都有对应的inode，里面包含了与该文件有关的一些信息。 inode的内容inode包含文件的元信息，具体来说有以下内容： 文件的字节数 文件拥有者的User ID 文件的Group ID 文件的读、写、执行权限 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。 链接数，即有多少文件名指向这个inode 文件数据block的位置 可以用stat命令，查看某个文件的inode信息： stat example.txt 总之，除了文件名以外的所有文件信息，都存在inode之中。至于为什么没有文件名，下文会有详细解释。 inode的大小inode也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是inode区（inode table），存放inode所包含的信息。 每个inode节点的大小，一般是128字节或256字节。inode节点的总数，在格式化时就给定，一般是每1KB或每2KB就设置一个inode。假定在一块1GB的硬盘中，每个inode节点的大小为128字节，每1KB就设置一个inode，那么inode table的大小就会达到128MB，占整块硬盘的12.8%。 查看每个硬盘分区的inode总数和已经使用的数量，可以使用df命令。 df -i 查看每个inode节点的大小，可以用如下命令： sudo dumpe2fs -h /dev/hda | grep “Inode size” 由于每个文件都必须有一个inode，因此有可能发生inode已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。 inode号码每个inode都有一个号码，操作系统用inode号码来识别不同的文件。 这里值得重复一遍，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。对于系统来说，文件名只是inode号码便于识别的别称或者绰号。 表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的inode号码；其次，通过inode号码，获取inode信息；最后，根据inode信息，找到文件数据所在的block，读出数据。 使用ls -i命令，可以看到文件名对应的inode号码： ls -i example.txt 目录文件Unix/Linux系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。 目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的inode号码。 ls命令只列出目录文件中的所有文件名： ls /etc ls -i命令列出整个目录文件，即文件名和inode号码： ls -i /etc 如果要查看文件的详细信息，就必须根据inode号码，访问inode节点，读取信息。ls -l命令列出文件的详细信息。 ls -l /etc 理解了上面这些知识，就能理解目录的权限。目录文件的读权限（r）和写权限（w），都是针对目录文件本身。由于目录文件内只有文件名和inode号码，所以如果只有读权限，只能获取文件名，无法获取其他信息，因为其他信息都储存在inode节点中，而读取inode节点内的信息需要目录文件的执行权限（x）。 硬链接一般情况下，文件名和inode号码是”一一对应”关系，每个inode号码对应一个文件名。但是，Unix/Linux系统允许，多个文件名指向同一个inode号码。 这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为”硬链接”（hard link）。 ln命令可以创建硬链接： ln 源文件 目标文件 运行上面这条命令以后，源文件与目标文件的inode号码相同，都指向同一个inode。inode信息中有一项叫做”链接数”，记录指向该inode的文件名总数，这时就会增加1。 反过来，删除一个文件名，就会使得inode节点中的”链接数”减1。当这个值减到0，表明没有文件名指向这个inode，系统就会回收这个inode号码，以及其所对应block区域。 这里顺便说一下目录文件的”链接数”。创建目录时，默认会生成两个目录项：”.”和”..”。前者的inode号码就是当前目录的inode号码，等同于当前目录的”硬链接”；后者的inode号码就是当前目录的父目录的inode号码，等同于父目录的”硬链接”。所以，任何一个目录的”硬链接”总数，总是等于2加上它的子目录总数（含隐藏目录）。 软链接除了硬链接以外，还有一种特殊情况。 文件A和文件B的inode号码虽然不一样，但是文件A的内容是文件B的路径。读取文件A时，系统会自动将访问者导向文件B。因此，无论打开哪一个文件，最终读取的都是文件B。这时，文件A就称为文件B的”软链接”（soft link）或者”符号链接（symbolic link）。 这意味着，文件A依赖于文件B而存在，如果删除了文件B，打开文件A就会报错：”No such file or directory“。这是软链接与硬链接最大的不同：文件A指向文件B的文件名，而不是文件B的inode号码，文件B的inode”链接数”不会因此发生变化。 ln -s命令可以创建软链接。 ln -s 源文文件或目录 目标文件或目录 inode的特殊作用由于inode号码与文件名分离，这种机制导致了一些Unix/Linux系统特有的现象。 有时，文件名包含特殊字符，无法正常删除。这时，直接删除inode节点，就能起到删除文件的作用。 移动文件或重命名文件，只是改变文件名，不影响inode号码。 打开一个文件以后，系统就以inode号码来识别这个文件，不再考虑文件名。因此，通常来说，系统无法从inode号码得知文件名。 第3点使得软件更新变得简单，可以在不关闭软件的情况下进行更新，不需要重启。因为系统通过inode号码，识别运行中的文件，不通过文件名。更新的时候，新版文件以同样的文件名，生成一个新的inode，不会影响到运行中的文件。等到下一次运行这个软件的时候，文件名就自动指向新版文件，旧版文件的inode则被回收。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>inode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp和udp的区别与应用场景]]></title>
    <url>%2F2019%2F07%2F08%2Ftcp-udp%2F</url>
    <content type="text"><![CDATA[基本概念 面向报文 面向报文的传输方式是应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。因此，应用程序必须选择合适大小的报文。若报文太长，则IP层需要分片，降低效率。若太短，会使IP太小。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。这也就是说，应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。UDP面向无连接。 面向字节流 面向字节流的话，虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序的传输数据看成是一连串的无结构的字节流。TCP有一个缓冲，当应用程序传送的数据块太长，TCP就可以把它划分短一些再传送。如果应用程序一次只发送一个字节，TCP也可以等待积累有足够多的字节后再构成报文段发送出去。TCP面向连接。 TCP和UDP介绍TCP（可靠 有序 无丢失 不重复） TCP是面向连接的； 每条TCP连接只能有两个端点，一对一通信； TCP提供可靠的交付服务，传输数据无差错，不丢失，不重复，且按时序到达； TCP提供全双工通信； 面向字节流，TCP根据对方给出的窗口和当前的网络拥塞程度决定一个报文应该包含多少个字节。 UDP 面向无连接； UDP使用尽最大努力交付，不保证可靠性。UDP是面向报文的，UDP对应用层交付下来的报文，既不合并，也不拆分，而是保留报文的边界； UDP面向报文，应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文； UDP没有拥塞控制； UDP支持一对一，一对多，多对一和多对多的交互通信。 UDP的首部开销小，只有8字节。 TCP 和UDP区别 TCP是面向连接(Connection oriented)的协议，UDP是无连接(Connection less)协议；TCP用三次握手建立连接：1) Client向server发送SYN；2) Server接收到SYN，回复Client一个SYN-ACK；3) Client接收到SYN_ACK，回复Server一个ACK。到此，连接建成。UDP发送数据前不需要建立连接。 TCP可靠，UDP不可靠；TCP丢包会自动重传，UDP不会。 TCP有序，UDP无序；消息在传输过程中可能会乱序，后发送的消息可能会先到达，TCP会对其进行重排序，UDP不会。 TCP无界，UDP有界；TCP通过字节流传输，UDP中每一个包都是单独的。 TCP有流量控制（拥塞控制），UDP没有；主要靠三次握手实现。 TCP传输慢，UDP传输快；因为TCP需要建立连接、保证可靠性和有序性，所以比较耗时。这就是为什么视频流、广播电视、在线多媒体游戏等选择使用UDP。 TCP是重量级的，UDP是轻量级的；TCP要建立连接、保证可靠性和有序性，就会传输更多的信息，如TCP的包头比较大。 TCP的头部比UDP大；TCP头部需要20字节，UDP头部只要8个字节 TCP和UDP协议的一些应用 TCP和UDP协议的比较 TCP/UDP编程模型 从上图也能清晰的看出，TCP通信需要服务器端侦听listen、接收客户端连接请求accept，等待客户端connect建立连接后才能进行数据包的收发（recv/send）工作。而UDP则服务器和客户端的概念不明显，服务器端即接收端需要绑定端口，等待客户端的数据的到来。后续便可以进行数据的收发（recvfrom/sendto）工作。 在前面讲解UDP时，提到了UDP保留了报文的边界，下面我们来谈谈TCP和UDP中报文的边界问题。在默认的阻塞模式下，TCP无边界，UDP有边界。 对于TCP协议，客户端连续发送数据，只要服务端的这个函数的缓冲区足够大，会一次性接收过来，即客户端是分好几次发过来，是有边界的，而服务端却一次性接收过来，所以证明是无边界的； 而对于UDP协议，客户端连续发送数据，即使服务端的这个函数的缓冲区足够大，也只会一次一次的接收，发送多少次接收多少次，即客户端分几次发送过来，服务端就必须按几次接收，从而证明，这种UDP的通讯模式是有边界的。 TCP/UDP的优缺点TCP的优点可靠，稳定，有序，无丢失，不重复， TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认机制（可靠）、滑动窗口（流量控制）、重传（可靠）、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源。 TCP的缺点慢，效率低，占用系统资源高，易被攻击 TCP在传递数据之前，要先建连接，这会消耗时间，而且在数据传递时，确认机制、重传机制、拥塞控制机制等都会消耗大量的时间，而且要在每台设备上维护所有的传输连接，事实上，每个连接都会占用系统的CPU、内存等硬件资源。 而且，因为TCP有确认机制、三次握手机制，这些也导致TCP容易被人利用，实现DOS、DDOS、CC等攻击。 UDP的优点快，比TCP稍安全 UDP没有TCP的握手、确认、窗口、重传、拥塞控制等机制，UDP是一个无状态的传输协议，所以它在传递数据时非常快。没有TCP的这些机制，UDP较TCP被攻击者利用的漏洞就要少一些。但UDP也是无法避免攻击的，比如：UDP Flood攻击…… UDP的缺点不可靠，不稳定 因为UDP没有TCP那些可靠的机制，在数据传递时，如果网络质量不好，就会很容易丢包。 TCP/UDP应用场景基于上面的优缺点，那么： 什么时候应该使用TCP： 当对网络通讯质量有要求的时候，比如：整个数据要准确无误的传递给对方，这往往用于一些要求可靠的应用，比如HTTP、HTTPS、FTP等传输文件的协议，POP、SMTP等邮件传输的协议。 在日常生活中，常见使用TCP协议的应用如下： 浏览器，用的HTTP FlashFXP，用的FTP Outlook，用的POP、SMTP Putty，用的Telnet、SSH QQ文件传输 那么什么时候应该使用UDP： 当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP。 比如，日常生活中，常见使用UDP协议的应用如下： QQ语音 QQ视频 TFTP 直播 DNS DHCP RIP 参考:传输层TCP和UDP的区别分析与应用场景【转载】TCP,UDP通信使用场景及区别比较TCP 和 UDP]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-数据流中的中位数]]></title>
    <url>%2F2019%2F07%2F04%2Fjianzhioffer-median%2F</url>
    <content type="text"><![CDATA[题目描述如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 解题思路 链接：https://www.nowcoder.com/questionTerminal/9be0172896bd43948f8a32fb954e1be1来源：牛客网 1、两个堆中的数据数目差不能超过1，这样可以使中位数只会出现在两个堆的交接处； 2、大顶堆的所有数据都小于小顶堆，这样就满足了排序要求。 解析： 用最小堆minHeap保存中位数的后半部分 用最大堆maxHeap保存中位数的前半部分 后半部分的值都大于前半部分 当前个数为偶数，两堆中数量相同当前个数为奇数，最小堆也就是中位数的后半部分的数量比最大堆个数多1 插入num： 当前个数为偶数（应该往中位数后半部分minHeap添加数）： 如果num小于前半部分的最大值，应该把num offer 到 maxHeap，然后把maxHeap中的最大值取出来，加入到minHeap 否则，把num offer 到minHeap 当前个数为奇数（应该往中位数前半部分maxHeap添加数）： 如果num大于minHeap中的最小值，先把num offer 到 minHeap，取出minHeap中的最小值加入到maxHeap 否则，把num offer 到 maxHeap 获取中位数： 如果是奇数，中位数为 minHeap.peek() * 1.0; 如果是偶数，中位数为 (maxHeap.peek() + minHeap.peek()) / 2.0; AC代码12345678910111213141516171819202122232425262728293031323334353637383940import java.util.PriorityQueue;import java.util.Comparator;public class Solution &#123; //优先级队列默认是最小堆,中位数的后半部分 private PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;&gt;(); //中位数的前半部分 private PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;&gt;(6, new Comparator&lt;Integer&gt;()&#123; @Override public int compare(Integer o1, Integer o2) &#123; return o2 - o1; &#125; &#125;); private int count = 0; public void Insert(Integer num) &#123; if((count &amp; 1) == 0)&#123; if(!maxHeap.isEmpty() &amp;&amp; num &lt; maxHeap.peek())&#123; maxHeap.offer(num); num = maxHeap.poll(); &#125; minHeap.offer(num); &#125; else&#123; if(num &gt; minHeap.peek())&#123; minHeap.offer(num); num = minHeap.poll(); &#125; maxHeap.offer(num); &#125; count++; &#125; public Double GetMedian() &#123; if((count &amp; 1) == 1) return minHeap.peek() * 1.0; return (maxHeap.peek() + minHeap.peek()) / 2.0; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-二叉搜索树的第K个节点]]></title>
    <url>%2F2019%2F07%2F04%2Fjianzhioffer-kth-node%2F</url>
    <content type="text"><![CDATA[题目描述给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 AC代码123456789101112131415161718public class Solution &#123; int order = 0; TreeNode kth = null; TreeNode KthNode(TreeNode pRoot, int k) &#123; if(pRoot == null) return kth; kthNodeCore(pRoot, k); return kth; &#125; private void kthNodeCore(TreeNode pRoot, int k)&#123; if(pRoot == null) return; kthNodeCore(pRoot.left, k); order++; if(order == k) kth = pRoot; kthNodeCore(pRoot.right, k); &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-把二叉树打印成多行]]></title>
    <url>%2F2019%2F07%2F04%2Fjianzhioffer-print-tree%2F</url>
    <content type="text"><![CDATA[题目描述从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 AC代码层次遍历二叉树用到的数据结构是队列。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.ArrayList;import java.util.Queue;import java.util.LinkedList;/*public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); ArrayList&lt;ArrayList&lt;Integer&gt;&gt; resList = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); int curNode = 1;//当前层具有的结点数目 int nextNode = 0;//下一层具有的结点数目 ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; if(pRoot == null) return resList; queue.offer(pRoot); TreeNode tmp = null; ArrayList&lt;Integer&gt; curList = new ArrayList&lt;Integer&gt;(); while(!queue.isEmpty())&#123; tmp = queue.poll(); curList.add(tmp.val); curNode--; if(tmp.left != null) &#123; queue.offer(tmp.left); nextNode++; &#125; if(tmp.right != null) &#123; queue.offer(tmp.right); nextNode++; &#125; if(curNode == 0)&#123; resList.add(curList); curList = new ArrayList(); curNode = nextNode; nextNode = 0; &#125; &#125; return resList; &#125; &#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式详解]]></title>
    <url>%2F2019%2F07%2F04%2Fjava-design-pattern2%2F</url>
    <content type="text"><![CDATA[参考：https://github.com/CyC2018/CS-Notes 概述设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。 拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。 源码以及 UML 图 创建型单例（Singleton）Intent确保一个类只有一个实例，并提供该实例的全局访问点。 Class Diagram使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。 私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。 ImplementationⅠ 懒汉式-线程不安全以下实现中，私有静态变量 uniqueInstance 被延迟实例化，这样做的好处是，如果没有用到该类，那么就不会实例化 uniqueInstance，从而节约资源。 这个实现在多线程环境下是不安全的，如果多个线程能够同时进入 if (uniqueInstance == null) ，并且此时 uniqueInstance 为 null，那么会有多个线程执行 uniqueInstance = new Singleton(); 语句，这将导致实例化多次 uniqueInstance。 1234567891011121314public class Singleton &#123; private static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125;&#125; Ⅱ 饿汉式-线程安全线程不安全问题主要是由于 uniqueInstance 被实例化多次，采取直接实例化 uniqueInstance 的方式就不会产生线程不安全问题。 但是直接实例化的方式也丢失了延迟实例化带来的节约资源的好处。 1private static Singleton uniqueInstance = new Singleton(); Ⅲ 懒汉式-线程安全只需要对 getUniqueInstance() 方法加锁，那么在一个时间点只能有一个线程能够进入该方法，从而避免了实例化多次 uniqueInstance。 但是当一个线程进入该方法之后，其它试图进入该方法的线程都必须等待，即使 uniqueInstance 已经被实例化了。这会让线程阻塞时间过长，因此该方法有性能问题，不推荐使用。 123456public static synchronized Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance;&#125; Ⅳ 双重校验锁-线程安全uniqueInstance 只需要被实例化一次，之后就可以直接使用了。加锁操作只需要对实例化那部分的代码进行，只有当 uniqueInstance 没有被实例化时，才需要进行加锁。 双重校验锁先判断 uniqueInstance 是否已经被实例化，如果没有被实例化，那么才对实例化语句进行加锁。 123456789101112131415161718public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 考虑下面的实现，也就是只使用了一个 if 语句。在 uniqueInstance == null 的情况下，如果两个线程都执行了 if 语句，那么两个线程都会进入 if 语句块内。虽然在 if 语句块内有加锁操作，但是两个线程都会执行 uniqueInstance = new Singleton(); 这条语句，只是先后的问题，那么就会进行两次实例化。因此必须使用双重校验锁，也就是需要使用两个 if 语句。 12345if (uniqueInstance == null) &#123; synchronized (Singleton.class) &#123; uniqueInstance = new Singleton(); &#125;&#125; uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1&gt;3&gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 Ⅴ 静态内部类实现当 Singleton 类加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance() 方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。 这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。 12345678910111213public class Singleton &#123; private Singleton() &#123; &#125; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; public static Singleton getUniqueInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; Ⅵ 枚举实现123456789101112131415161718192021222324252627282930313233343536373839public enum Singleton &#123; INSTANCE; private String objName; public String getObjName() &#123; return objName; &#125; public void setObjName(String objName) &#123; this.objName = objName; &#125; public static void main(String[] args) &#123; // 单例测试 Singleton firstSingleton = Singleton.INSTANCE; firstSingleton.setObjName("firstName"); System.out.println(firstSingleton.getObjName()); Singleton secondSingleton = Singleton.INSTANCE; secondSingleton.setObjName("secondName"); System.out.println(firstSingleton.getObjName()); System.out.println(secondSingleton.getObjName()); // 反射获取实例测试 try &#123; Singleton[] enumConstants = Singleton.class.getEnumConstants(); for (Singleton enumConstant : enumConstants) &#123; System.out.println(enumConstant.getObjName()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 1234firstNamesecondNamesecondNamesecondName 该实现在多次序列化再进行反序列化之后，不会得到多个实例。而其它实现需要使用 transient 修饰所有字段，并且实现序列化和反序列化的方法。 该实现可以防止反射攻击。在其它实现中，通过 setAccessible() 方法可以将私有构造函数的访问级别设置为 public，然后调用构造函数从而实例化对象，如果要防止这种攻击，需要在构造函数中添加防止多次实例化的代码。该实现是由 JVM 保证只会实例化一次，因此不会出现上述的反射攻击。 Examples Logger Classes Configuration Classes Accesing resources in shared mode Factories implemented as Singletons JDK java.lang.Runtime#getRuntime() java.awt.Desktop#getDesktop() java.lang.System#getSecurityManager() 2. 简单工厂（Simple Factory）Intent在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。 Class Diagram简单工厂把实例化的操作单独放到一个类中，这个类就成为简单工厂类，让简单工厂类来决定应该用哪个具体子类来实例化。 这样做能把客户类和具体子类的实现解耦，客户类不再需要知道有哪些子类以及应当实例化哪个子类。客户类往往有多个，如果不使用简单工厂，那么所有的客户类都要知道所有子类的细节。而且一旦子类发生改变，例如增加子类，那么所有的客户类都要进行修改。 Implementation12public interface Product &#123;&#125; 12public class ConcreteProduct implements Product &#123;&#125; 12public class ConcreteProduct1 implements Product &#123;&#125; 12public class ConcreteProduct2 implements Product &#123;&#125; 以下的 Client 类包含了实例化的代码，这是一种错误的实现。如果在客户类中存在这种实例化代码，就需要考虑将代码放到简单工厂中。 123456789101112131415public class Client &#123; public static void main(String[] args) &#123; int type = 1; Product product; if (type == 1) &#123; product = new ConcreteProduct1(); &#125; else if (type == 2) &#123; product = new ConcreteProduct2(); &#125; else &#123; product = new ConcreteProduct(); &#125; // do something with the product &#125;&#125; 以下的 SimpleFactory 是简单工厂实现，它被所有需要进行实例化的客户类调用。 1234567891011public class SimpleFactory &#123; public Product createProduct(int type) &#123; if (type == 1) &#123; return new ConcreteProduct1(); &#125; else if (type == 2) &#123; return new ConcreteProduct2(); &#125; return new ConcreteProduct(); &#125;&#125; 12345678public class Client &#123; public static void main(String[] args) &#123; SimpleFactory simpleFactory = new SimpleFactory(); Product product = simpleFactory.createProduct(1); // do something with the product &#125;&#125; 3. 工厂方法（Factory Method）Intent定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化操作推迟到子类。 Class Diagram在简单工厂中，创建对象的是另一个类，而在工厂方法中，是由子类来创建对象。 下图中，Factory 有一个 doSomething() 方法，这个方法需要用到一个产品对象，这个产品对象由 factoryMethod() 方法创建。该方法是抽象的，需要由子类去实现。 Implementation1234567public abstract class Factory &#123; abstract public Product factoryMethod(); public void doSomething() &#123; Product product = factoryMethod(); // do something with the product &#125;&#125; 12345public class ConcreteFactory extends Factory &#123; public Product factoryMethod() &#123; return new ConcreteProduct(); &#125;&#125; 12345public class ConcreteFactory1 extends Factory &#123; public Product factoryMethod() &#123; return new ConcreteProduct1(); &#125;&#125; 12345public class ConcreteFactory2 extends Factory &#123; public Product factoryMethod() &#123; return new ConcreteProduct2(); &#125;&#125; JDK java.util.Calendar java.util.ResourceBundle java.text.NumberFormat java.nio.charset.Charset java.net.URLStreamHandlerFactory java.util.EnumSet javax.xml.bind.JAXBContext 4. 抽象工厂（Abstract Factory）Intent提供一个接口，用于创建 相关的对象家族 。 Class Diagram抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同。 抽象工厂模式用到了工厂方法模式来创建单一对象，AbstractFactory 中的 createProductA() 和 createProductB() 方法都是让子类来实现，这两个方法单独来看就是在创建一个对象，这符合工厂方法模式的定义。 至于创建对象的家族这一概念是在 Client 体现，Client 要通过 AbstractFactory 同时调用两个方法来创建出两个对象，在这里这两个对象就有很大的相关性，Client 需要同时创建出这两个对象。 从高层次来看，抽象工厂使用了组合，即 Cilent 组合了 AbstractFactory，而工厂方法模式使用了继承。 Implementation12public class AbstractProductA &#123;&#125; 12public class AbstractProductB &#123;&#125; 12public class ProductA1 extends AbstractProductA &#123;&#125; 12public class ProductA2 extends AbstractProductA &#123;&#125; 12public class ProductB1 extends AbstractProductB &#123;&#125; 12public class ProductB2 extends AbstractProductB &#123;&#125; 1234public abstract class AbstractFactory &#123; abstract AbstractProductA createProductA(); abstract AbstractProductB createProductB();&#125; 123456789public class ConcreteFactory1 extends AbstractFactory &#123; AbstractProductA createProductA() &#123; return new ProductA1(); &#125; AbstractProductB createProductB() &#123; return new ProductB1(); &#125;&#125; 123456789public class ConcreteFactory2 extends AbstractFactory &#123; AbstractProductA createProductA() &#123; return new ProductA2(); &#125; AbstractProductB createProductB() &#123; return new ProductB2(); &#125;&#125; 12345678public class Client &#123; public static void main(String[] args) &#123; AbstractFactory abstractFactory = new ConcreteFactory1(); AbstractProductA productA = abstractFactory.createProductA(); AbstractProductB productB = abstractFactory.createProductB(); // do something with productA and productB &#125;&#125; JDK javax.xml.parsers.DocumentBuilderFactory javax.xml.transform.TransformerFactory javax.xml.xpath.XPathFactory 5. 生成器（Builder）Intent封装一个对象的构造过程，并允许按步骤构造。 Class Diagram Implementation以下是一个简易的 StringBuilder 实现，参考了 JDK 1.8 源码。 12345678910111213141516171819202122232425262728293031323334public class AbstractStringBuilder &#123; protected char[] value; protected int count; public AbstractStringBuilder(int capacity) &#123; count = 0; value = new char[capacity]; &#125; public AbstractStringBuilder append(char c) &#123; ensureCapacityInternal(count + 1); value[count++] = c; return this; &#125; private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) expandCapacity(minimumCapacity); &#125; void expandCapacity(int minimumCapacity) &#123; int newCapacity = value.length * 2 + 2; if (newCapacity - minimumCapacity &lt; 0) newCapacity = minimumCapacity; if (newCapacity &lt; 0) &#123; if (minimumCapacity &lt; 0) // overflow throw new OutOfMemoryError(); newCapacity = Integer.MAX_VALUE; &#125; value = Arrays.copyOf(value, newCapacity); &#125;&#125; 1234567891011public class StringBuilder extends AbstractStringBuilder &#123; public StringBuilder() &#123; super(16); &#125; @Override public String toString() &#123; // Create a copy, don't share the array return new String(value, 0, count); &#125;&#125; 12345678910public class Client &#123; public static void main(String[] args) &#123; StringBuilder sb = new StringBuilder(); final int count = 26; for (int i = 0; i &lt; count; i++) &#123; sb.append((char) ('a' + i)); &#125; System.out.println(sb.toString()); &#125;&#125; 1abcdefghijklmnopqrstuvwxyz JDK java.lang.StringBuilder java.nio.ByteBuffer java.lang.StringBuffer java.lang.Appendable Apache Camel builders 6. 原型模式（Prototype）Intent使用原型实例指定要创建对象的类型，通过复制这个原型来创建新对象。 Class Diagram Implementation123public abstract class Prototype &#123; abstract Prototype myClone();&#125; 123456789101112131415161718public class ConcretePrototype extends Prototype &#123; private String filed; public ConcretePrototype(String filed) &#123; this.filed = filed; &#125; @Override Prototype myClone() &#123; return new ConcretePrototype(filed); &#125; @Override public String toString() &#123; return filed; &#125;&#125; 1234567public class Client &#123; public static void main(String[] args) &#123; Prototype prototype = new ConcretePrototype("abc"); Prototype clone = prototype.myClone(); System.out.println(clone.toString()); &#125;&#125; 1abc JDK java.lang.Object#clone() 三、行为型1. 责任链（Chain Of Responsibility）Intent使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链发送该请求，直到有一个对象处理它为止。 Class Diagram Handler：定义处理请求的接口，并且实现后继链（successor） ### Implementation 123456789101112public abstract class Handler &#123; protected Handler successor; public Handler(Handler successor) &#123; this.successor = successor; &#125; protected abstract void handleRequest(Request request);&#125; 123456789101112131415161718public class ConcreteHandler1 extends Handler &#123; public ConcreteHandler1(Handler successor) &#123; super(successor); &#125; @Override protected void handleRequest(Request request) &#123; if (request.getType() == RequestType.TYPE1) &#123; System.out.println(request.getName() + " is handle by ConcreteHandler1"); return; &#125; if (successor != null) &#123; successor.handleRequest(request); &#125; &#125;&#125; 123456789101112131415161718public class ConcreteHandler2 extends Handler &#123; public ConcreteHandler2(Handler successor) &#123; super(successor); &#125; @Override protected void handleRequest(Request request) &#123; if (request.getType() == RequestType.TYPE2) &#123; System.out.println(request.getName() + " is handle by ConcreteHandler2"); return; &#125; if (successor != null) &#123; successor.handleRequest(request); &#125; &#125;&#125; 123456789101112131415161718192021public class Request &#123; private RequestType type; private String name; public Request(RequestType type, String name) &#123; this.type = type; this.name = name; &#125; public RequestType getType() &#123; return type; &#125; public String getName() &#123; return name; &#125;&#125; 123public enum RequestType &#123; TYPE1, TYPE2&#125; 1234567891011121314public class Client &#123; public static void main(String[] args) &#123; Handler handler1 = new ConcreteHandler1(null); Handler handler2 = new ConcreteHandler2(handler1); Request request1 = new Request(RequestType.TYPE1, "request1"); handler2.handleRequest(request1); Request request2 = new Request(RequestType.TYPE2, "request2"); handler2.handleRequest(request2); &#125;&#125; 12request1 is handle by ConcreteHandler1request2 is handle by ConcreteHandler2 JDK java.util.logging.Logger#log() Apache Commons Chain javax.servlet.Filter#doFilter() 2. 命令（Command）Intent将命令封装成对象中，具有以下作用： 使用命令来参数化其它对象 将命令放入队列中进行排队 将命令的操作记录到日志中 支持可撤销的操作 Class Diagram Command：命令 Receiver：命令接收者，也就是命令真正的执行者 Invoker：通过它来调用命令 Client：可以设置命令与命令的接收者 ### Implementation 设计一个遥控器，可以控制电灯开关。 123public interface Command &#123; void execute();&#125; 123456789101112public class LightOnCommand implements Command &#123; Light light; public LightOnCommand(Light light) &#123; this.light = light; &#125; @Override public void execute() &#123; light.on(); &#125;&#125; 123456789101112public class LightOffCommand implements Command &#123; Light light; public LightOffCommand(Light light) &#123; this.light = light; &#125; @Override public void execute() &#123; light.off(); &#125;&#125; 12345678910public class Light &#123; public void on() &#123; System.out.println("Light is on!"); &#125; public void off() &#123; System.out.println("Light is off!"); &#125;&#125; 1234567891011121314151617181920212223242526272829/** * 遥控器 */public class Invoker &#123; private Command[] onCommands; private Command[] offCommands; private final int slotNum = 7; public Invoker() &#123; this.onCommands = new Command[slotNum]; this.offCommands = new Command[slotNum]; &#125; public void setOnCommand(Command command, int slot) &#123; onCommands[slot] = command; &#125; public void setOffCommand(Command command, int slot) &#123; offCommands[slot] = command; &#125; public void onButtonWasPushed(int slot) &#123; onCommands[slot].execute(); &#125; public void offButtonWasPushed(int slot) &#123; offCommands[slot].execute(); &#125;&#125; 123456789101112public class Client &#123; public static void main(String[] args) &#123; Invoker invoker = new Invoker(); Light light = new Light(); Command lightOnCommand = new LightOnCommand(light); Command lightOffCommand = new LightOffCommand(light); invoker.setOnCommand(lightOnCommand, 0); invoker.setOffCommand(lightOffCommand, 0); invoker.onButtonWasPushed(0); invoker.offButtonWasPushed(0); &#125;&#125; JDK java.lang.Runnable Netflix Hystrix javax.swing.Action 3. 解释器（Interpreter）Intent为语言创建解释器，通常由语言的语法和语法分析来定义。 Class Diagram TerminalExpression：终结符表达式，每个终结符都需要一个 TerminalExpression。 Context：上下文，包含解释器之外的一些全局信息。 ### Implementation 以下是一个规则检验器实现，具有 and 和 or 规则，通过规则可以构建一颗解析树，用来检验一个文本是否满足解析树定义的规则。 例如一颗解析树为 D And (A Or (B C))，文本 “D A” 满足该解析树定义的规则。 这里的 Context 指的是 String。 123public abstract class Expression &#123; public abstract boolean interpret(String str);&#125; 12345678910111213141516171819public class TerminalExpression extends Expression &#123; private String literal = null; public TerminalExpression(String str) &#123; literal = str; &#125; public boolean interpret(String str) &#123; StringTokenizer st = new StringTokenizer(str); while (st.hasMoreTokens()) &#123; String test = st.nextToken(); if (test.equals(literal)) &#123; return true; &#125; &#125; return false; &#125;&#125; 1234567891011121314public class AndExpression extends Expression &#123; private Expression expression1 = null; private Expression expression2 = null; public AndExpression(Expression expression1, Expression expression2) &#123; this.expression1 = expression1; this.expression2 = expression2; &#125; public boolean interpret(String str) &#123; return expression1.interpret(str) &amp;&amp; expression2.interpret(str); &#125;&#125; 12345678910111213public class OrExpression extends Expression &#123; private Expression expression1 = null; private Expression expression2 = null; public OrExpression(Expression expression1, Expression expression2) &#123; this.expression1 = expression1; this.expression2 = expression2; &#125; public boolean interpret(String str) &#123; return expression1.interpret(str) || expression2.interpret(str); &#125;&#125; 123456789101112131415161718192021222324252627public class Client &#123; /** * 构建解析树 */ public static Expression buildInterpreterTree() &#123; // Literal Expression terminal1 = new TerminalExpression("A"); Expression terminal2 = new TerminalExpression("B"); Expression terminal3 = new TerminalExpression("C"); Expression terminal4 = new TerminalExpression("D"); // B C Expression alternation1 = new OrExpression(terminal2, terminal3); // A Or (B C) Expression alternation2 = new OrExpression(terminal1, alternation1); // D And (A Or (B C)) return new AndExpression(terminal4, alternation2); &#125; public static void main(String[] args) &#123; Expression define = buildInterpreterTree(); String context1 = "D A"; String context2 = "A B"; System.out.println(define.interpret(context1)); System.out.println(define.interpret(context2)); &#125;&#125; 12truefalse JDK java.util.Pattern java.text.Normalizer All subclasses of java.text.Format javax.el.ELResolver 4. 迭代器（Iterator）Intent提供一种顺序访问聚合对象元素的方法，并且不暴露聚合对象的内部表示。 Class Diagram Aggregate 是聚合类，其中 createIterator() 方法可以产生一个 Iterator； Iterator 主要定义了 hasNext() 和 next() 方法。 Client 组合了 Aggregate，为了迭代遍历 Aggregate，也需要组合 Iterator。 ### Implementation 123public interface Aggregate &#123; Iterator createIterator();&#125; 12345678910111213141516public class ConcreteAggregate implements Aggregate &#123; private Integer[] items; public ConcreteAggregate() &#123; items = new Integer[10]; for (int i = 0; i &lt; items.length; i++) &#123; items[i] = i; &#125; &#125; @Override public Iterator createIterator() &#123; return new ConcreteIterator&lt;Integer&gt;(items); &#125;&#125; 123456public interface Iterator&lt;Item&gt; &#123; Item next(); boolean hasNext();&#125; 12345678910111213141516171819public class ConcreteIterator&lt;Item&gt; implements Iterator &#123; private Item[] items; private int position = 0; public ConcreteIterator(Item[] items) &#123; this.items = items; &#125; @Override public Object next() &#123; return items[position++]; &#125; @Override public boolean hasNext() &#123; return position &lt; items.length; &#125;&#125; 12345678910public class Client &#123; public static void main(String[] args) &#123; Aggregate aggregate = new ConcreteAggregate(); Iterator&lt;Integer&gt; iterator = aggregate.createIterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125; JDK java.util.Iterator java.util.Enumeration 5. 中介者（Mediator）Intent集中相关对象之间复杂的沟通和控制方式。 Class Diagram Mediator：中介者，定义一个接口用于与各同事（Colleague）对象通信。 Colleague：同事，相关对象 ### Implementation Alarm（闹钟）、CoffeePot（咖啡壶）、Calendar（日历）、Sprinkler（喷头）是一组相关的对象，在某个对象的事件产生时需要去操作其它对象，形成了下面这种依赖结构： 使用中介者模式可以将复杂的依赖结构变成星形结构： 123public abstract class Colleague &#123; public abstract void onEvent(Mediator mediator);&#125; 1234567891011public class Alarm extends Colleague &#123; @Override public void onEvent(Mediator mediator) &#123; mediator.doEvent("alarm"); &#125; public void doAlarm() &#123; System.out.println("doAlarm()"); &#125;&#125; 12345678910public class CoffeePot extends Colleague &#123; @Override public void onEvent(Mediator mediator) &#123; mediator.doEvent("coffeePot"); &#125; public void doCoffeePot() &#123; System.out.println("doCoffeePot()"); &#125;&#125; 12345678910public class Calender extends Colleague &#123; @Override public void onEvent(Mediator mediator) &#123; mediator.doEvent("calender"); &#125; public void doCalender() &#123; System.out.println("doCalender()"); &#125;&#125; 12345678910public class Sprinkler extends Colleague &#123; @Override public void onEvent(Mediator mediator) &#123; mediator.doEvent("sprinkler"); &#125; public void doSprinkler() &#123; System.out.println("doSprinkler()"); &#125;&#125; 123public abstract class Mediator &#123; public abstract void doEvent(String eventType);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ConcreteMediator extends Mediator &#123; private Alarm alarm; private CoffeePot coffeePot; private Calender calender; private Sprinkler sprinkler; public ConcreteMediator(Alarm alarm, CoffeePot coffeePot, Calender calender, Sprinkler sprinkler) &#123; this.alarm = alarm; this.coffeePot = coffeePot; this.calender = calender; this.sprinkler = sprinkler; &#125; @Override public void doEvent(String eventType) &#123; switch (eventType) &#123; case "alarm": doAlarmEvent(); break; case "coffeePot": doCoffeePotEvent(); break; case "calender": doCalenderEvent(); break; default: doSprinklerEvent(); &#125; &#125; public void doAlarmEvent() &#123; alarm.doAlarm(); coffeePot.doCoffeePot(); calender.doCalender(); sprinkler.doSprinkler(); &#125; public void doCoffeePotEvent() &#123; // ... &#125; public void doCalenderEvent() &#123; // ... &#125; public void doSprinklerEvent() &#123; // ... &#125;&#125; 1234567891011public class Client &#123; public static void main(String[] args) &#123; Alarm alarm = new Alarm(); CoffeePot coffeePot = new CoffeePot(); Calender calender = new Calender(); Sprinkler sprinkler = new Sprinkler(); Mediator mediator = new ConcreteMediator(alarm, coffeePot, calender, sprinkler); // 闹钟事件到达，调用中介者就可以操作相关对象 alarm.onEvent(mediator); &#125;&#125; 1234doAlarm()doCoffeePot()doCalender()doSprinkler() JDK All scheduleXXX() methods of java.util.Timer java.util.concurrent.Executor#execute() submit() and invokeXXX() methods of java.util.concurrent.ExecutorService scheduleXXX() methods of java.util.concurrent.ScheduledExecutorService java.lang.reflect.Method#invoke() 6. 备忘录（Memento）Intent在不违反封装的情况下获得对象的内部状态，从而在需要时可以将对象恢复到最初状态。 Class Diagram Originator：原始对象 Caretaker：负责保存好备忘录 Menento：备忘录，存储原始对象的的状态。备忘录实际上有两个接口，一个是提供给 Caretaker 的窄接口：它只能将备忘录传递给其它对象；一个是提供给 Originator 的宽接口，允许它访问到先前状态所需的所有数据。理想情况是只允许 Originator 访问本备忘录的内部状态。 ### Implementation 以下实现了一个简单计算器程序，可以输入两个值，然后计算这两个值的和。备忘录模式允许将这两个值存储起来，然后在某个时刻用存储的状态进行恢复。 实现参考：Memento Pattern - Calculator Example - Java Sourcecode 1234567891011121314151617/** * Originator Interface */public interface Calculator &#123; // Create Memento PreviousCalculationToCareTaker backupLastCalculation(); // setMemento void restorePreviousCalculation(PreviousCalculationToCareTaker memento); int getCalculationResult(); void setFirstNumber(int firstNumber); void setSecondNumber(int secondNumber);&#125; 123456789101112131415161718192021222324252627282930313233343536/** * Originator Implementation */public class CalculatorImp implements Calculator &#123; private int firstNumber; private int secondNumber; @Override public PreviousCalculationToCareTaker backupLastCalculation() &#123; // create a memento object used for restoring two numbers return new PreviousCalculationImp(firstNumber, secondNumber); &#125; @Override public void restorePreviousCalculation(PreviousCalculationToCareTaker memento) &#123; this.firstNumber = ((PreviousCalculationToOriginator) memento).getFirstNumber(); this.secondNumber = ((PreviousCalculationToOriginator) memento).getSecondNumber(); &#125; @Override public int getCalculationResult() &#123; // result is adding two numbers return firstNumber + secondNumber; &#125; @Override public void setFirstNumber(int firstNumber) &#123; this.firstNumber = firstNumber; &#125; @Override public void setSecondNumber(int secondNumber) &#123; this.secondNumber = secondNumber; &#125;&#125; 123456789/** * Memento Interface to Originator * * This interface allows the originator to restore its state */public interface PreviousCalculationToOriginator &#123; int getFirstNumber(); int getSecondNumber();&#125; 123456/** * Memento interface to CalculatorOperator (Caretaker) */public interface PreviousCalculationToCareTaker &#123; // no operations permitted for the caretaker&#125; 1234567891011121314151617181920212223242526/** * Memento Object Implementation * &lt;p&gt; * Note that this object implements both interfaces to Originator and CareTaker */public class PreviousCalculationImp implements PreviousCalculationToCareTaker, PreviousCalculationToOriginator &#123; private int firstNumber; private int secondNumber; public PreviousCalculationImp(int firstNumber, int secondNumber) &#123; this.firstNumber = firstNumber; this.secondNumber = secondNumber; &#125; @Override public int getFirstNumber() &#123; return firstNumber; &#125; @Override public int getSecondNumber() &#123; return secondNumber; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435/** * CareTaker object */public class Client &#123; public static void main(String[] args) &#123; // program starts Calculator calculator = new CalculatorImp(); // assume user enters two numbers calculator.setFirstNumber(10); calculator.setSecondNumber(100); // find result System.out.println(calculator.getCalculationResult()); // Store result of this calculation in case of error PreviousCalculationToCareTaker memento = calculator.backupLastCalculation(); // user enters a number calculator.setFirstNumber(17); // user enters a wrong second number and calculates result calculator.setSecondNumber(-290); // calculate result System.out.println(calculator.getCalculationResult()); // user hits CTRL + Z to undo last operation and see last result calculator.restorePreviousCalculation(memento); // result restored System.out.println(calculator.getCalculationResult()); &#125;&#125; 123110-273110 JDK java.io.Serializable 7. 观察者（Observer）Intent定义对象之间的一对多依赖，当一个对象状态改变时，它的所有依赖都会收到通知并且自动更新状态。 主题（Subject）是被观察的对象，而其所有依赖者（Observer）称为观察者。 ### Class Diagram 主题（Subject）具有注册和移除观察者、并通知所有观察者的功能，主题是通过维护一张观察者列表来实现这些操作的。 观察者（Observer）的注册功能需要调用主题的 registerObserver() 方法。 ### Implementation 天气数据布告板会在天气信息发生改变时更新其内容，布告板有多个，并且在将来会继续增加。 1234567public interface Subject &#123; void registerObserver(Observer o); void removeObserver(Observer o); void notifyObserver();&#125; 12345678910111213141516171819202122232425262728293031323334353637public class WeatherData implements Subject &#123; private List&lt;Observer&gt; observers; private float temperature; private float humidity; private float pressure; public WeatherData() &#123; observers = new ArrayList&lt;&gt;(); &#125; public void setMeasurements(float temperature, float humidity, float pressure) &#123; this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; notifyObserver(); &#125; @Override public void registerObserver(Observer o) &#123; observers.add(o); &#125; @Override public void removeObserver(Observer o) &#123; int i = observers.indexOf(o); if (i &gt;= 0) &#123; observers.remove(i); &#125; &#125; @Override public void notifyObserver() &#123; for (Observer o : observers) &#123; o.update(temperature, humidity, pressure); &#125; &#125;&#125; 123public interface Observer &#123; void update(float temp, float humidity, float pressure);&#125; 1234567891011public class StatisticsDisplay implements Observer &#123; public StatisticsDisplay(Subject weatherData) &#123; weatherData.reisterObserver(this); &#125; @Override public void update(float temp, float humidity, float pressure) &#123; System.out.println("StatisticsDisplay.update: " + temp + " " + humidity + " " + pressure); &#125;&#125; 1234567891011public class CurrentConditionsDisplay implements Observer &#123; public CurrentConditionsDisplay(Subject weatherData) &#123; weatherData.registerObserver(this); &#125; @Override public void update(float temp, float humidity, float pressure) &#123; System.out.println("CurrentConditionsDisplay.update: " + temp + " " + humidity + " " + pressure); &#125;&#125; 12345678910public class WeatherStation &#123; public static void main(String[] args) &#123; WeatherData weatherData = new WeatherData(); CurrentConditionsDisplay currentConditionsDisplay = new CurrentConditionsDisplay(weatherData); StatisticsDisplay statisticsDisplay = new StatisticsDisplay(weatherData); weatherData.setMeasurements(0, 0, 0); weatherData.setMeasurements(1, 1, 1); &#125;&#125; 1234CurrentConditionsDisplay.update: 0.0 0.0 0.0StatisticsDisplay.update: 0.0 0.0 0.0CurrentConditionsDisplay.update: 1.0 1.0 1.0StatisticsDisplay.update: 1.0 1.0 1.0 JDK java.util.Observer java.util.EventListener javax.servlet.http.HttpSessionBindingListener RxJava 8. 状态（State）Intent允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它所属的类。 Class Diagram ### Implementation 糖果销售机有多种状态，每种状态下销售机有不同的行为，状态可以发生转移，使得销售机的行为也发生改变。 123456789101112131415161718192021public interface State &#123; /** * 投入 25 分钱 */ void insertQuarter(); /** * 退回 25 分钱 */ void ejectQuarter(); /** * 转动曲柄 */ void turnCrank(); /** * 发放糖果 */ void dispense();&#125; 123456789101112131415161718192021222324252627282930public class HasQuarterState implements State &#123; private GumballMachine gumballMachine; public HasQuarterState(GumballMachine gumballMachine) &#123; this.gumballMachine = gumballMachine; &#125; @Override public void insertQuarter() &#123; System.out.println("You can't insert another quarter"); &#125; @Override public void ejectQuarter() &#123; System.out.println("Quarter returned"); gumballMachine.setState(gumballMachine.getNoQuarterState()); &#125; @Override public void turnCrank() &#123; System.out.println("You turned..."); gumballMachine.setState(gumballMachine.getSoldState()); &#125; @Override public void dispense() &#123; System.out.println("No gumball dispensed"); &#125;&#125; 1234567891011121314151617181920212223242526272829public class NoQuarterState implements State &#123; GumballMachine gumballMachine; public NoQuarterState(GumballMachine gumballMachine) &#123; this.gumballMachine = gumballMachine; &#125; @Override public void insertQuarter() &#123; System.out.println("You insert a quarter"); gumballMachine.setState(gumballMachine.getHasQuarterState()); &#125; @Override public void ejectQuarter() &#123; System.out.println("You haven't insert a quarter"); &#125; @Override public void turnCrank() &#123; System.out.println("You turned, but there's no quarter"); &#125; @Override public void dispense() &#123; System.out.println("You need to pay first"); &#125;&#125; 12345678910111213141516171819202122232425262728public class SoldOutState implements State &#123; GumballMachine gumballMachine; public SoldOutState(GumballMachine gumballMachine) &#123; this.gumballMachine = gumballMachine; &#125; @Override public void insertQuarter() &#123; System.out.println("You can't insert a quarter, the machine is sold out"); &#125; @Override public void ejectQuarter() &#123; System.out.println("You can't eject, you haven't inserted a quarter yet"); &#125; @Override public void turnCrank() &#123; System.out.println("You turned, but there are no gumballs"); &#125; @Override public void dispense() &#123; System.out.println("No gumball dispensed"); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334public class SoldState implements State &#123; GumballMachine gumballMachine; public SoldState(GumballMachine gumballMachine) &#123; this.gumballMachine = gumballMachine; &#125; @Override public void insertQuarter() &#123; System.out.println("Please wait, we're already giving you a gumball"); &#125; @Override public void ejectQuarter() &#123; System.out.println("Sorry, you already turned the crank"); &#125; @Override public void turnCrank() &#123; System.out.println("Turning twice doesn't get you another gumball!"); &#125; @Override public void dispense() &#123; gumballMachine.releaseBall(); if (gumballMachine.getCount() &gt; 0) &#123; gumballMachine.setState(gumballMachine.getNoQuarterState()); &#125; else &#123; System.out.println("Oops, out of gumballs"); gumballMachine.setState(gumballMachine.getSoldOutState()); &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class GumballMachine &#123; private State soldOutState; private State noQuarterState; private State hasQuarterState; private State soldState; private State state; private int count = 0; public GumballMachine(int numberGumballs) &#123; count = numberGumballs; soldOutState = new SoldOutState(this); noQuarterState = new NoQuarterState(this); hasQuarterState = new HasQuarterState(this); soldState = new SoldState(this); if (numberGumballs &gt; 0) &#123; state = noQuarterState; &#125; else &#123; state = soldOutState; &#125; &#125; public void insertQuarter() &#123; state.insertQuarter(); &#125; public void ejectQuarter() &#123; state.ejectQuarter(); &#125; public void turnCrank() &#123; state.turnCrank(); state.dispense(); &#125; public void setState(State state) &#123; this.state = state; &#125; public void releaseBall() &#123; System.out.println("A gumball comes rolling out the slot..."); if (count != 0) &#123; count -= 1; &#125; &#125; public State getSoldOutState() &#123; return soldOutState; &#125; public State getNoQuarterState() &#123; return noQuarterState; &#125; public State getHasQuarterState() &#123; return hasQuarterState; &#125; public State getSoldState() &#123; return soldState; &#125; public int getCount() &#123; return count; &#125;&#125; 123456789101112131415161718192021222324252627public class Client &#123; public static void main(String[] args) &#123; GumballMachine gumballMachine = new GumballMachine(5); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.ejectQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.ejectQuarter(); gumballMachine.insertQuarter(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); &#125;&#125; 12345678910111213141516171819202122232425You insert a quarterYou turned...A gumball comes rolling out the slot...You insert a quarterQuarter returnedYou turned, but there's no quarterYou need to pay firstYou insert a quarterYou turned...A gumball comes rolling out the slot...You insert a quarterYou turned...A gumball comes rolling out the slot...You haven't insert a quarterYou insert a quarterYou can't insert another quarterYou turned...A gumball comes rolling out the slot...You insert a quarterYou turned...A gumball comes rolling out the slot...Oops, out of gumballsYou can't insert a quarter, the machine is sold outYou turned, but there are no gumballsNo gumball dispensed 9. 策略（Strategy）Intent定义一系列算法，封装每个算法，并使它们可以互换。 策略模式可以让算法独立于使用它的客户端。 Class Diagram Strategy 接口定义了一个算法族，它们都实现了 behavior() 方法。 Context 是使用到该算法族的类，其中的 doSomething() 方法会调用 behavior()，setStrategy(Strategy) 方法可以动态地改变 strategy 对象，也就是说能动态地改变 Context 所使用的算法。 ### 与状态模式的比较 状态模式的类图和策略模式类似，并且都是能够动态改变对象的行为。但是状态模式是通过状态转移来改变 Context 所组合的 State 对象，而策略模式是通过 Context 本身的决策来改变组合的 Strategy 对象。所谓的状态转移，是指 Context 在运行过程中由于一些条件发生改变而使得 State 对象发生改变，注意必须要是在运行过程中。 状态模式主要是用来解决状态转移的问题，当状态发生转移了，那么 Context 对象就会改变它的行为；而策略模式主要是用来封装一组可以互相替代的算法族，并且可以根据需要动态地去替换 Context 使用的算法。 Implementation设计一个鸭子，它可以动态地改变叫声。这里的算法族是鸭子的叫声行为。 123public interface QuackBehavior &#123; void quack();&#125; 123456public class Quack implements QuackBehavior &#123; @Override public void quack() &#123; System.out.println("quack!"); &#125;&#125; 123456public class Squeak implements QuackBehavior&#123; @Override public void quack() &#123; System.out.println("squeak!"); &#125;&#125; 1234567891011121314public class Duck &#123; private QuackBehavior quackBehavior; public void performQuack() &#123; if (quackBehavior != null) &#123; quackBehavior.quack(); &#125; &#125; public void setQuackBehavior(QuackBehavior quackBehavior) &#123; this.quackBehavior = quackBehavior; &#125;&#125; 12345678910public class Client &#123; public static void main(String[] args) &#123; Duck duck = new Duck(); duck.setQuackBehavior(new Squeak()); duck.performQuack(); duck.setQuackBehavior(new Quack()); duck.performQuack(); &#125;&#125; 12squeak!quack! JDK java.util.Comparator#compare() javax.servlet.http.HttpServlet javax.servlet.Filter#doFilter() 10. 模板方法（Template Method）Intent定义算法框架，并将一些步骤的实现延迟到子类。 通过模板方法，子类可以重新定义算法的某些步骤，而不用改变算法的结构。 Class Diagram ### Implementation 冲咖啡和冲茶都有类似的流程，但是某些步骤会有点不一样，要求复用那些相同步骤的代码。 123456789101112131415161718192021public abstract class CaffeineBeverage &#123; final void prepareRecipe() &#123; boilWater(); brew(); pourInCup(); addCondiments(); &#125; abstract void brew(); abstract void addCondiments(); void boilWater() &#123; System.out.println("boilWater"); &#125; void pourInCup() &#123; System.out.println("pourInCup"); &#125;&#125; 1234567891011public class Coffee extends CaffeineBeverage &#123; @Override void brew() &#123; System.out.println("Coffee.brew"); &#125; @Override void addCondiments() &#123; System.out.println("Coffee.addCondiments"); &#125;&#125; 1234567891011public class Tea extends CaffeineBeverage &#123; @Override void brew() &#123; System.out.println("Tea.brew"); &#125; @Override void addCondiments() &#123; System.out.println("Tea.addCondiments"); &#125;&#125; 123456789public class Client &#123; public static void main(String[] args) &#123; CaffeineBeverage caffeineBeverage = new Coffee(); caffeineBeverage.prepareRecipe(); System.out.println("-----------"); caffeineBeverage = new Tea(); caffeineBeverage.prepareRecipe(); &#125;&#125; 123456789boilWaterCoffee.brewpourInCupCoffee.addCondiments-----------boilWaterTea.brewpourInCupTea.addCondiments JDK java.util.Collections#sort() java.io.InputStream#skip() java.io.InputStream#read() java.util.AbstractList#indexOf() 11. 访问者（Visitor）Intent为一个对象结构（比如组合结构）增加新能力。 Class Diagram Visitor：访问者，为每一个 ConcreteElement 声明一个 visit 操作 ConcreteVisitor：具体访问者，存储遍历过程中的累计结果 ObjectStructure：对象结构，可以是组合结构，或者是一个集合。 ### Implementation 123public interface Element &#123; void accept(Visitor visitor);&#125; 1234567891011121314class CustomerGroup &#123; private List&lt;Customer&gt; customers = new ArrayList&lt;&gt;(); void accept(Visitor visitor) &#123; for (Customer customer : customers) &#123; customer.accept(visitor); &#125; &#125; void addCustomer(Customer customer) &#123; customers.add(customer); &#125;&#125; 123456789101112131415161718192021222324public class Customer implements Element &#123; private String name; private List&lt;Order&gt; orders = new ArrayList&lt;&gt;(); Customer(String name) &#123; this.name = name; &#125; String getName() &#123; return name; &#125; void addOrder(Order order) &#123; orders.add(order); &#125; public void accept(Visitor visitor) &#123; visitor.visit(this); for (Order order : orders) &#123; order.accept(visitor); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930public class Order implements Element &#123; private String name; private List&lt;Item&gt; items = new ArrayList(); Order(String name) &#123; this.name = name; &#125; Order(String name, String itemName) &#123; this.name = name; this.addItem(new Item(itemName)); &#125; String getName() &#123; return name; &#125; void addItem(Item item) &#123; items.add(item); &#125; public void accept(Visitor visitor) &#123; visitor.visit(this); for (Item item : items) &#123; item.accept(visitor); &#125; &#125;&#125; 12345678910111213141516public class Item implements Element &#123; private String name; Item(String name) &#123; this.name = name; &#125; String getName() &#123; return name; &#125; public void accept(Visitor visitor) &#123; visitor.visit(this); &#125;&#125; 1234567public interface Visitor &#123; void visit(Customer customer); void visit(Order order); void visit(Item item);&#125; 123456789101112131415161718192021222324252627public class GeneralReport implements Visitor &#123; private int customersNo; private int ordersNo; private int itemsNo; public void visit(Customer customer) &#123; System.out.println(customer.getName()); customersNo++; &#125; public void visit(Order order) &#123; System.out.println(order.getName()); ordersNo++; &#125; public void visit(Item item) &#123; System.out.println(item.getName()); itemsNo++; &#125; public void displayResults() &#123; System.out.println("Number of customers: " + customersNo); System.out.println("Number of orders: " + ordersNo); System.out.println("Number of items: " + itemsNo); &#125;&#125; 1234567891011121314151617181920212223public class Client &#123; public static void main(String[] args) &#123; Customer customer1 = new Customer("customer1"); customer1.addOrder(new Order("order1", "item1")); customer1.addOrder(new Order("order2", "item1")); customer1.addOrder(new Order("order3", "item1")); Order order = new Order("order_a"); order.addItem(new Item("item_a1")); order.addItem(new Item("item_a2")); order.addItem(new Item("item_a3")); Customer customer2 = new Customer("customer2"); customer2.addOrder(order); CustomerGroup customers = new CustomerGroup(); customers.addCustomer(customer1); customers.addCustomer(customer2); GeneralReport visitor = new GeneralReport(); customers.accept(visitor); visitor.displayResults(); &#125;&#125; 123456789101112131415customer1order1item1order2item1order3item1customer2order_aitem_a1item_a2item_a3Number of customers: 2Number of orders: 4Number of items: 6 JDK javax.lang.model.element.Element and javax.lang.model.element.ElementVisitor javax.lang.model.type.TypeMirror and javax.lang.model.type.TypeVisitor 12. 空对象（Null）Intent使用什么都不做的空对象来代替 NULL。 一个方法返回 NULL，意味着方法的调用端需要去检查返回值是否是 NULL，这么做会导致非常多的冗余的检查代码。并且如果某一个调用端忘记了做这个检查返回值，而直接使用返回的对象，那么就有可能抛出空指针异常。 Class Diagram ### Implementation 123public abstract class AbstractOperation &#123; abstract void request();&#125; 123456public class RealOperation extends AbstractOperation &#123; @Override void request() &#123; System.out.println("do something"); &#125;&#125; 123456public class NullOperation extends AbstractOperation&#123; @Override void request() &#123; // do nothing &#125;&#125; 12345678910111213public class Client &#123; public static void main(String[] args) &#123; AbstractOperation abstractOperation = func(-1); abstractOperation.request(); &#125; public static AbstractOperation func(int para) &#123; if (para &lt; 0) &#123; return new NullOperation(); &#125; return new RealOperation(); &#125;&#125; 四、结构型1. 适配器（Adapter）Intent把一个类接口转换成另一个用户需要的接口。 ### Class Diagram ### Implementation 鸭子（Duck）和火鸡（Turkey）拥有不同的叫声，Duck 的叫声调用 quack() 方法，而 Turkey 调用 gobble() 方法。 要求将 Turkey 的 gobble() 方法适配成 Duck 的 quack() 方法，从而让火鸡冒充鸭子！ 123public interface Duck &#123; void quack();&#125; 123public interface Turkey &#123; void gobble();&#125; 123456public class WildTurkey implements Turkey &#123; @Override public void gobble() &#123; System.out.println("gobble!"); &#125;&#125; 123456789101112public class TurkeyAdapter implements Duck &#123; Turkey turkey; public TurkeyAdapter(Turkey turkey) &#123; this.turkey = turkey; &#125; @Override public void quack() &#123; turkey.gobble(); &#125;&#125; 1234567public class Client &#123; public static void main(String[] args) &#123; Turkey turkey = new WildTurkey(); Duck duck = new TurkeyAdapter(turkey); duck.quack(); &#125;&#125; JDK java.util.Arrays#asList() java.util.Collections#list() java.util.Collections#enumeration() javax.xml.bind.annotation.adapters.XMLAdapter 2. 桥接（Bridge）Intent将抽象与实现分离开来，使它们可以独立变化。 Class Diagram Abstraction：定义抽象类的接口 Implementor：定义实现类接口 ### Implementation RemoteControl 表示遥控器，指代 Abstraction。 TV 表示电视，指代 Implementor。 桥接模式将遥控器和电视分离开来，从而可以独立改变遥控器或者电视的实现。 1234567public abstract class TV &#123; public abstract void on(); public abstract void off(); public abstract void tuneChannel();&#125; 12345678910111213141516public class Sony extends TV &#123; @Override public void on() &#123; System.out.println("Sony.on()"); &#125; @Override public void off() &#123; System.out.println("Sony.off()"); &#125; @Override public void tuneChannel() &#123; System.out.println("Sony.tuneChannel()"); &#125;&#125; 12345678910111213141516public class RCA extends TV &#123; @Override public void on() &#123; System.out.println("RCA.on()"); &#125; @Override public void off() &#123; System.out.println("RCA.off()"); &#125; @Override public void tuneChannel() &#123; System.out.println("RCA.tuneChannel()"); &#125;&#125; 12345678910111213public abstract class RemoteControl &#123; protected TV tv; public RemoteControl(TV tv) &#123; this.tv = tv; &#125; public abstract void on(); public abstract void off(); public abstract void tuneChannel();&#125; 1234567891011121314151617181920212223public class ConcreteRemoteControl1 extends RemoteControl &#123; public ConcreteRemoteControl1(TV tv) &#123; super(tv); &#125; @Override public void on() &#123; System.out.println("ConcreteRemoteControl1.on()"); tv.on(); &#125; @Override public void off() &#123; System.out.println("ConcreteRemoteControl1.off()"); tv.off(); &#125; @Override public void tuneChannel() &#123; System.out.println("ConcreteRemoteControl1.tuneChannel()"); tv.tuneChannel(); &#125;&#125; 1234567891011121314151617181920212223public class ConcreteRemoteControl2 extends RemoteControl &#123; public ConcreteRemoteControl2(TV tv) &#123; super(tv); &#125; @Override public void on() &#123; System.out.println("ConcreteRemoteControl2.on()"); tv.on(); &#125; @Override public void off() &#123; System.out.println("ConcreteRemoteControl2.off()"); tv.off(); &#125; @Override public void tuneChannel() &#123; System.out.println("ConcreteRemoteControl2.tuneChannel()"); tv.tuneChannel(); &#125;&#125; 123456789101112public class Client &#123; public static void main(String[] args) &#123; RemoteControl remoteControl1 = new ConcreteRemoteControl1(new RCA()); remoteControl1.on(); remoteControl1.off(); remoteControl1.tuneChannel(); RemoteControl remoteControl2 = new ConcreteRemoteControl2(new Sony()); remoteControl2.on(); remoteControl2.off(); remoteControl2.tuneChannel(); &#125;&#125; JDK AWT (It provides an abstraction layer which maps onto the native OS the windowing support.) JDBC 3. 组合（Composite）Intent将对象组合成树形结构来表示“整体/部分”层次关系，允许用户以相同的方式处理单独对象和组合对象。 Class Diagram组件（Component）类是组合类（Composite）和叶子类（Leaf）的父类，可以把组合类看成是树的中间节点。 组合对象拥有一个或者多个组件对象，因此组合对象的操作可以委托给组件对象去处理，而组件对象可以是另一个组合对象或者叶子对象。 ### Implementation 1234567891011121314151617public abstract class Component &#123; protected String name; public Component(String name) &#123; this.name = name; &#125; public void print() &#123; print(0); &#125; abstract void print(int level); abstract public void add(Component component); abstract public void remove(Component component);&#125; 123456789101112131415161718192021222324252627282930public class Composite extends Component &#123; private List&lt;Component&gt; child; public Composite(String name) &#123; super(name); child = new ArrayList&lt;&gt;(); &#125; @Override void print(int level) &#123; for (int i = 0; i &lt; level; i++) &#123; System.out.print("--"); &#125; System.out.println("Composite:" + name); for (Component component : child) &#123; component.print(level + 1); &#125; &#125; @Override public void add(Component component) &#123; child.add(component); &#125; @Override public void remove(Component component) &#123; child.remove(component); &#125;&#125; 1234567891011121314151617181920212223public class Leaf extends Component &#123; public Leaf(String name) &#123; super(name); &#125; @Override void print(int level) &#123; for (int i = 0; i &lt; level; i++) &#123; System.out.print("--"); &#125; System.out.println("left:" + name); &#125; @Override public void add(Component component) &#123; throw new UnsupportedOperationException(); // 牺牲透明性换取单一职责原则，这样就不用考虑是叶子节点还是组合节点 &#125; @Override public void remove(Component component) &#123; throw new UnsupportedOperationException(); &#125;&#125; 123456789101112131415161718public class Client &#123; public static void main(String[] args) &#123; Composite root = new Composite("root"); Component node1 = new Leaf("1"); Component node2 = new Composite("2"); Component node3 = new Leaf("3"); root.add(node1); root.add(node2); root.add(node3); Component node21 = new Leaf("21"); Component node22 = new Composite("22"); node2.add(node21); node2.add(node22); Component node221 = new Leaf("221"); node22.add(node221); root.print(); &#125;&#125; 1234567Composite:root--left:1--Composite:2----left:21----Composite:22------left:221--left:3 JDK javax.swing.JComponent#add(Component) java.awt.Container#add(Component) java.util.Map#putAll(Map) java.util.List#addAll(Collection) java.util.Set#addAll(Collection) 4. 装饰（Decorator）Intent为对象动态添加功能。 Class Diagram装饰者（Decorator）和具体组件（ConcreteComponent）都继承自组件（Component），具体组件的方法实现不需要依赖于其它对象，而装饰者组合了一个组件，这样它可以装饰其它装饰者或者具体组件。所谓装饰，就是把这个装饰者套在被装饰者之上，从而动态扩展被装饰者的功能。装饰者的方法有一部分是自己的，这属于它的功能，然后调用被装饰者的方法实现，从而也保留了被装饰者的功能。可以看到，具体组件应当是装饰层次的最低层，因为只有具体组件的方法实现不需要依赖于其它对象。 ### Implementation 设计不同种类的饮料，饮料可以添加配料，比如可以添加牛奶，并且支持动态添加新配料。每增加一种配料，该饮料的价格就会增加，要求计算一种饮料的价格。 下图表示在 DarkRoast 饮料上新增新添加 Mocha 配料，之后又添加了 Whip 配料。DarkRoast 被 Mocha 包裹，Mocha 又被 Whip 包裹。它们都继承自相同父类，都有 cost() 方法，外层类的 cost() 方法调用了内层类的 cost() 方法。 123public interface Beverage &#123; double cost();&#125; 123456public class DarkRoast implements Beverage &#123; @Override public double cost() &#123; return 1; &#125;&#125; 123456public class HouseBlend implements Beverage &#123; @Override public double cost() &#123; return 1; &#125;&#125; 123public abstract class CondimentDecorator implements Beverage &#123; protected Beverage beverage;&#125; 1234567891011public class Milk extends CondimentDecorator &#123; public Milk(Beverage beverage) &#123; this.beverage = beverage; &#125; @Override public double cost() &#123; return 1 + beverage.cost(); &#125;&#125; 1234567891011public class Mocha extends CondimentDecorator &#123; public Mocha(Beverage beverage) &#123; this.beverage = beverage; &#125; @Override public double cost() &#123; return 1 + beverage.cost(); &#125;&#125; 123456789public class Client &#123; public static void main(String[] args) &#123; Beverage beverage = new HouseBlend(); beverage = new Mocha(beverage); beverage = new Milk(beverage); System.out.println(beverage.cost()); &#125;&#125; 13.0 设计原则类应该对扩展开放，对修改关闭：也就是添加新功能时不需要修改代码。饮料可以动态添加新的配料，而不需要去修改饮料的代码。 不可能把所有的类设计成都满足这一原则，应当把该原则应用于最有可能发生改变的地方。 JDK java.io.BufferedInputStream(InputStream) java.io.DataInputStream(InputStream) java.io.BufferedOutputStream(OutputStream) java.util.zip.ZipOutputStream(OutputStream) java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap 5. 外观（Facade）Intent提供了一个统一的接口，用来访问子系统中的一群接口，从而让子系统更容易使用。 Class Diagram ### Implementation 观看电影需要操作很多电器，使用外观模式实现一键看电影功能。 12345678910111213public class SubSystem &#123; public void turnOnTV() &#123; System.out.println("turnOnTV()"); &#125; public void setCD(String cd) &#123; System.out.println("setCD( " + cd + " )"); &#125; public void startWatching()&#123; System.out.println("startWatching()"); &#125;&#125; 123456789public class Facade &#123; private SubSystem subSystem = new SubSystem(); public void watchMovie() &#123; subSystem.turnOnTV(); subSystem.setCD("a movie"); subSystem.startWatching(); &#125;&#125; 123456public class Client &#123; public static void main(String[] args) &#123; Facade facade = new Facade(); facade.watchMovie(); &#125;&#125; 设计原则最少知识原则：只和你的密友谈话。也就是说客户对象所需要交互的对象应当尽可能少。 6. 享元（Flyweight）Intent利用共享的方式来支持大量细粒度的对象，这些对象一部分内部状态是相同的。 Class Diagram Flyweight：享元对象 IntrinsicState：内部状态，享元对象共享内部状态 ExtrinsicState：外部状态，每个享元对象的外部状态不同 ### Implementation 123public interface Flyweight &#123; void doOperation(String extrinsicState);&#125; 123456789101112131415public class ConcreteFlyweight implements Flyweight &#123; private String intrinsicState; public ConcreteFlyweight(String intrinsicState) &#123; this.intrinsicState = intrinsicState; &#125; @Override public void doOperation(String extrinsicState) &#123; System.out.println("Object address: " + System.identityHashCode(this)); System.out.println("IntrinsicState: " + intrinsicState); System.out.println("ExtrinsicState: " + extrinsicState); &#125;&#125; 123456789101112public class FlyweightFactory &#123; private HashMap&lt;String, Flyweight&gt; flyweights = new HashMap&lt;&gt;(); Flyweight getFlyweight(String intrinsicState) &#123; if (!flyweights.containsKey(intrinsicState)) &#123; Flyweight flyweight = new ConcreteFlyweight(intrinsicState); flyweights.put(intrinsicState, flyweight); &#125; return flyweights.get(intrinsicState); &#125;&#125; 12345678910public class Client &#123; public static void main(String[] args) &#123; FlyweightFactory factory = new FlyweightFactory(); Flyweight flyweight1 = factory.getFlyweight("aa"); Flyweight flyweight2 = factory.getFlyweight("aa"); flyweight1.doOperation("x"); flyweight2.doOperation("y"); &#125;&#125; 123456Object address: 1163157884IntrinsicState: aaExtrinsicState: xObject address: 1163157884IntrinsicState: aaExtrinsicState: y JDKJava 利用缓存来加速大量小对象的访问时间。 java.lang.Integer#valueOf(int) java.lang.Boolean#valueOf(boolean) java.lang.Byte#valueOf(byte) java.lang.Character#valueOf(char) 7. 代理（Proxy）Intent控制对其它对象的访问。 Class Diagram代理有以下四类： 远程代理（Remote Proxy）：控制对远程对象（不同地址空间）的访问，它负责将请求及其参数进行编码，并向不同地址空间中的对象发送已经编码的请求。 虚拟代理（Virtual Proxy）：根据需要创建开销很大的对象，它可以缓存实体的附加信息，以便延迟对它的访问，例如在网站加载一个很大图片时，不能马上完成，可以用虚拟代理缓存图片的大小信息，然后生成一张临时图片代替原始图片。 保护代理（Protection Proxy）：按权限控制对象的访问，它负责检查调用者是否具有实现一个请求所必须的访问权限。 智能代理（Smart Reference）：取代了简单的指针，它在访问对象时执行一些附加操作：记录对象的引用次数；当第一次引用一个对象时，将它装入内存；在访问一个实际对象前，检查是否已经锁定了它，以确保其它对象不能改变它。 ### Implementation 以下是一个虚拟代理的实现，模拟了图片延迟加载的情况下使用与图片大小相等的临时内容去替换原始图片，直到图片加载完成才将图片显示出来。 123public interface Image &#123; void showImage();&#125; 123456789101112131415161718192021222324252627282930313233public class HighResolutionImage implements Image &#123; private URL imageURL; private long startTime; private int height; private int width; public int getHeight() &#123; return height; &#125; public int getWidth() &#123; return width; &#125; public HighResolutionImage(URL imageURL) &#123; this.imageURL = imageURL; this.startTime = System.currentTimeMillis(); this.width = 600; this.height = 600; &#125; public boolean isLoad() &#123; // 模拟图片加载，延迟 3s 加载完成 long endTime = System.currentTimeMillis(); return endTime - startTime &gt; 3000; &#125; @Override public void showImage() &#123; System.out.println("Real Image: " + imageURL); &#125;&#125; 123456789101112131415161718192021public class ImageProxy implements Image &#123; private HighResolutionImage highResolutionImage; public ImageProxy(HighResolutionImage highResolutionImage) &#123; this.highResolutionImage = highResolutionImage; &#125; @Override public void showImage() &#123; while (!highResolutionImage.isLoad()) &#123; try &#123; System.out.println("Temp Image: " + highResolutionImage.getWidth() + " " + highResolutionImage.getHeight()); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; highResolutionImage.showImage(); &#125;&#125; 12345678910public class ImageViewer &#123; public static void main(String[] args) throws Exception &#123; String image = "http://image.jpg"; URL url = new URL(image); HighResolutionImage highResolutionImage = new HighResolutionImage(url); ImageProxy imageProxy = new ImageProxy(highResolutionImage); imageProxy.showImage(); &#125;&#125; JDK java.lang.reflect.Proxy RMI 参考资料 弗里曼. Head First 设计模式 [M]. 中国电力出版社, 2007. Gamma E. 设计模式: 可复用面向对象软件的基础 [M]. 机械工业出版社, 2007. Bloch J. Effective java[M]. Addison-Wesley Professional, 2017. Design Patterns Design patterns implemented in Java The breakdown of design patterns in JDK]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库系统原理]]></title>
    <url>%2F2019%2F07%2F04%2Fdatabase-principle%2F</url>
    <content type="text"><![CDATA[参考：https://github.com/CyC2018/CS-Notes 事务概念事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。 ACID原子性（Atomicity）事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。 回滚可以用回滚日志来实现，回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistency）数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对一个数据的读取结果都是相同的。 隔离性（Isolation）一个事务所做的修改在最终提交以前，对其它事务是不可见的。 持久性（Durability）一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 使用重做日志来保证持久性。 事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库崩溃的情况。 AUTOCOMMITMySQL 默认采用自动提交模式。也就是说，如果不显式使用START TRANSACTION语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。 并发一致性问题在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。 丢失修改T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 读脏数据(读取一个事务没有提交的结果)T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 不可重复读（读取一个事务已经提交的结果导致两次读取不一致）T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 幻影读T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 产生并发不一致性问题主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。 封锁封锁粒度MySQL 中提供了两种封锁粒度：行级锁以及表级锁。 应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高。 但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此封锁粒度越小，系统开销就越大。 在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。 封锁类型读写锁 排它锁（Exclusive），简写为 X 锁，又称写锁。 共享锁（Shared），简写为 S 锁，又称读锁。 有以下两个规定： 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。 锁的兼容关系如下： - X S X × × S × √ 意向锁使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。 在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。 意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定： 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。 各种锁的兼容关系如下： - X IX S IS X × × × × IX × √ × √ S × × √ √ IS × √ √ √ 解释如下： 任意 IS/IX 锁之间都是兼容的，因为它们只是表示想要对表加锁，而不是真正加锁； S 锁只与 S 锁和 IS 锁兼容，也就是说事务 T 想要对数据行加 S 锁，其它事务可以已经获得对表或者表中的行的 S 锁。 封锁协议三级封锁协议一级封锁协议 事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。 可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。 T1 T2 lock-x(A) read A=20 lock-x(A) wait write A=19 . commit . unlock-x(A) . obtain read A=19 write A=21 commit unlock-x(A) 二级封锁协议 在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。 可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。 T1 T2 lock-x(A) read A=20 write A=19 lock-s(A) wait rollback . A=20 . unlock-x(A) . obtain read A=20 unlock-s(A) commit 三级封锁协议 在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。 可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。 T1 T2 lock-s(A) read A=20 lock-x(A) wait read A=20 . commit . unlock-s(A) . obtain read A=20 write A=19 commit unlock-X(A) 两段锁协议加锁和解锁分为两个阶段进行。 可串行化调度是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。 事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。 1lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B) 但不是必要条件，例如以下操作不满足两段锁协议，但是它还是可串行化调度。 1lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C) MySQL 隐式与显示锁定MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。 InnoDB 也可以使用特定的语句进行显示锁定： 12SELECT ... LOCK In SHARE MODE;SELECT ... FOR UPDATE; 隔离级别未提交读（READ UNCOMMITTED）事务中的修改，即使没有提交，对其它事务也是可见的。 提交读（READ COMMITTED）一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。 可重复读（REPEATABLE READ）保证在同一个事务中多次读取同样数据的结果是一样的。 可串行化（SERIALIZABLE）强制事务串行执行。 隔离级别 脏读 不可重复读 幻影读 加锁读 未提交读 √ √ √ × 提交读 × √ √ × 可重复读 × × √ × 可串行化 × × × √ 多版本并发控制多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。 版本号 系统版本号：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号：事务开始时的系统版本号。 隐藏的列MVCC 在每行记录后面都保存着两个隐藏的列，用来存储两个版本号： 创建版本号：指示创建一个数据行的快照时的系统版本号； 删除版本号：如果该快照的删除版本号大于当前事务版本号表示该快照有效，否则表示该快照已经被删除了。 Undo 日志MVCC 使用到的快照存储在 Undo 日志中，该日志通过回滚指针把一个数据行（Record）的所有快照连接起来。 实现过程以下实现过程针对可重复读隔离级别。 当开始新一个事务时，该事务的版本号肯定会大于当前所有数据行快照的创建版本号，理解这一点很关键。 SELECT多个事务必须读取到同一个数据行的快照，并且这个快照是距离现在最近的一个有效快照。但是也有例外，如果有一个事务正在修改该数据行，那么它可以读取事务本身所做的修改，而不用和其它事务的读取结果一致。 把没有对一个数据行做修改的事务称为 T，T 所要读取的数据行快照的创建版本号必须小于 T 的版本号，因为如果大于或者等于 T 的版本号，那么表示该数据行快照是其它事务的最新修改，因此不能去读取它。除此之外，T 所要读取的数据行快照的删除版本号必须大于 T 的版本号，因为如果小于等于 T 的版本号，那么表示该数据行快照是已经被删除的，不应该去读取它。 INSERT将当前系统版本号作为数据行快照的创建版本号。 DELETE将当前系统版本号作为数据行快照的删除版本号。 UPDATE将当前系统版本号作为更新前的数据行快照的删除版本号，并将当前系统版本号作为更新后的数据行快照的创建版本号。可以理解为先执行 DELETE 后执行 INSERT。 快照读与当前读快照读使用 MVCC 读取的是快照中的数据，这样可以减少加锁所带来的开销。 1select * from table ...; 当前读读取的是最新的数据，需要加锁。以下第一个语句需要加 S 锁，其它都需要加 X 锁。 12345select * from table where ? lock in share mode;select * from table where ? for update;insert;update;delete; Next-Key LocksNext-Key Locks 是 MySQL 的 InnoDB 存储引擎的一种锁实现。 MVCC 不能解决幻读的问题，Next-Key Locks 就是为了解决这个问题而存在的。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key Locks 可以解决幻读问题。 Record Locks锁定一个记录上的索引，而不是记录本身。 如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Locks 依然可以使用。 Gap Locks锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。 1SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE; Next-Key Locks它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间： 12345(negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) 关系数据库设计理论函数依赖记 A-&gt;B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。 如果 {A1，A2，… ，An} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。 对于 A-&gt;B，如果能找到 A 的真子集 A’，使得 A’-&gt; B，那么 A-&gt;B 就是部分函数依赖，否则就是完全函数依赖。 对于 A-&gt;B，B-&gt;C，则 A-&gt;C 是一个传递函数依赖。 异常以下的学生课程关系的函数依赖为 Sno, Cname -&gt; Sname, Sdept, Mname, Grade，键码为 {Sno, Cname}。也就是说，确定学生和课程之后，就能确定其它信息。 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 不符合范式的关系，会产生很多异常，主要有以下四种异常： 冗余数据：例如 学生-2 出现了两次。 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改。 删除异常：删除一个信息，那么也会丢失其它信息。例如删除了 课程-1 需要删除第一行和第三行，那么 学生-1 的信息就会丢失。 插入异常：例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入。 范式范式理论是为了解决以上提到四种异常。 高级别范式的依赖于低级别的范式，1NF 是最低级别的范式。 第一范式 (1NF)属性不可分。 第二范式 (2NF)每个非主属性完全函数依赖于键码。 可以通过分解来满足。 分解前 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 以上学生课程关系中，{Sno, Cname} 为键码，有如下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname Sno, Cname-&gt; Grade Grade 完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。 Sname, Sdept 和 Mname 都部分依赖于键码，当一个学生选修了多门课时，这些数据就会出现多次，造成大量冗余数据。 分解后 关系-1 Sno Sname Sdept Mname 1 学生-1 学院-1 院长-1 2 学生-2 学院-2 院长-2 3 学生-3 学院-2 院长-2 有以下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname 关系-2 Sno Cname Grade 1 课程-1 90 2 课程-2 80 2 课程-1 100 3 课程-2 95 有以下函数依赖： Sno, Cname -&gt; Grade 第三范式 (3NF)非主属性不传递函数依赖于键码。 上面的 关系-1 中存在以下传递函数依赖： Sno -&gt; Sdept -&gt; Mname 可以进行以下分解： 关系-11 Sno Sname Sdept 1 学生-1 学院-1 2 学生-2 学院-2 3 学生-3 学院-2 关系-12 Sdept Mname 学院-1 院长-1 学院-2 院长-2 ER 图Entity-Relationship，有三个组成部分：实体、属性、联系。 用来进行关系型数据库系统的概念设计。 实体的三种联系包含一对一，一对多，多对多三种。 如果 A 到 B 是一对多关系，那么画个带箭头的线段指向 B； 如果是一对一，画两个带箭头的线段； 如果是多对多，画两个不带箭头的线段。 下图的 Course 和 Student 是一对多的关系。 表示出现多次的关系一个实体在联系出现几次，就要用几条线连接。 下图表示一个课程的先修关系，先修关系出现两个 Course 实体，第一个是先修课程，后一个是后修课程，因此需要用两条线来表示这种关系。 联系的多向性虽然老师可以开设多门课，并且可以教授多名学生，但是对于特定的学生和课程，只有一个老师教授，这就构成了一个三元联系。 一般只使用二元联系，可以把多元联系转换为二元联系。 表示子类用一个三角形和两条线来连接类和子类，与子类有关的属性和联系都连到子类上，而与父类和子类都有关的连到父类上。 参考资料 AbrahamSilberschatz, HenryF.Korth, S.Sudarshan, 等. 数据库系统概念 [M]. 机械工业出版社, 2006. 施瓦茨. 高性能 MYSQL(第3版)[M]. 电子工业出版社, 2013. 史嘉权. 数据库系统概论[M]. 清华大学出版社有限公司, 2006. The InnoDB Storage Engine Transaction isolation levels Concurrency Control The Nightmare of Locking, Blocking and Isolation Levels! Database Normalization and Normal Forms with an Example The basics of the InnoDB undo logging and history system MySQL locking for the busy web developer 浅入浅出 MySQL 和 InnoDB Innodb 中的事务隔离级别和锁的关系]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-按之字形顺序打印二叉树]]></title>
    <url>%2F2019%2F07%2F04%2Fjianzhioffer-zigzag-print-tree%2F</url>
    <content type="text"><![CDATA[题目描述请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 AC代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.util.ArrayList;import java.util.Queue;import java.util.LinkedList;import java.util.Collections;/*public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; //层次遍历需要用到队列的特性 Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); //保存遍历结果 ArrayList&lt;ArrayList&lt;Integer&gt;&gt; resList = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); //打印顺序，奇数为从右往左，偶数为从左往右 int order = 1; //当前层剩余的结点个数，即还剩多少个节点没有遍历 int curNode = 1; //下一层总共有多少个结点 int nextNode = 0; public ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; //程序的鲁棒性 if(pRoot == null) return resList; //先加入根节点 queue.offer(pRoot); TreeNode tmp = null; ArrayList&lt;Integer&gt; curList = new ArrayList&lt;Integer&gt;(); //队列为空，结束 while(!queue.isEmpty())&#123; tmp = queue.poll(); curList.add(tmp.val); curNode--; if(tmp.left != null) &#123; queue.offer(tmp.left); nextNode++; &#125; if(tmp.right != null) &#123; queue.offer(tmp.right); nextNode++; &#125; //当前层已经遍历完成 if(curNode == 0)&#123; order++; if((order &amp; 1) == 1) Collections.reverse(curList); resList.add(curList); curList = new ArrayList(); curNode = nextNode; nextNode = 0; &#125; &#125; return resList; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存概述]]></title>
    <url>%2F2019%2F07%2F03%2Fcache-summary%2F</url>
    <content type="text"><![CDATA[参考：https://github.com/CyC2018/CS-Notes 缓存特征命中率当某个请求能够通过访问缓存而得到响应时，称为缓存命中。 缓存命中率越高，缓存的利用率也就越高。 最大空间缓存通常位于内存中，内存的空间通常比磁盘空间小的多，因此缓存的最大空间不可能非常大。 当缓存存放的数据量超过最大空间时，就需要淘汰部分数据来存放新到达的数据。 淘汰策略 FIFO（First In First Out）：先进先出策略，在实时性的场景下，需要经常访问最新的数据，那么就可以使用 FIFO，使得最先进入的数据（最晚的数据）被淘汰。 LRU（Least Recently Used）：最近最久未使用策略，优先淘汰最久未使用的数据，也就是上次被访问时间距离现在最久的数据。该策略可以保证内存中的数据都是热点数据，也就是经常被访问的数据，从而保证缓存命中率。 LRU以下是基于 双向链表 + HashMap 的 LRU 算法实现，对算法的解释如下： 访问某个节点时，将其从原来的位置删除，并重新插入到链表头部。这样就能保证链表尾部存储的就是最近最久未使用的节点，当节点数量大于缓存最大空间时就淘汰链表尾部的节点。 为了使删除操作时间复杂度为 O(1)，就不能采用遍历的方式找到某个节点。HashMap 存储着 Key 到节点的映射，通过 Key 就能以 O(1) 的时间得到节点，然后再以 O(1) 的时间将其从双向队列中删除。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129public class LRU&lt;K, V&gt; implements Iterable&lt;K&gt; &#123; private Node head; private Node tail; private HashMap&lt;K, Node&gt; map; private int maxSize; private class Node &#123; Node pre; Node next; K k; V v; public Node(K k, V v) &#123; this.k = k; this.v = v; &#125; &#125; public LRU(int maxSize) &#123; this.maxSize = maxSize; this.map = new HashMap&lt;&gt;(maxSize * 4 / 3); head = new Node(null, null); tail = new Node(null, null); head.next = tail; tail.pre = head; &#125; public V get(K key) &#123; if (!map.containsKey(key)) &#123; return null; &#125; Node node = map.get(key); unlink(node); appendHead(node); return node.v; &#125; public void put(K key, V value) &#123; //如果存在，则先在双向链表中删除这个节点 if (map.containsKey(key)) &#123; Node node = map.get(key); unlink(node); &#125; Node node = new Node(key, value); map.put(key, node); appendHead(node); //数量超出maxSize，删除尾结点 if (map.size() &gt; maxSize) &#123; Node toRemove = removeTail(); map.remove(toRemove.k); &#125; &#125; private void unlink(Node node) &#123; Node pre = node.pre; Node next = node.next; pre.next = next; next.pre = pre; //下面两句代码很关键 node.pre = null; node.next = null; &#125; private void appendHead(Node node) &#123; Node next = head.next; node.next = next; next.pre = node; node.pre = head; head.next = node; &#125; //删除的是 tail 的前一个节点 private Node removeTail() &#123; //要删除的节点 Node node = tail.pre; Node pre = node.pre; tail.pre = pre; pre.next = tail; //node被释放的关键操作，必不可少 node.pre = null; node.next = null; return node; &#125; @Override public Iterator&lt;K&gt; iterator() &#123; return new Iterator&lt;K&gt;() &#123; private Node cur = head.next; @Override public boolean hasNext() &#123; return cur != tail; &#125; @Override public K next() &#123; Node node = cur; cur = cur.next; return node.k; &#125; &#125;; &#125;&#125; 缓存位置浏览器当 HTTP 响应允许进行缓存时，浏览器会将 HTML、CSS、JavaScript、图片等静态资源进行缓存。 ISP网络服务提供商（ISP）是网络访问的第一跳，通过将数据缓存在 ISP 中能够大大提高用户的访问速度。 反向代理反向代理位于服务器之前，请求与响应都需要经过反向代理。通过将数据缓存在反向代理，在用户请求反向代理时就可以直接使用缓存进行响应。 本地缓存使用 Guava Cache 将数据缓存在服务器本地内存中，服务器代码可以直接读取本地内存中的缓存，速度非常快。 分布式缓存使用 Redis、Memcache 等分布式缓存将数据缓存在分布式缓存系统中。 相对于本地缓存来说，分布式缓存单独部署，可以根据需求分配硬件资源。不仅如此，服务器集群都可以访问分布式缓存，而本地缓存需要在服务器集群之间进行同步，实现难度和性能开销上都非常大。 数据库缓存MySQL 等数据库管理系统具有自己的查询缓存机制来提高查询效率。 CDN内容分发网络（Content distribution network，CDN）是一种互连的网络系统，它利用更靠近用户的服务器从而更快更可靠地将 HTML、CSS、JavaScript、音乐、图片、视频等静态资源分发给用户。 CDN 主要有以下优点： 更快地将数据分发给用户； 通过部署多台服务器，从而提高系统整体的带宽性能； 多台服务器可以看成是一种冗余机制，从而具有高可用性。 缓存问题缓存穿透指的是对某个一定不存在的数据进行请求，该请求将会穿透缓存到达数据库。 解决方案： 对这些不存在的数据缓存一个空数据； 对这类请求进行过滤。 缓存雪崩指的是由于数据没有被加载到缓存中，或者缓存数据在同一时间大面积失效（过期），又或者缓存服务器宕机，导致大量的请求都到达数据库。 在有缓存的系统中，系统非常依赖于缓存，缓存分担了很大一部分的数据请求。当发生缓存雪崩时，数据库无法处理这么大的请求，导致数据库崩溃。 解决方案： 为了防止缓存在同一时间大面积过期导致的缓存雪崩，可以通过观察用户行为，合理设置缓存过期时间来实现(过期时间加随机值)； 为了防止缓存服务器宕机出现的缓存雪崩，可以使用分布式缓存，分布式缓存中每一个节点只缓存部分的数据，当某个节点宕机时可以保证其它节点的缓存仍然可用。 也可以进行缓存预热，避免在系统刚启动不久由于还未将大量数据进行缓存而导致缓存雪崩。 缓存一致性缓存一致性要求数据更新的同时缓存数据也能够实时更新。 解决方案： 在数据更新的同时立即去更新缓存； 在读缓存之前先判断缓存是否是最新的，如果不是最新的先进行更新。 要保证缓存一致性需要付出很大的代价，缓存数据最好是那些对一致性要求不高的数据，允许缓存数据存在一些脏数据。 数据分布哈希分布哈希分布就是将数据计算哈希值之后，按照哈希值分配到不同的节点上。例如有 N 个节点，数据的主键为 key，则将该数据分配的节点序号为：hash(key)%N。 传统的哈希分布算法存在一个问题：当节点数量变化时，也就是 N 值变化，那么几乎所有的数据都需要重新分布，将导致大量的数据迁移。 顺序分布(范围分布)将数据划分为多个连续的部分，按数据的 ID 或者时间分布到不同节点上。例如 User 表的 ID 范围为 1 ~ 7000，使用顺序分布可以将其划分成多个子表，对应的主键范围为 1 ~ 1000，1001 ~ 2000，…，6001 ~ 7000。 顺序分布相比于哈希分布的主要优点如下： 能保持数据原有的顺序； 并且能够准确控制每台服务器存储的数据量，从而使得存储空间的利用率最大。 映射表用一张表存储数据和节点的映射关系。 一致性哈希Distributed Hash Table（DHT） 是一种哈希分布方式，其目的是为了克服传统哈希分布在服务器节点数量变化时大量数据迁移的问题。 基本原理将哈希空间 [0, 2n-1] 看成一个哈希环，每个服务器节点都配置到哈希环上。每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中顺时针方向第一个大于等于该哈希值的节点上。 一致性哈希在增加或者删除节点时只会影响到哈希环中相邻的节点，例如下图中新增节点 X，只需要将它前一个节点 C 上的数据重新进行分布即可，对于节点 A、B、D 都没有影响。 虚拟节点上面描述的一致性哈希存在数据分布不均匀的问题，节点存储的数据量有可能会存在很大的不同。 数据不均匀主要是因为节点在哈希环上分布的不均匀，这种情况在节点数量很少的情况下尤其明显。 解决方式是通过增加虚拟节点，然后将虚拟节点映射到真实节点上。虚拟节点的数量比真实节点来得多，那么虚拟节点在哈希环上分布的均匀性就会比原来的真实节点好，从而使得数据分布也更加均匀。 参考资料 大规模分布式存储系统 缓存那些事 一致性哈希算法 内容分发网络 How Aspiration CDN helps to improve your website loading speed?]]></content>
      <categories>
        <category>cache</category>
      </categories>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-对称的二叉树]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-symmetric-binary-trees%2F</url>
    <content type="text"><![CDATA[题目描述请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 AC代码12345678910111213141516171819202122232425262728293031/*public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; boolean isSymmetrical(TreeNode pRoot) &#123; if(pRoot == null) return true; if(pRoot.left == null &amp;&amp; pRoot.right == null) return true; return helper(pRoot.left, pRoot.right); &#125; private boolean helper(TreeNode p, TreeNode q)&#123; if(p == null &amp;&amp; q == null) return true; if(p != null &amp;&amp; q != null)&#123; if(p.val == q.val) return helper(p.left, q.right) &amp;&amp; helper(p.right, q.left); return false; &#125; return false; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-二叉树的下一个结点]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-next-node%2F</url>
    <content type="text"><![CDATA[题目描述给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 解题思路 这个题其实求的是中序线索二叉树某一结点的下一个结点 如果一个结点有右子树，那么中序遍历中该结点的下一个结点为右子树中最左边的结点 如果一个结点没有右子树，且它的父节点的左子树是该结点，那么父节点是该结点在中序遍历中的下一个结点 如果一个结点没有右子树，且它的父节点的左子树不是该结点，则一直向上寻找父节点，直到找到一个是它父节点的左子节点的结点，该结点即为所求。 其他情况为 null。 AC代码12345678910111213141516171819202122232425262728293031323334353637/*public class TreeLinkNode &#123; int val; TreeLinkNode left = null; TreeLinkNode right = null; TreeLinkNode next = null; TreeLinkNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public TreeLinkNode GetNext(TreeLinkNode pNode) &#123; if(pNode == null) return null; TreeLinkNode tmp = null; //有右子树 if(pNode.right != null)&#123; tmp = pNode.right; while(tmp.left != null) tmp = tmp.left; return tmp; &#125; //有父节点 if(pNode.next != null)&#123; //父节点的左节点是该节点 tmp = pNode.next; if(tmp.left == pNode) return tmp; while(tmp != null &amp;&amp; tmp.left != pNode)&#123; pNode = tmp; tmp = tmp.next; &#125; return tmp; &#125; return null; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-链表中环的入口结点]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-entrance-node%2F</url>
    <content type="text"><![CDATA[题目描述给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 解题思路 假设x为环前面的路程（黑色路程），a为环入口到相遇点的路程（蓝色路程，假设顺时针走）， c为环的长度（蓝色+橙色路程）当快慢指针相遇的时候： 此时慢指针走的路程为Sslow = x + m * c + a快指针走的路程为Sfast = x + n * c + a2 Sslow = Sfast2 * ( x + mc + a ) = (x + n *c + a)从而可以推导出：x = (n - 2 * m )c - a= (n - 2 m -1 )c + c - a即环前面的路程 = 数个环的长度（为可能为0） + c - a什么是c - a？这是相遇点后，环后面部分的路程。（橙色路程）所以，我们可以让一个指针从起点A开始走，让一个指针从相遇点B开始继续往后走，2个指针速度一样，那么，当从原点的指针走到环入口点的时候（此时刚好走了x）从相遇点开始走的那个指针也一定刚好到达环入口点。所以2者会相遇，且恰好相遇在环的入口点。 最后，判断是否有环，且找环的算法复杂度为： 时间复杂度：O(n)空间复杂度：O(1) AC代码12345678910111213141516171819202122232425262728293031323334/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode EntryNodeOfLoop(ListNode pHead) &#123; //头结点为空，或者只有头结点的情况 if(pHead == null || pHead.next == null) return null; ListNode p1 = pHead;//慢指针 ListNode p2 = pHead;//快指针 while(p2 != null &amp;&amp; p2.next != null)&#123; p1 = p1.next; p2 = p2.next.next; //快慢指针相遇 if(p1 == p2)&#123; p1 = pHead; while(p1 != p2)&#123; p1 = p1.next; p2 = p2.next; &#125; return p1; &#125; &#125; return null; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-字符流中第一个不重复的字符]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-first-char%2F</url>
    <content type="text"><![CDATA[题目描述请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 输出描述如果当前字符流没有存在出现一次的字符，返回#字符。 AC代码 创建一个长度为256的char数组(ascii字符用一个字节表示)。 数组的下标表示字符（数字可以表示字符），数组中的值由3种数字组成 0，代表该字符没有出现过 -1，表示该字符出现过不止一次 其他值，表示该字符是第一次出现的字符当中的第几个 每次遍历数组，寻找只出现过一次且出现次序最小的字符。 12345678910111213141516171819202122232425public class Solution &#123; //记录一个字符第一次出现的次序 int index = 0; int[] chars = new int[256]; //Insert one char from stringstream public void Insert(char ch) &#123; if(chars[ch] == 0) chars[ch] = ++index; else chars[ch] = -1; //-1代表重复出现 &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; int min = Integer.MAX_VALUE; char c = '#'; for(int i = 0;i &lt; 255;i++)&#123; //一个字符出现过，且只出现过一次，且出现的次序比之前的小 if(chars[i] != 0 &amp;&amp; chars[i] != -1 &amp;&amp; chars[i] &lt; min)&#123; min = chars[i]; c = (char)i; &#125; &#125; return c; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-表示数值的字符串]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-num-str%2F</url>
    <content type="text"><![CDATA[题目描述请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,“5e2”,”-123”,“3.1416”和”-1E-16”都表示数值。 但是”12e”,“1a3.14”,“1.2.3”,”±5”和”12e+4.3”都不是。 AC代码 利用 java中的正则表达式匹配可以很容易的写出来 123456789101112public class Solution &#123; public boolean isNumeric(char[] str) &#123; String s=String.valueOf(str); /* 1. X? X，一次或一次也没有 2. X* X，零次或多次 3. java 正则表达式中 . 表示任何字符。若要匹配 . 要用转义 \\. 4. X+ X，一次或多次 */ return s.matches("[+-]?[0-9]*(\\.[0-9]+)?([eE][+-]?[0-9]+)?"); &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-构建乘积数组]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-product-array%2F</url>
    <content type="text"><![CDATA[题目描述给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]。不能使用除法。 解题思路 B[i]的值可以看作上图的矩阵中每行的乘积。每个B[i]都可以分为两部分（1左边的位一部分，右边的又是一部分）。下三角用连乘可以很容求得，同理可得上三角。先算下三角中的连乘，即我们先算出B[i]中的一部分，然后倒过来按上三角中的分布规律，把另一部分也乘进去。 AC代码123456789101112131415161718import java.util.ArrayList;public class Solution &#123; public int[] multiply(int[] A) &#123; if(A == null) return null; int len = A.length; int[] B = new int[len]; //下三角 B[0] = 1; for(int i = 1;i &lt; len;i++) B[i] = B[i - 1] * A[i - 1]; //上三角 int tmp = 1; for(int i = len - 2;i &gt;= 0;i--)&#123; tmp *= A[i + 1]; B[i] *= tmp; &#125; return B; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis概述]]></title>
    <url>%2F2019%2F07%2F03%2Fredis-summary%2F</url>
    <content type="text"><![CDATA[参考：https://github.com/CyC2018/CS-Notes 概述Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。 键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。 Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。 数据类型(值的类型) 数据类型 可以存储的值 操作 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 对单个或者多个元素 进行修剪，只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 What Redis data structures look like STRING set hello worldOKget hello“world”del hello(integer) 1get hello(nil) LIST 123456789101112131415161718192021&gt; rpush list-key item(integer) 1&gt; rpush list-key item2(integer) 2&gt; rpush list-key item(integer) 3&gt; lrange list-key 0 -11) "item"2) "item2"3) "item"&gt; lindex list-key 1"item2"&gt; lpop list-key"item"&gt; lrange list-key 0 -11) "item2"2) "item" SET 123456789101112131415161718192021222324252627&gt; sadd set-key item(integer) 1&gt; sadd set-key item2(integer) 1&gt; sadd set-key item3(integer) 1&gt; sadd set-key item(integer) 0&gt; smembers set-key1) "item"2) "item2"3) "item3"&gt; sismember set-key item4(integer) 0&gt; sismember set-key item(integer) 1&gt; srem set-key item2(integer) 1&gt; srem set-key item2(integer) 0&gt; smembers set-key1) "item"2) "item3" HASH 123456789101112131415161718192021222324&gt; hset hash-key sub-key1 value1(integer) 1&gt; hset hash-key sub-key2 value2(integer) 1&gt; hset hash-key sub-key1 value1(integer) 0&gt; hgetall hash-key1) "sub-key1"2) "value1"3) "sub-key2"4) "value2"&gt; hdel hash-key sub-key2(integer) 1&gt; hdel hash-key sub-key2(integer) 0&gt; hget hash-key sub-key1"value1"&gt; hgetall hash-key1) "sub-key1"2) "value1" ZSET 12345678910111213141516171819202122232425&gt; zadd zset-key 728 member1(integer) 1&gt; zadd zset-key 982 member0(integer) 1&gt; zadd zset-key 982 member0(integer) 0&gt; zrange zset-key 0 -1 withscores1) "member1"2) "728"3) "member0"4) "982"&gt; zrangebyscore zset-key 0 800 withscores1) "member1"2) "728"&gt; zrem zset-key member1(integer) 1&gt; zrem zset-key member1(integer) 0&gt; zrange zset-key 0 -1 withscores1) "member0"2) "982" 数据结构字典dictht 是一个散列表结构，使用拉链法保存哈希冲突。 123456789/* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */typedef struct dictht &#123; dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used;&#125; dictht; 12345678910typedef struct dictEntry &#123; void *key; union &#123; void *val; uint64_t u64; int64_t s64; double d; &#125; v; struct dictEntry *next;&#125; dictEntry; Redis 的字典 dict 中包含两个哈希表 dictht，这是为了方便进行 rehash 操作。在扩容时，将其中一个 dictht 上的键值对 rehash 到另一个 dictht 上面，完成之后释放空间并交换两个 dictht 的角色。 1234567typedef struct dict &#123; dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */&#125; dict; rehash 操作不是一次性完成，而是采用渐进方式，这是为了避免一次性执行过多的 rehash 操作给服务器带来过大的负担。 渐进式 rehash 通过记录 dict 的 rehashidx 完成，它从 0 开始，然后每执行一次 rehash 都会递增。例如在一次 rehash 中，要把 dict[0] rehash 到 dict[1]，这一次会把 dict[0] 上 table[rehashidx] 的键值对 rehash 到 dict[1] 上，dict[0] 的 table[rehashidx] 指向 null，并令 rehashidx++。 在 rehash 期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式 rehash。 采用渐进式 rehash 会导致字典中的数据分散在两个 dictht 上，因此对字典的查找操作也需要到对应的 dictht 去执行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* Performs N steps of incremental rehashing. Returns 1 if there are still * keys to move from the old to the new hash table, otherwise 0 is returned. * * Note that a rehashing step consists in moving a bucket (that may have more * than one key as we use chaining) from the old to the new hash table, however * since part of the hash table may be composed of empty spaces, it is not * guaranteed that this function will rehash even a single bucket, since it * will visit at max N*10 empty buckets in total, otherwise the amount of * work it does would be unbound and the function may block for a long time. */int dictRehash(dict *d, int n) &#123; int empty_visits = n * 10; /* Max number of empty buckets to visit. */ if (!dictIsRehashing(d)) return 0; while (n-- &amp;&amp; d-&gt;ht[0].used != 0) &#123; dictEntry *de, *nextde; /* Note that rehashidx can't overflow as we are sure there are more * elements because ht[0].used != 0 */ assert(d-&gt;ht[0].size &gt; (unsigned long) d-&gt;rehashidx); while (d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) &#123; d-&gt;rehashidx++; if (--empty_visits == 0) return 1; &#125; de = d-&gt;ht[0].table[d-&gt;rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while (de) &#123; uint64_t h; nextde = de-&gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask; de-&gt;next = d-&gt;ht[1].table[h]; d-&gt;ht[1].table[h] = de; d-&gt;ht[0].used--; d-&gt;ht[1].used++; de = nextde; &#125; d-&gt;ht[0].table[d-&gt;rehashidx] = NULL; d-&gt;rehashidx++; &#125; /* Check if we already rehashed the whole table... */ if (d-&gt;ht[0].used == 0) &#123; zfree(d-&gt;ht[0].table); d-&gt;ht[0] = d-&gt;ht[1]; _dictReset(&amp;d-&gt;ht[1]); d-&gt;rehashidx = -1; return 0; &#125; /* More to rehash... */ return 1;&#125; 跳跃表是有序集合的底层实现之一。 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。 在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程。 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快速，因为不需要进行旋转等操作来维护平衡性； 更容易实现； 支持无锁操作。 使用场景计数器可以对 String 进行自增自减运算，从而实现计数器功能。 Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。 缓存将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。 查找表例如 DNS 记录就很适合使用 Redis 进行存储。 查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效,因为缓存不作为可靠的数据来源。 消息队列List 是一个双向链表，可以通过 lpop 和 lpush 写入和读取消息。 不过最好使用 Kafka、RabbitMQ 等消息中间件。 会话缓存可以使用 Redis 来统一存储多台应用服务器的会话信息。 当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。 分布式锁实现在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。 可以使用 Reids 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。 其它Set 可以实现交集、并集等操作，从而实现共同好友等功能。 ZSet 可以实现有序性操作，从而实现排行榜等功能。 Redis 与 Memcached两者都是非关系型内存键值数据库，主要有以下不同： 数据类型Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型，可以更灵活地解决问题。 数据持久化Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。 分布式Memcached 不支持分布式，只能通过在客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。 Redis Cluster 实现了分布式的支持。 内存管理机制 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。 Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 键的过期时间Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。 对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。 数据淘汰策略可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。 Reids 具体有 6 种淘汰策略： 策略 描述 volatile-lru 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集中任意选择数据淘汰 allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰 allkeys-random 从所有数据集中任意选择数据进行淘汰 noeviction 禁止驱逐数据 作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法实际实现上并非针对所有 key，而是抽样一小部分并且从中选出被淘汰的 key。 使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。 Redis 4.0 引入了 volatile-lfu 和 allkeys-lfu 淘汰策略，LFU 策略通过统计访问频率，将访问频率最少的键值对淘汰。 持久化Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 RDB 持久化将某个时间点的所有数据都存放到硬盘上。 可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。 如果系统发生故障，将会丢失最后一次创建快照之后的数据。 如果数据量很大，保存快照的时间会很长。 AOF 持久化将写命令添加到 AOF 文件（Append Only File）的末尾。 使用 AOF 持久化需要设置同步选项，从而确保写命令什么时候会同步到磁盘文件上。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项： 选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统来决定何时同步 always 选项会严重减低服务器的性能； everysec 选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响； no 选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量。 随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。 事务一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其它客户端的命令请求。 事务中的多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。 Redis 最简单的事务实现方式是使用 MULTI 和 EXEC 命令将事务操作包围起来。 事件Redis 服务器是一个事件驱动程序。 文件事件服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。 Redis 基于 Reactor 模式开发了自己的网络事件处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。 时间事件服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。 事件的调度与执行服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下： 12345678910111213141516def aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： 12345678def main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 复制通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。 一个从服务器只能有一个主服务器，并且不支持主主复制。 连接过程 主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令； 从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令； 主服务器每执行一次写命令，就向从服务器发送相同的写命令。 主从链随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。 SentinelSentinel（哨兵）可以监听集群中的服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。 分片分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，这种方法在解决某些问题时可以获得线性级别的性能提升。 假设有 4 个 Reids 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，… ，有不同的方式来选择一个指定的键存储在哪个实例中。 最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。 还有一种方式是哈希分片，使用CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。 根据执行分片的位置，可以分为三种分片方式： 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。 服务器分片：Redis Cluster。 一个简单的论坛系统分析该论坛系统功能如下： 可以发布文章； 可以对文章进行点赞； 在首页可以按文章的发布时间或者文章的点赞数进行排序显示。 文章信息文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。 Redis 没有关系型数据库中的表这一概念来将同种类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。 点赞功能当有用户为一篇文章点赞时，除了要对该文章的 votes 字段进行加 1 操作，还必须记录该用户已经对该文章进行了点赞，防止用户点赞次数超过 1。可以建立文章的已投票用户集合来进行记录。 为了节约内存，规定一篇文章发布满一周之后，就不能再对它进行投票，而文章的已投票集合也会被删除，可以为文章的已投票集合设置一个一周的过期时间就能实现这个规定。 对文章进行排序为了按发布时间和点赞数进行排序，可以建立一个文章发布时间的有序集合和一个文章点赞数的有序集合。（下图中的 score 就是这里所说的点赞数；下面所示的有序集合分值并不直接是时间和点赞数，而是根据时间和点赞数间接计算出来的） 参考资料 Carlson J L. Redis in Action[J]. Media.johnwiley.com.au, 2013. 黄健宏. Redis 设计与实现 [M]. 机械工业出版社, 2014. REDIS IN ACTION Skip Lists: Done Right 论述 Redis 和 Memcached 的差异 Redis 3.0 中文版- 分片 Redis 应用场景 Using Redis as an LRU cache]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ping原理和ICMP协议]]></title>
    <url>%2F2019%2F07%2F03%2Fping-icmp%2F</url>
    <content type="text"><![CDATA[本文转载自:ping 原理与ICMP协议 ping 的原理​ ping 程序是用来探测主机到主机之间是否可通信，如果不能ping到某台主机，表明不能和这台主机建立连接。ping 使用的是ICMP协议，它发送icmp回送请求消息给目的主机。ICMP协议规定：目的主机必须返回ICMP回送应答消息给源主机。如果源主机在一定时间内收到应答，则认为主机可达。​ ICMP协议通过IP协议发送的，IP协议是一种无连接的，不可靠的数据包协议。在Unix/Linux，序列号从0开始计数，依次递增。而Windows ping程序的ICMP序列号是没有规律。​ ICMP协议在实际传输中数据包：20字节IP首部 + 8字节ICMP首部+ 数据​ ICMP报文格式:IP首部(20字节)+8位类型+8位代码+16位校验和+(不同的类型和代码，格式也有所不同) Ping工作过程​ 假定主机A的IP地址是192.168.1.1，主机B的IP地址是192.168.1.2，都在同一子网内，则当你在主机A上运行“Ping 192.168.1.2”后，都发生了些什么呢? 首先，Ping命令会构建一个固定格式的ICMP请求数据包，然后由ICMP协议将这个数据包连同地址“192.168.1.2”一起交给IP层协议（和ICMP一样，实际上是一组后台运行的进程），IP层协议将以地址“192.168.1.2”作为目的地址，本机IP地址作为源地址，加上一些其他的控制信息，构建一个IP数据包，并在一个映射表中查找出IP地址192.168.1.2所对应的物理地址（也叫MAC地址，熟悉网卡配置的朋友不会陌生，这是数据链路层协议构建数据链路层的传输单元——帧所必需的），一并交给数据链路层。后者构建一个数据帧，目的地址是IP层传过来的物理地址，源地址则是本机的物理地址，还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。其中映射表由ARP实现。ARP(Address Resolution Protocol)是地址解析协议,是一种将IP地址转化成物理地址的协议。ARP具体说来就是将网络层（IP层，也就是相当于OSI的第三层）地址解析为数据连接层（MAC层，也就是相当于OSI的第二层）的MAC地址。 主机B收到这个数据帧后，先检查它的目的地址，并和本机的物理地址对比，如符合，则接收；否则丢弃。接收后检查该数据帧，将IP数据包从帧中提取出来，交给本机的IP层协议。同样，IP层检查后，将有用的信息提取后交给ICMP协议，后者处理后，马上构建一个ICMP应答包，发送给主机A，其过程和主机A发送ICMP请求包到主机B一模一样。即先由IP地址，在网络层传输，然后再根据mac地址由数据链路层传送到目的主机。 ICMPICMP协议介绍前面讲到了，IP协议并不是一个可靠的协议，它不保证数据被送达，那么，自然的，保证数据送达的工作应该由其他的模块来完成。其中一个重要的模块就是ICMP(网络控制报文)协议。 当传送IP数据包发生错误－－比如主机不可达，路由不可达等等，ICMP协议将会把错误信息封包，然后传送回给主机。给主机一个处理错误的机会，这 也就是为什么说建立在IP层以上的协议是可能做到安全的原因。ICMP数据包由8bit的错误类型和8bit的代码和16bit的校验和组成。而前 16bit就组成了ICMP所要传递的信息。 尽管在大多数情况下，错误的包传送应该给出ICMP报文，但是在特殊情况下，是不产生ICMP错误报文的。如下 ICMP差错报文不会产生ICMP差错报文（出IMCP查询报文）（防止IMCP的无限产生和传送） 目的地址是广播地址或多播地址的IP数据报。 作为链路层广播的数据报。 不是IP分片的第一片。 源地址不是单个主机的数据报。这就是说，源地址不能为零地址、环回地址、广播地 址或多播地址。 虽然里面的一些规定现在还不是很明白，但是所有的这一切规定，都是为了防止产生ICMP报文的无限传播而定义的。 ICMP协议大致分为两类，一种是查询报文，一种是差错报文。其中查询报文有以下几种用途: ping查询 子网掩码查询（用于无盘工作站在初始化自身的时候初始化子网掩码） 时间戳查询（可以用来同步时间） 而差错报文则产生在数据传送发生错误的时候。就不赘述了。 ICMP的应用-ping（IP）ping可以说是ICMP的最著名的应用，当我们某一个网站上不去的时候。通常会ping一下这个网站。ping会回显出一些有用的信息。一般的信息如下: Reply from 10.4.24.1: bytes=32 time&lt;1ms TTL=255Reply from 10.4.24.1: bytes=32 time&lt;1ms TTL=255Reply from 10.4.24.1: bytes=32 time&lt;1ms TTL=255Reply from 10.4.24.1: bytes=32 time&lt;1ms TTL=255 Ping statistics for 10.4.24.1: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),Approximate round trip times in milli-seconds: Minimum = 0ms, Maximum = 0ms, Average = 0ms ping这个单词源自声纳定位，而这个程序的作用也确实如此，它利用ICMP协议包来侦测另一个主机是否可达。原理是用类型码为0的ICMP发请 求，受到请求的主机则用类型码为8的ICMP回应。ping程序来计算间隔时间，并计算有多少个包被送达。用户就可以判断网络大致的情况。我们可以看到， ping给出来了传送的时间和TTL的数据。我给的例子不太好，因为走的路由少，有兴趣地可以ping一下国外的网站比如sf.net，就可以观察到一些 丢包的现象，而程序运行的时间也会更加的长。ping还给我们一个看主机到目的主机的路由的机会。这是因为，ICMP的ping请求数据报在每经过一个路由器的时候，路由器都会把自己的ip放到该数据报中。而目的主机则会把这个ip列表复制到回应icmp数据包中发回给主机。但是，无论如何，ip头所能纪录的路由列表是非常的有限。如果要观察路由， 我们还是需要使用更好的工具，就是要讲到的Traceroute(windows下面的名字叫做tracert)。 ICMP的应用-Traceroute（UDP）Traceroute是用来侦测主机到目的主机之间所经路由情况的重要工具，也是最便利的工具。前面说到，尽管ping工具也可以进行侦测，但是，因为ip头的限制，ping不能完全的记录下所经过的路由器。所以Traceroute正好就填补了这个缺憾。 Traceroute的原理是非常非常的有意思，它收到目的主机的IP后，首先给目的主机发送一个TTL=1（还记得TTL是什么吗？）的UDP(后面就 知道UDP是什么了)数据包，而经过的第一个路由器收到这个数据包以后，就自动把TTL减1，而TTL变为0以后，路由器就把这个包给抛弃了，并同时产生 一个主机不可达的ICMP数据报给主机。主机收到这个数据报以后再发一个TTL=2的UDP数据报给目的主机，然后刺激第二个路由器给主机发ICMP数据 报。如此往复直到到达目的主机。这样，traceroute就拿到了所有的路由器ip。从而避开了ip头只能记录有限路由IP的问题。 有人要问，我怎么知道UDP到没到达目的主机呢？这就涉及一个技巧的问题，TCP和UDP协议有一个端口号定义，而普通的网络程序只监控少数的几个号码较 小的端口，比如说80,比如说23,等等。而traceroute发送的是端口号&gt;30000(真变态)的UDP报，所以到达目的主机的时候，目的主机只能发送一个端口不可达的ICMP数据报给主机。主机接到这个报告以后就知道，主机到了。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>ping</tag>
        <tag>ICMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-82:Remove Duplicates from Sorted List II]]></title>
    <url>%2F2019%2F07%2F03%2Fleetcode-82%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/remove-duplicates-from-sorted-list-ii/ 这道题目和leetcode-83:Remove Duplicates from Sorted List(排序链表中删除重复结点)的区别是，这道题要求把重复的数字一个都不保留，全都删除。 题目描述 题目难度：Medium Given a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Example 1 Input: 1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5Output: 1-&gt;2-&gt;5 Example 2 Input: 1-&gt;1-&gt;1-&gt;2-&gt;3Output: 2-&gt;3 AC代码1234567891011121314151617181920212223242526272829303132/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode deleteDuplicates(ListNode head) &#123; //head 为空，或者只有一个结点，那么直接返回head if(head == null || head.next == null) return head; //创建新的结点，并和原来的链表相连 ListNode newHead = new ListNode(0); newHead.next =head; ListNode pre = newHead, cur = head; while(cur != null)&#123; while(cur.next != null &amp;&amp; cur.val == cur.next.val) cur = cur.next; //cur结点没有重复的情况 if(pre.next == cur)&#123; pre = cur; cur = cur.next; &#125; //cur结点有重复的情况下，pre结点不动 else&#123; cur = cur.next; pre.next = cur; &#125; &#125; return newHead.next; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-83:Remove Duplicates from Sorted List(排序链表中删除重复结点)]]></title>
    <url>%2F2019%2F07%2F02%2Fleetcode-83%2F</url>
    <content type="text"><![CDATA[题目描述 题目难度：Easy Given a sorted linked list, delete all duplicates such that each element appear only once. Example 1 Input: 1-&gt;1-&gt;2Output: 1-&gt;2 Example 2 Input: 1-&gt;1-&gt;2-&gt;3-&gt;3Output: 1-&gt;2-&gt;3 AC代码1234567891011121314151617181920/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode deleteDuplicates(ListNode head) &#123; if(head == null) return head; ListNode curNode = head; while(curNode.next != null)&#123; if(curNode.val == curNode.next.val) curNode.next = curNode.next.next; else curNode = curNode.next; &#125; return head; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-数组中重复的数字]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-repeated-number%2F</url>
    <content type="text"><![CDATA[题目描述在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 AC代码112345678910111213141516171819202122232425public class Solution &#123; // Parameters: // numbers: an array of integers // length: the length of array numbers // duplication: (Output) the duplicated number in the array number,length of duplication array is 1,so using duplication[0] = ? in implementation; // Here duplication like pointor in C/C++, duplication[0] equal *duplication in C/C++ // 这里要特别注意~返回任意重复的一个，赋值duplication[0] // Return value: true if the input is valid, and there are some duplications in the array number // otherwise false public boolean duplicate(int numbers[],int length,int [] duplication) &#123; if(numbers == null || numbers.length == 0 || numbers.length == 1) return false; for(int i = 0;i &lt; numbers.length;i++)&#123; while(numbers[i] != i)&#123; if(numbers[numbers[i]] == numbers[i])&#123; //判断数组下标为 numbers[i] 的元素是否为 numbers[i]，是的话表示重复 duplication[0] = numbers[i]; return true; &#125; int temp = numbers[i]; //要注意这一块的交换 numbers[i] = numbers[temp]; numbers[temp] = temp; &#125; &#125; return false; &#125;&#125; AC代码2123456789101112131415161718192021222324252627public class Solution &#123; // Parameters: // numbers: an array of integers // length: the length of array numbers // duplication: (Output) the duplicated number in the array number,length of duplication array is 1,so using duplication[0] = ? in implementation; // Here duplication like pointor in C/C++, duplication[0] equal *duplication in C/C++ // 这里要特别注意~返回任意重复的一个，赋值duplication[0] // Return value: true if the input is valid, and there are some duplications in the array number // otherwise false public boolean duplicate(int numbers[],int length,int [] duplication) &#123; if(numbers == null || numbers.length == 0) return false; for(int i = 0;i &lt; length;i++)&#123; if(i == numbers[i]) continue; int j = i, temp; while(j != numbers[j])&#123; if(numbers[numbers[j]] == numbers[j])&#123; duplication[0] = numbers[j]; return true; &#125; temp = numbers[j]; numbers[numbers[j]] = numbers[j]; j = temp; &#125; &#125; return false; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中索引无效的情况]]></title>
    <url>%2F2019%2F07%2F02%2Fmysql-index-Invalid%2F</url>
    <content type="text"><![CDATA[最佳左前缀原则——如果索引了多列，要遵守最左前缀原则。指的是查询要从索引的最左前列开始并且不跳过索引中的列。违反了最佳左前缀原则，索引会失效，进行全表扫描 不在索引列上做任何操作（计算，函数【avg，max，min等】，（自动或者手动）类型装换），会导致索引失效而导致全表扫描 mysql使用不等于(!= 或者&lt;&gt;)的时候，无法使用索引，会导致索引失效 mysql中使用is not null 或者 is null会导致无法使用索引 mysql中like查询是以%开头，索引会失效变成全表扫描，覆盖索引。 mysql中，如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)。要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql使用全表扫描要比使用索引快,则不会使用到索引 参考：mysql索引失效情况]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cookie和session区别]]></title>
    <url>%2F2019%2F07%2F02%2Fcookie-session%2F</url>
    <content type="text"><![CDATA[本文转载自：你真的了解 Cookie 和 Session 吗 我在做面试官的时候，曾经问过很多朋友这个问题： Cookie 和 Session 有什么区别呢？大部分的面试者应该都可以说上一两句，比如：什么是 Cookie？什么是 Session？两者的区别等。 但如果再往深入探讨的话，就慢慢有一些朋友不太了解了，谈起原理时就很少有朋友全部回答准确。今天和大家一起深入聊聊有关 Cookie 和 Session 的话题 。 第一层楼什么是 Cookie 和 Session ?初级程序员高频面试题。 什么是 Cookie HTTP Cookie（也叫 Web Cookie或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。 Cookie 主要用于以下三个方面： 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） 什么是 Session Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。 第二层楼Cookie 和 Session 有什么不同？ 作用范围不同，Cookie 保存在客户端（浏览器），Session 保存在服务器端。 session会服务器上保存一段时间，这个会产生一个问题——当用户访问增多，会占用你服务器的资源，从而影响性能。 存取方式的不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等。 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。 存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。 前两层楼内容，绝大部分同学都可以准确回答 第三层楼为什么需要 Cookie 和 Session，他们有什么关联？ 说起来为什么需要 Cookie ，这就需要从浏览器开始说起，我们都知道浏览器是没有状态的(HTTP 协议无状态)，这意味着浏览器并不知道是张三还是李四在和服务端打交道。这个时候就需要有一个机制来告诉服务端，本次操作用户是否登录，是哪个用户在执行的操作，那这套机制的实现就需要 Cookie 和 Session 的配合。 那么 Cookie 和 Session 是如何配合的呢？我画了一张图大家可以先了解下。 用户第一次请求服务器的时候，服务器根据用户提交的相关信息，创建对应的 Session ，请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器，浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入到 Cookie 中，同时 Cookie 记录此 SessionID 属于哪个域名。 当用户第二次访问服务器的时候，请求会自动判断此域名下是否存在 Cookie 信息，如果存在自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。 根据以上流程可知，SessionID 是连接 Cookie 和 Session 的一道桥梁，大部分系统也是根据此原理来验证用户登录状态。 三层楼的内容，大部分同学可以讲清楚。 第四层楼既然服务端是根据 Cookie 中的信息判断用户是否登录，那么如果浏览器中禁止了 Cookie，如何保障整个机制的正常运转。 第一种方案，每次请求中都携带一个 SessionID 的参数，也可以 Post 的方式提交，也可以在请求的地址后面拼接 xxx?SessionID=123456...。 第二种方案，Token 机制。Token 机制多用于 App 客户端和服务器交互的模式，也可以用于 Web 端做用户状态管理。 Token 的意思是“令牌”，是服务端生成的一串字符串，作为客户端进行请求的一个标识。Token 机制和 Cookie 和 Session 的使用机制比较类似。 当用户第一次登录后，服务器根据提交的用户信息生成一个 Token，响应时将 Token 返回给客户端，以后客户端只需带上这个 Token 前来请求数据即可，无需再次登录验证。 四层楼的内容，一部分同学可以讲清楚。 第五层楼如何考虑分布式 Session 问题？ 在互联网公司为了可以支撑更大的流量，后端往往需要多台服务器共同来支撑前端用户请求，那如果用户在 A 服务器登录了，第二次请求跑到服务 B 就会出现登录失效问题。 分布式 Session 一般会有以下几种解决方案： Nginx ip_hash 策略，服务端使用 Nginx 代理，每个请求按访问 IP 的 hash 分配，这样来自同一 IP 固定访问一个后台服务器，避免了在服务器 A 创建 Session，第二次分发到服务器 B 的现象。 Session 复制，任何一个服务器上的 Session 发生改变（增删改），该节点会把这个 Session 的所有内容序列化，然后广播给所有其它节点。 共享 Session，服务端无状态话，将用户的 Session 等信息使用缓存中间件来统一管理，保障分发到每一个服务器的响应结果都一致。 建议采用第三种方案。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Cookie</tag>
        <tag>Session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http状态码]]></title>
    <url>%2F2019%2F07%2F02%2Fhttp-status-code%2F</url>
    <content type="text"><![CDATA[HTTP状态码含义： 状态码 含义 100 客户端应当继续发送请求。这个临时响应是用来通知客户端它的部分请求已经被服务器接收，且仍未被拒绝。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。 101 服务器已经理解了客户端的请求，并将通过Upgrade 消息头通知客户端采用不同的协议来完成这个请求。在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。 只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的HTTP 版本比旧版本更有优势，或者切换到一个实时且同步的协议以传送利用此类特性的资源。 102 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。 200 请求已成功，请求所希望的响应头或数据体将随此响应返回。 201 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其 URI 已经随Location 头信息返回。假如需要的资源无法及时建立的话，应当返回 ‘202 Accepted’。 202 服务器已接受请求，但尚未处理。正如它可能被拒绝一样，最终该请求可能会也可能不会被执行。在异步操作的场合下，没有比发送这个状态码更方便的做法了。 返回202状态码的响应的目的是允许服务器接受其他过程的请求（例如某个每天只执行一次的基于批处理的操作），而不必让客户端一直保持与服务器的连接直到批处理操作全部完成。在接受请求处理并返回202状态码的响应应当在返回的实体中包含一些指示处理当前状态的信息，以及指向处理状态监视器或状态预测的指针，以便用户能够估计操作是否已经完成。 203 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超级。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。响应可能通过实体头部的形式，返回新的或更新后的元信息。如果存在这些头部信息，则应当与所请求的变量相呼应。 如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。 由于204响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。 205 服务器成功处理了请求，且没有返回任何内容。但是与204响应不同，返回此状态码的响应要求请求者重置文档视图。该响应主要是被用于接受用户输入后，立即重置表单，以便用户能够轻松地开始另一次输入。 与204响应一样，该响应也被禁止包含任何消息体，且以消息头后的第一个空行结束。 206 服务器已经成功处理了部分 GET 请求。类似于 FlashGet 或者迅雷这类的 HTTP 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。 该请求必须包含 Range 头信息来指示客户端希望得到的内容范围，并且可能包含 If-Range 来作为请求条件。 响应必须包含如下的头部域： Content-Range 用以指示本次响应中返回的内容的范围；如果是 Content-Type 为 multipart/byteranges 的多段下载，则每一 multipart 段中都应包含 Content-Range 域用以指示本段的内容范围。假如响应中包含 Content-Length，那么它的数值必须匹配它返回的内容范围的真实字节数。 Date ETag 和/或 Content-Location，假如同样的请求本应该返回200响应。 Expires, Cache-Control，和/或 Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了 If-Range 强缓存验证，那么本次响应不应该包含其他实体头；假如本响应的请求使用了 If-Range 弱缓存验证，那么本次响应禁止包含其他实体头；这避免了缓存的实体内容和更新了的实体头信息之间的不一致。否则，本响应就应当包含所有本应该返回200响应中应当返回的所有实体头部域。 假如 ETag 或 Last-Modified 头部不能精确匹配的话，则客户端缓存应禁止将206响应返回的内容与之前任何缓存过的内容组合在一起。 任何不支持 Range 以及 Content-Range 头的缓存都禁止缓存206响应返回的内容。 207 由WebDAV(RFC 2518)扩展的状态码，代表之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。 300 被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。 除非这是一个 HEAD 请求，否则该响应应当包括一个资源特性及地址的列表的实体，以便用户或浏览器从中选择最合适的重定向地址。这个实体的格式由 Content-Type 定义的格式所决定。浏览器可能根据响应的格式以及浏览器自身能力，自动作出最合适的选择。当然，RFC 2616规范并没有规定这样的自动选择该如何进行。 如果服务器本身已经有了首选的回馈选择，那么在 Location 中应当指明这个回馈的 URI；浏览器可能会将这个 Location 值作为自动重定向的地址。此外，除非额外指定，否则这个响应也是可缓存的。 301 被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。除非额外指定，否则这个响应也是可缓存的。 新的永久性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，因此浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：对于某些使用 HTTP/1.0 协议的浏览器，当它们发送的 POST 请求得到了一个301响应的话，接下来的重定向请求将会变成 GET 方式。 302 请求的资源现在临时从不同的 URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：虽然RFC 1945和RFC 2068规范不允许客户端在重定向时改变请求的方法，但是很多现存的浏览器将302响应视作为303响应，并且使用 GET 方式访问在 Location 中规定的 URI，而无视原先请求的方法。状态码303和307被添加了进来，用以明确服务器期待客户端进行何种反应。 303 对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的POST请求输出重定向到一个新的资源。这个新的 URI 不是原始资源的替代引用。同时，303响应禁止被缓存。当然，第二个请求（重定向）可能被缓存。 新的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 注意：许多 HTTP/1.1 版以前的 浏览器不能正确理解303状态。如果需要考虑与这些浏览器之间的互动，302状态码应该可以胜任，因为大多数的浏览器处理302响应时的方式恰恰就是上述规范要求客户端处理303响应时应当做的。 304 如果客户端发送了一个带条件的 GET 请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。304响应禁止包含消息体，因此始终以消息头后的第一个空行结尾。 该响应必须包含以下的头信息： Date，除非这个服务器没有时钟。假如没有时钟的服务器也遵守这些规则，那么代理服务器以及客户端可以自行将 Date 字段添加到接收到的响应头中去（正如RFC 2068中规定的一样），缓存机制将会正常工作。 ETag 和/或 Content-Location，假如同样的请求本应返回200响应。 Expires, Cache-Control，和/或Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了强缓存验证，那么本次响应不应该包含其他实体头；否则（例如，某个带条件的 GET 请求使用了弱缓存验证），本次响应禁止包含其他实体头；这避免了缓存了的实体内容和更新了的实体头信息之间的不一致。 假如某个304响应指明了当前某个实体没有缓存，那么缓存系统必须忽视这个响应，并且重复发送不包含限制条件的请求。 假如接收到一个要求更新某个缓存条目的304响应，那么缓存系统必须更新整个条目以反映所有在响应中被更新的字段的值。 305 被请求的资源必须通过指定的代理才能被访问。Location 域中将给出指定的代理所在的 URI 信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能建立305响应。 注意：RFC 2068中没有明确305响应是为了重定向一个单独的请求，而且只能被原始服务器建立。忽视这些限制可能导致严重的安全后果。 306 在最新版的规范中，306状态码已经不再被使用。 307 请求的资源现在临时从不同的URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的URI 应当在响应的 Location 域中返回。除非这是一个HEAD 请求，否则响应的实体中应当包含指向新的URI 的超链接及简短说明。因为部分浏览器不能识别307响应，因此需要添加上述必要信息以便用户能够理解并向新的 URI 发出访问请求。 如果这不是一个GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 400 1、语义有误，当前请求无法被服务器理解。除非进行修改，否则客户端不应该重复提交这个请求。 2、请求参数有误。 401 当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的 Authorization 头信息的请求。如果当前请求已经包含了 Authorization 证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。参见RFC 2617。 402 该状态码是为了将来可能的需求而预留的。 403 服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个 HEAD 请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。 404 请求失败，请求所希望得到的资源未被在服务器上发现。没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。 405 请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个Allow 头信息用以表示出当前资源能够接受的请求方法的列表。 鉴于 PUT，DELETE 方法会对服务器上的资源进行写操作，因而绝大部分的网页服务器都不支持或者在默认配置下不允许上述请求方法，对于此类请求均会返回405错误。 406 请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。 除非这是一个 HEAD 请求，否则该响应就应当返回一个包含可以让用户或者浏览器从中选择最合适的实体特性以及地址列表的实体。实体的格式由 Content-Type 头中定义的媒体类型决定。浏览器可以根据格式及自身能力自行作出最佳选择。但是，规范中并没有定义任何作出此类自动选择的标准。 407 与401响应类似，只不过客户端必须在代理服务器上进行身份验证。代理服务器必须返回一个 Proxy-Authenticate 用以进行身份询问。客户端可以返回一个 Proxy-Authorization 信息头用以验证。参见RFC 2617。 408 请求超时。客户端没有在服务器预备等待的时间内完成一个请求的发送。客户端可以随时再次提交这一请求而无需进行任何更改。 409 由于和被请求的资源的当前状态之间存在冲突，请求无法完成。这个代码只允许用在这样的情况下才能被使用：用户被认为能够解决冲突，并且会重新提交新的请求。该响应应当包含足够的信息以便用户发现冲突的源头。 冲突通常发生于对 PUT 请求的处理中。例如，在采用版本检查的环境下，某次 PUT 提交的对特定资源的修改请求所附带的版本信息与之前的某个（第三方）请求向冲突，那么此时服务器就应该返回一个409错误，告知用户请求无法完成。此时，响应实体中很可能会包含两个冲突版本之间的差异比较，以便用户重新提交归并以后的新版本。 410 被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。这样的状况应当被认为是永久性的。如果可能，拥有链接编辑功能的客户端应当在获得用户许可后删除所有指向这个地址的引用。如果服务器不知道或者无法确定这个状况是否是永久的，那么就应该使用404状态码。除非额外说明，否则这个响应是可缓存的。 410响应的目的主要是帮助网站管理员维护网站，通知用户该资源已经不再可用，并且服务器拥有者希望所有指向这个资源的远端连接也被删除。这类事件在限时、增值服务中很普遍。同样，410响应也被用于通知客户端在当前服务器站点上，原本属于某个个人的资源已经不再可用。当然，是否需要把所有永久不可用的资源标记为’410 Gone’，以及是否需要保持此标记多长时间，完全取决于服务器拥有者。 411 服务器拒绝在没有定义 Content-Length 头的情况下接受请求。在添加了表明请求消息体长度的有效 Content-Length 头之后，客户端可以再次提交该请求。 412 服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。这个状态码允许客户端在获取资源时在请求的元信息（请求头字段数据）中设置先决条件，以此避免该请求方法被应用到其希望的内容以外的资源上。 413 服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。此种情况下，服务器可以关闭连接以免客户端继续发送此请求。 如果这个状况是临时的，服务器应当返回一个 Retry-After 的响应头，以告知客户端可以在多少时间以后重新尝试。 414 请求的URI 长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。这比较少见，通常的情况包括： 本应使用POST方法的表单提交变成了GET方法，导致查询字符串（Query String）过长。 重定向URI “黑洞”，例如每次重定向把旧的 URI 作为新的 URI 的一部分，导致在若干次重定向后 URI 超长。 客户端正在尝试利用某些服务器中存在的安全漏洞攻击服务器。这类服务器使用固定长度的缓冲读取或操作请求的 URI，当 GET 后的参数超过某个数值后，可能会产生缓冲区溢出，导致任意代码被执行[1]。没有此类漏洞的服务器，应当返回414状态码。 415 对于当前请求的方法和所请求的资源，请求中提交的实体并不是服务器中所支持的格式，因此请求被拒绝。 416 如果请求中包含了 Range 请求头，并且 Range 中指定的任何数据范围都与当前资源的可用范围不重合，同时请求中又没有定义 If-Range 请求头，那么服务器就应当返回416状态码。 假如 Range 使用的是字节范围，那么这种情况就是指请求指定的所有数据范围的首字节位置都超过了当前资源的长度。服务器也应当在返回416状态码的同时，包含一个 Content-Range 实体头，用以指明当前资源的长度。这个响应也被禁止使用 multipart/byteranges 作为其 Content-Type。 417 在请求头 Expect 中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服务器，它有明显的证据证明在当前路由的下一个节点上，Expect 的内容无法被满足。 421 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 请求格式正确，但是由于含有语义错误，无法响应。（RFC 4918 WebDAV）423 Locked 当前资源被锁定。（RFC 4918 WebDAV） 424 由于之前的某个请求发生的错误，导致当前请求失败，例如 PROPPATCH。（RFC 4918 WebDAV） 425 在WebDav Advanced Collections 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中。 426 客户端应当切换到TLS/1.0。（RFC 2817） 449 由微软扩展，代表请求应当在执行完适当的操作后进行重试。 500 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。 501 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。 502 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。如果能够预计延迟时间，那么响应中可以包含一个 Retry-After 头用以标明这个延迟时间。如果没有给出这个 Retry-After 信息，那么客户端应当以处理500响应的方式处理它。 注意：503状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。 504 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。 注意：某些代理服务器在DNS查询超时时会返回400或者500错误 505 服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本。这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。 506 由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误：被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。 507 服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。WebDAV (RFC 4918) 509 服务器达到带宽限制。这不是一个官方的状态码，但是仍被广泛使用。 510 获取资源所需要的策略并没有没满足。（RFC 2774） 参考：http://tool.oschina.net/commons?type=5]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-236:Lowest Common Ancestor of a Binary Tree（LCA，最近公共祖先节点）]]></title>
    <url>%2F2019%2F07%2F02%2Fleetcode-236%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/ 题目描述 题目难度：Medium Given a binary tree, find the lowest common ancestor (LCA) of two given nodes in the tree. According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).” Given the following binary tree: root = [3,5,1,6,2,0,8,null,null,7,4] Example 1Input: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1Output: 3Explanation: The LCA of nodes 5 and 1 is 3. Example 2Input: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4Output: 5Explanation: The LCA of nodes 5 and 4 is 5, since a node can be a descendant of itself according to the LCA definition. NoteAll of the nodes’ values will be unique.p and q are different and both values will exist in the binary tree. AC代码1最简单粗暴的解法：从下往上依次遍历树中每个节点，第一次出现子树(包括节点本身)既有p又有q的结点为LCA所求结点。时间复杂度：O(n * n) 123456789101112131415161718192021222324class Solution &#123; private TreeNode resNode = null;public TreeNode findCommonNode(TreeNode root, TreeNode p, TreeNode q)&#123; if(root == null) return null; if(p == null &amp;&amp; q == null) return null; if(root.left != null) findCommonNode(root.left, p, q); if(root.right != null) findCommonNode(root.right, p, q); if(hasNode(root, p) &amp;&amp; hasNode(root, q)) if(resNode == null) resNode = root; return resNode;&#125;private boolean hasNode(TreeNode root, TreeNode node)&#123; if(root == null) return false; if(root == node) return true; if(root.left == node || root.right == node) return true; return hasNode(root.left, node) || hasNode(root.right, node);&#125; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; return findCommonNode(root, p, q); &#125;&#125; AC代码2leetcode大神的解法，以递归的方式解决LCA问题。时间复杂度：O(n)空间复杂度：O(n)。主要是递归栈的深度。 12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; /** 以下三种情况会返回： 1. root == null，return null 2. root 只等于 p 或者 q 其中一个结点 3. q 和 p 相等的情况下，root == q == p */ if(root == null || root == p || root == q) &#123; return root; &#125; TreeNode left = lowestCommonAncestor(root.left, p , q); TreeNode right = lowestCommonAncestor(root.right, p , q); /** 1. 如果 left == null，返回 right(不管right是否为null) 2. left != null，right == null，返回 left 3. left 和 right 都不等于null，说明 left 和 right 只和 q 或者 p 其中一个结点相等，root为p和q的最近公共子节点，返回root */ return left == null ? right: right == null ? left: root;&#125;&#125; AC代码3建立两队列，一个队列保存从根节点到p的路径，另一个队列保存从根节点到q的路径。两个路径中第一个不相同的节点的前一个节点即为LCA所求。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root == null || root == p || root == q) &#123; return root; &#125; LinkedList&lt;TreeNode&gt; q1 = new LinkedList&lt;&gt;(); LinkedList&lt;TreeNode&gt; q2 = new LinkedList&lt;&gt;(); getTrace(root, q1, p); getTrace(root, q2, q); return LCA(q1, q2);&#125; private boolean getTrace(TreeNode root, LinkedList&lt;TreeNode&gt; queue, TreeNode node)&#123; if(root == null) return false; if(root == node)&#123; queue.offer(root); return true; &#125; queue.offer(root); if(getTrace(root.left, queue, node) || getTrace(root.right, queue, node)) return true; queue.pollLast(); return false; &#125; private TreeNode LCA(Queue&lt;TreeNode&gt; q1, Queue&lt;TreeNode&gt; q2)&#123; TreeNode node = null; while(true)&#123; TreeNode n1 = q1.poll(); TreeNode n2 = q2.poll(); if(n1 == n2) node = n1; else break; &#125; return node; &#125;&#125; AC代码4离线Tarjan算法 后续有时间再研究。]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-23:Merge k Sorted Lists(合并k个有序链表)]]></title>
    <url>%2F2019%2F07%2F02%2Fleetcode-23%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/merge-k-sorted-lists/ 题目描述 题目难度：Hard Merge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity. ExampleInput: [1-&gt;4-&gt;5,1-&gt;3-&gt;4,2-&gt;6] Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 AC代码leetcode 优秀的解法：灵活运用二路归并排序的思想。 时间复杂度：O(log(k) * n) ,k为链表长度，n为单个链表长度 12345678910111213141516171819202122232425262728293031323334353637383940/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode mergeKLists(ListNode[] lists) &#123; return partition(lists, 0, lists.length - 1); &#125; private ListNode partition(ListNode[] lists, int start, int end)&#123; if(start == end) return lists[start]; if(start &lt; end)&#123; int mid = (start + end) &gt;&gt; 1; ListNode l1 = partition(lists, start, mid); ListNode l2 = partition(lists, mid + 1, end); return mergeCore(l1, l2); &#125; else return null; &#125; //合并两个链表，返回合并后的头结点 private ListNode mergeCore(ListNode l1, ListNode l2)&#123; if(l1 != null &amp;&amp; l2 != null)&#123; if(l1.val &lt; l2.val)&#123; l1.next = mergeCore(l1.next, l2); return l1; &#125; else&#123; l2.next = mergeCore(l1, l2.next); return l2; &#125; &#125; if(l1 != null) return l1; else return l2; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-不用加减乘除做加法]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-addition2%2F</url>
    <content type="text"><![CDATA[题目描述写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 AC代码用位运算 1234567891011121314public class Solution &#123; public int Add(int num1,int num2) &#123; int row_sum = 0; // 不含进位的和 int carry = 0; // 进位 while(num2 != 0)&#123; // 当进位为 0 时停止循环，条件row_sum + carry == num1 + num2 始终成立 row_sum = num1 ^ num2; // 不含进位的和可以用异或 ^ 实现 carry = (num1 &amp; num2) &lt;&lt; 1; // 通过 &amp; ，可以确定哪里有进位，然后左移一位表示进位 //计算 row_sum 和进位的和 num1 = row_sum; num2 = carry; &#125; return num1; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-求1+2+3+...+n]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-addition%2F</url>
    <content type="text"><![CDATA[题目描述求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 AC代码利用递归的思想 短路与&amp;&amp;可以充当if else的功能 1234567public class Solution &#123; public int Sum_Solution(int n) &#123; int sum = n; boolean ans = (n &gt; 0) &amp;&amp; ((sum += Sum_Solution(n - 1)) &gt; 0); return sum; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-孩子们的游戏(圆圈中最后剩下的数)]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-Joseph-ring%2F</url>
    <content type="text"><![CDATA[题目描述每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) AC代码1这道题其实就是一个约瑟夫环。 1234567891011121314151617181920public class Solution &#123; public int LastRemaining_Solution(int n, int m) &#123; if(n == 0 || m == 0) return -1; boolean[] selected = new boolean[n]; int count = 0; int cur = -1; //循环次数为 n - 1次，会把selected中n-1个数置为true，剩下的最后一个false即为所求 while(count != n - 1)&#123; for(int i = 0;i &lt; m;i++)&#123; cur = (cur + 1) % n; while(selected[cur] == true) cur = (cur + 1) % n; &#125; selected[cur] = true; count++; &#125; for(int i = 0;i &lt; n;i++) if(selected[i] == false) return i; return -1; &#125;&#125; AC代码2原理请看：https://blog.csdn.net/crazy__chen/article/details/45115911 12345678910111213public class Solution&#123; public int LastRemaining_Solution(int n, int m) &#123; if(n==0||m==0)return -1; int s=0; for(int i=2;i&lt;=n;i++) &#123; s=(s+m)%i; &#125; return s ; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-扑克牌顺子]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-Playing-cards%2F</url>
    <content type="text"><![CDATA[题目描述LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 解题思路 将原数组升序排序 计算0的个数zeros 计算两两非0元素之间的间隔之和gaps（1和3之间的间隔为3 - 1 - 1 = 1，代表数字1和3之间插入一个数字2，可形成顺子），如果出现两个非0元素相等，直接返回false（不可能形成顺子） 计算zeros和gaps的差值。如果zeros大于等于gaps，表示zeros可以填补形成顺子所需的数字，返回true；否则返回false。 AC代码12345678910111213141516171819import java.util.Arrays; public class Solution &#123; public boolean isContinuous(int [] numbers) &#123; if(numbers == null || numbers.length == 0) return false; Arrays.sort(numbers); //升序排序 int numsOfZero = 0;//0的个数 int gaps = 0;//间隔的个数 for(int i = 0;i &lt; numbers.length;i++)&#123; //两相邻数字相等，不可能形成顺子 if(i != 0 &amp;&amp; numbers[i] != 0 &amp;&amp; numbers[i - 1] == numbers[i]) return false; //间隔的计算，两相邻数字都不为0，且相差要大于1 if(i != 0 &amp;&amp; numbers[i - 1] != 0 &amp;&amp; numbers[i] - numbers[i - 1] != 1) gaps += numbers[i] - numbers[i - 1] - 1; if(numbers[i] == 0) numsOfZero++; &#125; if(gaps &lt;= numsOfZero) return true; return false; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库中聚集索引和非聚集索引的区别]]></title>
    <url>%2F2019%2F07%2F02%2Fdatabase-Clustered-index-Nonclustered-index%2F</url>
    <content type="text"><![CDATA[本文转载自：聚集索引和非聚集索引 索引简介众所周知，索引是关系型数据库中给数据库表中一列或多列的值排序后的存储结构，SQL的主流索引结构有B+树以及Hash结构，聚集索引以及非聚集索引用的是B+树索引。 SQL Sever索引类型有：唯一索引，主键索引，聚集索引，非聚集索引。 MySQL 索引类型有：唯一索引，主键（聚集）索引，非聚集索引，全文索引。 聚集索引聚集（clustered）索引，也叫聚簇索引。 定义：数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引。 由于聚集索引规定数据在表中的物理存储顺序，因此一个表只能包含一个聚集索引 单单从定义来看是不是显得有点抽象，打个比方，一个表就像是我们以前用的新华字典，聚集索引就像是拼音目录，而每个字存放的页码就是我们的数据物理地址，我们如果要查询一个“哇”字，我们只需要查询“哇”字对应在新华字典拼音目录对应的页码，就可以查询到对应的“哇”字所在的位置，而拼音目录对应的A-Z的字顺序，和新华字典实际存储的字的顺序A-Z也是一样的，如果我们中文新出了一个字，拼音开头第一个是B，那么他插入的时候也要按照拼音目录顺序插入到A字的后面。 非聚集索引非聚集（unclustered）索引。 定义：该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同，一个表中可以拥有多个非聚集索引。 其实按照定义，除了聚集索引以外的索引都是非聚集索引，只是人们想细分一下非聚集索引，分成普通索引，唯一索引，全文索引。如果非要把非聚集索引类比成现实生活中的东西，那么非聚集索引就像新华字典的偏旁字典，他结构顺序与实际存放顺序不一定一致。 聚集索引和非聚集索引的区别 聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个 聚集索引存储记录是物理上连续存在，而非聚集索引是逻辑上的连续，物理存储并不连续 聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。 聚集索引表记录的排列顺序与索引的排列顺序一致 优点是查询速度快，可以进行范围查找，因为一旦具有第一个索引值的纪录被找到，具有连续索引值的记录也一定物理的紧跟其后。 缺点是对表进行修改速度较慢，这是为了保持表中的记录的物理顺序与索引的顺序一致，而把记录插入到数据页的相应位置，必须在数据页中进行数据重排， 降低了执行速度。 建议使用聚集索引的场合为：a. 此列包含有限数目的不同值；b. 查询的结果返回一个区间的值；c. 查询的结果返回某值相同的大量结果集。 非聚集索引指定了表中记录的逻辑顺序，但记录的物理顺序和索引的顺序不一致 聚集索引和非聚集索引都采用了B+（平衡树）树的结构，但非聚集索引的叶子层并不与实际的数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针的方式。非聚集索引比聚集索引层次多，添加记录不会引起数据顺序的重组。建议使用非聚集索引的场合为：a. 此列包含了大量数目不同的值；b. 查询的结束返回的是少量的结果集；c. order by 子句中使用了该列。 表中经常有一个列或列的组合，其值能唯一地标识表中的每一行。这样的一列或多列称为表的主键.(默认为聚集索引) 聚集索引确定表中数据的物理顺序。聚集索引类似于电话簿，后者按姓氏排列数据。但该索引可以包含多个列（组合索引），就像电话簿按姓氏和名字进行组织一样。 聚集索引相当于我们书本上前面的目录的一样，它可以方便快速的找到你想找的内容，而非聚集索引就相当于书最后几页的解释，它是对书中某个语句或者是生词的解释，就像我们上学时候的地理说一样，书后面都有各种地理名称的英文解释；《数据库原理》里面的解释：聚集索引的顺序就是数据的物理存储顺序，而非聚集索引的顺序和数据物理排列无关。因为数据在物理存放时只能有一种排列方式，所以一个表只能有一个聚集索引。在SQL SERVER中，索引是通过二叉树的数据结构来描述的；我们可以如此理解这个两种索引：聚集索引的叶节点就是数据节点，而非聚集索引的叶节点仍然是索引节点，只不过其包含一个指向对应数据块的指针。聚集索引会降低 insert，和update操作的性能，所以，是否使用聚集索引要全面衡量。 参考文章：https://www.jianshu.com/p/5681ebd5b0efhttps://www.cnblogs.com/weixing/p/4317774.htmlhttps://www.cnblogs.com/s-b-b/p/8334593.html]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>聚集索引</tag>
        <tag>非聚集索引</tag>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-翻转单词顺序列]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-flip-word-order%2F</url>
    <content type="text"><![CDATA[题目描述牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ AC代码123456789101112131415161718192021222324252627282930public class Solution &#123; public String ReverseSentence(String str) &#123; if(str == null || str.length() == 0) return str; char[] chars = str.toCharArray(); int start = -1, end = -1; if(chars[0] != ' ') start = 0; for(int i = 0;i &lt; chars.length;i++)&#123; if(i &gt; 0 &amp;&amp; chars[i] != ' ' &amp;&amp; chars[i - 1] == ' ') start = i; if(chars[i] != ' ')&#123; if((i + 1 &lt; chars.length &amp;&amp; chars[i + 1] == ' ') || i + 1 == chars.length)&#123; end = i; reverse(chars, start, end); &#125; &#125; &#125; reverse(chars, 0, chars.length - 1); return new String(chars); &#125; //翻转chars数组中start至end(包括start和end)之间的字符 private void reverse(char[] chars, int start, int end)&#123; while(start &lt; end)&#123; char c = chars[start]; chars[start] = chars[end]; chars[end] = c; start++; end--; &#125; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引原理]]></title>
    <url>%2F2019%2F07%2F02%2Fdatabase-index-principle%2F</url>
    <content type="text"><![CDATA[本文转载自：深入浅出数据库索引原理 使用索引很简单，只要能写创建表的语句，就肯定能写创建索引的语句，要知道这个世界上是不存在不会创建表的服务器端程序员的。然而， 会使用索引是一回事， 而深入理解索引原理又能恰到好处使用索引又是另一回事，这完全是两个天差地别的境界（我自己也还没有达到这层境界）。很大一部份程序员对索引的了解仅限于到“加索引能使查询变快”这个概念为止。 为什么要给表加上主键？ 为什么加索引后会使查询变快？ 为什么加索引后会使写入、修改、删除变慢？ 什么情况下要同时在两个字段上建索引？ 这些问题他们可能不一定能说出答案。知道这些问题的答案有什么好处呢？如果开发的应用使用的数据库表中只有1万条数据，那么了解与不了解真的没有差别， 然而， 如果开发的应用有几百上千万甚至亿级别的数据，那么不深入了解索引的原理， 写出来程序就根本跑不动，就好比如果给货车装个轿车的引擎，这货车还能拉的动货吗？ 接下来就讲解一下上面提出的几个问题，希望对阅读者有帮助。 网上很多讲解索引的文章对索引的描述是这样的「索引就像书的目录， 通过书的目录就准确的定位到了书籍具体的内容」，这句话描述的非常正确， 但就像脱了裤子放屁，说了跟没说一样，通过目录查找书的内容自然是要比一页一页的翻书找来的快，同样使用的索引的人难到会不知道，通过索引定位到数据比直接一条一条的查询来的快，不然他们为什么要建索引。 想要理解索引原理必须清楚一种数据结构「平衡树」(非二叉)，也就是b tree或者 b+ tree，重要的事情说三遍：“平衡树，平衡树，平衡树”。当然， 有的数据库也使用哈希桶作用索引的数据结构 ， 然而， 主流的RDBMS都是把平衡树当做数据表默认的索引数据结构的。 我们平时建表的时候都会为表加上主键， 在某些关系数据库中， 如果建表时不指定主键，数据库会拒绝建表的语句执行。 事实上， 一个没加主键的表，并不能被称之为「表」。一个没加主键的表，它的数据无序的放置在磁盘存储器上，一行一行的排列的很整齐， 跟我认知中的「表」很接近。如果给表加上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是上面说的「平衡树」结构，换句话说，就是整个表就变成了一个索引。没错， 再说一遍， 整个表变成了一个索引，也就是所谓的「聚集索引」。 这就是为什么一个表只能有一个主键， 一个表只能有一个「聚集索引」，因为主键的作用就是把「表」的数据格式转换成「索引（平衡树）」的格式放置。 上图就是带有主键的表（聚集索引）的结构图。图画的不是很好， 将就着看。其中树的所有结点（底部除外）的数据都是由主键字段中的数据构成，也就是通常我们指定主键的id字段。最下面部分是真正表中的数据。 假如我们执行一个SQL语句： select * from table where id = 1256; 首先根据索引定位到1256这个值所在的叶结点，然后再通过叶结点取到id等于1256的数据行。 这里不讲解平衡树的运行细节， 但是从上图能看出，树一共有三层， 从根节点至叶节点只需要经过三次查找就能得到结果。如下图 假如一张表有一亿条数据 ，需要查找其中某一条数据，按照常规逻辑， 一条一条的去匹配的话， 最坏的情况下需要匹配一亿次才能得到结果，用大O标记法就是O(n)最坏时间复杂度，这是无法接受的，而且这一亿条数据显然不能一次性读入内存供程序使用， 因此， 这一亿次匹配在不经缓存优化的情况下就是一亿次IO开销，以现在磁盘的IO能力和CPU的运算能力， 有可能需要几个月才能得出结果 。如果把这张表转换成平衡树结构（一棵非常茂盛和节点非常多的树），假设这棵树有10层，那么只需要10次IO开销就能查找到所需要的数据， 速度以指数级别提升，用大O标记法就是O(log n)，n是记录总树，底数是树的分叉数，结果就是树的层次数。换言之，查找次数是以树的分叉数为底，记录总数的对数，用公式来表示就是 用程序来表示就是Math.Log(100000000,10)，100000000是记录数，10是树的分叉数（真实环境下分叉数远不止10）， 结果就是查找次数，这里的结果从亿降到了个位数。因此，利用索引会使数据库查询有惊人的性能提升。 然而， 事物都是有两面的， 索引能让数据库查询数据的速度上升， 而使写入数据的速度下降，原因很简单的， 因为平衡树这个结构必须一直维持在一个正确的状态， 增删改数据都会改变平衡树各节点中的索引数据内容，破坏树结构， 因此，在每次数据改变时， DBMS必须去重新梳理树（索引）的结构以确保它的正确，这会带来不小的性能开销，也就是为什么索引会给查询以外的操作带来副作用的原因。 讲完聚集索引 ， 接下来聊一下非聚集索引， 也就是我们平时经常提起和使用的常规索引。 非聚集索引和聚集索引一样， 同样是采用平衡树作为索引的数据结构。索引树结构中各节点的值来自于表中的索引字段， 假如给user表的name字段加上索引 ， 那么索引就是由name字段中的值构成，在数据改变时， DBMS需要一直维护索引结构的正确性。如果给表中多个字段加上索引 ， 那么就会出现多个独立的索引结构，每个索引（非聚集索引）互相之间不存在关联。 如下图 每次给字段建一个新索引， 字段中的数据就会被复制一份出来， 用于生成索引。 因此， 给表添加索引，会增加表的体积， 占用磁盘存储空间。 非聚集索引和聚集索引的区别在于， 通过聚集索引可以查到需要查找的数据， 而通过非聚集索引可以查到记录对应的主键值 ， 再使用主键的值通过聚集索引查找到需要的数据，如下图 不管以任何方式查询表， 最终都会利用主键通过聚集索引来定位到数据， 聚集索引（主键）是通往真实数据所在的唯一路径。 然而， 有一种例外可以不使用聚集索引就能查询出所需要的数据， 这种非主流的方法 称之为「覆盖索引」查询， 也就是平时所说的复合索引或者多字段索引查询。 文章上面的内容已经指出， 当为字段建立索引以后， 字段中的内容会被同步到索引之中， 如果为一个索引指定两个字段， 那么这个两个字段的内容都会被同步至索引之中。 先看下面这个SQL语句 //建立索引create index index_birthday on user_info(birthday);//查询生日在1991年11月1日出生用户的用户名select user_name from user_info where birthday = ‘1991-11-1’ 这句SQL语句的执行过程如下 首先，通过非聚集索引index_birthday查找birthday等于1991-11-1的所有记录的主键ID值 然后，通过得到的主键ID值执行聚集索引查找，找到主键ID值对就的真实数据（数据行）存储的位置 最后， 从得到的真实数据中取得user_name字段的值返回， 也就是取得最终的结果 我们把birthday字段上的索引改成双字段的覆盖索引 create index index_birthday_and_user_name on user_info(birthday, user_name); 这句SQL语句的执行过程就会变为 通过非聚集索引index_birthday_and_user_name查找birthday等于1991-11-1的叶节点的内容，然而， 叶节点中除了有user_name表主键ID的值以外， user_name字段的值也在里面， 因此不需要通过主键ID值的查找数据行的真实所在， 直接取得叶节点中user_name的值返回即可。 通过这种覆盖索引直接查找的方式， 可以省略不使用覆盖索引查找的后面两个步骤， 大大的提高了查询性能，如下图 数据库索引的大致工作原理就是像文中所述， 然而细节方面可能会略有偏差，这但并不会对概念阐述的结果产生影响 。 最后， 推荐三本关系数据库方面的书籍， 文中所讲解的概念内容都是来自于此。 《SQL Server2005技术内幕之T-SQL查询》 这本书虽然是针对SQL Server写的， 但是里面的大部份内容同样适用于其它关系数据库，此书对查询编写的技巧和优化讲解的非常透彻。 《关系数据库系统概论》第四版 王珊和萨师煊写的那本， 是大学计算机教材， 讲的通俗易懂， 在国内计算机书图书出版领域质量是排的上号的。 《数据库系统概念》 这本书在数据库领域非常出名， 被称之为帆船书， 书中内容博大精深，非一朝一夕可参透的。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode_416:Partition Equal Subset Sum]]></title>
    <url>%2F2019%2F07%2F01%2Fleetcode-416%2F</url>
    <content type="text"><![CDATA[Partition Equal Subset Sum 题目链接：https://leetcode.com/problems/partition-equal-subset-sum/ 题目描述Given a non-empty array containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal. Note: Each of the array element will not exceed 100. The array size will not exceed 200. 题目难度:Medium 测试用例Example 1 Input: [1, 5, 11, 5]Output: trueExplanation: The array can be partitioned as [1, 5, 5] and [11]. Example 2 Input: [1, 2, 3, 5]Output: falseExplanation: The array cannot be partitioned into equal sum subsets. AC代码1123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123; public boolean canPartition(int[] nums) &#123; int sum = 0; for (int num : nums) &#123; sum += num; &#125; //和为奇数，直接返回false if ((sum &amp; 1) == 1) &#123; return false; &#125; Arrays.sort(nums); return dfs(nums, sum/2, 0); &#125; /** function: 判断在nums数组中，下标从 start开始到数组结束，是否存在和为 target 的子集（可以不连续） nums: 原始数组 target: 目标值，即nums数组中选择子集，使其和为target, start: 数目遍历的开始索引 return: nums数组中存在子集，使其和为target,返回true;否则返回false */ private boolean dfs(int[] nums, int target, int start) &#123; if(target == 0) return true; for(int i = start; i &lt; nums.length; i++) &#123; if(target - nums[i] &lt; 0) break; //提前结束 if(i != start &amp;&amp; nums[i] == nums[i-1]) //很关键的一步判断，防止做多余的操作 continue; if(dfs(nums, target - nums[i], i+1)) return true; &#125; return false; &#125; &#125; AC代码2该问题我们可以利用0-1背包问题的思想进行求解。 0-1背包问题请看这里：0-1背包问题 假设给定元素个数为n的数组nums，数组元素的和为sum，对应于背包问题，等价于有n个物品，每个物品的重量和价值均为nums[i]，背包的限重为sum/2，求解背包中的物品价值是否可以为sum/2？ 12345678910111213141516171819202122232425262728293031class Solution &#123; private boolean knapSack(int[] nums,int sum)&#123; int size = nums.length; //dp[i] 表示下标从0到i(包括i)的所有数中是否存在和为i的子集。 boolean[] dp = new boolean[sum + 1]; for (int i = 0;i &lt;= sum;i ++)&#123; dp[i] = i == nums[0]; &#125; for (int i = 1;i &lt; size;i++)&#123; for (int j = sum;j &gt;= nums[i];j--)&#123; dp[j] = dp[j] || dp[j-nums[i]]; &#125; &#125; return dp[sum]; &#125; public boolean canPartition(int[] nums) &#123; int sum = 0; for (int item : nums)&#123; sum += item; &#125; //如果数组元素和不是2的倍数，直接返回false if ((sum &amp; 1) == 1) &#123; return false; &#125; return knapSack(nums,sum/2); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0-1背包问题]]></title>
    <url>%2F2019%2F07%2F01%2F0-1-Backpack-problem%2F</url>
    <content type="text"><![CDATA[本文转载自：彻底理解0-1背包问题 0-1背包问题给定n个重量为w1,w2,w3,…,wn,价值为v1,v2,v3,…,vn,的物品和容量为C的背包，求这个物品中一个最有价值的子集，使得在满足背包的容量的前提下，包内的总价值最大。 0-1背包问题指的是每个物品只能使用一次 递归方法首先我们用递归的方式来尝试解决这个问题。我们用F(n,C) 表示将前n个物品放进容量为C的背包里，得到的最大的价值。 我们用自顶向下的角度来看，假如我们已经进行到了最后一步（即求解将n个物品放到背包里获得的最大价值），此时我们便有两种选择: 不放第n个物品，此时总价值为F(n−1,C) 放置第n个物品，此时总价值为vn+F(n−1,C−wn) 两种选择中总价值最大的方案就是我们的最终方案，递推式（有时也称之为状态转移方程）如下: F(i,C)=max(F(i−1,C),v(i)+F(i−1,C−w(i))) (前提是 w[i] &lt;= C) 代码如下： 1234567891011121314151617181920212223242526272829303132333435public class KnapSack01 &#123; /** * 解决背包问题的递归函数 * * @param w 物品的重量数组 * @param v 物品的价值数组 * @param index 当前待选择的物品索引 * @param capacity 当前背包有效容量 * @return 最大价值 */ private static int solveKS(int[] w, int[] v, int index, int capacity) &#123; //基准条件：如果索引无效或者容量不足，直接返回当前价值0 if (index &lt; 0 || capacity &lt;= 0) return 0; //不放第index个物品所得价值 int res = solveKS(w, v, index - 1, capacity); //放第index个物品所得价值（前提是：第index个物品可以放得下） if (w[index] &lt;= capacity) &#123; res = Math.max(res, v[index] + solveKS(w, v, index - 1, capacity - w[index])); &#125; return res; &#125; public static int knapSack(int[] w, int[] v, int C) &#123; int size = w.length; return solveKS(w, v, size - 1, C); &#125; public static void main(String[] args)&#123; int[] w = &#123;2,1,3,2&#125;; int[] v = &#123;12,10,20,15&#125;; System.out.println(knapSack(w,v,5)); &#125;&#125; 记忆化搜索我们用递归方法可以很简单的实现以上代码，但是有个严重的问题就是，直接采用自顶向下的递归算法会导致要不止一次的解决公共子问题，因此效率是相当低下的。我们可以将已经求得的子问题的结果保存下来，这样对子问题只会求解一次，这便是记忆化搜索。下面在上述代码的基础上加上记忆化搜索 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class KnapSack01 &#123; private static int[][] memo; /** * 解决背包问题的递归函数 * * @param w 物品的重量数组 * @param v 物品的价值数组 * @param index 当前待选择的物品索引 * @param capacity 当前背包有效容量 * @return 最大价值 */ private static int solveKS(int[] w, int[] v, int index, int capacity) &#123; //基准条件：如果索引无效或者容量不足，直接返回当前价值0 if (index &lt; 0 || capacity &lt;= 0) return 0; //如果此子问题已经求解过，则直接返回上次求解的结果 if (memo[index][capacity] != 0) &#123; return memo[index][capacity]; &#125; //不放第index个物品所得价值 int res = solveKS(w, v, index - 1, capacity); //放第index个物品所得价值（前提是：第index个物品可以放得下） if (w[index] &lt;= capacity) &#123; res = Math.max(res, v[index] + solveKS(w, v, index - 1, capacity - w[index])); &#125; //添加子问题的解，便于下次直接使用 memo[index][capacity] = res; return res; &#125; public static int knapSack(int[] w, int[] v, int C) &#123; int size = w.length; memo = new int[size][C + 1]; return solveKS(w, v, size - 1, C); &#125; public static void main(String[] args) &#123; int[] w = &#123;2, 1, 3, 2&#125;; int[] v = &#123;12, 10, 20, 15&#125;; System.out.println(knapSack(w, v, 5)); &#125;&#125; 动态规划算法 12345678910111213141516171819202122232425262728293031public class KnapSack01 &#123; public static int knapSack(int[] w, int[] v, int C) &#123; int size = w.length; if (size == 0) &#123; return 0; &#125; int[][] dp = new int[size][C + 1]; //初始化第一行 //仅考虑容量为i的背包放第0个物品的情况 for (int i = 0; i &lt;= C; i++) &#123; dp[0][i] = w[0] &lt;= i ? v[0] : 0; &#125; //填充其他行和列 for (int i = 1; i &lt; size; i++) &#123; for (int j = 0; j &lt;= C; j++) &#123; dp[i][j] = dp[i - 1][j]; if (w[i] &lt;= j) &#123; dp[i][j] = Math.max(dp[i][j], v[i] + dp[i - 1][j - w[i]]); &#125; &#125; &#125; return dp[size - 1][C]; &#125; public static void main(String[] args) &#123; int[] w = &#123;2, 1, 3, 2&#125;; int[] v = &#123;12, 10, 20, 15&#125;; System.out.println(knapSack(w, v, 5)); &#125;&#125; 空间复杂度的极致优化上面的动态规划算法使用了O(n*C)的空间复杂度（因为我们使用了二维数组来记录子问题的解），其实我们完全可以只使用一维数组来存放结果，但同时我们需要注意的是，为了防止计算结果被覆盖，我们必须从后向前分别进行计算。 我们仍然假设背包空间为5，根据 F(i,C)=max(F(i−1,C),v(i)+F(i−1,C−w(i))) 我们可以知道，当我们利用一维数组进行记忆化的时候，我们只需要使用到当前位置的值和该位置之前的值，举个例子假设我们要计算F(i,4),我们需要用到的值为F(i−1,4) ,和F(i−1,4−w(i)),因此为了防止结果被覆盖，我们需要从后向前依次计算结果最终的动态规划代码如下: 12345678910111213141516171819202122232425262728public class KnapSack01 &#123; public static int knapSack(int[] w, int[] v, int C) &#123; int size = w.length; if (size == 0) &#123; return 0; &#125; int[] dp = new int[C + 1]; //初始化第一行 //仅考虑容量为C的背包放第0个物品的情况 for (int i = 0; i &lt;= C; i++) &#123; dp[i] = w[0] &lt;= i ? v[0] : 0; &#125; for (int i = 1; i &lt; size; i++) &#123; for (int j = C; j &gt;= w[i]; j--) &#123; dp[j] = Math.max(dp[j], v[i] + dp[j - w[i]]); &#125; &#125; return dp[C]; &#125; public static void main(String[] args) &#123; int[] w = &#123;2, 1, 3, 2&#125;; int[] v = &#123;12, 10, 20, 15&#125;; System.out.println(knapSack(w, v, 5)); &#125;&#125; 利用背包问题的思想解决问题leetcode 416 Partition Equal Subset Sum 给定一个仅包含正整数的非空数组，确定该数组是否可以分成两部分，要求两部分的和相等 问题分析该问题我们可以利用背包问题的思想进行求解。 假设给定元素个数为n的数组arr，数组元素的和为sum，对应于背包问题，等价于有n个物品，每个物品的重量和价值均为为arr[i]，背包的限重为sum/2，求解背包中的物品最大价值为多少？ 123456789101112131415161718192021222324252627282930class Solution &#123; private boolean knapSack(int[] nums,int sum)&#123; int size = nums.length; boolean[] dp = new boolean[sum + 1]; for (int i = 0;i &lt;= sum;i ++)&#123; dp[i] = i == nums[0]; &#125; for (int i = 1;i &lt; size;i++)&#123; for (int j = sum;j &gt;= nums[i];j--)&#123; dp[j] = dp[j] || dp[j-nums[i]]; &#125; &#125; return dp[sum]; &#125; public boolean canPartition(int[] nums) &#123; int sum = 0; for (int item : nums)&#123; sum += item; &#125; //如果数组元素和不是2的倍数，直接返回false if (sum % 2 != 0) return false; return knapSack(nums,sum/2); &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2019%2F07%2F01%2FLinux-common-command%2F</url>
    <content type="text"><![CDATA[本文转载自：Linux最常用150个命令汇总 线上查询及帮助命令(2个) man 查看命令帮助，命令的词典，更复杂的还有info，但不常用。 help 查看Linux内置命令的帮助，比如cd命令。 文件和目录操作命令(18个) ls 全拼list，功能是列出目录的内容及其内容属性信息。 cd 全拼change directory，功能是从当前工作目录切换到指定的工作目录。 cp 全拼copy，其功能为复制文件或目录。 find 查找的意思，用于查找目录及目录下的文件。 mkdir 全拼make directories，其功能是创建目录。 mv 全拼move，其功能是移动或重命名文件。 pwd 全拼print working directory，其功能是显示当前工作目录的绝对路径。 rename 用于重命名文件。 rm 全拼remove，其功能是删除一个或多个文件或目录。 rmdir 全拼remove empty directories，功能是删除空目录。 touch 创建新的空文件，改变已有文件的时间戳属性。 tree 功能是以树形结构显示目录下的内容。 basename 显示文件名或目录名。 dirname 显示文件或目录路径。 chattr 改变文件的扩展属性。 lsattr 查看文件扩展属性。 file 显示文件的类型。 md5sum 计算和校验文件的MD5值。 查看文件及内容处理命令（21个） cat 全拼concatenate，功能是用于连接多个文件并且打印到屏幕输出或重定向到指定文件中。 tac tac是cat的反向拼写，因此命令的功能为反向显示文件内容。 more 分页显示文件内容。 less 分页显示文件内容，more命令的相反用法。 head 显示文件内容的头部。 tail 显示文件内容的尾部。 cut 将文件的每一行按指定分隔符分割并输出。 split 分割文件为不同的小片段。 paste 按行合并文件内容。 sort 对文件的文本内容排序。 uniq 去除重复行。oldboy wc 统计文件的行数、单词数或字节数。 iconv 转换文件的编码格式。 dos2unix 将DOS格式文件转换成UNIX格式。 diff 全拼difference，比较文件的差异，常用于文本文件。 vimdiff 命令行可视化文件比较工具，常用于文本文件。 rev 反向输出文件内容。 grep/egrep 过滤字符串，三剑客老三。 join 按两个文件的相同字段合并。 tr 替换或删除字符。 vi/vim 命令行文本编辑器。 文件压缩及解压缩命令（4个） tar 打包压缩。oldboy unzip 解压文件。 gzip gzip压缩工具。 zip 压缩工具。 信息显示命令（11个） uname 显示操作系统相关信息的命令。 hostname 显示或者设置当前系统的主机名。 dmesg 显示开机信息，用于诊断系统故障。 uptime 显示系统运行时间及负载。 stat 显示文件或文件系统的状态。 du 计算磁盘空间使用情况。 df 报告文件系统磁盘空间的使用情况。 top 实时显示系统资源使用情况。 free 查看系统内存。 date 显示与设置系统时间。 cal 查看日历等时间信息。 搜索文件命令（4个） which 查找二进制命令，按环境变量PATH路径查找。 find 从磁盘遍历查找文件或目录。 whereis 查找二进制命令，按环境变量PATH路径查找。 locate 从数据库 (/var/lib/mlocate/mlocate.db) 查找命令，使用updatedb更新库。 用户管理命令（10个） useradd 添加用户。 usermod 修改系统已经存在的用户属性。 userdel 删除用户。 groupadd 添加用户组。 passwd 修改用户密码。 chage 修改用户密码有效期限。 id 查看用户的uid,gid及归属的用户组。 su 切换用户身份。 visudo 编辑/etc/sudoers文件的专属命令。 sudo 以另外一个用户身份（默认root用户）执行事先在sudoers文件允许的命令。 基础网络操作命令（11个） telnet 使用TELNET协议远程登录。 ssh 使用SSH加密协议远程登录。 scp 全拼secure copy，用于不同主机之间复制文件。 wget 命令行下载文件。 ping 测试主机之间网络的连通性。 route 显示和设置linux系统的路由表。 ifconfig 查看、配置、启用或禁用网络接口的命令。 ifup 启动网卡。 ifdown 关闭网卡。 netstat 查看网络状态。 ss 查看网络状态。 深入网络操作命令（9个） nmap 网络扫描命令。 lsof 全名list open files，也就是列举系统中已经被打开的文件。 mail 发送和接收邮件。 mutt 邮件管理命令。 nslookup 交互式查询互联网DNS服务器的命令。 dig 查找DNS解析过程。 host 查询DNS的命令。 traceroute 追踪数据传输路由状况。 tcpdump 命令行的抓包工具。 有关磁盘与文件系统的命令（16个） mount 挂载文件系统。 umount 卸载文件系统。 fsck 检查并修复Linux文件系统。 dd 转换或复制文件。 dumpe2fs 导出ext2/ext3/ext4文件系统信息。 dump ext2/3/4文件系统备份工具。 fdisk 磁盘分区命令，适用于2TB以下磁盘分区。 parted 磁盘分区命令，没有磁盘大小限制，常用于2TB以下磁盘分区。 mkfs 格式化创建Linux文件系统。 partprobe 更新内核的硬盘分区表信息。 e2fsck 检查ext2/ext3/ext4类型文件系统。 mkswap 创建Linux交换分区。 swapon 启用交换分区。 swapoff 关闭交换分区。 sync 将内存缓冲区内的数据写入磁盘。 resize2fs 调整ext2/ext3/ext4文件系统大小。 系统权限及用户授权相关命令（4个） chmod 改变文件或目录权限。 chown 改变文件或目录的属主和属组。 chgrp 更改文件用户组。 umask 显示或设置权限掩码。 查看系统用户登陆信息的命令（7个） whoami 显示当前有效的用户名称，相当于执行id -un命令。 who 显示目前登录系统的用户信息。 w 显示已经登陆系统的用户列表，并显示用户正在执行的指令。 last 显示登入系统的用户。 lastlog 显示系统中所有用户最近一次登录信息。 users 显示当前登录系统的所有用户的用户列表。 finger 查找并显示用户信息。 内置命令及其它（19个） echo 打印变量，或直接输出指定的字符串 printf 将结果格式化输出到标准输出。 rpm 管理rpm包的命令。 yum 自动化简单化地管理rpm包的命令。 watch 周期性的执行给定的命令，并将命令的输出以全屏方式显示。 alias 设置系统别名。 unalias 取消系统别名。 date 查看或设置系统时间。 lear 清除屏幕，简称清屏。 history 查看命令执行的历史纪录。 eject 弹出光驱。 time 计算命令执行时间。 nc 功能强大的网络工具。 xargs 将标准输入转换成命令行参数。 exec 调用并执行指令的命令。 export 设置或者显示环境变量。 unset 删除变量或函数。 type 用于判断另外一个命令是否是内置命令。 bc 命令行科学计算器 系统管理与性能监视命令(9个) chkconfig 管理Linux系统开机启动项。 vmstat 虚拟内存统计。 mpstat 显示各个可用CPU的状态统计。 iostat 统计系统IO。 sar 全面地获取系统的CPU、运行队列、磁盘 I/O、分页（交换区）、内存、 CPU中断和网络等性能数据。 ipcs 用于报告Linux中进程间通信设施的状态，显示的信息包括消息列表、共享内存和信号量的信息。 ipcrm 用来删除一个或更多的消息队列、信号量集或者共享内存标识。 strace 用于诊断、调试Linux用户空间跟踪器。我们用它来监控用户空间进程和内核的交互，比如系统调用、信号传递、进程状态变更等。 ltrace 命令会跟踪进程的库函数调用,它会显现出哪个库函数被调用。 关机/重启/注销和查看系统信息的命令（6个） shutdown 关机。 halt 关机。 poweroff 关闭电源。 logout 退出当前登录的Shell。 exit 退出当前登录的Shell。 Ctrl+d 退出当前登录的Shell的快捷键。 进程管理相关命令（15个） bg 将一个在后台暂停的命令，变成继续执行 （在后台执行）。 fg 将后台中的命令调至前台继续运行。 jobs 查看当前有多少在后台运行的命令。 kill 终止进程。 killall 通过进程名终止进程。 pkill 通过进程名终止进程。 crontab 定时任务命令。 ps 显示进程的快照。 pstree 树形显示进程。 nice/renice 调整程序运行的优先级。 nohup 忽略挂起信号运行指定的命令。 pgrep 查找匹配条件的进程。 runlevel 查看系统当前运行级别。 init 切换运行级别。 service 启动、停止、重新启动和关闭系统服务，还可以显示所有系统服务的当前状态。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中兴2020届秋招提前批在线笔试题:排列员工工资顺序]]></title>
    <url>%2F2019%2F07%2F01%2Fzhongxin-algorithm%2F</url>
    <content type="text"><![CDATA[本文转载自：题目：排列员工工资顺序（Java和C++）（中兴在线笔试题） 题目描述某公司中有N名员工。给定所有员工工资的清单，财务人员要按照特定的顺序排列员工的工资。他按照工资的频次降序排列。即给定清单中所有频次较高的工资将在频次较低的工资之前出现。如果相同数量的员工都有相同的工资，则将按照给定清单中该工资第一次出现的顺序排列。 写一个算法来帮助财务人员排列员工工资的顺序。 输入该函数/方法的输入包括两个参数—— num，一个整数，表示员工的人数 salaries，一个正整数列表，表示N名员工的工资 输出返回一个正整数列表，该列表按照员工工资的频次排序。 约束条件： 1&lt;num&lt;100000 1&lt;salaries&lt;1000000000 0&lt;i&lt;num 示例 输入： num=19 salaries=[10000,20000,40000,30000,30000,30000,40000,20000,50000, 50000,50000,50000,60000,60000,60000,70000,80000,90000,100000] 输出： [50000,50000,50000,50000,30000,30000,30000,60000,60000, 60000,20000,20000,40000,40000,10000,70000,80000,90000,100000] 分析本题要求把根据数据出现的频数从高到低排序，所以基本的思路是算出每个数的频次，然后直接从高到低排序。 可是这样会出现一个问题，频次相同的数据没法排序。比如 20 40 20 40 50 这组数据分别得到频次 2 2 2 2 1，本身就是一个降序排序，所以最后结果还是20 40 20 40 50。 那怎么办呢？我们可以上面那种数据中间那个20为例，即20 40 20 40 50。首先我们来判断20的前半部分是否存在20，如果存在20的话则插入到该20的后面即可，即变为20 20 40 40 50。 所有排序过程均使用插入排序。插入排序的详细原理读者可以自行百度，插入排序从后往前移动，移动过程中不会改变原有已经排列好的数字的顺序。 思路 计算每个数的频数。 根据频数的大小，对数据从高到低排序。排序的方法使用插入排序。 对第2部排好序的结果再排序。这次从前向后遍历，判断该数的前半部分是否存在该数，存在的话就插入到最近和它相同数的后面。 代码实现Java实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class salaryArrangement &#123; public static void main(String[] args) &#123; int num = 19; int slalay[] = &#123; 10000, 20000, 40000, 30000, 30000, 30000, 40000, 20000, 50000, 50000, 50000, 50000, 60000, 60000, 60000, 70000, 80000, 90000, 100000 &#125;;// 测试数据 slalay = b(num, slalay); // 保存结果 int i; System.out.println("输出结果为："); for (i = 0; i &lt; num; i++)// 输出结果 &#123; System.out.print(" "); System.out.print(slalay[i]); &#125; &#125; public static int[] b(int num, int slalay[]) &#123; int i, j, k; int temp_frequency; int temp_slalay; int frequency[] = new int[num]; // 频次 for (i = 0; i &lt; num; i++) // 给频次赋初值 &#123; frequency[i] = 0; &#125; for (i = 0; i &lt; num; i++) // 对已排序的数据统计频次 &#123; for (j = 0; j &lt; num; j++) &#123; if (slalay[i] == slalay[j]) &#123; frequency[i]++; &#125; &#125; &#125; for (i = 1; i &lt; num; i++) // 对频次进行插入排序，同时根据频数交换的顺序排列原数据 &#123; temp_frequency = frequency[i]; temp_slalay = slalay[i]; j = i - 1; while (j &gt;= 0 &amp;&amp; temp_frequency &gt; frequency[j]) //频次递减排序 &#123; frequency[j + 1] = frequency[j]; slalay[j + 1] = slalay[j]; // 变换原数据 j--; &#125; frequency[j + 1] = temp_frequency; slalay[j + 1] = temp_slalay; &#125; for (i = 1; i &lt; num; i++) // 再进行一次插入排序 &#123; temp_slalay = slalay[i]; j = i - 1; k = i - 1; while (k &gt;= 0) &#123; if (temp_slalay == slalay[k]) // 判断前半部分子序列是否存在当前当前数据 &#123; while (j &gt;= 0 &amp;&amp; temp_slalay != slalay[j]) // 插入到相同的数的后面 &#123; slalay[j + 1] = slalay[j]; j--; &#125; slalay[j + 1] = temp_slalay; break; // 退出循环判断下个数 &#125; k--; &#125; &#125; return slalay; &#125;&#125; C++实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include &lt;stdio.h&gt; int* b(int num, int* slalay) &#123; int i, j, k; int temp_frequency; int temp_slalay; int frequency[num]; // 频次 for (i = 0; i &lt; num; i++) // 给频次赋初值 &#123; frequency[i] = 0; &#125; for (i = 0; i &lt; num; i++) // 对已排序的数据统计频次 &#123; for (j = 0; j &lt; num; j++) &#123; if (slalay[i] == slalay[j]) &#123; frequency[i]++; &#125; &#125; &#125; for (i = 1; i &lt; num; i++) // 对频次进行插入排序，同时根据频数交换的顺序排列原数据 &#123; temp_frequency = frequency[i]; temp_slalay = slalay[i]; j = i - 1; while (j &gt;= 0 &amp;&amp; temp_frequency &gt; frequency[j]) //频次递减排序 &#123; frequency[j + 1] = frequency[j]; slalay[j + 1] = slalay[j]; // 变换原数据 j--; &#125; frequency[j + 1] = temp_frequency; slalay[j + 1] = temp_slalay; &#125; for (i = 1; i &lt; num; i++) // 再进行一次插入排序 &#123; temp_slalay = slalay[i]; j = i - 1; k = i - 1; while (k &gt;= 0) &#123; if (temp_slalay == slalay[k]) // 判断前半部分子序列是否存在当前当前数据 &#123; while (j &gt;= 0 &amp;&amp; temp_slalay != slalay[j]) // 插入到相同的数的后面 &#123; slalay[j + 1] = slalay[j]; j--; &#125; slalay[j + 1] = temp_slalay; break; // 退出循环判断下个数 &#125; k--; &#125; &#125; return slalay;&#125; int main()&#123; int num = 19; int slalay[19] = &#123; 10000, 20000, 40000, 30000, 30000, 30000, 40000, 20000, 50000, 50000, 50000, 50000, 60000,60000, 60000, 70000, 80000, 90000, 100000 &#125;;// 测试数据 int *slalayResult; slalayResult = b(num, slalay); // 保存结果 int i; printf("输出结果为："); for (i = 0; i &lt; num; i++)// 输出结果 &#123; printf("%d ",slalayResult[i]); &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识azkaban]]></title>
    <url>%2F2019%2F07%2F01%2Fazkaban-introduce%2F</url>
    <content type="text"><![CDATA[本文转载自：Azkaban 简介 本文简单介绍一下Azkaban及其特点。azkaban是一个开源的任务调度系统，用于负责任务的调度运行（如数据仓库调度），用以替代linux中的crontab。 Azkaban是什么？Azkaban是什么? Azkaban是一套简单的任务调度服务，整体包括三部分webserver、dbserver、executorserver。 Azkaban是linkin的开源项目，开发语言为Java。 Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。 Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。 Azkaban典型使用场景 实际当中经常有这些场景：每天有一个大任务，这个大任务可以分成A,B,C,D四个小任务，A,B任务之间没有依赖关系，C任务依赖A,B任务的结果，D任务依赖C任务的结果。一般的做法是，开两个终端同时执行A,B，两个都执行完了再执行C，最后再执行D。这样的话，整个的执行过程都需要人工参加，并且得盯着各任务的进度。但是我们的很多任务都是在深更半夜执行的，通过写脚本设置crontab执行。其实，整个过程类似于一个有向无环图（DAG）。每个子任务相当于大任务中的一个流，任务的起点可以从没有度的节点开始执行，任何没有通路的节点之间可以同时执行，比如上述的A,B。总结起来的话，我们需要的就是一个工作流的调度器，而azkaban就是能解决上述问题的一个调度器。 Azkaban官网 https://azkaban.github.io/ Azkaban的功能特点它具有如下功能特点： 1、Web用户界面 2、方便上传工作流 3、方便设置任务之间的关系 4、工作流调度 5、认证/授权 6、能够杀死并重启工作流 7、模块化和可插拔的插件机制 8、项目工作区 9、工作流和任务的日志记录和审计 Azkaban的架构Azkaban是一种类似于Oozie的工作流控制引擎，可以用来解决多个Hadoop（或Spark等）离线计算任务之间的依赖关系问题。也可以用其代替crontab来对周期性任务进行调度，并且更为直观，可靠，同时提供了美观的可视化管理界面。Azkaban由三部分构成： Relational Database(Mysql)azkaban将大多数状态信息都存于MySQL中,Azkaban Web Server 和 Azkaban Executor Server也需要访问DB。 Azkaban Web Server提供了Web UI，是azkaban的主要管理者，包括 project 的管理，认证，调度，对工作流执行过程的监控等。 Azkaban Executor Server调度工作流和任务，记录工作流活任务的日志，之所以将AzkabanWebServer和AzkabanExecutorServer分开，主要是因为在某个任务流失败后，可以更方便的将重新执行。而且也更有利于Azkaban系统的升级 MySQL实例：Azkaban使用MySQL来存储项目和执行。 Azkaban Web服务器：Azkaban使用Jetty作为Web服务器，用作控制器以及提供Web界面 Azkaban执行服务器：Azkaban执行服务器执行提交工作流。 参考资料： http://www.cnblogs.com/zlslch/p/6294321.html http://www.cnblogs.com/zlslch/p/6296079.html http://www.cnblogs.com/zlslch/p/6296214.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java之线程池]]></title>
    <url>%2F2019%2F06%2F29%2Fjava-ThreadPool%2F</url>
    <content type="text"><![CDATA[本文转载自：Java并发编程：线程池的使用 在前面的文章中，我们使用线程的时候就去创建一个线程，这样实现起来非常简便，但是就会有一个问题： 如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，这样频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间。 那么有没有一种办法使得线程可以复用，就是执行完一个任务，并不被销毁，而是可以继续执行其他的任务？ 在Java中可以通过线程池来达到这样的效果。今天我们就来详细讲解一下Java的线程池，首先我们从最核心的ThreadPoolExecutor类中的方法讲起，然后再讲述它的实现原理，接着给出了它的使用示例，最后讨论了一下如何合理配置线程池的大小。 Java中的ThreadPoolExecutor类java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类，因此如果要透彻地了解Java中的线程池，必须先了解这个类。下面我们来看一下ThreadPoolExecutor类的具体实现源码。 在ThreadPoolExecutor类中提供了四个构造方法： 123456789101112131415public class ThreadPoolExecutor extends AbstractExecutorService &#123; ..... public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,RejectedExecutionHandler handler); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory,RejectedExecutionHandler handler); ...&#125; 从上面的代码可以得知，ThreadPoolExecutor继承了AbstractExecutorService类，并提供了四个构造器，事实上，通过观察每个构造器的源码具体实现，发现前面三个构造器都是调用的第四个构造器进行的初始化工作。 下面解释下一下构造器中各个参数的含义： corePoolSize：核心池的大小，这个参数跟后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数，这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性： 1234567TimeUnit.DAYS; //天TimeUnit.HOURS; //小时TimeUnit.MINUTES; //分钟TimeUnit.SECONDS; //秒TimeUnit.MILLISECONDS; //毫秒TimeUnit.MICROSECONDS; //微妙TimeUnit.NANOSECONDS; //纳秒 workQueue：一个阻塞队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列有以下几种选择： 123ArrayBlockingQueue;LinkedBlockingQueue;SynchronousQueue; ArrayBlockingQueue和PriorityBlockingQueue使用较少，一般使用LinkedBlockingQueue和Synchronous。线程池的排队策略与BlockingQueue有关。 threadFactory：线程工厂，主要用来创建线程； handler：表示当拒绝处理任务时的策略，有以下四种取值： 1234ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 具体参数的配置与线程池的关系将在下一节讲述。 从上面给出的hreadPoolExecutor类的代码可以知道，ThreadPoolExecutor继承了AbstractExecutorService，我们来看一下AbstractExecutorService的实现： 123456789101112131415161718192021222324252627public abstract class AbstractExecutorService implements ExecutorService &#123; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; &#125;; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; &#125;; public Future&lt;?&gt; submit(Runnable task) &#123;&#125;; public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; &#125;; public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; &#125;; private &lt;T&gt; T doInvokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException &#123; &#125;; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException &#123; &#125;; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; &#125;; public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException &#123; &#125;; public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException &#123; &#125;;&#125; AbstractExecutorService是一个抽象类，它实现了ExecutorService接口。 我们接着看ExecutorService接口的实现： 12345678910111213141516171819202122public interface ExecutorService extends Executor &#123; void shutdown(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future&lt;?&gt; submit(Runnable task); &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 而ExecutorService又是继承了Executor接口，我们看一下Executor接口的实现： 123public interface Executor &#123; void execute(Runnable command);&#125; 到这里，大家应该明白了ThreadPoolExecutor、AbstractExecutorService、ExecutorService和Executor几个之间的关系了。 Executor是一个顶层接口，在它里面只声明了一个方法execute(Runnable)，返回值为void，参数为Runnable类型，从字面意思可以理解，就是用来执行传进去的任务的； 然后ExecutorService接口继承了Executor接口，并声明了一些方法：submit、invokeAll、invokeAny以及shutDown等； 抽象类AbstractExecutorService实现了ExecutorService接口，基本实现了ExecutorService中声明的所有方法； 然后ThreadPoolExecutor继承了类AbstractExecutorService。 在ThreadPoolExecutor类中有几个非常重要的方法： 1234execute()submit()shutdown()shutdownNow() execute()方法实际上是Executor中声明的方法，在ThreadPoolExecutor进行了具体的实现，这个方法是ThreadPoolExecutor的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 submit()方法是在ExecutorService中声明的方法，在AbstractExecutorService就已经有了具体的实现，在ThreadPoolExecutor中并没有对其进行重写，这个方法也是用来向线程池提交任务的，但是它和execute()方法不同，它能够返回任务执行的结果，去看submit()方法的实现，会发现它实际上还是调用的execute()方法，只不过它利用了Future来获取任务执行结果（Future相关内容将在下一篇讲述）。 shutdown()和shutdownNow()是用来关闭线程池的。 还有很多其他的方法： 比如：getQueue() 、getPoolSize() 、getActiveCount()、getCompletedTaskCount()等获取与线程池相关属性的方法，有兴趣的朋友可以自行查阅API。 深入剖析线程池实现原理 在上一节我们从宏观上介绍了ThreadPoolExecutor，下面我们来深入解析一下线程池的具体实现原理，将从下面几个方面讲解： 1.线程池状态 2.任务的执行 3.线程池中的线程初始化 4.任务缓存队列及排队策略 5.任务拒绝策略 6.线程池的关闭 7.线程池容量的动态调整 线程池状态 在ThreadPoolExecutor中定义了一个volatile变量，另外定义了几个static final变量表示线程池的各个状态： 12345volatile int runState;static final int RUNNING = 0;static final int SHUTDOWN = 1;static final int STOP = 2;static final int TERMINATED = 3; runState表示当前线程池的状态，它是一个volatile变量用来保证线程之间的可见性； 下面的几个static final变量表示runState可能的几个取值。 当创建线程池后，初始时，线程池处于RUNNING状态； 如果调用了shutdown()方法，则线程池处于SHUTDOWN状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕； 如果调用了shutdownNow()方法，则线程池处于STOP状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务； 当线程池处于SHUTDOWN或STOP状态，并且所有工作线程已经销毁，任务缓存队列已经清空或执行结束后，线程池被设置为TERMINATED状态。 任务的执行 在了解将任务提交给线程池到任务执行完毕整个过程之前，我们先来看一下ThreadPoolExecutor类中其他的一些比较重要成员变量： 12345678910111213141516171819private final BlockingQueue&lt;Runnable&gt; workQueue; //任务缓存队列，用来存放等待执行的任务private final ReentrantLock mainLock = new ReentrantLock(); //线程池的主要状态锁，对线程池状态（比如线程池大小 //、runState等）的改变都要使用这个锁private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); //用来存放工作集 private volatile long keepAliveTime; //线程存货时间 private volatile boolean allowCoreThreadTimeOut; //是否允许为核心线程设置存活时间private volatile int corePoolSize; //核心池的大小（即线程池中的线程数目大于这个参数时，提交的任务会被放进任务缓存队列）private volatile int maximumPoolSize; //线程池最大能容忍的线程数 private volatile int poolSize; //线程池中当前的线程数 private volatile RejectedExecutionHandler handler; //任务拒绝策略 private volatile ThreadFactory threadFactory; //线程工厂，用来创建线程 private int largestPoolSize; //用来记录线程池中曾经出现过的最大线程数 private long completedTaskCount; //用来记录已经执行完毕的任务个数 每个变量的作用都已经标明出来了，这里要重点解释一下corePoolSize、maximumPoolSize、largestPoolSize三个变量。 ​ corePoolSize在很多地方被翻译成核心池大小，其实我的理解这个就是线程池的大小。举个简单的例子： 假如有一个工厂，工厂里面有10个工人，每个工人同时只能做一件任务。 因此只要当10个工人中有工人是空闲的，来了任务就分配给空闲的工人做； 当10个工人都有任务在做时，如果还来了任务，就把任务进行排队等待； 如果说新任务数目增长的速度远远大于工人做任务的速度，那么此时工厂主管可能会想补救措施，比如重新招 4个临时工人进来； 然后就将任务也分配给这4个临时工人做； 如果说着14个工人做任务的速度还是不够，此时工厂主管可能就要考虑不再接收新的任务或者抛弃前面的一些任务了。 当这14个工人当中有人空闲时，而新任务增长的速度又比较缓慢，工厂主管可能就考虑辞掉4个临时工了，只保持原来的10个工人，毕竟请额外的工人是要花钱的。 这个例子中的corePoolSize就是10，而maximumPoolSize就是14（10+4）。 也就是说corePoolSize就是线程池大小，maximumPoolSize在我看来是线程池的一种补救措施，即任务量突然过大时的一种补救措施。 不过为了方便理解，在本文后面还是将corePoolSize翻译成核心池大小。 ​ largestPoolSize只是一个用来起记录作用的变量，用来记录线程池中曾经有过的最大线程数目，跟线程池的容量没有任何关系。 下面我们进入正题，看一下任务从提交到最终执行完毕经历了哪些过程。 在ThreadPoolExecutor类中，最核心的任务提交方法是execute()方法，虽然通过submit也可以提交任务，但是实际上submit方法里面最终调用的还是execute()方法，所以我们只需要研究execute()方法的实现原理即可： 123456789101112public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) &#123; if (runState == RUNNING &amp;&amp; workQueue.offer(command)) &#123; if (runState != RUNNING || poolSize == 0) ensureQueuedTaskHandled(command); &#125; else if (!addIfUnderMaximumPoolSize(command)) reject(command); // is shutdown or saturated &#125;&#125; 上面的代码可能看起来不是那么容易理解，下面我们一句一句解释： 首先，判断提交的任务command是否为null，若是null，则抛出空指针异常； 接着是这句，这句要好好理解一下： 1if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) 由于是或条件运算符，所以先计算前半部分的值，如果线程池中当前线程数不小于核心池大小，那么就会直接进入下面的if语句块了。 如果线程池中当前线程数小于核心池大小，则接着执行后半部分，也就是执行 1addIfUnderCorePoolSize(command) 如果执行完addIfUnderCorePoolSize这个方法返回false，则继续执行下面的if语句块，否则整个方法就直接执行完毕了。 如果执行完addIfUnderCorePoolSize这个方法返回false，然后接着判断： 1if (runState == RUNNING &amp;&amp; workQueue.offer(command)) 如果当前线程池处于RUNNING状态，则将任务放入任务缓存队列；如果当前线程池不处于RUNNING状态或者任务放入缓存队列失败，则执行： 1addIfUnderMaximumPoolSize(command) 如果执行addIfUnderMaximumPoolSize方法失败，则执行reject()方法进行任务拒绝处理。 回到前面： 1if (runState == RUNNING &amp;&amp; workQueue.offer(command)) 这句的执行，如果说当前线程池处于RUNNING状态且将任务放入任务缓存队列成功，则继续进行判断： 1if (runState != RUNNING || poolSize == 0) 这句判断是为了防止在将此任务添加进任务缓存队列的同时其他线程突然调用shutdown或者shutdownNow方法关闭了线程池的一种应急措施。如果是这样就执行： 1ensureQueuedTaskHandled(command) 进行应急处理，从名字可以看出是保证添加到任务缓存队列中的任务得到处理。 我们接着看2个关键方法的实现：addIfUnderCorePoolSize和addIfUnderMaximumPoolSize： 123456789101112131415private boolean addIfUnderCorePoolSize(Runnable firstTask) &#123; Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (poolSize &lt; corePoolSize &amp;&amp; runState == RUNNING) t = addThread(firstTask); //创建线程去执行firstTask任务 &#125; finally &#123; mainLock.unlock(); &#125; if (t == null) return false; t.start(); return true;&#125; 这个是addIfUnderCorePoolSize方法的具体实现，从名字可以看出它的意图就是当低于核心池大小时执行的方法。下面看其具体实现，首先获取到锁，因为这地方涉及到线程池状态的变化，先通过if语句判断当前线程池中的线程数目是否小于核心池大小，有朋友也许会有疑问：前面在execute()方法中不是已经判断过了吗，只有线程池当前线程数目小于核心池大小才会执行addIfUnderCorePoolSize方法的，为何这地方还要继续判断？原因很简单，前面的判断过程中并没有加锁，因此可能在execute方法判断的时候poolSize小于corePoolSize，而判断完之后，在其他线程中又向线程池提交了任务，就可能导致poolSize不小于corePoolSize了，所以需要在这个地方继续判断。然后接着判断线程池的状态是否为RUNNING，原因也很简单，因为有可能在其他线程中调用了shutdown或者shutdownNow方法。然后就是执行 1t = addThread(firstTask); 这个方法也非常关键，传进去的参数为提交的任务，返回值为Thread类型。然后接着在下面判断t是否为空，为空则表明创建线程失败（即poolSize&gt;=corePoolSize或者runState不等于RUNNING），否则调用t.start()方法启动线程。 我们来看一下addThread方法的实现： 123456789101112private Thread addThread(Runnable firstTask) &#123; Worker w = new Worker(firstTask); Thread t = threadFactory.newThread(w); //创建一个线程，执行任务 if (t != null) &#123; w.thread = t; //将创建的线程的引用赋值为w的成员变量 workers.add(w); int nt = ++poolSize; //当前线程数加1 if (nt &gt; largestPoolSize) largestPoolSize = nt; &#125; return t;&#125; 在addThread方法中，首先用提交的任务创建了一个Worker对象，然后调用线程工厂threadFactory创建了一个新的线程t，然后将线程t的引用赋值给了Worker对象的成员变量thread，接着通过workers.add(w)将Worker对象添加到工作集当中。 下面我们看一下Worker类的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364private final class Worker implements Runnable &#123; private final ReentrantLock runLock = new ReentrantLock(); private Runnable firstTask; volatile long completedTasks; Thread thread; Worker(Runnable firstTask) &#123; this.firstTask = firstTask; &#125; boolean isActive() &#123; return runLock.isLocked(); &#125; void interruptIfIdle() &#123; final ReentrantLock runLock = this.runLock; if (runLock.tryLock()) &#123; try &#123; if (thread != Thread.currentThread()) thread.interrupt(); &#125; finally &#123; runLock.unlock(); &#125; &#125; &#125; void interruptNow() &#123; thread.interrupt(); &#125; private void runTask(Runnable task) &#123; final ReentrantLock runLock = this.runLock; runLock.lock(); try &#123; if (runState &lt; STOP &amp;&amp; Thread.interrupted() &amp;&amp; runState &gt;= STOP) boolean ran = false; beforeExecute(thread, task); //beforeExecute方法是ThreadPoolExecutor类的一个方法，没有具体实现，用户可以根据 //自己需要重载这个方法和后面的afterExecute方法来进行一些统计信息，比如某个任务的执行时间等 try &#123; task.run(); ran = true; afterExecute(task, null); ++completedTasks; &#125; catch (RuntimeException ex) &#123; if (!ran) afterExecute(task, ex); throw ex; &#125; &#125; finally &#123; runLock.unlock(); &#125; &#125; public void run() &#123; try &#123; Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) &#123; runTask(task); task = null; &#125; &#125; finally &#123; workerDone(this); //当任务队列中没有任务时，进行清理工作 &#125; &#125;&#125; 它实际上实现了Runnable接口，因此上面的Thread t = threadFactory.newThread(w);效果跟下面这句的效果基本一样： 1Thread t = new Thread(w); 相当于传进去了一个Runnable任务，在线程t中执行这个Runnable。 既然Worker实现了Runnable接口，那么自然最核心的方法便是run()方法了： 123456789101112public void run() &#123; try &#123; Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) &#123; runTask(task); task = null; &#125; &#125; finally &#123; workerDone(this); &#125;&#125; 从run方法的实现可以看出，它首先执行的是通过构造器传进来的任务firstTask，在调用runTask()执行完firstTask之后，在while循环里面不断通过getTask()去取新的任务来执行，那么去哪里取呢？自然是从任务缓存队列里面去取，getTask是ThreadPoolExecutor类中的方法，并不是Worker类中的方法，下面是getTask方法的实现： 123456789101112131415161718192021222324252627Runnable getTask() &#123; for (;;) &#123; try &#123; int state = runState; if (state &gt; SHUTDOWN) return null; Runnable r; if (state == SHUTDOWN) // Help drain queue r = workQueue.poll(); else if (poolSize &gt; corePoolSize || allowCoreThreadTimeOut) //如果线程数大于核心池大小或者允许为核心池线程设置空闲时间， //则通过poll取任务，若等待一定的时间取不到任务，则返回null r = workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS); else r = workQueue.take(); if (r != null) return r; if (workerCanExit()) &#123; //如果没取到任务，即r为null，则判断当前的worker是否可以退出 if (runState &gt;= SHUTDOWN) // Wake up others interruptIdleWorkers(); //中断处于空闲状态的worker return null; &#125; // Else retry &#125; catch (InterruptedException ie) &#123; // On interruption, re-check runState &#125; &#125;&#125; 在getTask中，先判断当前线程池状态，如果runState大于SHUTDOWN（即为STOP或者TERMINATED），则直接返回null。 如果runState为SHUTDOWN或者RUNNING，则从任务缓存队列取任务。 如果当前线程池的线程数大于核心池大小corePoolSize或者允许为核心池中的线程设置空闲存活时间，则调用poll(time,timeUnit)来取任务，这个方法会等待一定的时间，如果取不到任务就返回null。 然后判断取到的任务r是否为null，为null则通过调用workerCanExit()方法来判断当前worker是否可以退出，我们看一下workerCanExit()的实现： 12345678910111213141516private boolean workerCanExit() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); boolean canExit; //如果runState大于等于STOP，或者任务缓存队列为空了 //或者 允许为核心池线程设置空闲存活时间并且线程池中的线程数目大于1 try &#123; canExit = runState &gt;= STOP || workQueue.isEmpty() || (allowCoreThreadTimeOut &amp;&amp; poolSize &gt; Math.max(1, corePoolSize)); &#125; finally &#123; mainLock.unlock(); &#125; return canExit;&#125; 也就是说如果线程池处于STOP状态、或者任务队列已为空或者允许为核心池线程设置空闲存活时间并且线程数大于1时，允许worker退出。如果允许worker退出，则调用interruptIdleWorkers()中断处于空闲状态的worker，我们看一下interruptIdleWorkers()的实现： 12345678910oid interruptIdleWorkers() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) //实际上调用的是worker的interruptIfIdle()方法 w.interruptIfIdle(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; 从实现可以看出，它实际上调用的是worker的interruptIfIdle()方法，在worker的interruptIfIdle()方法中： 123456789101112void interruptIfIdle() &#123; final ReentrantLock runLock = this.runLock; if (runLock.tryLock()) &#123; //注意这里，是调用tryLock()来获取锁的，因为如果当前worker正在执行任务，锁已经被获取了，是无法获取到锁的 //如果成功获取了锁，说明当前worker处于空闲状态 try &#123; if (thread != Thread.currentThread()) thread.interrupt(); &#125; finally &#123; runLock.unlock(); &#125; &#125;&#125; 这里有一个非常巧妙的设计方式，假如我们来设计线程池，可能会有一个任务分派线程，当发现有线程空闲时，就从任务缓存队列中取一个任务交给空闲线程执行。但是在这里，并没有采用这样的方式，因为这样会要额外地对任务分派线程进行管理，无形地会增加难度和复杂度，这里直接让执行完任务的线程去任务缓存队列里面取任务来执行。 我们再看addIfUnderMaximumPoolSize方法的实现，这个方法的实现思想和addIfUnderCorePoolSize方法的实现思想非常相似，唯一的区别在于addIfUnderMaximumPoolSize方法是在线程池中的线程数达到了核心池大小并且往任务队列中添加任务失败的情况下执行的： 123456789101112131415private boolean addIfUnderMaximumPoolSize(Runnable firstTask) &#123; Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (poolSize &lt; maximumPoolSize &amp;&amp; runState == RUNNING) t = addThread(firstTask); &#125; finally &#123; mainLock.unlock(); &#125; if (t == null) return false; t.start(); return true;&#125; 看到没有，其实它和addIfUnderCorePoolSize方法的实现基本一模一样，只是if语句判断条件中的poolSize &lt; maximumPoolSize不同而已。 到这里，大部分朋友应该对任务提交给线程池之后到被执行的整个过程有了一个基本的了解，下面总结一下： 首先，要清楚corePoolSize和maximumPoolSize的含义； 其次，要知道Worker是用来起到什么作用的； 要知道任务提交给线程池之后的处理策略，这里总结一下主要有4点： 如果当前线程池中的线程数目小于corePoolSize，则每来一个任务，就会创建一个线程去执行这个任务； 如果当前线程池中的线程数目&gt;=corePoolSize，则每来一个任务，会尝试将其添加到任务缓存队列当中，若添加成功，则该任务会等待空闲线程将其取出去执行；若添加失败（一般来说是任务缓存队列已满），则会尝试创建新的线程去执行这个任务； 如果当前线程池中的线程数目达到maximumPoolSize，则会采取任务拒绝策略进行处理； 如果线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止，直至线程池中的线程数目不大于corePoolSize；如果允许为核心池中的线程设置存活时间，那么核心池中的线程空闲时间超过keepAliveTime，线程也会被终止。 线程池中的线程初始化 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。 在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程； prestartAllCoreThreads()：初始化所有核心线程 下面是这2个方法的实现： 12345678910public boolean prestartCoreThread() &#123; return addIfUnderCorePoolSize(null); //注意传进去的参数是null&#125; public int prestartAllCoreThreads() &#123; int n = 0; while (addIfUnderCorePoolSize(null))//注意传进去的参数是null ++n; return n;&#125; 注意上面传进去的参数是null，根据第2小节的分析可知如果传进去的参数为null，则最后执行线程会阻塞在getTask方法中的 1r = workQueue.take(); 即等待任务队列中有任务。 ### 任务缓存队列及排队策略 在前面我们多次提到了任务缓存队列，即workQueue，它用来存放等待执行的任务。 workQueue的类型为BlockingQueue&lt;Runnable&gt;，通常可以取下面三种类型： 1）ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小； 2）LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE； 3）synchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。 任务拒绝策略 当线程池的任务缓存队列已满并且线程池中的线程数目达到maximumPoolSize，如果还有任务到来就会采取任务拒绝策略，通常有以下四种策略： 1234ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 线程池的关闭 ThreadPoolExecutor提供了两个方法，用于线程池的关闭，分别是shutdown()和shutdownNow()，其中： shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务 线程池容量的动态调整 ThreadPoolExecutor提供了动态调整线程池容量大小的方法：setCorePoolSize()和setMaximumPoolSize()， setCorePoolSize：设置核心池大小 setMaximumPoolSize：设置线程池最大能创建的线程数目大小 当上述参数从小变大时，ThreadPoolExecutor进行线程赋值，还可能立即创建新的线程来执行任务。 使用示例 前面我们讨论了关于线程池的实现原理，这一节我们来看一下它的具体使用： 12345678910111213141516171819202122232425262728293031323334public class Test &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); for(int i=0;i&lt;15;i++)&#123; MyTask myTask = new MyTask(i); executor.execute(myTask); System.out.println("线程池中线程数目："+executor.getPoolSize()+"，队列中等待执行的任务数目："+ executor.getQueue().size()+"，已执行玩别的任务数目："+executor.getCompletedTaskCount()); &#125; executor.shutdown(); &#125;&#125; class MyTask implements Runnable &#123; private int taskNum; public MyTask(int num) &#123; this.taskNum = num; &#125; @Override public void run() &#123; System.out.println("正在执行task "+taskNum); try &#123; Thread.currentThread().sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("task "+taskNum+"执行完毕"); &#125;&#125; 执行结果： 正在执行task 0线程池中线程数目：1，队列中等待执行的任务数目：0，已执行玩别的任务数目：0线程池中线程数目：2，队列中等待执行的任务数目：0，已执行玩别的任务数目：0正在执行task 1线程池中线程数目：3，队列中等待执行的任务数目：0，已执行玩别的任务数目：0正在执行task 2线程池中线程数目：4，队列中等待执行的任务数目：0，已执行玩别的任务数目：0正在执行task 3线程池中线程数目：5，队列中等待执行的任务数目：0，已执行玩别的任务数目：0正在执行task 4线程池中线程数目：5，队列中等待执行的任务数目：1，已执行玩别的任务数目：0线程池中线程数目：5，队列中等待执行的任务数目：2，已执行玩别的任务数目：0线程池中线程数目：5，队列中等待执行的任务数目：3，已执行玩别的任务数目：0线程池中线程数目：5，队列中等待执行的任务数目：4，已执行玩别的任务数目：0线程池中线程数目：5，队列中等待执行的任务数目：5，已执行玩别的任务数目：0线程池中线程数目：6，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 10线程池中线程数目：7，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 11线程池中线程数目：8，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 12线程池中线程数目：9，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 13线程池中线程数目：10，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 14task 3执行完毕task 0执行完毕task 2执行完毕task 1执行完毕正在执行task 8正在执行task 7正在执行task 6正在执行task 5task 4执行完毕task 10执行完毕task 11执行完毕task 13执行完毕task 12执行完毕正在执行task 9task 14执行完毕task 8执行完毕task 5执行完毕task 7执行完毕task 6执行完毕task 9执行完毕 从执行结果可以看出，当线程池中线程的数目大于5时，便将任务放入任务缓存队列里面，当任务缓存队列满了之后，便创建新的线程。如果上面程序中，将for循环中改成执行20个任务，就会抛出任务拒绝异常了。 不过在java doc中，并不提倡我们直接使用ThreadPoolExecutor，而是使用Executors类中提供的几个静态方法来创建线程池： 123Executors.newCachedThreadPool(); //创建一个缓冲池，缓冲池容量大小为Integer.MAX_VALUEExecutors.newSingleThreadExecutor(); //创建容量为1的缓冲池Executors.newFixedThreadPool(int); //创建固定容量大小的缓冲池 下面是这三个静态方法的具体实现; 12345678910111213141516public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 从它们的具体实现来看，它们实际上也是调用了ThreadPoolExecutor，只不过参数都已配置好了。 newFixedThreadPool(有界线程池)创建的线程池corePoolSize和maximumPoolSize值是相等的，它使用的LinkedBlockingQueue； 创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但是，在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 newSingleThreadExecutor(单一线程池)将corePoolSize和maximumPoolSize都设置为1，也使用的LinkedBlockingQueue； 创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO,LIFO, 优先级)执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。 newCachedThreadPool(无界线程池)将corePoolSize设置为0，将maximumPoolSize设置为Integer.MAX_VALUE，使用的SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过60秒，就销毁线程。 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。这种类型的线程池特点是： 工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger.MAX_VALUE), 这样可灵活的往线程池中添加线程。 如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪。 实际中，如果Executors提供的三个静态方法能满足要求，就尽量使用它提供的三个方法，因为自己去手动配置ThreadPoolExecutor的参数有点麻烦，要根据实际任务的类型和数量来进行配置。 另外，如果ThreadPoolExecutor达不到要求，可以自己继承ThreadPoolExecutor类进行重写。 如何合理配置线程池的大小 本节来讨论一个比较重要的话题：如何合理配置线程池大小，仅供参考。 一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1 如果是IO密集型任务，参考值可以设置为2*NCPU 当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。 参考资料： http://ifeve.com/java-threadpool/ http://blog.163.com/among_1985/blog/static/275005232012618849266/ http://developer.51cto.com/art/201203/321885.htm http://blog.csdn.net/java2000_wl/article/details/22097059 http://blog.csdn.net/cutesource/article/details/6061229 http://blog.csdn.net/xieyuooo/article/details/8718741 《JDK API 1.6》]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识ALS]]></title>
    <url>%2F2019%2F06%2F28%2FALS%2F</url>
    <content type="text"><![CDATA[前言Spark平台推出至今已经地带到2.4.x版本，很多地方都有了重要的更新，加入了很多新的东西。但是在协同过滤这一块却一直以来都只有ALS一种算法。同样是大规模计算平台，Hadoop中的机器学习算法库Mahout就集成了多种推荐算法，不但有user-cf和item-cf这种经典算法，还有KNN、SVD，Slope one这些，可谓随意挑选，简繁由君。ALS算法是2008年以来，用的比较多的协同过滤算法。它已经集成到Spark的Mllib库中，使用起来比较方便。 ALS算法算法介绍ALS的意思是交替最小二乘法（Alternating Least Squares），它只是一种优化算法的名字，被用在求解spark中所提供的推荐系统模型的最优解。spark中协同过滤的文档中一开始就说了。从协同过滤的分类来说，ALS算法属于User-Item CF，也叫做混合CF。它同时考虑了User和Item两个方面。用户和商品的关系，可以抽象为如下的三元组：&lt;User,Item,Rating&gt;。其中，Rating是用户对商品的评分，表征用户对该商品的喜好程度。 这是一个基于模型的协同过滤（model-based CF），其实它是一种近几年推荐系统界大火的隐语义模型中的一种。它的基本思想是对稀疏矩阵进行模型分解，评估出缺失项的值，以此来得到一个基本的训练模型。然后依照此模型可以针对新的用户和物品数据进行评估。ALS是采用交替的最小二乘法来算出缺失项的。交替的最小二乘法是在最小二乘法的基础上发展而来的。 隐语义模型又叫潜在因素模型，它试图通过数量相对少的未被观察到的底层原因，来解释大量用户和产品之间可观察到的交互。 原理SVD基于模型的协同过滤算法操作起来就是通过降维的方法来补全用户-物品矩阵，对矩阵中没有出现的值进行估计。基于这种思想的早期推荐系统常用的一种方法是SVD（奇异值分解）。该方法在矩阵分解之前需要先把评分矩阵R缺失值补全，补全之后稀疏矩阵R表示成稠密矩阵R’，然后将R’分解成如下形式： R’ = UTSV然后再选取U中的K列和V中的S行作为隐特征的个数，达到降维的目的。K的选取通常用启发式策略。 这种方法有两个缺点 补全成稠密矩阵之后需要耗费巨大的存储空间，在实际中，用户对物品的行为信息何止千万，对这样的稠密矩阵的存储是不现实的。 SVD的计算复杂度很高，更不用说这样的大规模稠密矩阵了。所以关于SVD的研究很多都是在小数据集上进行的。 ALS隐语义模型也是基于矩阵分解的，但是和SVD不同，它是把原始矩阵分解成两个矩阵相乘而不是三个。 假设我们有一批用户数据，其中包含m个User和n个Item，则我们定义Rating矩阵，其中的元素表示第u个User对第i个Item的评分。 在实际使用中，由于n和m的数量都十分巨大，因此R矩阵的规模很容易就会突破1亿项。这时候，传统的矩阵分解方法对于这么大的数据量已经是很难处理了。 另一方面，一个用户也不可能给所有商品评分，因此，R矩阵注定是个稀疏矩阵。矩阵中所缺失的评分，又叫做missing item。 针对这样的特点，我们可以假设用户和商品之间存在若干关联维度（比如用户年龄、性别、受教育程度和商品的外观、价格等），我们只需要将R矩阵投射到这些维度上即可。这个投射的数学表示是： Rm×n=Um×k×Vk×n 这里的表明这个投射只是一个近似的空间变换。 不懂这个空间变换的同学，可参见《机器学习（十二）》中的“奇异值分解”的内容，或是本节中的“主成分分析”的内容。 现在的问题就变成了确定U和V ，我们把 U叫做用户因子矩阵V叫做物品因子矩阵 一般情况下，k的值远小于n和m的值，从而达到了数据降维的目的。 幸运的是，我们并不需要显式的定义这些关联维度，而只需要假定它们存在即可，因此这里的关联维度又被称为Latent factor。k的典型取值一般是20～200。 这种方法被称为概率矩阵分解算法(probabilistic matrix factorization，PMF)。ALS算法是PMF在数值计算方面的应用。 通常上式不能达到精确相等的程度，我们要做的就是要最小化他们之间的差距，从而又变成了一个最优化问题。 求解最优化问题我们很容易就想到了随机梯度下降，其中有一种方法就是这样，通过优化如下损失函数来找到X和Y中合适的参数： 其中puk就是U矩阵中u行k列的参数，度量了用户u和第k个隐类的关系；qik是V矩阵中i行k列的参数，度量了物品i和第k个隐类的关系。这种方式也是一种很流行的方法，有很多对它的相关扩展，比如加上偏置项的LFM。 然而ALS用的是另一种求解方法，它先用随机初始化的方式固定一个矩阵，例如Y然后通过最小化等式两边差的平方来更新另一个矩阵X。 得到X之后，又可以固定X用相同的方法求Y，如此交替进行，直到最后收敛或者达到用户指定的迭代次数为止，是为“交替”是也。 因为这个迭代过程，交替优化X和Y，因此又被称作交替最小二乘算法（Alternating Least Squares，ALS）。 从上式可以看出，X的第i行是A的第i行和Y的函数，因此可以很容易地分开计算X的每一行，这就为并行计算提供了很大的便捷，也正是如此，Spark这种面向大规模计算的平台选择了这个算法。 在Intro to Implicit Matrix Factorization: Classic ALS with Sketchfab Models中，作者用了embarrassingly parallel来形容这个算法，意思是高度易并行化的——它的每个子任务之间没有什么依赖关系。 在现实中，不可能每个用户都和所有的物品都有行为关系，事实上，有交互关系的用户-物品对只占很小的一部分，换句话说，用户-物品关系列表是非常稀疏的。和SVD这种矩阵分解不同，ALS所用的矩阵分解技术在分解之前不用把系数矩阵填充成稠密矩阵之后再分解，这不但大大减少了存储空间，而且spark可以利用这种稀疏性用简单的线性代数计算求解。这几点使得本算法在大规模数据上计算非常快，解释了为什么spark mllib目前只有ALS一种推荐算法。 ALS算法的缺点 它是一个离线算法。 无法准确评估新加入的用户或商品。这个问题也被称为Cold Start问题。 显性反馈和隐性反馈基于矩阵分解的协同过滤的标准方法将用户项矩阵中的条目视为由用户给予该项的明确偏好，例如，给予电影评级的用户。在许多真实世界的用例中，通常只能访问隐式反馈（例如查看，点击，购买，喜欢，共享等）。 隐式反馈：用户给商品评分是个非常简单粗暴的用户行为。在实际的电商网站中，还有大量的用户行为，同样能够间接反映用户的喜好，比如用户的购买记录、搜索关键字，甚至是鼠标的移动。我们将这些间接用户行为称之为隐式反馈（implicit feedback），以区别于评分这样的显式反馈（explicit feedback）。 隐式反馈有以下几个特点：1.没有负面反馈（negative feedback）。用户一般会直接忽略不喜欢的商品，而不是给予负面评价。2.隐式反馈包含大量噪声。比如，电视机在某一时间播放某一节目，然而用户已经睡着了，或者忘了换台。3.显式反馈表现的是用户的喜好（preference），而隐式反馈表现的是用户的信任（confidence）。比如用户最喜欢的一般是电影，但观看时间最长的却是连续剧。大米购买的比较频繁，量也大，但未必是用户最想吃的食物。4.隐式反馈非常难以量化。 用于spark.ml处理这些数据的方法取自隐式反馈数据集的协作过滤。本质上，这种方法不是直接对收视率矩阵进行建模，而是将数据视为代表实力的数字观察用户操作（例如点击次数或某人观看电影的累计持续时间）。然后，这些数字与观察到的用户偏好的信心水平相关，而不是给予项目的明确评分。该模型然后试图找出可用于预测用户对物品的预期偏好的潜在因素。 在推荐系统中用户和物品的交互数据分为显性反馈和隐性反馈数据。在ALS中这两种情况也是被考虑了进来的，分别可以训练如下两种模型： 显性反馈模型val model1 = ALS.train(ratings, rank, numIterations, lambda) 隐性反馈模型val model2 = ALS.trainImplicit(ratings, rank, numIterations, lambda, alpha) 参数 numBlocks是为了并行化计算而将用户和项目划分到的块的数量（默认为10）。rank是模型中潜在因素的数量（默认为10）。maxIter是要运行的最大迭代次数（默认为10）。regParam指定ALS中的正则化参数（默认为1.0）。implicitPrefs指定是使用显式反馈 ALS变体还是使用 隐式反馈数据（默认为false使用显式反馈的手段）。alpha是一个适用于ALS的隐式反馈变量的参数，该变量管理偏好观察值的 基线置信度（默认值为1.0)nonnegative指定是否对最小二乘使用非负约束（默认为false）。注意： ALS的基于DataFrame的API目前仅支持用户和项目ID的整数。用户和项目ID列支持其他数字类型，但ID必须在整数值范围内。 从上面可以看到，隐式模型多了一个置信参数，这就涉及到ALS中对于隐式反馈模型的处理方式了——有的文章称为“加权的正则化矩阵分解”，它的损失函数如下： 我们知道，在隐反馈模型中是没有评分的，所以在式子中rui被pui所取代，pui是偏好的表示，仅仅表示用户和物品之间有没有交互，而不表示评分高低或者喜好程度。比如用户和物品之间有交互就让pui等于1，没有就等于0。函数中还有一个cui的项，它用来表示用户偏爱某个商品的置信程度，比如交互次数多的权重就会增加。如果我们用dui来表示交互次数的话，那么就可以把置信程度表示成如下公式： 参考：【推荐系统系列6】ALS推荐算法原理ALS算法解析]]></content>
      <categories>
        <category>推荐算法</category>
      </categories>
      <tags>
        <tag>ALS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql最左匹配原则]]></title>
    <url>%2F2019%2F06%2F27%2Fmysql-leftmost-matching-principle%2F</url>
    <content type="text"><![CDATA[转载自：mysql索引最左匹配原则的理解?作者：沈杰 下面是一个表结构，有三个字段，分别是id,name,cid 1234567CREATE TABLE `student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `cid` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_cid_INX` (`name`,`cid`),) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8 索引方面：id是主键，(name,cid)是一个多列索引。 先看下面两条SQL语句的执行计划： 1EXPLAIN SELECT * FROM student WHERE cid=1; 1EXPLAIN SELECT * FROM student WHERE cid=1 AND name=&apos;小红&apos;; sql查询用到索引的条件是必须要遵守最左前缀原则，为什么上面两个查询还能用到索引？-————————————————————————————————————————– 讲上面问题之前，我先补充一些知识，因为我觉得你对索引理解是狭隘的：上述你的两个查询的explain结果中显示用到索引的情况类型是不一样的。,可观察explain结果中的type字段。你的查询中分别是： type: indextype: ref 解释： index：这种类型表示是mysql会对整个该索引进行扫描。要想用到这种类型的索引，对这个索引并无特别要求，只要是索引，或者某个复合索引的一部分，mysql都可能会采用index类型的方式扫描。但是呢，缺点是效率不高，mysql会从索引中的第一个数据一个个的查找到最后一个数据，直到找到符合判断条件的某个索引。 所以：对于第一条语句： 1EXPLAIN SELECT * FROM student WHERE cid=1; 判断条件是cid=1,而cid是(name,cid)复合索引的一部分，没有问题，可以进行index类型的索引扫描方式。explain显示结果使用到了索引，是index类型的方式。 -————————————————————————————————————————– ref：这种类型表示mysql会根据特定的算法快速查找到某个符合条件的索引，而不是会对索引中每一个数据都进行一 一的扫描判断，也就是所谓你平常理解的使用索引查询会更快的取出数据。而要想实现这种查找，索引却是有要求的，要实现这种能快速查找的算法，索引就要满足特定的数据结构。简单说，也就是索引字段的数据必须是有序的，才能实现这种类型的查找，才能利用到索引。 有些了解的人可能会问，索引不都是一个有序排列的数据结构么。不过答案说的还不够完善，那只是针对单个索引，而复合索引的情况有些同学可能就不太了解了。 下面就说下复合索引： 以该表的(name,cid)复合索引为例,它内部结构简单说就是下面这样排列的： mysql创建复合索引的规则是首先会对复合索引的最左边的，也就是第一个name字段的数据进行排序，在第一个字段的排序基础上，然后再对后面第二个的cid字段进行排序。其实就相当于实现了类似 order by name cid这样一种排序规则。 所以：第一个name字段是绝对有序的，而第二字段就是无序的了。所以通常情况下，直接使用第二个cid字段进行条件判断是用不到索引的，当然，可能会出现上面的使用index类型的索引。这就是所谓的mysql为什么要强调最左前缀原则的原因。 那么什么时候才能用到呢? 当然是cid字段的索引数据也是有序的情况下才能使用咯，什么时候才是有序的呢？观察可知，当然是在name字段是等值匹配的情况下，cid才是有序的。发现没有，观察两个name名字为 c 的cid字段是不是有序的呢。从上往下分别是4 5。 这也就是mysql索引规则中要求复合索引要想使用第二个索引，必须先使用第一个索引的原因。（而且第一个索引必须是等值匹配）。 -————————————————————————————————————————– 所以对于这条sql查询： 1EXPLAIN SELECT * FROM student WHERE cid=1 AND name=&apos;小红&apos;; 没有错，而且复合索引中的两个索引字段都能很好的利用到了！因为语句中最左面的name字段进行了等值匹配，所以cid是有序的，也可以利用到索引了。 你可能会问：我建的索引是(name,cid)。而我查询的语句是cid=1 AND name=’小红’; 我是先查询cid，再查询name的，不是先从最左面查的呀？ 好吧，我再解释一下这个问题：首先可以肯定的是把条件判断反过来变成这样 name=’小红’ and cid=1; 最后所查询的结果是一样的。那么问题产生了？既然结果是一样的，到底以何种顺序的查询方式最好呢？ 所以，而此时那就是我们的mysql查询优化器该登场了，mysql查询优化器会判断纠正这条sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。所以，当然是我们能尽量的利用到索引时的查询顺序效率最高咯，所以mysql查询优化器会最终以这种顺序进行查询执行。 在最左前缀匹配原则，有一个非常重要的原则：mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 作者：Jokerone_ 链接：https://www.jianshu.com/p/b7911e0394b0 来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>最左匹配原则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Collaborative-Filtering协同过滤详解]]></title>
    <url>%2F2019%2F06%2F27%2FCollaborative-Filtering%2F</url>
    <content type="text"><![CDATA[基本思想基于用户的协同过滤算法是通过用户的历史行为数据发现用户对商品或内容的喜欢(如商品购买，收藏，内容评论或分享)，并对这些喜好进行度量和打分。根据不同用户对相同商品或内容的态度和偏好程度计算用户之间的关系。在有相同喜好的用户间进行商品推荐。简单的说就是如果A,B两个用户都购买了x、y、z三本图书，并且给出了5星的好评。那么A和B就属于同一类用户。可以将A看过的图书w也推荐给用户B。 基于用户协同过滤算法的原理图 所以，协同过滤算法主要分为两个步骤： 1、寻找相似的用户集合； 2、寻找集合中用户喜欢的且目标用户没有的进行推荐。 具体实现寻找用户间的相似度Jaccard公式Jaccard系数主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以Jaccard系数只关心个体间共同具有的特征是否一致这个问题。如果比较X与Y的Jaccard相似系数，只比较xn和yn中相同的个数。 Jaccard公式 皮尔逊相关系数皮尔逊相关系统是比欧几里德距离更加复杂的可以判断人们兴趣相似度的一种方法。它在数据不是很规范时，会倾向于给出更好的结果。假定有两个变量X、Y，那么两变量间的皮尔逊相关系数可通过以下公式计算： 公式一： 皮尔逊相关系数公式一公式二： 皮尔逊相关系数公式二公式三： 皮尔逊相关系数公式三公式四： 皮尔逊相关系数公式四 上述四个公式等价，其中E是数学期望，cov表示协方差，N表示变量取值的个数。 欧几里德距离假定两个用户X、Y，均为n维向量，表示用户对n个商品的评分，那么X与Y的欧几里德距离就是： 多维欧几里德距离公式数值越小则代表相似度越高，但是对于不同的n，计算出来的距离不便于控制，所以需要进行如下转换： 相似度公式使得结果分布在(0,1]上，数值越大，相似度越高。 余弦距离余弦距离，也称为余弦相似度，是用向量空间中两个向量余弦值作为衡量两个个体间差异大小的度量值。 与前面的欧几里德距离相似，用户X、Y为两个n维向量，套用余弦公式，其余弦距离表示为： 余弦距离公式即两个向量夹角的余弦值。但是相比欧式距离，余弦距离更加注意两个向量在方向上的相对差异，而不是在空间上的绝对距离，具体可以借助下图来感受两者间的区别： 余弦距离与欧式距离的区别 推荐物品在选取上述方法中的一种得到各个用户相似度后，针对目标用户u，我们选出最相似的k个用户，用集合S(u,k)表示，将S中所有用户喜欢的物品提取出来并去除目标用户u已经喜欢的物品。然后对余下的物品进行评分与相似度加权，得到的结果进行排序。最后由排序结果对目标用户u进行推荐。其中，对于每个可能推荐的物品i，用户u对其的感兴趣的程度可以用如下公式计算： 用户u对物品i感兴趣的程度rvi表示用户v对i的喜欢程度，即对i的评分，wuv表示用户u和v之间的相似度。 收集用户偏好要从用户的行为和偏好中发现规律，并基于此给予推荐，如何收集用户的偏好信息成为系统推荐效果最基础的决定因素。用户有很多方式向系统提供自己的偏好信息，而且不同的应用也可能大不相同，下面举例进行介绍： 表 1 用户行为和用户偏好 用户行为 类型 特征 作用 评分 显式 整数量化的偏好，可能的取值是 [0, n]；n 一般取值为 5 或者是 10 通过用户对物品的评分，可以精确的得到用户的偏好 投票 显式 布尔量化的偏好，取值是 0 或 1 通过用户对物品的投票，可以较精确的得到用户的偏好 转发 显式 布尔量化的偏好，取值是 0 或 1 通过用户对物品的投票，可以精确的得到用户的偏好,如果是站内，同时可以推理得到被转发人的偏好（不精确） 保存书签 显示 布尔量化的偏好，取值是 0 或 1 通过用户对物品的投票，可以精确的得到用户的偏好 标记标签(Tag) 显示 一些单词，需要对单词进行分析，得到偏好 通过分析用户的标签，可以得到用户对项目的理解，同时可以分析出用户的情感：喜欢还是讨厌 评论 显示 一段文字，需要进行文本分析，得到偏好 通过分析用户的评论，可以得到用户的情感：喜欢还是讨厌 点击流( 查看 ) 隐式 一组用户的点击，用户对物品感兴趣，需要进行分析，得到偏好 用户的点击一定程度上反映了用户的注意力，所以它也可以从一定程度上反映用户的喜好 页面停留时间 隐式 一组时间信息，噪音大，需要进行去噪，分析，得到偏好 用户的页面停留时间一定程度上反映了用户的注意力和喜好，但噪音偏大，不好利用 购买 隐式 布尔量化的偏好，取值是 0 或 1 用户的购买是很明确的说明这个项目它感兴趣 以上列举的用户行为都是比较通用的，推荐引擎设计人员可以根据自己应用的特点添加特殊的用户行为，并用他们表示用户对物品的喜好| 在一般应用中，我们提取的用户行为一般都多于一种，关于如何组合这些不同的用户行为，基本上有以下两种方式： 将不同的行为分组：一般可以分为“查看”和“购买”等等，然后基于不同的行为，计算不同的用户 / 物品相似度。类似于当当网或者 Amazon 给出的“购买了该图书的人还购买了 …”，“查看了图书的人还查看了 …” 根据不同行为反映用户喜好的程度将它们进行加权，得到用户对于物品的总体喜好。一般来说，显式的用户反馈比隐式的权值大，但比较稀疏，毕竟进行显示反馈的用户是少数；同时相对于“查看”，“购买”行为反映用户喜好的程度更大，但这也因应用而异。 收集了用户行为数据，我们还需要对数据进行一定的预处理，其中最核心的工作就是：减噪和归一化。 减噪：用户行为数据是用户在使用应用过程中产生的，它可能存在大量的噪音和用户的误操作，我们可以通过经典的数据挖掘算法过滤掉行为数据中的噪音，这样可以是我们的分析更加精确。 归一化：如前面讲到的，在计算用户对物品的喜好程度时，可能需要对不同的行为数据进行加权。但可以想象，不同行为的数据取值可能相差很大，比如，用户的查看数据必然比购买数据大的多，如何将各个行为的数据统一在一个相同的取值范围中，从而使得加权求和得到的总体喜好更加精确，就需要我们进行归一化处理。最简单的归一化处理，就是将各类数据除以此类中的最大值，以保证归一化后的数据取值在 [0,1] 范围中 进行了预处理后，根据不同应用的行为分析方法，可以选择分组或者加权处理，之后我们可以得到一个用户偏好的二维矩阵，一维是用户列表，另一维是物品列表，值是用户对物品的偏好，一般是 [0,1] 或者 [-1, 1] 的浮点数值。 基于用户的 CF（User CF）基于用户的协同过滤(user-basedCF)是基于这样一个事实：每个用户都有与其具有相似兴趣爱好和购买行为的用户群，这些相似用户(邻居用户)的购买项目可以作为对当前用户(目标用户)进行项目推荐的基础。因此，这种方法也被称为基于邻居的协同过滤或最近邻居算法。 基于用户的 CF 的基本思想相当简单，基于用户对物品的偏好找到相邻邻居用户，然后将邻居用户喜欢的推荐给当前用户。计算上，就是将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，找到 K 邻居后，根据邻居的相似度权重以及他们对物品的偏好，预测当前用户没有偏好的未涉及物品，计算得到一个排序的物品列表作为推荐。图 2 给出了一个例子，对于用户 A，根据用户的历史偏好，这里只计算得到一个邻居 – 用户 C，然后将用户 C 喜欢的物品 D 推荐给用户 A。 图 2 基于用户的 CF 的基本原理 基于物品的 CF（Item CF）基于物品的 CF 的原理和基于用户的 CF 类似，只是在计算邻居时采用物品本身，而不是从用户的角度，即基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。从计算的角度看，就是将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的偏好预测当前用户还没有表示偏好的物品，计算得到一个排序的物品列表作为推荐。图 3 给出了一个例子，对于物品 A，根据所有用户的历史偏好，喜欢物品 A 的用户都喜欢物品 C，得出物品 A 和物品 C 比较相似，而用户 C 喜欢物品 A，那么可以推断出用户 C 可能也喜欢物品 C。 基于物品的协同过滤的一个优点是容易解释推荐原因，第二个是电商网站中物品的相似度是相对不变的，物品相似度的矩阵维护起来相对容易。 图 3 基于物品的 CF 的基本原理 User CF vs. Item CF前面介绍了User CF 和Item CF 的基本原理，下面我们分几个不同的角度深入看看它们各自的优缺点和适用场景： 计算复杂度Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。但我们往往忽略了这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。 适用场景在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。 相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。 推荐多样性和精度研究推荐引擎的学者们在相同的数据集合上分别用 User CF 和Item CF计算推荐结果，发现推荐列表中，只有 50% 是一样的，还有 50% 完全不同。但是这两个算法确有相似的精度，所以可以说，这两个算法是很互补的。 关于推荐的多样性，有两种度量方法： 第一种度量方法是从单个用户的角度度量，就是说给定一个用户，查看系统给出的推荐列表是否多样，也就是要比较推荐列表中的物品之间两两的相似度，不难想到，对这种度量方法，Item CF 的多样性显然不如 User CF 的好，因为 Item CF 的推荐就是和以前看的东西最相似的。 第二种度量方法是考虑系统的多样性，也被称为覆盖率 (Coverage)，它是指一个推荐系统是否能够提供给所有用户丰富的选择。在这种指标下，Item CF 的多样性要远远好于 User CF, 因为 User CF 总是倾向于推荐热门的，从另一个侧面看，也就是说，Item CF 的推荐有很好的新颖性，很擅长推荐长尾里的物品。所以，尽管大多数情况，Item CF 的精度略小于 User CF， 但如果考虑多样性，Item CF 却比 User CF 好很多。 如果你对推荐的多样性还心存疑惑，那么下面我们再举个实例看看 User CF 和 Item CF 的多样性到底有什么差别。首先，假设每个用户兴趣爱好都是广泛的，喜欢好几个领域的东西，不过每个用户肯定也有一个主要的领域，对这个领域会比其他领域更加关心。给定一个用户，假设他喜欢 3 个领域 A,B,C，A 是他喜欢的主要领域，这个时候我们来看 User CF 和 Item CF 倾向于做出什么推荐：如果用 User CF, 它会将 A,B,C 三个领域中比较热门的东西推荐给用户；而如果用 ItemCF，它会基本上只推荐 A 领域的东西给用户。所以我们看到因为 User CF 只推荐热门的，所以它在推荐长尾里项目方面的能力不足；而 Item CF 只推荐 A 领域给用户，这样他有限的推荐列表中就可能包含了一定数量的不热门的长尾物品，同时 Item CF 的推荐对这个用户而言，显然多样性不足。但是对整个系统而言，因为不同的用户的主要兴趣点不同，所以系统的覆盖率会比较好。 从人们需求的角度来看，大多数的需求会集中在头部，而这部分我们可以称之为流行，而分布在尾部的需求是个性化的，零散的小量的需求。而这部分差异化的、少量的需求会在需求曲线上面形成一条长长的“尾巴”，而所谓长尾效应就在于它的数量上，将所有非流行的市场累加起来就会形成一个比流行市场还大的市场。 从上面的分析，可以很清晰的看到，这两种推荐都有其合理性，但都不是最好的选择，因此他们的精度也会有损失。其实对这类系统的最好选择是，如果系统给这个用户推荐 30 个物品，既不是每个领域挑选 10 个最热门的给他，也不是推荐 30 个 A 领域的给他，而是比如推荐 15 个 A 领域的给他，剩下的 15 个从 B,C 中选择。所以结合 User CF 和 Item CF 是最优的选择，结合的基本原则就是当采用 Item CF 导致系统对个人推荐的多样性不足时，我们通过加入 User CF 增加个人推荐的多样性，从而提高精度，而当因为采用 User CF 而使系统的整体多样性不足时，我们可以通过加入 Item CF 增加整体的多样性，同样同样可以提高推荐的精度。 用户对推荐算法的适应度前面我们大部分都是从推荐引擎的角度考虑哪个算法更优，但其实我们更多的应该考虑作为推荐引擎的最终使用者 — 应用用户对推荐算法的适应度。 对于 User CF，推荐的原则是假设用户会喜欢那些和他有相同喜好的用户喜欢的东西，但如果一个用户没有相同喜好的朋友，那 User CF 的算法的效果就会很差，所以一个用户对的 CF 算法的适应度是和他有多少共同喜好用户成正比的。 Item CF 算法也有一个基本假设，就是用户会喜欢和他以前喜欢的东西相似的东西，那么我们可以计算一个用户喜欢的物品的自相似度。一个用户喜欢物品的自相似度大，就说明他喜欢的东西都是比较相似的，也就是说他比较符合 Item CF 方法的基本假设，那么他对 Item CF 的适应度自然比较好；反之，如果自相似度小，就说明这个用户的喜好习惯并不满足 Item CF 方法的基本假设，那么对于这种用户，用 Item CF 方法做出好的推荐的可能性非常低。 总结 UserCF ItemCF 性能 适用于用户较少的场合，如果用户很多，计算用户相似度矩阵代价很大 适用于物品数明显小于用户数的场合，如果物品很多（网页），计算物品相似度矩阵代价很大 领域 时效性较强，用户个性化兴趣不太明显的领域 长尾物品丰富，用户个性化需求强烈的领域 实时性 用户有新行为，不一定造成推荐结果的立即变化 用户有新行为，一定会导致推荐结果的实时变化 冷启动 在新用户对很少的物品产生行为后，不能立即对他进行个性化推荐，因为用户相似度表是每隔一段时间离线计算的 新用户只要对一个物品产生行为，就可以给他推荐和该物品相关的其他物品 新物品上线后一段时间，一旦有用户对物品产生行为，就可以将新物品推荐给和对它产生行为的用户兴趣相似的其他用户 但没有办法在不离线更新物品相似度表的情况下将新物品推荐给用户 推荐理由 很难提供令用户信服的推荐解释 利用用户的历史行为给用户做推荐解释，可以令用户比较信服 矩阵分解 Spark推荐模型库当前只包含基于矩阵分解（matrix factorization）的实现，由此我们也将重点关注这类模型。它们有吸引人的地方。首先，这些模型在协同过滤中的表现十分出色。而在Netflix Prize等知名比赛中的表现也很拔尖 显式矩阵分解 要找到和“用户物品”矩阵近似的k维（低阶）矩阵，最终要求出如下两个矩阵：一个用于表示用户的U × k维矩阵，以及一个表征物品的I × k维矩阵。 这两个矩阵也称作因子矩阵。它们的乘积便是原始评级矩阵的一个近似。值得注意的是，原始评级矩阵通常很稀疏，但因子矩阵却是稠密的。 特点：因子分解类模型的好处在于，一旦建立了模型，对推荐的求解便相对容易。但也有弊端，即当用户和物品的数量很多时，其对应的物品或是用户的因子向量可能达到数以百万计。 这将在存储和计算能力上带来挑战。另一个好处是，这类模型的表现通常都很出色。 隐式矩阵分解（关联因子分确定，可能随时会变化）隐式模型仍然会创建一个用户因子矩阵和一个物品因子矩阵。但是，模型所求解的是偏好矩阵而非评级矩阵的近似。类似地，此时用户因子向量和物品因子向量的点积所得到的分数 也不再是一个对评级的估值，而是对某个用户对某一物品偏好的估值（该值的取值虽并不严格地处于0到1之间，但十分趋近于这个区间） 最小二乘法（Alternating Least Squares ALS）：解决矩阵分解的最优化方法ALS的实现原理是迭代式求解一系列最小二乘回归问题。在每一次迭代时，固定用户因子矩阵或是物品因子矩阵中的一个，然后用固定的这个矩阵以及评级数据来更新另一个矩阵。 之后，被更新的矩阵被固定住，再更新另外一个矩阵。如此迭代，直到模型收敛（或是迭代了预设好的次数）。 参考：Collaborative Filtering(协同过滤)算法详解协同过滤算法：基于用户和基于物品的优缺点比较]]></content>
      <categories>
        <category>推荐算法</category>
      </categories>
      <tags>
        <tag>协同过滤</tag>
        <tag>推荐算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识ElasticSearch]]></title>
    <url>%2F2019%2F06%2F27%2Fes-introduce%2F</url>
    <content type="text"><![CDATA[没有ES之前思考：大规模数据如何检索？当系统数据量上了10亿、100亿条的时候，我们在做系统架构的时候通常会从以下角度去考虑问题：1）用什么数据库好？(mysql、sybase、oracle、达梦、神通、mongodb、hbase…)2）如何解决单点故障；(lvs、F5、A10、Zookeeper、MQ)3）如何保证数据安全性；(热备、冷备、异地多活)4）如何解决检索难题；(数据库代理中间件：mysql-proxy、Cobar、MaxScale等;)5）如何解决统计分析问题；(离线、近实时) 传统数据库的应对解决方案对于关系型数据，我们通常采用以下或类似架构去解决查询瓶颈和写入瓶颈：解决要点：1）通过主从备份解决数据安全性问题；2）通过数据库代理中间件心跳监测，解决单点故障问题；3）通过代理中间件将查询语句分发到各个slave节点进行查询，并汇总结果 非关系型数据库的解决方案对于Nosql数据库，以mongodb为例，其它原理类似：解决要点：1）通过副本备份保证数据安全性；2）通过节点竞选机制解决单点问题；3）先从配置库检索分片信息，然后将请求分发到各个节点，最后由路由节点合并汇总结果 另辟蹊径——完全把数据放入内存怎么样？我们知道，完全把数据放在内存中是不可靠的，实际上也不太现实，当我们的数据达到PB级别时，按照每个节点96G内存计算，在内存完全装满的数据情况下，我们需要的机器是：1PB=1024T=1048576G节点数=1048576/96=10922个实际上，考虑到数据备份，节点数往往在2.5万台左右。成本巨大决定了其不现实！ 从前面讨论我们了解到，把数据放在内存也好，不放在内存也好，都不能完完全全解决问题。全部放在内存速度问题是解决了，但成本问题上来了。为解决以上问题，从源头着手分析，通常会从以下方式来寻找方法：1、存储数据时按有序存储；2、将数据和索引分离；3、压缩数据；这就引出了Elasticsearch。 Elasticsearch的概述Elasticsearch 是什么Elasticsearch（ES）是一个基于Lucene构建的开源、分布式、RESTful接口的全文搜索引擎。Elasticsearch还是一个分布式文档数据库，其中每个字段均可被索引，而且每个字段的数据均可被搜索，ES能够横向扩展至数以百计的服务器存储以及处理PB级的数据。可以在极短的时间内存储、搜索和分析大量的数据。通常作为具有复杂搜索场景情况下的核心发动机。Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 ElasticSearch就是为高可用和可扩展而生的。可以通过购置性能更强的服务器或者升级硬件来完成系统扩展，称为垂直或向上扩展（Vertical Scale/Scaling Up）。另一方面，增加更多的服务器来完成系统扩展，称为水平扩展或者向外扩展（Horizontal Scale/Scaling Out）。尽管ES能够利用更强劲的硬件，垂直扩展毕竟还是有它的极限。真正的可扩展性来自于水平扩展，通过向集群中添加更多的节点来分担负载，增加可靠性。ES天生就是分布式的：它知道如何管理多个节点来完成扩展和实现高可用性。这也意味你的应用不需要做任何的改动。 Lucene与ES关系 Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。 Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 ES主要解决问题1）检索相关数据；2）返回统计结果；3）速度要快。 ES工作原理当ElasticSearch的节点启动后，它会利用多播(multicast)(或者单播，如果用户更改了配置)寻找集群中的其它节点，并与之建立连接。这个过程如下图所示： ES数据架构的主要概念（与关系数据库Mysql对比）（1）关系型数据库中的数据库（DataBase），等价于ES中的索引（Index）（2）一个数据库下面有N张表（Table），等价于1个索引Index下面有N多类型（Type），（3）一个数据库表（Table）下的数据由多行（ROW）多列（column，属性）组成，等价于1个Type由多个文档（Document）和多Field组成。（4）在一个关系型数据库里面，schema定义了表、每个表的字段，还有表和字段之间的关系。 与之对应的，在ES中：Mapping定义索引下的Type的字段处理规则，即索引如何建立、索引类型、是否保存原始索引JSON文档、是否压缩原始JSON文档、是否需要分词处理、如何进行分词处理等。（5）在数据库中的增insert、删delete、改update、查search操作等价于ES中的增PUT/POST、删Delete、改_update、查GET. ES核心概念Near Realtime(NRT) 几乎实时Elasticsearch是一个几乎实时的搜索平台。意思是，从索引一个文档到这个文档可被搜索只需要一点点的延迟，这个时间一般为毫秒级。 Cluster：集群ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。 群集是一个或多个节点（服务器）的集合， 这些节点共同保存整个数据，并在所有节点上提供联合索引和搜索功能。一个集群由一个唯一集群ID确定，并指定一个集群名（默认为“elasticsearch”）。该集群名非常重要，因为节点可以通过这个集群名加入群集，一个节点只能是群集的一部分。 确保在不同的环境中不要使用相同的群集名称，否则可能会导致连接错误的群集节点。例如，你可以使用logging-dev、logging-stage、logging-prod分别为开发、阶段产品、生产集群做记录。 Node：节点形成集群的每个服务器称为节点。 节点是单个服务器实例，它是群集的一部分，可以存储数据，并参与群集的索引和搜索功能。就像一个集群，节点的名称默认为一个随机的通用唯一标识符（UUID），确定在启动时分配给该节点。如果不希望默认，可以定义任何节点名。这个名字对管理很重要，目的是要确定你的网络服务器对应于你的ElasticSearch群集节点。 我们可以通过群集名配置节点以连接特定的群集。默认情况下，每个节点设置加入名为“elasticSearch”的集群。这意味着如果你启动多个节点在网络上，假设他们能发现彼此都会自动形成和加入一个名为“elasticsearch”的集群。 在单个群集中，您可以拥有尽可能多的节点。此外，如果“elasticsearch”在同一个网络中，没有其他节点正在运行，从单个节点的默认情况下会形成一个新的单节点名为”elasticsearch”的集群。 Shard：分片 &amp; Replia：副本索引可以存储大量的数据，这些数据可能超过单个节点的硬件限制。例如，十亿个文件占用磁盘空间1TB的单指标可能不适合对单个节点的磁盘或可能太慢服务仅从单个节点的搜索请求。 为了解决这一问题，Elasticsearch提供细分你的指标分成多个块称为分片的能力。当你创建一个索引，你可以简单地定义你想要的分片数量。每个分片本身是一个全功能的、独立的“指数”，可以托管在集群中的任何节点。 Shards分片的重要性主要体现在以下两个特征： 分片允许您水平拆分或缩放内容的大小 分片允许你分配和并行操作的碎片（可能在多个节点上）从而提高性能/吞吐量 这个机制中的碎片是分布式的以及其文件汇总到搜索请求是完全由ElasticSearch管理，对用户来说是透明的。 在同一个集群网络或云环境上，故障是任何时候都会出现的，拥有一个故障转移机制以防分片和结点因为某些原因离线或消失是非常有用的，并且被强烈推荐。为此，Elasticsearch允许你创建一个或多个拷贝，你的索引分片进入所谓的副本或称作复制品的分片，简称Replicas。 Replicas的重要性主要体现在以下两个特征： 副本为分片或节点失败提供了高可用性。为此，需要注意的是，一个副本的分片不会分配在同一个节点作为原始的或主分片，副本是从主分片那里复制过来的。 副本允许用户扩展你的搜索量或吞吐量，因为搜索可以在所有副本上并行执行。 当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。 为提高查询吞吐量或实现高可用性，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。 全文检索全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。 Index索引索引是具有相似特性的文档集合。例如，可以为客户数据提供索引，为产品目录建立另一个索引，以及为订单数据建立另一个索引。索引由名称（必须全部为小写）标识，该名称用于在对其中的文档执行索引、搜索、更新和删除操作时引用索引。在单个群集中，您可以定义尽可能多的索引。 Type类型在索引中，可以定义一个或多个类型。类型是索引的逻辑类别/分区，其语义完全取决于您。一般来说，类型定义为具有公共字段集的文档。例如，假设你运行一个博客平台，并将所有数据存储在一个索引中。在这个索引中，您可以为用户数据定义一种类型，为博客数据定义另一种类型，以及为注释数据定义另一类型。 Document文档文档是可以被索引的信息的基本单位。例如，您可以为单个客户提供一个文档，单个产品提供另一个文档，以及单个订单提供另一个文档。本文件的表示形式为JSON（JavaScript Object Notation）格式，这是一种非常普遍的互联网数据交换格式。 在索引/类型中，您可以存储尽可能多的文档。请注意，尽管文档物理驻留在索引中，文档实际上必须索引或分配到索引中的类型。 Elasticsearch可以做什么?当你经营一家网上商店，你可以让你的客户搜索你卖的商品。在这种情况下，你可以使用ElasticSearch来存储你的整个产品目录和库存信息，为客户提供精准搜索，可以为客户推荐相关商品。 当你想收集日志或者交易数据的时候，需要分析和挖掘这些数据，寻找趋势，进行统计，总结，或发现异常。在这种情况下，你可以使用Logstash或者其他工具来进行收集数据，当这引起数据存储到ElasticsSearch中。你可以搜索和汇总这些数据，找到任何你感兴趣的信息。 对于程序员来说，比较有名的案例是GitHub，GitHub的搜索是基于ElasticSearch构建的，在github.com/search页面，你可以搜索项目、用户、issue、pull request，还有代码。共有40~50个索引库，分别用于索引网站需要跟踪的各种数据。虽然只索引项目的主分支（master），但这个数据量依然巨大，包括20亿个索引文档，30TB的索引文件。 ELK是什么？ELK=elasticsearch+Logstash+kibanaelasticsearch：后台分布式存储以及全文检索logstash: 日志加工、“搬运工”kibana：数据可视化展示。ELK架构为数据分布式存储、可视化查询和日志解析创建了一个功能强大的管理链。 三者相互配合，取长补短，共同完成分布式大数据处理工作。 ES特点和优势1）分布式实时文件存储，可将每一个字段存入索引，使其可以被检索到。2）实时分析的分布式搜索引擎。 分布式：索引分拆成多个分片，每个分片可有零个或多个副本。集群中的每个数据节点都可承载一个或多个分片，并且协调和处理各种操作；负载再平衡和路由在大多数情况下自动完成。 3）可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。也可以运行在单台PC上（已测试）4）支持插件机制，分词插件、同步插件、Hadoop插件、可视化插件等。 参考：Elasticsearch学习，请先看这一篇！ElasticSearch介绍和基本概念]]></content>
      <categories>
        <category>搜索</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中遍历HashMap的5种方式]]></title>
    <url>%2F2019%2F06%2F27%2Fjava-HashMap-access%2F</url>
    <content type="text"><![CDATA[本文转载自：Java中遍历HashMap的5种方式 本教程将为你展示Java中HashMap的几种典型遍历方式。 如果你使用Java8，由于该版本JDK支持lambda表达式，可以采用第5种方式来遍历。 如果你想使用泛型，可以参考方法3。如果你使用旧版JDK不支持泛型可以参考方法4。 通过ForEach循环进行遍历1234567891011121314151617mport java.io.IOException;import java.util.HashMap;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); map.put(1, 10); map.put(2, 20); // Iterating entries using a For Each loop for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue()); &#125; &#125;&#125; ForEach迭代键值对方式如果你只想使用键或者值，推荐使用如下方式 123456789101112131415161718192021import java.io.IOException;import java.util.HashMap;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); map.put(1, 10); map.put(2, 20); // 迭代键 for (Integer key : map.keySet()) &#123; System.out.println("Key = " + key); &#125; // 迭代值 for (Integer value : map.values()) &#123; System.out.println("Value = " + value); &#125; &#125;&#125; 使用带泛型的迭代器进行遍历123456789101112131415161718import java.io.IOException;import java.util.HashMap;import java.util.Iterator;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); map.put(1, 10); map.put(2, 20); Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; entries = map.entrySet().iterator(); while (entries.hasNext()) &#123; Map.Entry&lt;Integer, Integer&gt; entry = entries.next(); System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue()); &#125; &#125;&#125; 使用不带泛型的迭代器进行遍历12345678910111213141516171819202122import java.io.IOException;import java.util.HashMap;import java.util.Iterator;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map map = new HashMap(); map.put(1, 10); map.put(2, 20); Iterator&lt;Map.Entry&gt; entries = map.entrySet().iterator(); while (entries.hasNext()) &#123; Map.Entry entry = (Map.Entry) entries.next(); Integer key = (Integer) entry.getKey(); Integer value = (Integer) entry.getValue(); System.out.println("Key = " + key + ", Value = " + value); &#125; &#125;&#125; 通过Java8 Lambda表达式遍历1234567891011121314import java.io.IOException;import java.util.HashMap;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); map.put(1, 10); map.put(2, 20); map.forEach((k, v) -&gt; System.out.println("key: " + k + " value:" + v)); &#125;&#125; 输出 12key: 1 value:10key: 2 value:20 英文原文：https://www.javatips.net/blog/iterate-hashmap-using-java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库范式]]></title>
    <url>%2F2019%2F06%2F26%2Fdatabase-paradigms%2F</url>
    <content type="text"><![CDATA[转载自：[数据库] 理解数据库范式-通俗易懂 ​ 数据库范式是数据库设计中必不可少的知识，没有对范式的理解，就无法设计出高效率、优雅的数据库。甚至设计出错误的数据库。而想要理解并掌握范式却并不是那么容易。教科书中一般以关系代数的方法来解释数据库范式。这样做虽然能够十分准确的表达数据库范式，但比较抽象，不太直观，不便于理解，更难以记忆。 本文用较为直白的语言介绍范式，旨在便于理解和记忆，这样做可能会出现一些不精确的表述。但对于初学者应该是个不错的入门。我写下这些的目的主要是为了加强记忆，其实我也比较菜，我希望当我对一些概念生疏的时候，回过头来看看自己写的笔记，可以快速地进入状态。如果你发现其中用错误，请指正。 下面开始进入正题： 基础概念要理解范式，首先必须对知道什么是关系数据库，如果你不知道，我可以简单的不能再简单的说一下：关系数据库就是用二维表来保存数据。表和表之间可以……（省略10W字）。 然后你应该理解以下概念： 实体：现实世界中客观存在并可以被区别的事物。比如“一个学生”、“一本书”、“一门课”等等。值得强调的是这里所说的“事物”不仅仅是看得见摸得着的“东西”，它也可以是虚拟的，比如说“老师与学校的关系”。 属性：教科书上解释为：“实体所具有的某一特性”，由此可见，属性一开始是个逻辑概念，比如说，“性别”是“人”的一个属性。在关系数据库中，属性又是个物理概念，属性可以看作是“表的一列”。 元组：表中的一行就是一个元组。 分量：元组的某个属性值。在一个关系数据库中，它是一个操作原子，即关系数据库在做任何操作的时候，属性是“不可分的”。否则就不是关系数据库了。 码：表中可以唯一确定一个元组的某个属性（或者属性组），如果这样的码有不止一个，那么大家都叫候选码，我们从候选码中挑一个出来做老大，它就叫主码。 全码：如果一个码包含了所有的属性，这个码就是全码。 主属性：一个属性只要在任何一个候选码中出现过，这个属性就是主属性。 非主属性：与上面相反，没有在任何候选码中出现过，这个属性就是非主属性。 外码：一个属性（或属性组），它不是码，但是它别的表的码，它就是外码。 六个范式好了，上面已经介绍了我们掌握范式所需要的全部基础概念，下面我们就来讲范式。首先要明白，范式的包含关系。一个数据库设计如果符合第二范式，一定也符合第一范式。如果符合第三范式，一定也符合第二范式…… 第一范式（1NF）：属性不可分在前面已经介绍了属性值的概念，我们说，它是“不可分的”。而第一范式要求属性也不可分。那么它和属性值不可分有什么区别呢？给一个例子： 这个表中，属性值“分”了。“电话”这个属性里对于“小明”属性值分成了两个。 这两种情况都不满足第一范式。不满足第一范式的数据库，不是关系数据库！所以，我们在任何关系数据库管理系统中，做不出这样的“表”来。针对上述情况可以做成这样的表：这个表中，属性 “分”了。也就是“电话”分为了“手机”和“座机”两个属性。 第二范式（2NF）：符合1NF，并且，非主属性完全依赖于码。（注意是完全依赖不能是部分依赖，设有函数依赖W→A，若存在XW，有X→A成立，那么称W→A是局部依赖，否则就称W→A是完全函数依赖） 一个学生上一门课，一定是特定某个老师教。所以有（学生，课程）－&gt;老师； 一个学生上一门课，一定在特定某个教室。所以有（学生，课程）－&gt;教室； 一个学生上一门课，他老师的职称可以确定。所以有（学生，课程）－&gt;老师职称； 一个学生上一门课，一定是特定某个教材。所以有（学生，课程）－&gt;教材 一个学生上一门课，一定在特定时间。所以有（学生，课程）－&gt;上课时间 因此（学生，课程）是一个码。 然而，一个课程，一定指定了某个教材，一年级语文肯定用的是《小学语文1》，那么就有课程－&gt;教材。（学生，课程）是个码，课程却决定了教材，这就叫做不完全依赖，或者说部分依赖。出现这样的情况，就不满足第二范式！ 有什么不好吗？你可以想想： 1、校长要新增加一门课程叫“微积分”，教材是《大学数学》，怎么办？学生还没选课，而学生又是主属性，主属性不能空，课程怎么记录呢，教材记到哪呢? ……郁闷了吧?(插入异常) 2、下学期没学生学一年级语文（上）了，学一年级语文（下）去了，那么表中将不存在一年级语文（上），也就没了《小学语文1》。这时候，校长问：一年级语文（上）用的什么教材啊？……郁闷了吧?(删除异常) 3、校长说：一年级语文（上）换教材，换成《大学语文》。有10000个学生选了这门课，改动好大啊！改累死了……郁闷了吧？（修改/更新异常，在这里你可能觉得直接把教材《小学语文1》替换成《大学语文》不就可以了，但是替换操作虽然计算机运行速度很快，但是毕竟也要替换10000次，造成了很大的时间开销） 那应该怎么解决呢？投影分解，将一个表分解成两个或若干个表 第三范式（3NF）：符合2NF，并且，消除传递依赖（也就是每个非主属性都不传递依赖于候选键，判断传递函数依赖，指的是如果存在”A → B → C”的决定关系，则C传递函数依赖于A。）上面的“学生上课新表”符合2NF，但是它有传递依赖！在哪呢？问题就出在“老师”和“老师职称”这里。一个老师一定能确定一个老师职称。（学生，课程）-&gt;老师-&gt;职称。 有什么问题吗？想想： 1、老师升级了，变教授了，要改数据库，表中有N条，改了N次……（修改异常）2、没人选这个老师的课了，老师的职称也没了记录……（删除异常）3、新来一个老师，还没分配教什么课，他的职称记到哪？……（插入异常）那应该怎么解决呢？和上面一样，投影分解： BC范式（BCNF）：符合3NF，并且，主属性不依赖于主属性(也就是不存在任何字段对任一候选关键字段的传递函数依赖)BC范式既检查非主属性，又检查主属性。当只检查非主属性时，就成了第三范式。满足BC范式的关系都必然满足第三范式。 还可以这么说：若一个关系达到了第三范式，并且它只有一个候选码，或者它的每个候选码都是单属性，则该关系自然达到BC范式。 给你举个例子：假设仓库管理关系表 (仓库ID, 存储物品ID, 管理员ID, 数量)，且有一个管理员只在一个仓库工作；一个仓库可以存储多种物品。 这个数据库表中存在如下决定关系： (仓库ID, 存储物品ID) →(管理员ID, 数量) (管理员ID, 存储物品ID) → (仓库ID, 数量) 所以，(仓库ID, 存储物品ID)和(管理员ID, 存储物品ID)都是StorehouseManage的候选关键字，表中的唯一非关键字段为数量，它是符合第三范式的。但是，由于存在如下决定关系： (仓库ID) → (管理员ID) (管理员ID) → (仓库ID) 即存在关键字段决定关键字段的情况，所以其不符合BCNF范式。它会出现如下异常情况： (1) 删除异常： 当仓库被清空后，所有”存储物品ID”和”数量”信息被删除的同时，”仓库ID”和”管理员ID”信息也被删除了。 (2) 插入异常： 当仓库没有存储任何物品时，无法给仓库分配管理员。 (3) 更新异常： 如果仓库换了管理员，则表中所有行的管理员ID都要修改。 把仓库管理关系表分解为二个关系表： 仓库管理：StorehouseManage(仓库ID, 管理员ID)； 仓库：Storehouse(仓库ID, 存储物品ID, 数量)。 这样的数据库表是符合BCNF范式的，消除了删除异常、插入异常和更新异常。 一般，一个数据库设计符合3NF或BCNF就可以了。在BC范式以上还有第四范式、第五范式。 第四范式：要求把同一表内的多对多关系删除第五范式：从最终结构重新建立原始结构其实数据库设计范式这方面重点掌握的就是1NF、2NF、3NF、BCNF 四种范式之间存在如下关系： 这里主要区别3NF和BCNF，一句话就是3NF是要满足不存在非主属性对候选码的传递函数依赖，BCNF是要满足不存在任一属性（包含非主属性和主属性）对候选码的传递函数依赖。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql的优化问题]]></title>
    <url>%2F2019%2F06%2F26%2Fmysql-optimize%2F</url>
    <content type="text"><![CDATA[MySQL表设计原则表结构设计满足三大范式三大范式◆ 第一范式（1NF）：强调的是列的原子性，即列不能够再分成其他几列。 ◆ 第二范式（2NF）：首先是 1NF，另外包含两部分内容，一是表必须有一个主键；二是没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。 ◆ 第三范式（3NF）：首先是 2NF，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。 第二范式（2NF）和第三范式（3NF）的概念很容易混淆，区分它们的关键点在于 2NF：非主键列是否完全依赖于主键，还是依赖于主键的一部分； 3NF：非主键列是直接依赖于主键，还是直接依赖于非主键列。 范式的优点 范式化的数据库更新起来更加快； 范式化之后，只有很少的重复数据，只需要修改更少的数据； 范式化的表更小，可以在内存中执行； 很少的冗余数据，在查询的时候需要更少的distinct或者group by语句。 范式的缺点范式化的表，在查询的时候经常需要很多的关联，因为单独一个表内不存在冗余和重复数据。这导致，稍微复杂一些的查询语句在查询范式的schema上都可能需要较多次的关联。这会增加让查询的代价，也可能使一些索引策略无效。因为范式化将列存放在不同的表中，而这些列在一个表中本可以属于同一个索引。 适度冗余， 让query尽量减少join虽然optimizer会对query进行一定的优化，但有时候遇见复杂的join，优化效果并不令人满意，再加上本来join的性能开销，所以需要尽量的减少join，而需要通过冗余来实现。比如：有两个数据表分别为用户信息表和用户发帖表，在展示发帖列表时，如果没有冗余的话，两个表要join以取得想要的发帖信息和用户昵称，但如果考虑冗余，用户昵称占用空间不大，如果在发帖表里增加这么一个字段的话，在展示列表时就不用做join操作了，性能会得到很大的改善。 但冗余也会带来一些问题，比如在发帖表里增加了用户昵称字段，就得维护两份用户昵称数据，为了保证数据的一致性，在用户昵称发生改变时，就得向两个表做更新操作，程序中就得做更多的处理。但相比的话，更新频率显然不及查询频率，这样通过增加少量的更新操作会换来更大的性能提升，这也是在项目中经常采用的优化手段。 大字段垂直分拆所谓的大字段，没有一个很严格的标准，常用的是如果一个字段的大小占整条记录的50%以上，我们就视为其为大字段。大字段垂直分拆相比适度冗余是完全相反的操作，适度冗余是将别的表中的字段放进一个表中，而大字段分拆是将自身的大字段拆分出去放进另一个表中。 这两个优化策略貌似是矛盾的，但要根据具体的应用场景来分析，适度冗余是因为在频率较高的查询中要使用该字段，为了减少join的性能开销。而大字段垂直分拆是将在查询中不使用的大字段拿出去，虽然不使用该字段但mysql在查询时并不是只需要访问需要查询的那几个字段，而是读取所有的字段，所以即使不使用字段，mysql也会读取该字段，为了节省IO开销，所以将查询中不常使用的大字段分拆出去。比如：拿博客系统为例，常用的作法是将博客内容从博客列表里分拆出去建立一个博客内容表，因为访问博客列表时并不需要读取博客内容，分拆出去之后，访问博客列表的性能将会大大的提升。但同时访问博客内容时就得做一次join操作了，性能对比的话，join操作两个表是一对一的关系，性能开销会很低。 大表水平分拆举例说明：在一个论坛系统里，管理员经常会发一些帖子，这些帖子要求在每个分类列表里都要置顶。 设计方案一：在发帖表里增加一列用来标示是否是管理员发帖，这样在每个分类列表展示时就需要对发帖表查询两次，一次是置顶帖，一次是普通帖，然后将两次结果合并。如果发帖表内容较大时，查询置顶帖的性能开销会比较大。 设计方案二：将置顶帖存放在一个单独的置顶表里。因为置顶帖数量相比会很少，但访问频率很高，这样从发帖表里分拆开来，访问的性能开销会少很多。 选择合适的数据类型要选择合适的数据类型必须要先了解不同数据类型间的差异。 数字类型有整数类型和浮点数类型，还有一类是通过二进制格式以字符串来存放的数字类型，如DECIMAL(size,d)，其存放长度主要通过定义的size决定，size定义多大，则实际存放就有多长。默认的size为10，d为0。这种类型的存放长度较长而且完全可以用整形来代替实现，所以不推荐使用。 时间类型主要使用DATE，DATETIME和TIMESTAMP三种类型，TIMESTAMP占用存储空间最少，只要4个字节，其它两种类型都要占用8个字节。从存储内容来看，TIMESTAMP只能存储1970年之后的时间，另外两种都能存储从1001开始的时间。 特别要说明的是varchar类型，varchar(size)，在mysql5.0.3之前size表示的是字节数，mysql5.0.3之后size表示的是字符数。这里我们只关注mysql5.0.3之后的表示，size表示的字符数最大限制和字符集有关，如果是gbk编码，最大长度为(65535-1-2)/2=32766，减1的原因是实际行存储从第二个字节开始，减2的原因是varchar头部的2个字节表示长度，除2因为是gbk编码；如果是utf8编码，最大长度为(65535-1-2)/3=21844。 如果数据量一样，但数据类型更小的话，数据存放同样的数据就会占用更少的空间，这样检索同样的数据所带来的IO消耗自然会降低，性能也就很自然的得到提升。此外，mysql对不同类型的数据，处理方式也不一样，比如在运算或者排序操作中，越简单的数据类型操作性能越高，所以对于要频繁进行运算或者排序的字段尽量选择简单的数据类型。 参考：mysql表设计原则]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis内部数据结构详解——skiplist]]></title>
    <url>%2F2019%2F06%2F26%2Fredis-skiplist%2F</url>
    <content type="text"><![CDATA[本文是《Redis内部数据结构详解》系列的第六篇。在本文中，我们围绕一个Redis的内部数据结构——skiplist展开讨论。 Redis里面使用skiplist是为了实现sorted set这种对外的数据结构。sorted set提供的操作非常丰富，可以满足非常多的应用场景。这也意味着，sorted set相对来说实现比较复杂。同时，skiplist这种数据结构对于很多人来说都比较陌生，因为大部分学校里的算法课都没有对这种数据结构进行过详细的介绍。因此，为了介绍得足够清楚，本文会比这个系列的其它几篇花费更多的篇幅。 我们将大体分成三个部分进行介绍： 介绍经典的skiplist数据结构，并进行简单的算法分析。这一部分的介绍，与Redis没有直接关系。我会尝试尽量使用通俗易懂的语言进行描述。 讨论Redis里的skiplist的具体实现。为了支持sorted set本身的一些要求，在经典的skiplist基础上，Redis里的相应实现做了若干改动。 讨论sorted set是如何在skiplist, dict和ziplist基础上构建起来的。 我们在讨论中还会涉及到两个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 我们在讨论中会详细解释这两个配置的含义。 注：本文讨论的代码实现基于Redis源码的3.2分支。 skiplist数据结构简介skiplist本质上也是一种查找结构，用于解决算法中的查找问题（Searching），即根据给定的key，快速查到它所在的位置（或者对应的value）。 我们在《Redis内部数据结构详解》系列的第一篇中介绍dict的时候，曾经讨论过：一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。但skiplist却比较特殊，它没法归属到这两大类里面。 这种数据结构是由William Pugh发明的，最早出现于他在1990年发表的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。对细节感兴趣的同学可以下载论文原文来阅读。 skiplist，顾名思义，首先它是一个list。实际上，它是在有序链表的基础上发展起来的。 我们先来看一个有序链表，如下图（最左侧的灰色节点表示一个空的头结点）： 在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。 假如我们每相邻两个节点增加一个指针，让指针指向下下个节点，如下图： 这样所有新增加的指针连成了一个新的链表，但它包含的节点个数只有原来的一半（上图中是7, 19, 26）。现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如，我们想查找23，查找的路径是沿着下图中标红的指针所指向的方向进行的： 23首先和7比较，再和19比较，比它们都大，继续向后比较。 但23和26比较的时候，比26要小，因此回到下面的链表（原链表），与22比较。 23比22要大，沿下面的指针继续向后和26比较。23比26小，说明待查数据23在原链表中不存在，而且它的插入位置应该在22和26之间。 在这个查找过程中，由于新增加的指针，我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。 利用同样的方式，我们可以在上层新产生的链表上，继续为每相邻的两个节点增加一个指针，从而产生第三层链表。如下图： 在这个新的三层链表结构上，如果我们还是查找23，那么沿着最上层链表首先要比较的是19，发现23比19大，接下来我们就知道只需要到19的后面去继续查找，从而一下子跳过了19前面的所有节点。可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度。 skiplist正是受这种多层链表的想法的启发而设计出来的。实际上，按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到O(log n)。但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新蜕化成O(n)。删除数据也有同样的问题。 skiplist为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程： 从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。 根据上图中的skiplist结构，我们很容易理解这种数据结构的名字的由来。skiplist，翻译成中文，可以翻译成“跳表”或“跳跃表”，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度。 刚刚创建的这个skiplist总共包含4层链表，现在假设我们在它里面依然查找23，下图给出了查找路径： 需要注意的是，前面演示的各个节点的插入过程，实际上在插入之前也要先经历一个类似的查找过程，在确定插入位置后，再完成插入操作。 至此，skiplist的查找和插入操作，我们已经很清楚了。而删除操作与插入操作类似，我们也很容易想象出来。这些操作我们也应该能很容易地用代码实现出来。 当然，实际应用中的skiplist每个节点应该包含key和value两部分。前面的描述中我们没有具体区分key和value，但实际上列表中是按照key进行排序的，查找过程也是根据key在比较。 但是，如果你是第一次接触skiplist，那么一定会产生一个疑问：节点插入时随机出一个层数，仅仅依靠这样一个简单的随机数操作而构建出来的多层链表结构，能保证它有一个良好的查找性能吗？为了回答这个疑问，我们需要分析skiplist的统计性能。 在分析之前，我们还需要着重指出的是，执行插入操作时计算随机数的过程，是一个很关键的过程，它对skiplist的统计特性有着很重要的影响。这并不是一个普通的服从均匀分布的随机数，它的计算过程如下： 首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）。 如果一个节点有第i层(i&gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的概率为p。 节点最大的层数不允许超过一个最大值，记为MaxLevel。 这个计算随机层数的伪码如下所示： 123456randomLevel() level := 1 // random()返回一个[0...1)的随机数 while random() &lt; p and level &lt; MaxLevel do level := level + 1 return level randomLevel()的伪码中包含两个参数，一个是p，一个是MaxLevel。在Redis的skiplist实现中，这两个参数的取值为： 12p = 1/4MaxLevel = 32 skiplist的算法性能分析在这一部分，我们来简单分析一下skiplist的时间复杂度和空间复杂度，以便对于skiplist的性能有一个直观的了解。如果你不是特别偏执于算法的性能分析，那么可以暂时跳过这一小节的内容。 我们先来计算一下每个节点所包含的平均指针数目（概率期望）。节点包含的指针数目，相当于这个算法在空间上的额外开销(overhead)，可以用来度量空间复杂度。 根据前面randomLevel()的伪码，我们很容易看出，产生越高的节点层数，概率越低。定量的分析如下： 节点层数至少为1。而大于1的节点层数，满足一个概率分布。 节点层数恰好等于1的概率为1-p。 节点层数大于等于2的概率为p，而节点层数恰好等于2的概率为p(1-p)。 节点层数大于等于3的概率为p2，而节点层数恰好等于3的概率为p2(1-p)。 节点层数大于等于4的概率为p3，而节点层数恰好等于4的概率为p3(1-p)。 …… 因此，一个节点的平均层数（也即包含的平均指针数目），计算如下： 现在很容易计算出： 当p=1/2时，每个节点所包含的平均指针数目为2； 当p=1/4时，每个节点所包含的平均指针数目为1.33。这也是Redis里的skiplist实现在空间上的开销。 接下来，为了分析时间复杂度，我们计算一下skiplist的平均查找长度。查找长度指的是查找路径上跨越的跳数，而查找过程中的比较次数就等于查找长度加1。以前面图中标出的查找23的查找路径为例，从左上角的头结点开始，一直到结点22，查找长度为6。 为了计算查找长度，这里我们需要利用一点小技巧。我们注意到，每个节点插入的时候，它的层数是由随机函数randomLevel()计算出来的，而且随机的计算不依赖于其它节点，每次插入过程都是完全独立的。所以，从统计上来说，一个skiplist结构的形成与节点的插入顺序无关。 这样的话，为了计算查找长度，我们可以将查找过程倒过来看，从右下方第1层上最后到达的那个节点开始，沿着查找路径向左向上回溯，类似于爬楼梯的过程。我们假设当回溯到某个节点的时候，它才被插入，这虽然相当于改变了节点的插入顺序，但从统计上不影响整个skiplist的形成结构。 现在假设我们从一个层数为i的节点x出发，需要向左向上攀爬k层。这时我们有两种可能： 如果节点x有第(i+1)层指针，那么我们需要向上走。这种情况概率为p。 如果节点x没有第(i+1)层指针，那么我们需要向左走。这种情况概率为(1-p)。 这两种情形如下图所示： 用C(k)表示向上攀爬k个层级所需要走过的平均查找路径长度（概率期望），那么： 12C(0)=0C(k)=(1-p)×(上图中情况b的查找长度) + p×(上图中情况c的查找长度) 代入，得到一个差分方程并化简： 123C(k)=(1-p)(C(k)+1) + p(C(k-1)+1)C(k)=1/p+C(k-1)C(k)=k/p 这个结果的意思是，我们每爬升1个层级，需要在查找路径上走1/p步。而我们总共需要攀爬的层级数等于整个skiplist的总层数-1。 那么接下来我们需要分析一下当skiplist中有n个节点的时候，它的总层数的概率均值是多少。这个问题直观上比较好理解。根据节点的层数随机算法，容易得出： 第1层链表固定有n个节点； 第2层链表平均有n*p个节点； 第3层链表平均有n*p2个节点； … 所以，从第1层到最高层，各层链表的平均节点数是一个指数递减的等比数列。容易推算出，总层数的均值为log1/pn，而最高层的平均节点数为1/p。 综上，粗略来计算的话，平均查找长度约等于： C(log1/pn-1)=(log1/pn-1)/p 即，平均时间复杂度为O(log n)。 当然，这里的时间复杂度分析还是比较粗略的。比如，沿着查找路径向左向上回溯的时候，可能先到达左侧头结点，然后沿头结点一路向上；还可能先到达最高层的节点，然后沿着最高层链表一路向左。但这些细节不影响平均时间复杂度的最后结果。另外，这里给出的时间复杂度只是一个概率平均值，但实际上计算一个精细的概率分布也是有可能的。详情还请参见William Pugh的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。 skiplist与平衡树、哈希表的比较 skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。 在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 Redis中的skiplist实现在这一部分，我们讨论Redis中的skiplist实现。 在Redis中，skiplist被用于实现暴露给外部的一个数据结构：sorted set。准确地说，sorted set底层不仅仅使用了skiplist，还使用了ziplist和dict。这几个数据结构的关系，我们下一章再讨论。现在，我们先花点时间把sorted set的关键命令看一下。这些命令对于Redis里skiplist的实现，有重要的影响。 sorted set的命令举例sorted set是一个有序的数据集合，对于像类似排行榜这样的应用场景特别适合。 现在我们来看一个例子，用sorted set来存储代数课（algebra）的成绩表。原始数据如下： Alice 87.5 Bob 89.0 Charles 65.5 David 78.0 Emily 93.5 Fred 87.5 这份数据给出了每位同学的名字和分数。下面我们将这份数据存储到sorted set里面去： [ 对于上面的这些命令，我们需要的注意的地方包括： 前面的6个zadd命令，将6位同学的名字和分数(score)都输入到一个key值为algebra的sorted set里面了。注意Alice和Fred的分数相同，都是87.5分。 zrevrank命令查询Alice的排名（命令中的rev表示按照倒序排列，也就是从大到小），返回3。排在Alice前面的分别是Emily、Bob、Fred，而排名(rank)从0开始计数，所以Alice的排名是3。注意，其实Alice和Fred的分数相同，这种情况下sorted set会把分数相同的元素，按照字典顺序来排列。按照倒序，Fred排在了Alice的前面。 zscore命令查询了Charles对应的分数。 zrevrange命令查询了从大到小排名为0~3的4位同学。 zrevrangebyscore命令查询了分数在80.0和90.0之间的所有同学，并按分数从大到小排列。 总结一下，sorted set中的每个元素主要表现出3个属性： 数据本身（在前面的例子中我们把名字存成了数据）。 每个数据对应一个分数(score)。 根据分数大小和数据本身的字典排序，每个数据会产生一个排名(rank)。可以按正序或倒序。 Redis中skiplist实现的特殊性我们简单分析一下前面出现的几个查询命令： zrevrank由数据查询它对应的排名，这在前面介绍的skiplist中并不支持。 zscore由数据查询它对应的分数，这也不是skiplist所支持的。 zrevrange根据一个排名范围，查询排名在这个范围内的数据。这在前面介绍的skiplist中也不支持。 zrevrangebyscore根据分数区间查询数据集合，是一个skiplist所支持的典型的范围查找（score相当于key）。 实际上，Redis中sorted set的实现是这样的： 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个dict + 一个skiplist来实现的。简单来讲，dict用来查询数据到分数的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 这里sorted set的构成我们在下一章还会再详细地讨论。现在我们集中精力来看一下sorted set与skiplist的关系，： zscore的查询，不是由skiplist来提供的，而是由那个dict来提供的。 为了支持排名(rank)，Redis里对skiplist做了扩展，使得根据排名能够快速查到数据，或者根据分数查到数据之后，也同时很容易获得排名。而且，根据排名的查找，时间复杂度也为O(log n)。 zrevrange的查询，是根据排名查数据，由扩展后的skiplist来提供。 zrevrank是先在dict中由数据查到分数，再拿分数到skiplist中去查找，查到后也同时获得了排名。 前述的查询过程，也暗示了各个操作的时间复杂度： zscore只用查询一个dict，所以时间复杂度为O(1) zrevrank, zrevrange, zrevrangebyscore由于要查询skiplist，所以zrevrank的时间复杂度为O(log n)，而zrevrange, zrevrangebyscore的时间复杂度为O(log(n)+M)，其中M是当前查询返回的元素个数。 总结起来，Redis中的skiplist跟前面介绍的经典的skiplist相比，有如下不同： 分数(score)允许重复，即skiplist的key允许重复。这在最开始介绍的经典skiplist中是不允许的。 在比较时，不仅比较分数（相当于skiplist的key），还比较数据本身。在Redis的skiplist实现中，数据本身的内容唯一标识这份数据，而不是由key来唯一标识。另外，当多个元素分数相同的时候，还需要根据数据内容来进字典排序。 第1层链表不是一个单向链表，而是一个双向链表。这是为了方便以倒序方式获取一个范围内的元素。 在skiplist中可以很方便地计算出每个元素的排名(rank)。 skiplist的数据结构定义1234567891011121314151617#define ZSKIPLIST_MAXLEVEL 32#define ZSKIPLIST_P 0.25typedef struct zskiplistNode &#123; robj *obj; double score; struct zskiplistNode *backward; struct zskiplistLevel &#123; struct zskiplistNode *forward; unsigned int span; &#125; level[];&#125; zskiplistNode; typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; unsigned long length; int level;&#125; zskiplist; 这段代码出自server.h，我们来简要分析一下： 开头定义了两个常量，ZSKIPLIST_MAXLEVEL和ZSKIPLIST_P，分别对应我们前面讲到的skiplist的两个参数：一个是MaxLevel，一个是p。 zskiplistNode定义了skiplist的节点结构。 obj字段存放的是节点数据，它的类型是一个string robj。本来一个string robj可能存放的不是sds，而是long型，但zadd命令在将数据插入到skiplist里面之前先进行了解码，所以这里的obj字段里存储的一定是一个sds。有关robj的详情可以参见系列文章的第三篇：《Redis内部数据结构详解(3)——robj》。这样做的目的应该是为了方便在查找的时候对数据进行字典序的比较，而且，skiplist里的数据部分是数字的可能性也比较小。 score字段是数据对应的分数。 backward字段是指向链表前一个节点的指针（前向指针）。节点只有1个前向指针，所以只有第1层链表是一个双向链表。 level[]存放指向各层链表后一个节点的指针（后向指针）。每层对应1个后向指针，用forward字段表示。另外，每个后向指针还对应了一个span值，它表示当前的指针跨越了多少个节点。span用于计算元素排名(rank)，这正是前面我们提到的Redis对于skiplist所做的一个扩展。需要注意的是，level[]是一个柔性数组（flexible array member），因此它占用的内存不在zskiplistNode结构里面，而需要插入节点的时候单独为它分配。也正因为如此，skiplist的每个节点所包含的指针数目才是不固定的，我们前面分析过的结论——skiplist每个节点包含的指针数目平均为1/(1-p)——才能有意义。 zskiplist定义了真正的skiplist结构，它包含： 头指针header和尾指针tail。 链表长度length，即链表包含的节点总数。注意，新创建的skiplist包含一个空的头指针，这个头指针不包含在length计数中。 level表示skiplist的总层数，即所有节点层数的最大值。 下图以前面插入的代数课成绩表为例，展示了Redis中一个skiplist的可能结构： 注意：图中前向指针上面括号中的数字，表示对应的span的值。即当前指针跨越了多少个节点，这个计数不包括指针的起点节点，但包括指针的终点节点。 假设我们在这个skiplist中查找score=89.0的元素（即Bob的成绩数据），在查找路径中，我们会跨域图中标红的指针，这些指针上面的span值累加起来，就得到了Bob的排名(2+2+1)-1=4（减1是因为rank值以0起始）。需要注意这里算的是从小到大的排名，而如果要算从大到小的排名，只需要用skiplist长度减去查找路径上的span累加值，即6-(2+2+1)=1。 可见，在查找skiplist的过程中，通过累加span值的方式，我们就能很容易算出排名。相反，如果指定排名来查找数据（类似zrange和zrevrange那样），也可以不断累加span并时刻保持累加值不超过指定的排名，通过这种方式就能得到一条O(log n)的查找路径。 Redis中的sorted set我们前面提到过，Redis中的sorted set，是在skiplist, dict和ziplist基础上构建起来的: 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个叫zset的数据结构来实现的，这个zset包含一个dict + 一个skiplist。dict用来查询数据到分数(score)的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 在这里我们先来讨论一下前一种情况——基于ziplist实现的sorted set。在本系列前面关于ziplist的文章里，我们介绍过，ziplist就是由很多数据项组成的一大块连续内存。由于sorted set的每一项元素都由数据和score组成，因此，当使用zadd命令插入一个(数据, score)对的时候，底层在相应的ziplist上就插入两个数据项：数据在前，score在后。 ziplist的主要优点是节省内存，但它上面的查找操作只能按顺序查找（可以正序也可以倒序）。因此，sorted set的各个查询操作，就是在ziplist上从前向后（或从后向前）一步步查找，每一步前进两个数据项，跨域一个(数据, score)对。 随着数据的插入，sorted set底层的这个ziplist就可能会转成zset的实现（转换过程详见t_zset.c的zsetConvert）。那么到底插入多少才会转呢？ 还记得本文开头提到的两个Redis配置吗？ 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 这个配置的意思是说，在如下两个条件之一满足的时候，ziplist会转成zset（具体的触发条件参见t_zset.c中的zaddGenericCommand相关代码）： 当sorted set中的元素个数，即(数据, score)对的数目超过128的时候，也就是ziplist数据项超过256的时候。 当sorted set中插入的任意一个数据的长度超过了64的时候。 最后，zset结构的代码定义如下： 1234typedef struct zset &#123; dict *dict; zskiplist *zsl;&#125; zset; Redis为什么用skiplist而不用平衡树？在前面我们对于skiplist和平衡树、哈希表的比较中，其实已经不难看出Redis里使用skiplist而不用平衡树的原因了。现在我们看看，对于这个问题，Redis的作者 @antirez 是怎么说的： There are a few reasons: 1) They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. 2) A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. 3) They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 这段话原文出处： https://news.ycombinator.com/item?id=1171423 这里从内存占用、对范围查找的支持和实现难易程度这三方面总结的原因，我们在前面其实也都涉及到了。 系列下一篇我们将介绍intset，以及它与Redis对外暴露的数据类型set的关系，敬请期待。 其它精选文章： Redis内部数据结构详解(5)——quicklist Redis内部数据结构详解(4)——ziplist Redis内部数据结构详解(3)——robj Redis内部数据结构详解(2)——sds Redis内部数据结构详解(1)——dict 你需要了解深度学习和神经网络这项技术吗？ 技术的正宗与野路子 论人生之转折 编程世界的熵增原理 Android端外推送到底有多烦？ Android和iOS开发中的异步处理（四）——异步任务和队列 用树型模型管理App数字和红点提示 原创文章，转载请注明出处，并包含下面的二维码！否则拒绝转载！http://zhangtielei.com/posts/blog-redis-skiplist.html]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BitMap对海量无重复的整数排序]]></title>
    <url>%2F2019%2F06%2F26%2Fbitmap%2F</url>
    <content type="text"><![CDATA[转载自：bitmap对海量无重复的整数排序 现在有n个无重复的正整数（n 小于10的7次方），如果内存限制在1.5M以内，要求对着n个数进行排序。【编程珠玑第一章题目】 很显然，10的7次方个整数占用的空间为10 ^ 7 * 4字节，大约等于40M，而内存限制为1.5M，因此，无法将所有数字加载到内存，所以快速排序、堆排序等高效的排序算法就没法使用。这里可以使用bitmap方式，用1bit表示一个整数，那么，10^7个整数需要10^7位，也就是大约1.25M空间。 如下是bitmap对无重复整数的排序过程。 一次bitmap就可以将所有数据排完如果每个整数占一位，可以将所有的整数在内存中表示（如上述提到的那样），那么可以直接一次bitmap排序就完成了，时间复杂度为O(n)，空间复杂度为O(n位)。下面分别给出C和C++的bitset方式： C语言方式 下面代码以n = 100为例子；n是海量时，只要每个整数1bit可以一次在内存中表示所有整数的情况下，方法一样，将宏定义N的值改为海量数据的上限（比如10^7）即可： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//位图排序#include &lt;iostream&gt;#include &lt;bitset&gt;#define WIDTHWORD 32 //一个整数的宽度是32bit#define SHIFT 5 #define MASK 0x1F //0x1f == 31#define N 100 //对100个无重复的整数排序using namespace std; //申请一个N位的bitmapint bitmap[1 + N / WIDTHWORD]; //将bitmap的第value设置为1void set(int value) &#123; bitmap[value &gt;&gt; SHIFT] |= (1 &lt;&lt; (value &amp; MASK));&#125; //清除bitmap第value位上的1:设置为0void clear(int value) &#123; bitmap[value &gt;&gt; SHIFT] &amp;= ~(1 &lt;&lt; (value &amp; MASK));&#125; //测试bitmap第value位是否为1int test(int value) &#123; return bitmap[value &gt;&gt; SHIFT] &amp; (1 &lt;&lt; (value &amp; MASK));&#125; int main() &#123; int a[] = &#123;12, 5, 1, 89, 64, 49, 77, 91, 3, 0, 32, 50, 99&#125;; int length = sizeof(a) / sizeof(int); //将bitmap所有位设置为0 for (int i = 0; i &lt; N; ++i) &#123; clear(i); &#125; //bitmap中将待排序数组中值所在的位设置为1 for (int i = 0; i &lt; length; i++) set(a[i]); //输出排序后的结果 for (int i = 0; i &lt; N; ++i) &#123; if (test(i)) cout &lt;&lt; i &lt;&lt; &quot; &quot;; &#125;&#125; 如上代码中：N表示待排序整数的上限，例如本题要求的10^7。那么申请一个N位大小的bitmap： int bitmap[1 + N / WIDTHWORD];设置、清除、测试函数的含义可以参考文章：http://blog.163.com/xb_stone_yinyang/blog/static/2118160372013625112558579/，下面给出这几个函数的简要解释： 对于一个整数value，要将其对应到bitmap中的第value位，如果设置第value位为1呢？ 看设置函数：value &gt;&gt; SHIFT 是找到value在bitmap中对应的是第几个int型数的位置，例如整数100，它对应的是int数组（也就是bitmap）的第 100 &gt;&gt; 5 == 100 / 32 == 3个int型的位置（从0开始计数，每个int型占据32位）；然后再在int数组（也就是bitmap）的第3个位置中寻找需要将第几位设置为1： 1 &lt;&lt; (value &amp; 0x1f) == 1 &lt;&lt; 100 &amp; 31 == 1 &lt;&lt; 4，即要将1左移四位就是要设置为1的那一位；bitmap[value &gt;&gt; SHIFT] |= (1 &lt;&lt; (value &amp; MASK)); 最终完成将bitmap的第100位设置为1。 对于一个整数value，如何将其对应到bitmap中的那位的上的1清除掉呢？ 看清除函数，和设置函数一样，value &gt;&gt; SHIFT 是找到value在bitmap中对应的是第几个整型的位置；然后，1 &lt;&lt; (value &amp; 0x1f)在找到的那个整型的位置中判断要将该字节的哪一位设置为0；bitmap[value &gt;&gt; SHIFT] &amp;= ~(1 &lt;&lt; (value &amp; MASK));完成最终清除工作。 对于一个整数value，如何测试在bitmap中是否包含该数，也就是bitmap中第value位上是否为1？ 也是先找到value对应bitmap中第几个整型位置，然后在该位置中找到对应的位，再看该位上是否为1，为1表示bitmap中包含value。 程序排序结果： 使用C++的bitset1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;bitset&gt;#define N 100using namespace std; int main() &#123; int a[] = &#123;12, 5, 1, 89, 64, 49, 77, 91, 3, 0, 32, 50, 99&#125;; int length = sizeof(a) / sizeof(int); //直接使用C++bitset，申请Nbit的空间，每一位均设置为0 bitset&lt;N&gt; bitmap; //遍历待排序数组，将bitmap中对应位设置为1 for (int i = 0; i &lt; length; i++) bitmap.set(a[i], 1); //输入排序结果 for (int i = 0; i &lt; N; ++i) &#123; if (bitmap[i]) cout &lt;&lt; i &lt;&lt; " "; &#125;&#125; 需要多次bitmap排序 如果上限N更大或者进一步限制内存大小（例如，将内存限制在0.5M之内），那么一次bitmap就不能将所有数据排序。需要多次bitmap排序。 例如上面排序小于100的一些数，我们上面的一次bitmap，是申请100位的bitmap；但是，如果限制我们只能使用30位bitmap，那么就需要排序100 / 30 + 1次：第一次排序0 ~ 29之间的数，第二次排序30 ~ 59之间的数，第三次排序60 ~ 89之间的数，第四次排序90 ~ 100之间的数。如果是k次bitmap排序，那么时间复杂度为O(kn)，空间开销为O(n / k 位). 12345678910111213141516171819202122232425262728293031下面只给出C++方式，C方式类似：```cint main() &#123; int a[] = &#123;12, 5, 1, 89, 64, 49, 77, 91, 3, 0, 32, 50, 99&#125;; int length = sizeof(a) / sizeof(int); //假设还是有小于100的不重复整数需要排序，但是 //不能申请100位空间，只能申请30位空间，那么，需要 //排序的次数如下： int sort_times = N / 30 + 1; //那么，第一趟先排序0-29，第二趟排序30-59， //第三趟排序60-89，第四趟排序剩下的 bitset&lt;30&gt; bitmap; //只能申请30位的bitmap for (int times = 0; times &lt; sort_times; ++times) &#123; //一共进行四趟排序 bitmap.reset(); //记得每次排序前将bitmap清空为0 for (int i = 0; i &lt; length; i++) &#123; if (a[i] &gt;= 30 * times &amp;&amp; a[i] &lt; 30 * (times + 1)) bitmap.set(a[i] - 30 * times); &#125; for (int i = 0; i &lt; 30; ++i) &#123; if (bitmap[i]) cout &lt;&lt; i + 30 * times &lt;&lt; &quot; &quot;; &#125; &#125;&#125; 如果每个整数最多出现m次，如何排序?上述两部分讨论的是如果整数是不重复时的排序，那么，如果海量整数，每个整数允许重复，但是重复次数不超过m（例如m == 10），如何排序？方法：如果每个整数重复出现次数不超过10次，那么，可以用4位表示一个整数，用这四位统计该数出现次数，然后排序后输出该数时，输出m次即可。 除了排序，bitmap的其他用途如上，bitmap可以用于不重复正整数排序，那么，bitmap其他用途：1、找出不重复数：在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数。2、判断某数是否存在于海量整数中：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>BitMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中创建实例对象的方式]]></title>
    <url>%2F2019%2F06%2F26%2Fjava-create-instance%2F</url>
    <content type="text"><![CDATA[1、关键字 new。工厂模式是对这种方式的包装； 2、类实现克隆接口，克隆一个实例。原型模式是一个应用实例； 3、用该类的加载器，newInstance。java的反射，反射使用实例：Spring的依赖注入、切面编程中动态代理 4、sun.misc.Unsafe类，allocateInstance方法创建一个实例。（Java官方也不建议直接使用的Unsafe类，据说Oracle正在计划从Java 9中去掉Unsafe类） 5、实现序列化接口的类，通过IO流反序列化读取一个类，获得实例。 参考：Java创建类的实例的几种方法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis中基本数据类型与内部存储结构]]></title>
    <url>%2F2019%2F06%2F26%2Fredis-storage-structure%2F</url>
    <content type="text"><![CDATA[本文转载自：Redis-基本数据类型与内部存储结构 概览Redis是典型的Key-Value类型数据库，Key为字符类型，Value的类型常用的为五种类型：String、Hash 、List 、 Set 、 Ordered Set Redis内部内存管理 redisObject 核心对象 Redis 内部使用一个 redisObject 对象来表示所有的 key 和 value。 type ：代表一个 value 对象具体是何种数据类型。 encoding ：是不同数据类型在 redis 内部的存储方式，比如：type=string 代表 value 存储的是一个普通字符串，那么对应的 encoding 可以是 raw 或者是 int，如果是 int 则代表实际 redis 内部是按数值型类存储和表示这个字符串的，当然前提是这个字符串本身可以用数值表示，比如：”123” “456”这样的字符串。 vm 字段：只有打开了 Redis 的虚拟内存功能，此字段才会真正的分配内存，该功能默认是关闭状态的。 Redis 使用 redisObject 来表示所有的 key/value 数据是比较浪费内存的，当然这些内存管理成本的付出主要也是为了给 Redis 不同数据类型提供一个统一的管理接口，实际作者也提供了多种方法帮助我们尽量节省内存使用。 Key（键值） 官网Key链接：https://redis.io/commands#generic 过期删除过期数据的清除从来不容易，为每一条key设置一个timer，到点立刻删除的消耗太大，每秒遍历所有数据消耗也大，Redis使用了一种相对务实的做法： 当client主动访问key会先对key进行超时判断，过时的key会立刻删除。 如果clien永远都不再get那条key呢？ 它会在Master的后台，每秒10次的执行如下操作： 随机选取100个key校验是否过期，如果有25个以上的key过期了，立刻额外随机选取下100个key(不计算在10次之内)。可见，如果过期的key不多，它最多每秒回收200条左右，如果有超过25%的key过期了，它就会做得更多，但只要key不被主动get，它占用的内存什么时候最终被清理掉只有天知道。 常用操作 Key的长度限制：Key的最大长度不能超过1024字节，在实际开发时不要超过这个长度，但是Key的命名不能太短，要能唯一标识缓存的对，作者建议按照在关系型数据库中的库表唯一标识字段的方式来命令Key的值，用分号分割不同数据域，用点号作为单词连接。 Key的查询：Keys，返回匹配的key，支持通配符如 “keys a*” 、 “keys a?c”，但不建议在生产环境大数据量下使用。 对Key对应的Value进行的排序：Sort命令对集合按数字或字母顺序排序后返回或另存为list，还可以关联到外部key等。因为复杂度是最高的O(N+Mlog(M))*(N是集合大小，M 为返回元素的数量)，有时会安排到slave上执行。官网链接https://redis.io/commands/sort Key的超时操作：Expire（指定失效的秒数）/ExpireAt（指定失效的时间戳）/Persist（持久化）/TTL（返回还可存活的秒数），关于Key超时的操作。默认以秒为单位，也有p字头的以毫秒为单位的版本 String（字符串类型的Value） 可以是String，也可是是任意的byte[]类型的数组，如图片等。String 在 redis 内部存储默认就是一个字符串，被 redisObject 所引用，当遇到 incr,decr 等操作时会转成数值型进行计算，此时 redisObject 的 encoding 字段为int。 https://redis.io/commands#string 大小限制：最大为512Mb，基本可以存储任意图片啦。 常用命令的时间复杂度为O(1)，读写一样的快。 对String代表的数字进行增减操作（没有指定的Key则设置为0值，然后在进行操作）：Incr/IncrBy/IncrByFloat/Decr/DecrBy（原子性），** 可以用来做计数器，做自增序列，也可以用于限流，令牌桶计数等**。key不存在时会创建并贴心的设原值为0。IncrByFloat专门针对float。。 设置Value的安全性：SetNx命令仅当key不存在时才Set（原子性操作）。可以用来选举Master或做分布式锁：所有Client不断尝试使用SetNx master myName抢注Master，成功的那位不断使用Expire刷新它的过期时间。如果Master倒掉了key就会失效，剩下的节点又会发生新一轮抢夺。SetEx， Set + Expire 的简便写法，p字头版本以毫秒为单位。 获取：GetSet（原子性）， 设置新值，返回旧值。比如一个按小时计算的计数器，可以用GetSet获取计数并重置为0。这种指令在服务端做起来是举手之劳，客户端便方便很多。MGet/MSet/MSetNx， 一次get/set多个key。 其他操作：Append/SetRange/GetRange/StrLen，对文本进行扩展、替换、截取和求长度，只对特定数据格式如字段定长的有用，json就没什么用。 BitMap的用法：GetBit/SetBit/BitOp,与或非/BitCount， BitMap的玩法，比如统计今天的独立访问用户数时，每个注册用户都有一个offset，他今天进来的话就把他那个位设为1，用BitCount就可以得出今天的总人数。 Hash（HashMap，哈希映射表） Redis 的 Hash 实际是内部存储的 Value 为一个 HashMap，并提供了直接存取这个 Map 成员的接口。Hash将对象的各个属性存入Map里，可以只读取/更新对象的某些属性。另外不同的模块可以只更新自己关心的属性而不会互相并发覆盖冲突。 不同程序通过 key（用户 ID） + field（属性标签）就可以并发操作各自关心的属性数据 https://redis.io/commands#hash 实现原理Redis Hash 对应 Value 内部实际就是一个 HashMap，实际这里会有2种不同实现，** 这个 Hash 的成员比较少时 Redis 为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的 HashMap 结构，对应的 value redisObject 的 encoding 为 zipmap，当成员数量增大时会自动转成真正的 HashMap，此时 encoding 为 ht**。一般操作复杂度是O(1)，要同时操作多个field时就是O(N)，N是field的数量。 常用操作 O(1)操作：hget、hset等等 O(n)操作：hgetall Redis 可以直接取到全部的属性数据，但是如果内部 Map 的成员很多，那么涉及到遍历整个内部 Map 的操作， 由于 Redis 单线程模型的缘故，这个遍历操作可能会比较耗时，而另其它客户端的请求完全不响应，这点需要格外注意。 List（双向链表） Redis list 的应用场景非常多，也是 Redis 最重要的数据结构之一，比如 twitter 的关注列表，粉丝列表等都可以用 Redis 的 list 结构来实现，还提供了生产者消费者阻塞模式（B开头的命令,block,表示阻塞），常用于任务队列，消息队列等。 实现方式Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis 内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 用作消息队列中防止数据丢失的解决方法 如果消费者把job给Pop走了又没处理完就死机了怎么办？ 消息生产者保证不丢失 加多一个sorted set，分发的时候同时发到list与sorted set，以分发时间为score，用户把job做完了之后要用ZREM消掉sorted set里的job，并且定时从sorted set中取出超时没有完成的任务，重新放回list。 如果发生重复可以在sorted set中在查询确认一遍，或者将消息的消费接口设计成幂等性。 消息消费者保证不丢失 为每个worker多加一个的list，弹出任务时改用RPopLPush，将job同时放到worker自己的list中，完成时用LREM消掉。如果集群管理(如zookeeper)发现worker已经挂掉，就将worker的list内容重新放回主list 常用操作 复合操作：RPopLPush/ BRPopLPush，弹出来返回给client的同时，把自己又推入另一个list，是原子操作。 RPUSH key value [value …] 将一个或多个值 value 插入到列表 key 的表尾(最右边)。 12&gt; LRANGE KEY_NAME START END&gt; Redis Lrange 返回列表中指定区间内的元素，区间以偏移量 START 和 END(从0开始) 指定。 其中 0 表示列表的第一个元素， 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 按值进行的操作：LRem(按值删除元素)、LInsert(插在某个值的元素的前后)，复杂度是O(N)，N是List长度，因为List的值不唯一，所以要遍历全部元素，而Set只要O(log(N))。 按下表进行操作（下标从0开始，队列从左到右算，下标为负数时则从右到左，-1为右端第一个元素） 时间复杂度为O(N) LSet ：按下标设置元素值。（N为List的长度） LIndex：按下标返回元素。（N为index的值） LTrim：限制List的大小，保留指定范围的元素。（N是移除元素的个数） LRange：返回列表内指定范围下标的元素，常用于分页。（N = start+range） set（HashSet） Set就是HashSet，可以将重复的元素随便放入而Set会自动去重，底层实现也是HashMap，并且 set 提供了判断某个成员是否在一个 set 集合内的重要接口，这个也是 list 所不能提供的。 实现原理set 的内部实现是一个 value 永远为 null 的 HashMap，实际就是通过计算 hash 的方式来快速排重的，这也是 set 能提供判断一个成员是否在集合内的原因。 常用操作 增删改查：SAdd/SRem/SIsMember/SCard/SMove/SMembers等等。除了SMembers都是O(1)。 集合操作：SInter/SInterStore/SUnion/SUnionStore/SDiff/SDiffStore，各种集合操作。交集运算可以用来显示在线好友(在线用户 交集 好友列表)，共同关注(两个用户的关注列表的交集)。O(N)，并集和差集的N是集合大小之和，交集的N是小的那个集合的大小的2倍。 Sorted Set（插入有序Set集合） set 不是自动有序的，而** sorted set 可以通过用户额外提供一个优先级（score）的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么可以选择 sorted set 数据结构，比如 twitter 的 public **timeline 可以以发表时间作为 score 来存储，这样获取时就是自动按时间排好序的。 实现方式 内部使用 HashMap 和跳跃表（SkipList）来保证数据的存储和有序 Sorted Set的实现是HashMap(element-&gt;score, 用于实现ZScore及判断element是否在集合内)，和SkipList(score-&gt;element,按score排序)的混合体。SkipList有点像平衡二叉树那样，不同范围的score被分成一层一层，每层是一个按score排序的链表。 常用操作 ZAdd/ZRem是O(log(N))；ZRangeByScore/ZRemRangeByScore是O(log(N)+M)，N是Set大小，M是结果/操作元素的个数。复杂度的log取对数很关键，可以使，1000万大小的Set，复杂度也只是几十不到。但是，如果一次命中很多元素M很大则复杂度很高。 ZRange/ZRevRange，按排序结果的范围返回元素列表，可以为正数与倒数。 ZRangeByScore/ZRevRangeByScore，按score的范围返回元素，可以为正数与倒数。 ZRemRangeByRank/ZRemRangeByScore，按排序/按score的范围限删除元素。 ZCount，统计按score的范围的元素个数。 ZRank/ZRevRank ，显示某个元素的正/倒序的排名。 ZScore/ZIncrby，显示元素的Score值/增加元素的Score。 ZAdd(Add)/ZRem(Remove)/ZCard(Count)，ZInsertStore(交集)/ZUnionStore(并集)，与Set相比，少了IsMember和差集运算。 Redis使用与内存优化 上面的一些实现上的分析可以看出 redis 实际上的内存管理成本非常高，即占用了过多的内存，属于用空间换时间。作者对这点也非常清楚，所以提供了一系列的参数和手段来控制和节省内存 建议不要开启VM（虚拟内存）选项 VM 选项是作为 Redis 存储超出物理内存数据的一种数据在内存与磁盘换入换出的一个持久化策略，将严重地拖垮系统的运行速度，所以要关闭 VM 功能，请检查你的 redis.conf 文件中 vm-enabled 为 no。 设置最大内存选项最好设置下 redis.conf 中的 maxmemory 选项，该选项是告诉 Redis 当使用了多少物理内存后就开始拒绝后续的写入请求，该参数能很好的保护好你的 Redis 不会因为使用了过多的物理内存而导致 swap，最终严重影响性能甚至崩溃。 一般还需要设置内存饱和回收策略 volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 控制内存使用的参数 Redis 为不同数据类型分别提供了一组参数来控制内存使用 Hash redis.conf 配置文件中下面2项 *hash-max-zipmap-entries 64 * 含义是当 value 这个 Map 内部不超过多少个成员时会采用线性紧凑格式存储，默认是64，即 value 内部有64个以下的成员就是使用线性紧凑存储zipmap，超过该值自动转成真正的 HashMap(ht)。 hash-max-zipmap-value 512 hash-max-zipmap-value 含义是当 value 这个 Map 内部的每个成员值长度不超过 多少字节就会采用线性紧凑存储zipmap来节省空间。 以上2个条件任意一个条件超过设置值都会转换成真正的 HashMap，也就不会再节省内存了，但是也不是越大越好（空间和查改效率需要根据实际情况来权衡） List list-max-ziplist-entries 512list 数据类型多少节点以下会采用去指针的紧凑存储格式ziplist list-max-ziplist-value 64list 数据类型节点值大小小于多少字节会采用紧凑存储格式ziplist。 Set set-max-intset-entries 512set 数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储 Redis内部的优化 Redis 内部实现没有对内存分配方面做过多的优化，在一定程度上会存在内存碎片，不过大多数情况下这个不会成为 Redis 的性能瓶颈。 Redis 缓存了一定范围的常量数字作为资源共享，在很多数据类型是数值型则能极大减少内存开销，默认为1-10000，可以重新编译配置修改源代码中的一行宏定义 REDIS_SHARED_INTEGERS。 总结 根据业务需要选择合适的数据类型，并为不同的应用场景设置相应的紧凑存储参数。 当业务场景不需要数据持久化时，关闭所有的持久化方式可以获得最佳的性能以及最大的内存使用量。 如果需要使用持久化，根据是否可以容忍重启丢失部分数据在快照方式与语句追加方式之间选择其一，不要使用虚拟内存以及 diskstore 方式。 不要让你的 Redis 所在机器物理内存使用超过实际内存总量的3/5。 Redis 的持久化使用了 Buffer IO ，所谓 Buffer IO 是指 Redis 对持久化文件的写入和读取操作都会使用物理内存的 Page Cache，而当 Redis 的持久化文件过大操作系统会进行Swap，这时你的系统就会有内存还有余量但是系统不稳定或者崩溃的风险。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nagle算法]]></title>
    <url>%2F2019%2F06%2F26%2Fnagle%2F</url>
    <content type="text"><![CDATA[Nagle算法用于对缓冲区内的一定数量的消息进行自动连接。该处理过程(称为Nagling)，通过减少必须发送的封包的数量，提高了网络应用程序系统的效率。 Nagle算法的规则 （可参考tcp_output.c文件里tcp_nagle_check函数注释）： 如果包长度达到MSS，则允许发送； MSS是最大分段大小Maxitum Segment Size ，TCP双方建立连接的时候可以协商MTU是最大传输单元Maxitum Transmission Unit 如果该包含有FIN，则允许发送； 设置了TCP_NODELAY选项，则允许发送； 未设置TCP_CORK选项时，若所有发出去的包均被确认，或所有发出去的小数据包(包长度小于MSS)均被确认，则允许发送。 对于规则4，就是说一个TCP连接上最多只能有一个未被确认的小数据包，在该分组的确认到达之前，不能发送其他的小数据包。如果某个小分组的确认被延迟了，那么后续小分组的发送就会相应的延迟。也就是说延迟确认影响的并不只是被延迟确认的那个数据包，而是后续所有的应答包。 举个例子： 当开启nagle算法时，客户端首先发送大小为1字节的第一个分组，随后其它分组到达发送缓冲区，由于上一个分组的应答还没有收到，所以TCP会先缓存新来的这4个小分组，并将其重新分组，组成一个大小为8(2+3+1+2)字节的”较大的”小分组。当第一个小分组的应答收到后，客户端将这个8字节的分组发送。总共发送的报文段（分组）个数为2。 Nagle算法的门槛实际上Nagle算法并不是很复杂，他的主要职责是数据的累积，实际上有三个门槛： 缓冲区中的字节数达到了一定量； 等待了一定的时间（一般的Nagle算法都是等待200ms）； 紧急数据发送。 这三个门槛的任何一个达到都必须发送数据了。一般情况下，如果数据流量很大，第二个条件是永远不会起作用的，但当发送小的数据包时，第二个门槛就发挥作用了，防止数据被无限的缓存在缓冲区不是好事情哦。 Nagle算法的选项配置TCP_NODELAY和TCP_CORK都是禁用Nagle算法，只不过NODELAY完全关闭而TCP_CORK完全由自己决定发送时机。Linux文档上说两者不要同时设置。 TCP_NODELAY 选项设置该选项: setsockopt(s,IPPRO_TCP,TCP_NODELAY,(const char*)&amp;on,sizeof(int));读取该选项: getsockopt(s,IPPRO_TCP,TCP_NODELAY,(char*)&amp;on,&amp;optlen); 默认情况下, 发送数据采用Nagle 算法。Nagle 算法是指发送方发送的数据不会立即发出,而是先放在缓冲区, 等缓存区满了再发出. 发送完一批数据后, 会等待接收方对这批数据的回应,然后再发送下一批数据。 Nagle 算法适用于发送方需要发送大批量数据, 并且接收方会及时作出回应的场合, 这种算法通过减少传输数据的次数来提高通信效率。如果发送方持续地发送小批量的数据, 并且接收方不一定会立即发送响应数据, 那么Nagle算法会使发送方运行很慢。 TCP_CORK选项 TCP链接的过程中，默认开启Nagle算法，进行小包发送的优化。优化网络传输，兼顾网络延时和网络拥塞。这个时候可以置位TCP_NODELAY关闭Nagle算法，有数据包的话直接发送保证网络时效性。 在进行大量数据发送的时候可以置位TCP_CORK关闭Nagle算法保证网络利用性。尽可能的进行数据的组包，以最大mtu传输，如果发送的数据包大小过小则如果在0.6 到 0.8S范围内都没能组装成一个MTU时，直接发送。如果发送的数据包大小足够间隔在0.45内时，每次组装一个MTU进行发送。如果间隔大于0.4 到 0.8S则，每过来一个数据包就直接发送。 Nagle组织包的长度是由系统决定的，有时候我们知道我们会每个1分钟产生1字节，共1000字节。如果完全由Nagle算法来发送的话，可能还是会1字节1字节发送[这是一种极端情况，假设返回ACK时间不是很长]。这个时候首先设置TCP_CORK能够阻塞住TCP[尽量阻塞住]，等我们write完1000字节之后，取消TCP_CORK，这个时候就能够将1000字节一次发出。 总结 TCP_CORK选项与TCP_NODELAY一样，是控制Nagle化的。 打开TCP_NODELAY选项，则意味着无论数据包是多么的小，都立即发送（不考虑拥塞窗口）。 如果将TCP连接比喻为一个管道，那TCP_CORK选项的作用就像一个塞子。 设置TCP_CORK选项，就是用塞子塞住管道，而取消TCP_CORK选项，就是将塞子拔掉。当TCP_CORK选项被设置时，TCP链接不会发送任何的小包，即只有当数据量达到MSS时，才会被发送。一般当数据传输完成时，通常需要取消该选项，以防被塞住，这样才可以让不够MSS大小的包能及时发出去。 参考：Nagle算法–TCP缓冲区管理算法]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>Nagle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么单线程的redis这么快]]></title>
    <url>%2F2019%2F06%2F25%2Fwhy-redis-quick%2F</url>
    <content type="text"><![CDATA[纯内存访问，redis将所有数据都放在内存中，内存响应时间大约为100纳秒，这是redis达到每秒万级级别访问的重要基础。完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 非阻塞IO，redis使用epoll作为IO多路复用技术的实现，再加上redis自身事件处理模型将epoll中的链接、读写、关闭都转换为事件，不在网络IO上浪费过多的事件。 123多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快。 单线程避免了线程切换和竟态产生的消耗。避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；1.单线程简化数据结构和算法的实现。2.单线程避免线程切换和竟态产生的消耗。缺点：如果命令执行时间过程，会导致其它命令阻塞。 参考： 单线程的redis为什么达到每秒万级的处理速度？为什么说Redis是单线程的以及Redis为什么这么快！]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MSS和MTU的关系]]></title>
    <url>%2F2019%2F06%2F24%2Fmss-mtu%2F</url>
    <content type="text"><![CDATA[MTU（Maximum Transmission Unit）最大传输单元，在TCP/IP协议族中，指的是IP数据报能经过一个物理网络的最大报文长度，其中包括了IP首部(从20个字节到60个字节不等)，一般以太网的MTU设为1500字节，加上以太帧首部的长度14字节，也就是一个以太帧不会超过1500+14 = 1514字节。 MSS（Maximum Segment Size)，最大报文段大小，指的是TCP报文（一种IP协议的上层协议）的最大数据报长度，其中不包括TCP首部长度。MSS由TCP链接的过程中由双方协商得出，其中SYN字段中的选项部分包括了这个信息。如果MSS+TCP首部+IP首部大于MTU，那么IP报文就会存在分片，如果小于，那么就可以不需要分片正常发送。 参考: TCP 中的Sequence Number]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>MSS</tag>
        <tag>MTU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP方法GET和POST的区别]]></title>
    <url>%2F2019%2F06%2F24%2Fget-post%2F</url>
    <content type="text"><![CDATA[当第一次面试的时候被问到说一说get和post有什么区别。当时就说了一大堆很普遍很基础的答案，什么post比get安全，get比post传的少什么的。然而，面试官问，还有呢？ 好家伙，面试完回去百度，整理了网上一堆的get和post的区别。整理如下： 从http标准来看，get比post安全，这里的安全是针对服务器而言的，get用于获取数据，不会引起数据的变化，并且是安全和幂等(多次请求，效果一致)的。而post是有可能引起数据的变化。 从应用角度来看，get参数是放在URL后面(通过URL传递)，参数直接暴露在URL上，所以不能用来传递敏感信息，而post提交的数据会放置在Request body中 get请求会被浏览器主动缓存post不会，除非手动设置。从这里看，post安全性比get高。 GET请求只能进行url编码，而POST支持多种编码方式。 GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。 GET在浏览器回退时是无害的，而POST会再次提交请求。 GET产生的URL地址可以被Bookmark，而POST不可以。 对参数的数据类型，GET只接受ASCII字符，而POST没有限制。 Get方式的提交顶多是1024字节，理论上post没有限制，可以传较大量的数据。 Get一般是获取数据，post是向服务器提交修改的数据。 再然后呢，去面试的时候自然自信满满。结果，还有没有点别的，感觉有种答非所问的赶脚。好吧，回去继续百度深造。结果是这样的： GET和POST是什么?HTTP协议中的两种发送请求的方法。HTTP是什么?HTTP是基于TCP/IP的关于数据如何在万维网中如何通信的协议。HTTP的底层是TCP/IP。所以GET和POST的底层也是TCP/IP，也就是说，GET/POST都是TCP链接 。GET和POST能做的事情是一样一样的。你要给GET加上request body，给POST带上url参数，技术上是完全行的通的。 那所谓的区别呢？ 运输快递需要车辆，而TCP就像车辆，但如果车辆全部按自己的想法走，交通就会瘫痪，所以交通规则HTTP诞生了。HTTP规定了运输方式：get、post、head、options、put、delete等。当执行get请求时，车上贴get标签，货物放在上层运输。如果是post请求，车上贴post标签，货物放在下层运输。当然，get方式也可以把货物放在下层，但是这样是算get还是post呢？所以，HTTP只是个行为准则，而TCP才是GET和POST怎么实现的基本。 而关于传递的参数大小问题，是这样的。 过大的数据量会增加运输成本，超出的部分，概不处理，那还不如乖按照规定走。所以，浏览器通常都会限制url长度在2K个字节，而(大多数)服务器最多处理64K大小的url。如果你用GET服务，在request body偷偷藏了数据，不同服务器的处理方式也是不同的，有些服务器会帮你卸货，读出数据，有些服务器直接忽略，所以，虽然GET可以带request body，也不能保证一定能被接收到哦。 GET和POST还有一个重大区别，简单的说： GET产生一个TCP数据包;POST产生两个TCP数据包。对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200(返回数据);而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok(返回数据) 总结： 语义不同：get获取数据；post提交数据 GET是用来向获取服务器信息的，请求报文传输的信息只是用于描述所需资源的参数，返回的信息才是数据本身；POST是用来向服务器传递数据的，其请求报文传递的信息就是数据本身，返回的报文只是操作的结果。这是GET和POST最重要的区别，没有之一。 get在浏览器回退时是无害的，而post会再次请求 get产生的url地址可以被收藏，而post不会 get请求会被浏览器主动缓存，而post不会，除非手动设置 使用GET，你可能会有意无意就享受到从浏览器到代理到网络服务商再到服务器各个网络组件一层一层的透明缓存；而POST默认是不可缓存的,需要你显式使用缓存相关Header。 get请求只能进行url编码，而post支持多种编码方式 get请求参数会被完整保留在浏览历史记录里，而post中的参数不会被保留 get 请求在url中传送的参数有长度限制，而post没有 使用载荷的POST比使用URI的GET可以传递更多数据。HTTP协议本身没有限制传递信息的最大值，但具体的限制受各个网络组件的实现影响，一般而言，GET的长度限制更短。当然用来获取数据的GET方式，绝大多数情况也用不着传递那么多的数据。 对参数的数据类型，get只接受ascll字符，而post没有限制 get比post更安全，因为参数直接暴露在url上，所以不能用来传递敏感信息 简单来说HTTP的安全(Safe)指的是是否改变服务器资源的状态，即是我们平常说的有无副作用。因为提交数据的目的往往是为了改变服务器状态，所以POST不是安全的(Safe)；而GET是为了获取数据，所以它不应该改变服务器的状态，是安全的(Safe)。HTTP中的安全Safe(副作用)和大多数人平时想的安全Security(例如数据安全)，仅仅是共用一个中文词汇，实质上就是雷峰和雷峰塔的关系，从这个角度上来说GET反而比POST安全多了。为此我建议大家把安全留给更加常用的Security,中文使用一个更加少歧义的说法来描述GET和POST的第二个区别:POST是一个可能有副作用的方法，但GET应是没有副作用的的。如果要保证传输的安全，请使用HTTPS。 get参数通过url传递，post放在request body中 GET产生一个TCP数据包;POST产生两个TCP数据包。 有些浏览器在发送POST的Ajax请求时会在发送POST请求前先发送一个HEAD，而GET请求则是直接发送。 get是幂等的，post是非幂等的 ‘POST是非幂等的‘就是你提交完表单后，按F5后浏览器会弹框要求你重复确认是否刷新的原因。 参考：关于面试被问到post和get的区别？？？[原][经典面试题]带你深入理解HTTP中GET和POST的区别]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>get</tag>
        <tag>post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的CopyOnWriteArraylist]]></title>
    <url>%2F2019%2F06%2F24%2Fjava-CopyOnWriteArraylist%2F</url>
    <content type="text"><![CDATA[转载自：https://www.xttblog.com/?p=4006 CopyOnWriteArrayList简介CopyOnWriteArrayList是一个并发容器。有很多人称它是线程安全的，我认为这句话不严谨，缺少一个前提条件，那就是非复合场景下操作它是线程安全的。 Copy-On-Write 简称 COW，是一种用于程序设计中的优化策略。其基本思路是，从一开始大家都在共享同一个内容，当某个人想要修改这个内容的时候，才会真正把内容 Copy 出去形成一个新的内容然后再改，这是一种延时懒惰策略。 Java 并发包提供了很多线程安全的集合，有了他们的存在，使得我们在多线程开发下，大大简化了多线程开发的难度，但是如果不知道其中的原理，可能会引发意想不到的问题，所以知道其中的原理还是很有必要的。 CopyOnWriteArrayList原理CopyOnWrite 容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行 Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对 CopyOnWrite 容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以 CopyOnWrite 容器也是一种读写分离的思想，读和写不同的容器。 add 方法下面我们看看它的 add 方法的源码： 123456789101112131415161718public boolean add(E e) &#123; //1.获得独占锁 final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray();//2.获得Object[] int len = elements.length;//3.获得elements的长度 Object[] newElements = Arrays.copyOf(elements, len + 1);//4.复制到新的数组 newElements[len] = e;//5.将add的元素添加到新元素 setArray(newElements);//6.替换之前的数据 return true; &#125; finally &#123; lock.unlock();//7.释放独占锁 &#125;&#125;final Object[] getArray() &#123; return array;&#125; CopyOnWrite的名字就是这样来的。在写的时候，先 copy 一个，操作新的对象。然后在覆盖旧的对象，保证 volatile语义。 看完这个源码后，我们来看几个常见的面试题。 CopyOnWriteArrayList 有什么优点？ 读写分离，适合写少读多的场景。使用了独占锁，支持多线程下的并发写。 CopyOnWriteArrayList 是如何保证写时线程安全的？ 因为用了ReentrantLock独占锁，保证同时只有一个线程对集合进行修改操作。 CopyOnWrite 怎么理解？ 写时复制。就是在写的时候，先 copy 一个，操作新的对象。然后在覆盖旧的对象，保证 volatile 语义。新数组的长度等于旧数组的长度 + 1。 从 add 方法的源码中你可以看出 CopyOnWriteArrayList 的缺点是什么？ 占用内存，写时 copy 效率低。因为 CopyOnWrite 的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（注意:在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说 200M 左右，那么再写入 100M 数据进去，内存就会占用 300M，那么这个时候很有可能造成频繁的 Yong GC 和 Full GC。 get 方法get 的源码分析。 123456public E get(int index) &#123; return get(getArray(), index);&#125;final Object[] getArray() &#123; return array;&#125; get 方法很简单。但是会出现一个很致命的问题，那就是一致性问题。 当我们获得了 array 后，由于整个 get 方法没有独占锁，所以另外一个线程还可以继续执行修改的操作，比如执行了 remove 的操作，remove 和 add 一样，也会申请独占锁，并且复制出新的数组，删除元素后，替换掉旧的数组。而这一切 get 方法是不知道的，它不知道 array 数组已经发生了天翻地覆的变化。就像微信一样，虽然对方已经把你给删了，但是你不知道。这就是一个一致性问题。 CopyOnWrite 容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用 CopyOnWrite 容器。 set方法set 方法解读。 1234567891011121314151617181920212223public E set(int index, E element) &#123; //(1)获得独占锁 final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray();//(2)获得array E oldValue = get(elements, index);//(3)根据下标,获得旧的元素 if (oldValue != element) &#123;//(4)如果旧的元素不等于新的元素 int len = elements.length;//(5)获得旧数组的长度 Object[] newElements = Arrays.copyOf(elements, len);//(6)复制出新的数组 newElements[index] = element;//(7)修改 setArray(newElements);//(8)替换 &#125; else &#123; //(9)为了保证volatile 语义，即使没有修改，也要替换成新的数组 // Not quite a no-op; ensures volatile write semantics setArray(elements); &#125; return oldValue; &#125; finally &#123; lock.unlock();//(10)释放独占锁 &#125;&#125; 上面的代码，我写的都有注释，相信大家都能看明白。 set 的时候，同样会获得一个独占锁，来保证写的线程安全。修改操作，实际上操作的是 array 的一个副本，最后才把 array 给替换掉。所以，修改和 add 很相似。set、add、remove 是互斥的。 removeremove 方法解读。 1234567891011121314151617181920212223 final ReentrantLock lock = this.lock;//获得独占锁 lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length;// 旧数组的长度 E oldValue = get(elements, index); int numMoved = len - index - 1; if (numMoved == 0)//判断是否是删除数组尾部的最后一个元素 //则复制出一个长度为【旧数组的长度-1】的新数组 setArray(Arrays.copyOf(elements, len - 1)); else &#123; //如果要删除的元素不是最后一个，则分两次复制，随之替换。 Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); &#125; return oldValue; &#125; finally &#123; lock.unlock(); &#125;&#125; 研究 java 自带的一些数据结构，你会发现设计的都很巧妙。大师就是大师啊。 迭代器迭代器的主要源码如下： 12345678910111213141516171819202122public Iterator&lt;E&gt; iterator() &#123; return new COWIterator&lt;E&gt;(getArray(), 0);&#125;static final class COWIterator&lt;E&gt; implements ListIterator&lt;E&gt; &#123; private final Object[] snapshot; private int cursor; private COWIterator(Object[] elements, int initialCursor) &#123; cursor = initialCursor; snapshot = elements; &#125; // 判断是否还有下一个元素 public boolean hasNext() &#123; return cursor &lt; snapshot.length; &#125; //获取下个元素 @SuppressWarnings("unchecked") public E next() &#123; if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++]; &#125;&#125; 调用iterator 方法获取迭代器，内部会调用 COWIterator 的构造方法，此构造方法有两个参数，第一个参数就是 array 数组，第二个参数是下标，就是 0。随后构造方法中会把 array 数组赋值给snapshot变量。snapshot 是“快照”的意思，如果 Java 基础尚可的话，应该知道数组是引用类型，传递的是指针，如果有其他地方修改了数组，这里应该马上就可以反应出来，那为什么又会是 snapshot这样的命名呢？没错，如果其他线程没有对 CopyOnWriteArrayList 进行增删改的操作，那么 snapshot 就是本身的 array，但是如果其他线程对 CopyOnWriteArrayList 进行了增删改的操作，旧的数组会被新的数组给替换掉，但是 snapshot 还是原来旧的数组的引用。也就是说 当我们使用迭代器便利 CopyOnWriteArrayList 的时候，不能保证拿到的数据是最新的，这也是一致性问题。 CopyOnWriteArrayList 的使用场景通过源码分析，我们看出它的优缺点比较明显，所以使用场景也就比较明显。就是合适读多写少的场景。 CopyOnWriteArrayList 的缺点 由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致young gc或者 full gc。 不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个 set 操作后，读取到数据可能还是旧的，虽然CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求。 由于实际使用中可能没法保证 CopyOnWriteArrayList 到底要放置多少数据，万一数据稍微有点多，每次 add/set 都要重新复制数组，这个代价实在太高昂了。在高性能的互联网应用中，这种操作分分钟引起故障。 CopyOnWriteArrayList 的设计思想 读写分离，读和写分开 最终一致性 使用另外开辟空间的思路，来解决并发冲突]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>CopyOnWriteArrayList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中常用的线程安全的类]]></title>
    <url>%2F2019%2F06%2F24%2Fjava-Thread-safe-classes%2F</url>
    <content type="text"><![CDATA[概念线程安全就是当多线程访问时，采用了加锁的机制；即当一个线程访问该类的某个数据时，会对这个数据进行保护，其他线程不能对其访问，直到该线程读取完之后，其他线程才可以使用。防止出现数据不一致或者数据被污染的情况。线程不安全：就是不提供数据访问时的数据保护，多个线程能够同时操作某个数据，从而出现数据不一致或者数据污染的情况。 对于线程不安全的问题，一般会使用synchronized关键字加锁同步控制。 工作原理：jvm中有一个main memory(主内存)对象，每一个线程也有自己的working memory(工作内存)，一个线程对于一个变量variable进行操作的时候， 都需要在自己的working memory里创建一个copy,操作完之后再写入main memory。 当多个线程操作同一个变量variable，就可能出现不可预知的结果。 而用synchronized的关键是建立一个监控monitor，这个monitor可以是要修改的变量，也可以是其他自己认为合适的对象(方法)，然后通过给这个monitor加锁来实现线程安全，每个线程在获得这个锁之后，要执行完加载load到working memory 到 use &amp;&amp; 指派assign 到 存储store 再到 main memory的过程。才会释放它得到的锁。这样就实现了所谓的线程安全。 线程安全(Thread-safe)的集合对象： Vector HashTable StringBuffer 非线程安全的集合对象： ArrayList LinkedList HashMap HashSet TreeMap TreeSet StringBulider 相关集合对象比较Vector、ArrayList、LinkedList Vector：Vector与ArrayList一样，也是通过数组实现的，不同的是它支持线程的同步，即某一时刻只有一个线程能够写Vector，避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此，访问它比访问ArrayList慢。 ArrayList：a. 当操作是在一列数据的后面添加数据而不是在前面或者中间，并需要随机地访问其中的元素时，使用ArrayList性能比较好。b. ArrayList是最常用的List实现类，内部是通过数组实现的，它允许对元素进行快速随机访问。数组的缺点是每个元素之间不能有间隔，当数组大小不满足时需要增加存储能力，就要讲已经有数组的数据复制到新的存储空间中。当从ArrayList的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。因此，它适合随机查找和遍历，不适合插入和删除。 LinkedList：a. 当对一列数据的前面或者中间执行添加或者删除操作时，并且按照顺序访问其中的元素时，要使用LinkedList。b. LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，他还提供了List接口中没有定义的方法，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。 Vector和ArrayList在使用上非常相似，都可以用来表示一组数量可变的对象应用的集合，并且可以随机的访问其中的元素。 HashTable、HashMap、HashSetHashTable和HashMap采用的存储机制是一样的，不同的是： HashMap：a. 采用数组方式存储key-value构成的Entry对象，无容量限制；b. 基于key hash查找Entry对象存放到数组的位置，对于hash冲突采用链表的方式去解决；c. 在插入元素时，可能会扩大数组的容量，在扩大容量时须要重新计算hash，并复制对象到新的数组中；d. 是非线程安全的；e. 遍历使用的是Iterator迭代器； f. 键和值都允许为null，最多有一个键为null的元素 HashTable：a. 是线程安全的；b. 无论是key还是value都不允许有null值的存在；在HashTable中调用Put方法时，如果key为null，直接抛出NullPointerException异常；c. 遍历使用的是Enumeration列举； HashSet：a. 基于HashMap实现，无容量限制；b. 是非线程安全的；c. 不保证数据的有序； TreeSet、TreeMapTreeSet和TreeMap都是完全基于Map来实现的。 TreeSet：a. 基于TreeMap实现的，支持排序；b. 是非线程安全的； TreeMap：a. 典型的基于红黑树的Map实现，因此它要求一定要有key比较的方法，要么传入Comparator比较器实现，要么key对象实现Comparator接口；b. 是非线程安全的； StringBuffer和StringBuliderStringBuilder与StringBuffer都继承自AbstractStringBuilder类，在AbstractStringBuilder中也是使用字符数组保存字符串。 在执行速度方面的比较：StringBuilder &gt; StringBuffer ； 他们都是字符串变量，是可改变的对象，每当我们用它们对字符串做操作时，实际上是在一个对象上操作的，不像String一样创建一些对象进行操作，所以速度快； StringBuilder：线程非安全的； StringBuffer：线程安全的； 对于String、StringBuffer和StringBulider三者使用的总结：1.如果要操作少量的数据用 = String2.单线程操作字符串缓冲区 下操作大量数据 = StringBuilder3.多线程操作字符串缓冲区 下操作大量数据 = StringBuffer Java中常见的线程安全的类 通过synchronized 关键字给方法加上内置锁来实现线程安全 Timer，TimerTask，Vector，Stack，HashTable，StringBuffer 原子类Atomicxxx—包装类的线程安全类如AtomicLong，AtomicInteger等等 Atomicxxx 是通过Unsafe 类的native(CAS)方法实现线程安全的 BlockingQueue 和BlockingDequeBlockingDeque接口继承了BlockingQueue接口，BlockingQueue 接口的实现类有ArrayBlockingQueue ，LinkedBlockingQueue ，PriorityBlockingQueue, 而BlockingDeque接口的实现类有LinkedBlockingDeque, BlockingQueue和BlockingDeque 都是通过使用定义为final的ReentrantLock作为类属性显式加锁实现同步的 CopyOnWriteArrayList和 CopyOnWriteArraySet CopyOnWriteArraySet的内部实现是在其类内部声明一个final的CopyOnWriteArrayList属性，并在调用其构造函数时实例化该CopyOnWriteArrayList，CopyOnWriteArrayList采用的是显式地加上ReentrantLock实现同步，而CopyOnWriteArrayList(读写分离)容器的线程安全性在于在每次修改时都会创建并重新发布一个新的容器副本，从而实现可变性。 Concurrentxxx最常用的就是ConcurrentHashMap，当然还有ConcurrentSkipListSet和ConcurrentSkipListMap等等。 ConcurrentHashMap使用了一种完全不同的加锁策略来提供更高的并发性和伸缩性。ConcurrentHashMap并不是将每个方法都在同一个锁上同步并使得每次只能有一个线程访问容器，而是使用一种粒度更细的加锁机制——分段锁来实现更大程度的共享。在这种机制中，任意数量的读取线程可以并发访问Map，执行读取操作的线程和执行写入操作的线程可以并发地访问Map，并且一定数量的写入线程可以并发地修改Map，这使得在并发环境下吞吐量更高，而在单线程环境中只损失非常小的性能。 ThreadPoolExecutor ThreadPoolExecutor也是使用了ReentrantLock显式加锁同步。 Collections中的synchronizedCollection(Collection c)方法可将一个集合变为线程安全，其内部通过synchronized关键字加锁同步。 参考Java常见的线程安全的类]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP协议详解]]></title>
    <url>%2F2019%2F06%2F23%2FTCP%2F</url>
    <content type="text"><![CDATA[前言小到基于应用层做网络开发，大到生活中无处不在的网络。我们在享受这个便利的时候，没有人会关心它如此牢固的底层基石是如何搭建的。而这些基石中很重要的一环就是tcp协议。翻看一下“三次握手”和“四次挥手”，本以为这就是tcp了，其实不然。它仅仅解决了连接和关闭的问题，传输的问题才是tcp协议更重要，更难，更复杂的问题。回头看tcp协议的原理，会发现它为了承诺上层数据传输的“可靠”，不知要应对多少网络中复杂多变的情况。简单直白列举一下： 怎么保证数据都是可靠呢？—连接确认！关闭确认！收到数据确认！各种确认！！ 因为网络或其他原因，对方收不到数据怎么办？–超时重试 网络情况千变万化，超时时间怎么确定？–根据RTT动态计算 反反复复，不厌其烦的重试，导致网络拥塞怎么办？—慢启动，拥塞避免，快速重传，快速恢复 发送速度和接收速度不匹配怎么办？–滑动窗口 滑动窗口滑的过程中，他一直告诉我处理不过来了，不让传数据了怎么办？–ZWP 滑动窗口滑的过程中，他处理得慢，就理所当然的每次让我发很少的数据，导致网络利用率很低怎么办？—Nagle 其中任何一个小环节，都凝聚了无数的算法，我们没有能力理解各个算法的实现，但是需要了解下tcp实现者的思路历程。 梳理完所有内容，大概可以知道： tcp提供哪些机制保证了数据传输的可靠性？ tcp连接的“三次握手”和关闭的“四次挥手”流程是怎么样的？ tcp连接和关闭过程中，状态是如何变化的？ tcp头部有哪些字段，分别用来做什么的？ tcp的滑动窗口协议是什么？ 超时重传的机制是什么？ 如何避免传输拥塞？ 概述tcp连接的特点 提供面向连接的，1可靠1的1字节流1服务 为上层应用层提供服务，不关心具体传输的内容是什么，也不知道是二进制流，还是ascii字符。 tcp的可靠性如何保证 分块传送：数据被分割成最合适的数据块（UDP的数据报长度不变） 等待确认：通过定时器等待接收端发送确认请求，收不到确认则重发 确认回复：收到确认后发送确认回复(不是立即发送，通常推迟几分之一秒) 数据校验：保持首部和数据的校验和，检测数据传输过程有无变化 乱序排序：接收端能重排序数据，以正确的顺序交给应用端 重复丢弃：接收端能丢弃重复的数据包 流量缓冲：两端有固定大小的缓冲区（滑动窗口），防止速度不匹配丢数据 tcp的首部格式宏观位置 从应用层-&gt;传输层-&gt;网络层-&gt;链路层，每经过一次都会在报文中增加相应的首部。 TCP数据被封装在IP数据报中 首部格式 tcp首部数据通常包含20个字节（不包括任选字段） 第1-2两个字节：源端口号 第3-4两个字节：目的端口号 源端口号+ip首部中的源ip地址+目的端口号+ip首部中的目的ip地址，唯一的确定了一个tcp连接。对应编码级别的socket。 第5-8四个字节：32位序号。tcp提供全双工服务，两端都有各自的序号。 编号：解决网络包乱序的问题 序号如何生成：不能是固定写死的，否则断网重连时序号重复使用会乱套。tcp基于时钟生成一个序号，每4微秒加一，到2^32-1时又从0开始 第9-12四个字节：32位确认序列号。上次成功收到数据字节序号加1，ack为1才有效。确认号：解决丢包的问题 第13位字节：首部长度。因为任选字段长度可变 后面6bite：保留 随后6bite：标识位。控制各种状态 第15-16两个字节：窗口大小。接收端期望接收的字节数。解决流量控制的问题 第17-18两个字节：校验和。由发送端计算和存储，由接收端校验。解决数据正确性问题 第19-20两个字节：紧急指针 标识位说明 URG：为1时，表示紧急指针有效 ACK：确认标识，连接建立成功后，总为1。为1时确认号有效 PSH：接收方应尽快把这个报文交给应用层 RST：复位标识，重建连接 SYN：建立新连接时，该位为0 FIN：关闭连接标识 tcp选项格式 每个选项开始是1字节kind字段，说明选项的类型 kind为0和1的选项，只占一个字节 其他kind后有一字节len，表示该选项总长度（包括kind和len） kind为11，12，13表示tcp事务 MSS 最长报文大小 最常见的可选字段 MSS只能出现在SYN时传过来（第一次握手和第二次握手时） 指明本端能接收的最大长度的报文段 建立连接时，双方都要发送MSS 如果不发送，默认为536字节 连接的建立与释放连接建立的“三次握手”三次握手流程TCP协议中，主动发起请求的一端称为『客户端』，被动连接的一端称为『服务端』。不管是客户端还是服务端，TCP连接建立完后都能发送和接收数据。起初，服务器和客户端都为CLOSED状态。在通信开始前，双方都得创建各自的传输控制块（TCB）。 服务器创建完TCB后遍进入LISTEN状态，此时准备接收客户端发来的连接请求。 第一次握手客户端向服务端发送连接请求报文段。该报文段的头部中SYN=1，ACK=0，seq=x。请求发送后，客户端便进入SYN-SENT状态。 123PS1：SYN=1，ACK=0表示该报文段为连接请求报文。PS2：x为本次TCP通信的字节流的初始序号。TCP规定：SYN=1的报文段不能有数据部分，但要消耗掉一个序号。 第二次握手服务端收到连接请求报文段后，如果同意连接，则会发送一个应答：SYN=1，ACK=1，seq=y，ack=x+1。该应答发送完成后便进入SYN-RCVD状态。 123PS1：SYN=1，ACK=1表示该报文段为连接同意的应答报文，ACK为1表示ack字段有效。PS2：seq=y表示服务端作为发送者时，发送字节流的初始序号。PS3：ack=x+1表示服务端希望下一个数据报发送序号从x+1开始的字节。 第三次握手当客户端收到连接同意的应答后，还要向服务端发送一个确认报文段，表示：服务端发来的连接同意应答已经成功收到。该报文段的头部为：ACK=1，seq=x+1，ack=y+1。客户端发完这个报文段后便进入ESTABLISHED状态，服务端收到这个应答后也进入ESTABLISHED状态，此时连接的建立完成！ 为什么连接建立需要三次握手，而不是两次握手？防止失效的连接请求报文段被服务端接收，从而产生错误。 1PS：失效的连接请求：若客户端向服务端发送的连接请求丢失，客户端等待应答超时后就会再次发送连接请求，此时，上一个连接请求就是『失效的』。 若建立连接只需两次握手，客户端并没有太大的变化，仍然需要获得服务端的应答后才进入ESTABLISHED状态，而服务端在收到连接请求后就进入ESTABLISHED状态。此时如果网络拥塞，客户端发送的连接请求迟迟到不了服务端，客户端便超时重发请求，如果服务端正确接收并确认应答，双方便开始通信，通信结束后释放连接。此时，如果那个失效的连接请求抵达了服务端，由于只有两次握手，服务端收到请求就会进入ESTABLISHED状态，等待发送数据或主动发送数据。但此时的客户端早已进入CLOSED状态，服务端将会一直等待下去，这样浪费服务端连接资源。 之所以存在 3-way hanshake 的说法，是因为 TCP 是双向通讯协议，作为响应一方(Responder) 要想初始化发送通道，必须也进行一轮 SYN + ACK。由于 SYN ACK 在 TCP 分组头部是两个标识位，因此处于优化目的被合并了。所以达到双方都能进行收发的状态只需要 3 个分组。 在谢希仁著《计算机网络》第四版中讲“三次握手”的目的是“为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误”。在另一部经典的《计算机网络》一书中讲“三次握手”的目的是为了解决“网络中存在延迟的重复分组”的问题。这两种不用的表述其实阐明的是同一个问题。谢希仁版《计算机网络》中的例子是这样的，“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。 连接关闭的“四次挥手”四次挥手流程 TCP连接的释放一共需要四步，因此称为『四次挥手』。我们知道，TCP连接是双向的，因此在四次挥手中，前两次挥手用于断开一个方向的连接，后两次挥手用于断开另一方向的连接。 第一次挥手若A认为数据发送完成，则它需要向B发送连接释放请求。该请求只有报文头，头中携带的主要参数为：FIN=1，seq=u。此时，A将进入FIN-WAIT-1状态。 12PS1：FIN=1表示该报文段是一个连接释放请求。PS2：seq=u，u-1是A向B发送的最后一个字节的序号。 第二次挥手B收到连接释放请求后，会通知相应的应用程序，告诉它A向B这个方向的连接已经释放。此时B进入CLOSE-WAIT状态，并向A发送连接释放的应答，其报文头包含：ACK=1，seq=v，ack=u+1。 123PS1：ACK=1：除TCP连接请求报文段以外，TCP通信过程中所有数据报的ACK都为1，表示应答。PS2：seq=v，v-1是B向A发送的最后一个字节的序号。PS3：ack=u+1表示希望收到从第u+1个字节开始的报文段，并且已经成功接收了前u个字节。 A收到该应答，进入FIN-WAIT-2状态，等待B发送连接释放请求。 第二次挥手完成后，A到B方向的连接已经释放，B不会再接收数据，A也不会再发送数据。但B到A方向的连接仍然存在，B可以继续向A发送数据。 第三次挥手当B向A发完所有数据后，向A发送连接释放请求，请求头：FIN=1，ACK=1，seq=w，ack=u+1。B便进入LAST-ACK状态。 第四次挥手A收到释放请求后，向B发送确认应答，此时A进入TIME-WAIT状态。该状态会持续2MSL时间，若该时间段内没有B的重发请求的话，就进入CLOSED状态，撤销TCB。当B收到确认应答后，也便进入CLOSED状态，撤销TCB。 为什么A要先进入TIME-WAIT状态，等待2MSL时间后才进入CLOSED状态？为了保证B能收到A的确认应答。若A发完确认应答后直接进入CLOSED状态，那么如果该应答丢失，B等待超时后就会重新发送连接释放请求，但此时A已经关闭了，不会作出任何响应，因此B永远无法正常关闭。 time_wait状态 也称为2MSL等待状态，MSL=Maximum Segment LifetIme，报文段最大生存时间，根据不同的tcp实现自行设定。常用值为30s，1min，2min。linux一般为30s。 12345MSL是Maximum Segment Lifetime英文的缩写，中文可以译为“报文最大生存时间”，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为tcp报文（segment）是ip数据报（datagram）的数据部分，具体称谓请参见《数据在网络各层中的称呼》一文，而ip头中有一个TTL域，TTL是time to live的缩写，中文可以译为“生存时间”，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个ip数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减1，当此值为0则数据报将被丢弃，同时发送ICMP报文通知源主机。RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。 2MSL即两倍的MSL，TCP的TIME_WAIT状态也称为2MSL等待状态，当TCP的一端发起主动关闭，在发出最后一个ACK包后，即第3次握手完成后发送了第四次握手的ACK包后就进入了TIME_WAIT状态，必须在此状态上停留两倍的MSL时间，等待2MSL时间主要目的是怕最后一个ACK包对方没收到，那么对方在超时后将重发第三次握手的FIN包，主动关闭端接到重发的FIN包后可以再发一个ACK应答包。在TIME_WAIT状态时两端的端口不能使用，要等到2MSL时间结束才可继续使用。当连接处于2MSL等待阶段时任何迟到的报文段都将被丢弃。不过在实际应用中可以通过设置SO_REUSEADDR选项达到不必等待2MSL时间结束再使用此端口。TTL与MSL是有关系的但不是简单的相等的关系，MSL要大于等于TTL。 主动关闭的一方发送最后一个ack所处的状态 这个状态必须维持2MSL等待时间为什么SYN和FIN会消耗一个序列号为什么在建立连接的时候，发送的 SYN 包大小（payload）明明是0字节，但是接收端却返回 ACK = 1 ，还有断开连接的时候 FIN 包也被视为含有1字节的数据。 原因是 SYN 和 FIN 信号都是需要 acknowledgement (确认)的，也就是你必须回复这个信号，如果它不占有一个字节的话，要如何判断你是回复这个信号还是回复这个信号之前的包呢？ 例如：如果 FIN 信号不占用一个字节，回复 FIN 的 ack 包就可能被误认为是回复之前的数据包被重新发送了一次，第二次挥手无法完成，连接也就无法正常关闭了。 复位报文段一个报文段从源地址发往目的地址，只要出现错误，都会发出复位的报文段，首部字段的RST是用于“复位”的。这些错误包括以下情况 端口没有在监听 异常中止：通过发送RST而不是fin来中止连接 同时打开 两个应用程序同时执行主动打开，称为“同时打开“ 这种情况极少发生 两端同时发送SYN，同时进入SYN_SENT状态 打开一条连接而不是两条 要进行四次报文交换过程，“四次握手” 同时关闭 双方同时执行主动关闭 进行四次报文交换 状态和正常关闭不一样 服务器对于并发请求的处理 正等待连接的一端有一个固定长度的队列（长度叫做“积压值”，大多数情况长度为5） 该队列中的连接为：已经完成了三次握手，但还没有被应用层接收（应用层需要等待最后一个ack收到后才知道这个连接） 应用层接收请求的连接，将从该队列中移除 当新的请求到来时，先判断队列情况来决定是否接收这个连接 积压值的含义：tcp监听的端点已经被tcp接收，但是等待应用层接收的最大值。与系统允许的最大连接数，服务器接收的最大并发数无关 数据的传输tcp传输的数据分类 成块数据传输：量大，报文段常常满 交互数据传输：量小，报文段为微小分组，大量微小分组，在广域网传输会增加拥堵的出现 tcp处理的数据包括两类，有不同的特点，需要不同的传输技术 交互数据的传输技术经受时延的确认 概念：tcp收到数据时，并不立马发送ack确认，而是稍后发送 目的：将ack与需要沿该方向发送的数据一起发送，以减少开销 特点：接收方不必确认每一个收到的分组，ACK是累计的，它表示接收方已经正确收到了一直到确认序号-1的所有字节 延时时间：绝大多数为200ms。不能超过500ms Nagle算法 解决什么问题：微小分组导致在广域网出现的拥堵问题 核心：减少了通过广域网传输的小分组数目 原理：要求一个tcp连接上最多只能有一个未被确认的未完成的分组，该分组的确认到达之前，不能发送其他分组。tcp收集这些分组，确认到来之前以一个分组的形式发出去 优点：自适应。确认到达的快，数据发送越快。确认慢，发送更少的组。 使用注意：局域网很少使用该算法。且有些特殊场景需要禁用该算法 成块数据的传输 主要使用滑动窗口协议 滑动窗口协议概述 解决了什么问题：发送方和接收方速率不匹配时，保证可靠传输和包乱序的问题 机制：接收方根据目前缓冲区大小，通知发送方目前能接收的最大值。发送方根据接收方的处理能力来发送数据。通过这种协调机制，防止接收端处理不过来。 窗口大小：接收方发给发送端的这个值称为窗口大小 tcp缓冲区的数据结构 接收端： LastByteRead: 缓冲区读取到的位置 NextByteExpected：收到的连续包的最后一个位置 LastByteRcvd：收到的包的最后一个位置 中间空白区：数据没有到达 发送端： LastByteAcked: 被接收端ack的位置，表示成功发送确认 LastByteSent：发出去了，还没有收到成功确认的Ack LastByteWritten：上层应用正在写的地方 滑动窗口示意图初始时示意图 黑框表示滑动窗口 #1表示收到ack确认的数据 #2表示还没收到ack的数据 #3表示在窗口中还没有发出的（接收方还有空间） #4窗口以外的数据（接收方没空间） 滑动过程示意图 收到36的ack，并发出46-51的字节 拥塞窗口 解决什么问题：发送方发送速度过快，导致中转路由器拥堵的问题 机制：发送方增加一个拥塞窗口（cwnd），每次收到ack，窗口值加1。发送时，取拥塞窗口和接收方发来的窗口大小取最小值发送 起到发送方流量控制的作用 滑动窗口会引发的问题零窗口 如何发生： 接收端处理速度慢，发送端发送速度快。窗口大小慢慢被调为0 如何解决：ZWP技术。发送zwp包给接收方，让接收方ack他的窗口大小。 糊涂窗口综合征 如何发生：接收方太忙，取不完数据，导致发送方越来越小。最后只让发送方传几字节的数据。 缺点：数据比tcp和ip头小太多，网络利用率太低。 如何解决：避免对小的窗口大小做响应。 发送端：前面说到的Nagle算法。 接收端：窗口大小小于某个值，直接ack（0），阻止发送数据。窗口变大后再发。 超时与重传概述 tcp提供可靠的运输层，使用的方法是确认机制。 但是数据和确认都有可能丢失 tcp通过在发送时设置定时器解决这种问题 定时器时间到了还没收到确认，就重传该数据 tcp管理的定时器类型 重传定时器：等待收到确认 坚持定时器：使窗口大小信息保持不断流动 保活定时器：检测空闲连接崩溃或重启 2MSL定时器：检测time_wait状态 超时重传机制背景 接收端给发送端的Ack确认只会确认最后一个连续的包 比如发送1,2,3,4,5共五份数据，接收端收到1,2，于是回ack3，然后收到4（还没收到3），此时tcp不会跳过3直接确认4，否则发送端以为3也收到了。这时你能想到的方法是什么呢？tcp又是怎么处理的呢？ 被动等待的超时重传策略 直观的方法是：接收方不做任何处理，等待发送方超时，然后重传。 缺点：发送端不知道该重发3，还是重发3,4,5 如果发送方如果只发送3：节省宽度，但是慢 如果发送方如果发送3,4,5：快，但是浪费宽带 总之，都在被动等待超时，超时可能很长。所以tcp不采用此方法 主动的快速重传机制概述 名称为：Fast Retransmit 不以实际驱动，而以数据驱动重传 实现原理 如果包没有送达，就一直ack最后那个可能被丢的包 发送方连续收到3相同的ack，就重传。不用等待超时 图中发生1,2,3,4,5数据 数据1到达，发生ack2 数据2因为某些原因没有送到 后续收到3的时候，接收端并不是ack4，也不是等待。而是主动ack2 收到4,5同理，一直主动ack2 客户端收到三次ack2，就重传2 2收到后，结合之前收到的3,4,5，直接ack6 快速重传的利弊 解决了被动等待timeout的问题 无法解决重传之前的一个，还是所有的问题。 上面的例子中是重传2，还是重传2,3,4,5。因为并不清楚ack2是谁传回来的 SACK方法概述 为了解决快速重传的缺点，一种更好的SACK重传策略被提出 基于快速重传，同时在tcp头里加了一个SACK的东西 解决了什么问题：客户端应该发送哪些超时包的问题 实现原理 SACK记录一个数值范围，表示哪些数据收到了 linux2.4后默认打开该功能，之前版本需要配置tcp-sack参数 SACK只是一种辅助的方式，发送方不能完全依赖SACK。主要还是依赖ACK和timout Duplicate SACK(D-SACK) 使用SACK标识的范围，还可以知道告知发送方，有哪些数据被重复接收了 可以让发送方知道：是发出去的包丢了，还是回来的ack包丢了 超时时间的确定背景 路由器和网络流量均会变化 所以超时时间肯定不能设置为一个固定值 超时长：重发慢，效率低，性能差 超时短：并没有丢就重发，导致网络拥塞，导致更多超时和更多重发 tcp会追踪这些变化，并相应的动态改变超时时间（RTO） 如何动态改变 每次重传的时间间隔为上次的一倍，直到最大间隔为64s，称为“指数退避” 首次重传到最后放弃重传的时间间隔一般为9min 依赖以往的往返时间计算（RTT）动态的计算 往返时间（RTT）的计算方法 并不是简单的ack时间和发送时间的差值。因为有重传，网络阻塞等各种变化的因素。 而是通过采样多次数值，然后做估算 tcp使用的方法有： 被平滑的RTT估计器 被平滑的均值偏差估计器 重传时间的具体计算 计算往返时间（RTT），保存测量结果 通过测量结果维护一个被平滑的RTT估计器和被平滑的均值偏差估计器 根据这两个估计器计算下一次重传时间 超时重传引发的问题-拥塞为什么重传会引发拥塞 当网络延迟突然增加时，tcp会重传数据 但是过多的重传会导致网络负担加重，从而导致更大的延时和丢包，进入恶性循环 也就是tcp的拥塞问题 解决拥塞-拥塞控制的算法 慢启动：降低分组进入网络的传输速率 拥塞避免：处理丢失分组的算法 快速重传 快速恢复 其他定时器坚持定时器坚持定时器存在的意义 当窗口大小为0时，接收方会发送一个没有数据，只有窗口大小的ack 但是，如果这个ack丢失了会出现什么问题？双方可能因为等待而中止连接 坚持定时器周期性的向接收方查询窗口是否被增大。这些发出的报文段称为窗口探查 坚持定时器启动时机 发送方被通告接收方窗口大小为0时 与超时重传的相同和不同 相同：同样的重传时间间隔 不同：窗口探查从不放弃发送，直到窗口被打开或者进程被关闭。而超时重传到一定时间就放弃发送 保活定时器保活定时器存在的意义 当tcp上没有数据传输时，服务器如何检测到客户端是否还存活 参考：TCP协议详解TCP系列01—概述及协议头格式TCP三次握手 四次挥手什么是2MSL]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8中的HashMap]]></title>
    <url>%2F2019%2F06%2F21%2Fjava8-HashMap%2F</url>
    <content type="text"><![CDATA[本文转载自：Java 8系列之重新认识HashMap 摘要HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap，还可以使用HashTable。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 12简单来说，Hashtable通过给方法加synchronized实现线程安全。而ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素， 每个Segment守护一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。 分段锁可理解为，把整个Map分成了N个Segment，put和get的时候，根据key.hashCode()找到该使用哪个Segment，这个Segment做到了类似于Hashtable的线程安全，分段锁就是说用到哪部分就锁哪部分。ConcurrentHashMap键值不能为null。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现搞清楚HashMap，首先需要知道HashMap是什么，即它的存储结构-字段；其次弄明白它能干什么，即它的功能实现-方法。下面我们针对这两个方面详细展开讲解。 存储结构-字段从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： 1map.put(&quot;美团&quot;,&quot;小美&quot;); 系统将调用”美团”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考http://blog.csdn.net/liuqiyao_01/article/details/14475159，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考http://blog.csdn.net/v_july_v/article/details/6105630。 功能实现-方法HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 1. 确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 1234567891011//方法一：static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;//方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 2. 分析HashMap的put方法HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 3. 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213 void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值&#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125;&#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes"，"unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： 1234567891011121314151617181920public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， "C"); new Thread("Thread1") &#123; public void run() &#123; map.put(7, "B"); System.out.println(map); &#125;; &#125;.start(); new Thread("Thread2") &#123; public void run() &#123; map.put(3, "A); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。 线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 JDK1.8与JDK1.7的性能对比HashMap中，如果key经过hash算法得出的数组索引位置全部不相同，即Hash算法非常好，那样的话，getKey方法的时间复杂度就是O(1)，如果Hash算法技术的结果碰撞非常多，假如Hash算极其差，所有的Hash算法结果得出的索引位置一样，那样所有的键值对都集中到一个桶中，或者在一个链表中，或者在一个红黑树中，时间复杂度分别为O(n)和O(lgn)。 鉴于JDK1.8做了多方面的优化，总体性能优于JDK1.7，下面我们从两个方面用例子证明这一点。 Hash较均匀的情况为了便于测试，我们先写一个类Key，如下： 123456789101112131415161718192021222324252627class Key implements Comparable&lt;Key&gt; &#123; private final int value; Key(int value) &#123; this.value = value; &#125; @Override public int compareTo(Key o) &#123; return Integer.compare(this.value, o.value); &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Key key = (Key) o; return value == key.value; &#125; @Override public int hashCode() &#123; return value; &#125;&#125; 这个类复写了equals方法，并且提供了相当好的hashCode函数，任何一个值的hashCode都不会相同，因为直接使用value当做hashcode。为了避免频繁的GC，我将不变的Key实例缓存了起来，而不是一遍一遍的创建它们。代码如下： 123456789101112131415public class Keys &#123; public static final int MAX_KEY = 10_000_000; private static final Key[] KEYS_CACHE = new Key[MAX_KEY]; static &#123; for (int i = 0; i &lt; MAX_KEY; ++i) &#123; KEYS_CACHE[i] = new Key(i); &#125; &#125; public static Key of(int value) &#123; return KEYS_CACHE[value]; &#125;&#125; 现在开始我们的试验，测试需要做的仅仅是，创建不同size的HashMap（1、10、100、……10000000），屏蔽了扩容的情况，代码如下： 1234567891011121314151617181920static void test(int mapSize) &#123; HashMap&lt;Key, Integer&gt; map = new HashMap&lt;Key,Integer&gt;(mapSize); for (int i = 0; i &lt; mapSize; ++i) &#123; map.put(Keys.of(i), i); &#125; long beginTime = System.nanoTime(); //获取纳秒 for (int i = 0; i &lt; mapSize; i++) &#123; map.get(Keys.of(i)); &#125; long endTime = System.nanoTime(); System.out.println(endTime - beginTime); &#125; public static void main(String[] args) &#123; for(int i=10;i&lt;= 1000 0000;i*= 10)&#123; test(i); &#125; &#125; 在测试中会查找不同的值，然后度量花费的时间，为了计算getKey的平均时间，我们遍历所有的get方法，计算总的时间，除以key的数量，计算一个平均值，主要用来比较，绝对值可能会受很多环境因素的影响。结果如下： 通过观测测试结果可知，JDK1.8的性能要高于JDK1.7 15%以上，在某些size的区域上，甚至高于100%。由于Hash算法较均匀，JDK1.8引入的红黑树效果不明显，下面我们看看Hash不均匀的的情况。 Hash极不均匀的情况假设我们有一个非常差的Key，它们所有的实例都返回相同的hashCode值。这是使用HashMap最坏的情况。代码修改如下： 123456789class Key implements Comparable&lt;Key&gt; &#123; //... @Override public int hashCode() &#123; return 1; &#125;&#125; 仍然执行main方法，得出的结果如下表所示： 从表中结果中可知，随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表太长的时候，HashMap会动态的将它替换成一个红黑树，这话的话会将时间复杂度从O(n)降为O(logn)。hash算法均匀和不均匀所花费的时间明显也不相同，这两种情况的相对比较，可以说明一个好的hash算法的重要性。 测试环境：处理器为2.2 GHz Intel Core i7，内存为16 GB 1600 MHz DDR3，SSD硬盘，使用默认的JVM参数，运行在64位的OS X 10.10.1上。 遍历Map对象既然java中的所有map都实现了Map接口，以下方法适用于任何map实现（HashMap, TreeMap, LinkedHashMap, Hashtable, 等等）： 方法一： 在for-each循环中使用entries来遍历这是最常见的并且在大多数情况下也是最可取的遍历方式。在键值都需要时使用。但是如果你遍历的是一个空的map对象，for-each循环将抛出NullPointerException，因此在遍历前你总是应该检查空引用。 1234Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue());&#125; 方法二：在for-each循环中遍历keys或values如果只需要map中的键或者值，你可以通过性能稍好的keySet()或values()来实现遍历，而不是用entrySet()。 123456789Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();//遍历map中的键for (Integer key : map.keySet()) &#123; System.out.println("Key = " + key);&#125;//遍历map中的值for (Integer value : map.values()) &#123; System.out.println("Value = " + value);&#125; 方法三：使用Iterator遍历1234567Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();// 使用泛型Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; entries = map.entrySet().iterator();while (entries.hasNext()) &#123; Map.Entry&lt;Integer, Integer&gt; entry = entries.next(); System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue());&#125;1234567 你也可以在keySet和values上应用同样的方法。该种方式看起来冗余却有其优点所在。首先，在老版本java中这是惟一遍历map的方式。另一个好处是，你可以在遍历时调用iterator.remove()来**删除** entries，另两个方法则不能。 小结(1) 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 (3) HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 (4) JDK1.8引入红黑树大程度优化了HashMap的性能。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统之页面置换算法]]></title>
    <url>%2F2019%2F06%2F21%2Fpage-replacement-algorithm%2F</url>
    <content type="text"><![CDATA[操作系统为何要进行页面置换呢？这是由于操作系统给用户态的应用程序提供了一个虚拟的“大容量”内存空间，而实际的物理内存空间又没有那么大。所以操作系统就就“瞒着”应用程序，只把应用程序中“常用”的数据和代码放在物理内存中，而不常用的数据和代码放在了硬盘这样的存储介质上。如果应用程序访问的是“常用”的数据和代码，那么操作系统已经放置在内存中了，不会出现什么问题。但当应用程序访问它认为应该在内存中的的数据或代码时，如果这些数据或代码不在内存中，会产生缺页异常。这时，操作系统必须能够应对这种缺页异常，即尽快把应用程序当前需要的数据或代码放到内存中来，然后重新执行应用程序产生异常的访存指令。如果在把硬盘中对应的数据或代码调入内存前，操作系统发现物理内存已经没有空闲空间了，这时操作系统必须把它认为“不常用”的页换出到磁盘上去，以腾出内存空闲空间给应用程序所需的数据或代码。 操作系统迟早会碰到没有内存空闲空间而必须要置换出内存中某个“不常用”的页的情况。如何判断内存中哪些是“常用”的页，哪些是“不常用”的页，把“常用”的页保持在内存中，在物理内存空闲空间不够的情况下，把“不常用”的页置换到硬盘上就是页面置换算法着重考虑的问题。容易理解，一个好的页面置换算法会使得缺页异常次数少，也就意味着访问硬盘的次数也少，从而使得应用程序执行的效率就高。 从操作系统原理的角度看，有如下一些页面置换算法： 最优 (Optimal) 页面置换算法：由Belady于1966年提出的一种理论上的算法。其所选择的被淘汰页面，将是以后永不使用的或许是在最长的未来时间内不再被访问的页面。采用最佳置换算法，通常可保证获得最低的缺页率。但由于操作系统其实无法预知一个应用程序在执行过程中访问到的若干页中，哪一个页是未来最长时间内不再被访问的，因而该算法是无法实际实现，但可以此算法作为上限来评价其它的页面置换算法。 先进先出(First In First Out, FIFO)页面置换算法：该算法总是淘汰最先进入内存的页，即选择在内存中驻留时间最久的页予以淘汰。只需把一个应用程序在执行过程中已调入内存的页按先后次序链接成一个队列，队列头指向内存中驻留时间最久的页，队列尾指向最近被调入内存的页(队列尾是插入的位置)。这样需要淘汰页时，从队列头很容易查找到需要淘汰的页。FIFO算法只是在应用程序按线性顺序访问地址空间时效果才好，否则效率不高。因为那些常被访问的页，往往在内存中也停留得最久，结果它们因变“老”而不得不被置换出去。FIFO算法的另一个缺点是，它有一种异常现象（Belady现象），即在增加放置页的页帧的情况下，反而使缺页异常次数增多。 二次机会（Second Chance）页面置换算法：为了克服FIFO算法的缺点，人们对它进行了改进。此算法在页表项（PTE）中设置了一位访问位来表示此页表项对应的页当前是否被访问过。当该页被访问时，CPU中的MMU硬件将把访问位置“1”。当需要找到一个页淘汰时，对于最“老”的那个页面，操作系统去检查它的访问位。如果访问位是0，说明这个页面老且无用，应该立刻淘汰出局；如果访问位是1，这说明该页面曾经被访问过，因此就再给它一次机会。具体来说，先把访问位位清零，然后把这个页面放到队列的尾端，并修改它的装入时间，就好像它刚刚进入系统一样，然后继续往下搜索。二次机会算法的实质就是寻找一个比较古老的、而且从上一次缺页异常以来尚未被访问的页面。如果所有的页面都被访问过了，它就退化为纯粹的FIFO算法。 LRU(Least Recently Used，LRU)页面置换算法： FIFO置换算法性能之所以较差，是因为它所依据的条件是各个页调入内存的时间，而页调入的先后顺序并不能反映页是否“常用”的使用情况。最近最久未使用（LRU）置换算法，是根据页调入内存后的使用情况进行决策页是否“常用”。由于无法预测各页面将来的使用情况，只能利用“最近的过去”作为“最近的将来”的近似，因此，LRU置换算法是选择最近最久未使用的页予以淘汰。该算法赋予每个页一个访问字段，用来记录一个页面自上次被访问以来所经历的时间t,当须淘汰一个页面时，选择现有页面中其t值最大的，即最近最久未使用的页面予以淘汰。 时钟（Clock）页面置换算法：也称最近未使用 (Not Used Recently, NUR) 页面置换算法。虽然二次机会算法是一个较合理的算法，但它经常需要在链表中移动页面，这样做既降低了效率，又是不必要的。一个更好的办法是把各个页面组织成环形链表的形式，类似于一个钟的表面。然后把一个指针指向最古老的那个页面，或者说，最先进来的那个页面。时钟算法和第二次机会算法的功能是完全一样的，只是在具体实现上有所不同。时钟算法需要在页表项（PTE）中设置了一位访问位来表示此页表项对应的页当前是否被访问过。当该页被访问时，CPU中的MMU硬件将把访问位置“1”。然后将内存中所有的页都通过指针链接起来并形成一个循环队列。初始时，设置一个当前指针指向某页（比如最古老的那个页面）。操作系统需要淘汰页时，对当前指针指向的页所对应的页表项进行查询，如果访问位为“0”，则淘汰该页，把它换出到硬盘上；如果访问位为“1”，这将该页表项的此位置“0”，继续访问下一个页。该算法近似地体现了LRU的思想，且易于实现，开销少。但该算法需要硬件支持来设置访问位，且该算法在本质上与FIFO算法是类似的，惟一不同的是在clock算法中跳过了访问位为1的页。 改进的时钟（Enhanced Clock）页面置换算法：在时钟置换算法中，淘汰一个页面时只考虑了页面是否被访问过，但在实际情况中，还应考虑被淘汰的页面是否被修改过。因为淘汰修改过的页面还需要写回硬盘，使得其置换代价大于未修改过的页面。改进的时钟置换算法除了考虑页面的访问情况，还需考虑页面的修改情况。即该算法不但希望淘汰的页面是最近未使用的页，而且还希望被淘汰的页是在主存驻留期间其页面内容未被修改过的。这需要为每一页的对应页表项内容中增加一位引用位和一位修改位。当该页被访问时，CPU中的MMU硬件将把访问位置“1”。当该页被“写”时，CPU中的MMU硬件将把修改位置“1”。这样这两位就存在四种可能的组合情况：（0，0）表示最近未被引用也未被修改，首先选择此页淘汰；（0，1）最近未被使用，但被修改，其次选择；（1，0）最近使用而未修改，再次选择；（1，1）最近使用且修改，最后选择。该算法与时钟算法相比，可进一步减少磁盘的I/O操作次数，但为了查找到一个尽可能适合淘汰的页面，可能需要经过多次扫描，增加了算法本身的执行开销。 以上文章来自：https://github.com/chyyuu/simple_os_book]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的Future和FutureTask]]></title>
    <url>%2F2019%2F06%2F21%2Fjava-future-futuretask%2F</url>
    <content type="text"><![CDATA[Callable 接口线程的创建方式中有两种，一种是实现Runnable接口，另一种是继承Thread，但是这两种方式都有个缺点，那就是在任务执行完成之后无法获取返回结果，于是就有了Callable接口，Future接口与FutureTask类的配和取得返回的结果。 我们先回顾一下java.lang.Runnable接口，就声明了run(),其返回值为void，当然就无法获取结果。 123public interface Runnable &#123; public abstract void run(); &#125; 而Callable的接口定义如下 123public interface Callable&lt;V&gt; &#123; V call() throws Exception; &#125; 该接口声明了一个名称为call()的方法，同时这个方法可以有返回值V，也可以抛出异常。无论是Runnable接口的实现类还是Callable接口的实现类，都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行，ThreadPoolExecutor或ScheduledThreadPoolExecutor都实现了ExcutorService接口，而因此Callable需要和Executor框架中的ExcutorService结合使用，我们先看看ExecutorService提供的方法： &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future&lt;?&gt; submit(Runnable task); 第一个方法：submit提交一个实现Callable接口的任务，并且返回封装了异步计算结果的Future。 第二个方法：submit提交一个实现Runnable接口的任务，并且指定了在调用Future的get方法时返回的result对象。（不常用）第三个方法：submit提交一个实现Runnable接口的任务，并且返回封装了异步计算结果的Future。因此我们只要创建好我们的线程对象（实现Callable接口或者Runnable接口），然后通过上面3个方法提交给线程池去执行即可。还有点要注意的是，除了我们自己实现Callable对象外，我们还可以使用工厂类Executors来把一个Runnable对象包装成Callable对象。Executors工厂类提供的方法如下： 12public static Callable&lt;Object&gt; callable(Runnable task) public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) Future接口Future接口是用来获取异步计算结果的，说白了就是对具体的Runnable或者Callable对象任务执行的结果进行获取(get()),取消(cancel()),判断是否完成等操作。我们看看Future接口的源码： 1234567public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; &#125; 方法解析：V get() ：获取异步执行的结果，如果没有结果可用，此方法会阻塞直到异步计算完成。V get(Long timeout , TimeUnit unit) ：获取异步执行结果，如果没有结果可用，此方法会阻塞，但是会有时间限制，如果阻塞时间超过设定的timeout时间，该方法将抛出异常。 boolean isDone() ：如果任务执行结束，无论是正常结束或是中途取消还是发生异常，都返回true。boolean isCanceller() ：如果任务完成前被取消，则返回true。boolean cancel(boolean mayInterruptRunning)： 如果任务还没开始，执行cancel(…)方法将返回false； 如果任务已经启动，执行cancel(true)方法将以中断执行此任务线程的方式来试图停止任务，如果停止成功，返回true； 当任务已经启动，执行cancel(false)方法将不会对正在执行的任务线程产生影响(让线程正常执行到完成)，此时返回false； 当任务已经完成，执行cancel(…)方法将返回false。 mayInterruptRunning参数表示是否中断执行中的线程。 通过方法分析我们也知道实际上Future提供了3种功能： 能够中断执行中的任务 判断任务是否执行完成 获取任务执行完成后额结果。但是我们必须明白Future只是一个接口，我们无法直接创建对象，因此就需要其实现类FutureTask登场啦。FutureTask类我们先来看看FutureTask的实现1public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; FutureTask类实现了RunnableFuture接口，我们看一下RunnableFuture接口的实现： 123public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run(); &#125; 分析：FutureTask除了实现了Future接口外还实现了Runnable接口（即可以通过Runnable接口实现线程，也可以通过Future取得线程执行完后的结果），因此FutureTask也可以直接提交给Executor执行。 最后我们给出FutureTask的两种构造函数： 1234public FutureTask(Callable&lt;V&gt; callable) &#123; &#125; public FutureTask(Runnable runnable, V result) &#123; &#125; Callable/Future/FutureTask的使用(封装了异步获取结果的Future!!!)通过上面的介绍，我们对Callable，Future，FutureTask都有了比较清晰的了解了，那么它们到底有什么用呢？我们前面说过通过这样的方式去创建线程的话，最大的好处就是能够返回结果，加入有这样的场景，我们现在需要计算一个数据，而这个数据的计算比较耗时，而我们后面的程序也要用到这个数据结果，那么这个时Callable岂不是最好的选择？我们可以开设一个线程去执行计算，而主线程继续做其他事，而后面需要使用到这个数据时，我们再使用Future获取不就可以了吗？下面我们就来编写一个这样的实例 使用Callable+Future获取执行结果Callable实现类如下： com.zejian.Executor; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.concurrent.Callable; /** * @author zejian * @time 2016年3月15日 下午2:02:42 * @decrition Callable接口实例 */ public class CallableDemo implements Callable&lt;Integer&gt; &#123; private int sum; @Override public Integer call() throws Exception &#123; System.out.println(&quot;Callable子线程开始计算啦！&quot;); Thread.sleep(2000); for(int i=0 ;i&lt;5000;i++)&#123; sum=sum+i; &#125; System.out.println(&quot;Callable子线程计算结束！&quot;); return sum; &#125; &#125; Callable执行测试类如下： package com.zejian.Executor; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.Future; /** * @author zejian * @time 2016年3月15日 下午2:05:43 * @decrition callable执行测试类 */ public class CallableTest &#123; public static void main(String[] args) &#123; //创建线程池 ExecutorService es = Executors.newSingleThreadExecutor(); //创建Callable对象任务 CallableDemo calTask=new CallableDemo(); //提交任务并获取执行结果 Future&lt;Integer&gt; future =es.submit(calTask); //关闭线程池 es.shutdown(); try &#123; Thread.sleep(2000); System.out.println(&quot;主线程在执行其他任务&quot;); if(future.get()!=null)&#123; //输出获取到的结果 System.out.println(&quot;future.get()--&gt;&quot;+future.get()); &#125;else&#123; //输出获取到的结果 System.out.println(&quot;future.get()未获取到结果&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;主线程在执行完成&quot;); &#125; &#125; 执行结果： 12345Callable子线程开始计算啦！主线程在执行其他任务Callable子线程计算结束！future.get()--&gt;12497500主线程在执行完成 使用Callable+FutureTask获取执行结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.zejian.Executor; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.Future; import java.util.concurrent.FutureTask; /** * @author zejian * @time 2016年3月15日 下午2:05:43 * @decrition callable执行测试类 */ public class CallableTest &#123; public static void main(String[] args) &#123; // //创建线程池 // ExecutorService es = Executors.newSingleThreadExecutor(); // //创建Callable对象任务 // CallableDemo calTask=new CallableDemo(); // //提交任务并获取执行结果 // Future&lt;Integer&gt; future =es.submit(calTask); // //关闭线程池 // es.shutdown(); //创建线程池 ExecutorService es = Executors.newSingleThreadExecutor(); //创建Callable对象任务 CallableDemo calTask=new CallableDemo(); //创建FutureTask FutureTask&lt;Integer&gt; futureTask=new FutureTask&lt;&gt;(calTask); //执行任务 es.submit(futureTask); //关闭线程池 es.shutdown(); try &#123; Thread.sleep(2000); System.out.println(&quot;主线程在执行其他任务&quot;); if(futureTask.get()!=null)&#123; //输出获取到的结果 System.out.println(&quot;futureTask.get()--&gt;&quot;+futureTask.get()); &#125;else&#123; //输出获取到的结果 System.out.println(&quot;futureTask.get()未获取到结果&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;主线程在执行完成&quot;); &#125; &#125; 执行结果： 12345Callable子线程开始计算啦！主线程在执行其他任务Callable子线程计算结束！futureTask.get()--&gt;12497500主线程在执行完成 作者：LittleCadet来源：CSDN原文：https://blog.csdn.net/sx1119183530/article/details/79735348版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LRU原理和Redis实现]]></title>
    <url>%2F2019%2F06%2F21%2Flur-redis%2F</url>
    <content type="text"><![CDATA[文章转载自：https://zhuanlan.zhihu.com/p/34133067 LRU简介LRU是内存不够的场景下，淘汰旧内容的一种策略。LRU ,Least Recent Used，淘汰掉最不经常使用的。可以稍微多补充两句，因为计算机体系结构中，最大的最可靠的存储是硬盘，它容量很大，并且内容可以固化，但是访问速度很慢，所以需要把使用的内容载入内存中；内存速度很快，但是容量有限，并且断电后内容会丢失，并且为了进一步提升性能，还有CPU内部的 L1 Cache，L2 Cache等概念。因为速度越快的地方，它的单位成本越高，容量越小，新的内容不断被载入，旧的内容肯定要被淘汰，所以就有这样的使用背景。 LRU原理在一般标准的操作系统教材里，会用下面的方式来演示LRU原理，假设内存只能容纳3个页大小，按照 7 0 1 2 0 3 0 4的次序访问页。假设内存按照栈的方式来描述访问时间，在上面的，是最近访问的，在下面的是，最远时间访问的，LRU就是这样工作的。 但是如果让我们自己设计一个基于 LRU 的缓存，这样设计可能问题很多，这段内存按照访问时间进行了排序，会有大量的内存拷贝操作，所以性能肯定是不能接受的。 那么如何设计一个LRU缓存，使得放入和移除都是O(1) 的，我们需要把访问次序维护起来，但是不能通过内存中的真实排序来反应，有一种方案就是使用双向链表。 基于 HashMap 和 双向链表实现 LRU整体的设计思路是，可以使用 HashMap存储 key，这样可以做到 save 和 get key的时间都是 O(1)，而 HashMap的 Value 指向双向链表实现的 LRU的 Node 节点，如图所示。 LRU 存储是基于双向链表实现的，下面的图演示了它的原理。其中head 代表双向链表的表头，tail 代表尾部。首先预先设置 LRU 的容量，如果存储满了，可以通过 O(1) 的时间淘汰掉双向链表的尾部，每次新增和访问数据，都可以通过 O(1)的效率把新的节点增加到对头，或者把已经存在的节点移动到队头。 下面展示了，预设大小是 3 的，LRU存储的在存储和访问过程中的变化。为了简化图复杂度，图中没有展示 HashMap部分的变化，仅仅演示了上图LRU双向链表的变化。我们对这个LRU缓存的操作序列如下： 123456789101112131415save(&quot;key1&quot;, 7)save(&quot;key2&quot;, 0)save(&quot;key3&quot;, 1)save(&quot;key4&quot;, 2)get(&quot;key2&quot;)save(&quot;key5&quot;, 3)get(&quot;key2&quot;)save(&quot;key6&quot;, 4) 相应的 LRU 双向链表部分变化如下： s = save, g = get 总结一下核心操作的步骤: save(key, value)，首先在HashMap 找到 Key对应的节点，如果节点存在，更新节点的值，并把这个节点移动队头。如果不存在，需要构造新的节点，并且尝试把节点塞到队头，如果LRU空间不足，则通过tail 淘汰掉队尾的节点，同时在 HashMap中移除 Key。 get(key)，通过 HashMap 找到LRU链表节点，因为根据LRU 原理，这个节点是最新访问的，所以要把节点插入到队头，然后返回缓存的值。 完整基于 Java 的代码参考如下 123456class DLinkedNode &#123; String key; int value; DLinkedNode pre; DLinkedNode post;&#125; LRU Cache 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class LRUCache &#123; private Hashtable&lt;Integer, DLinkedNode&gt; cache = new Hashtable&lt;Integer, DLinkedNode&gt;(); private int count; private int capacity; private DLinkedNode head, tail; public LRUCache(int capacity) &#123; this.count = 0; this.capacity = capacity; head = new DLinkedNode(); head.pre = null; tail = new DLinkedNode(); tail.post = null; head.post = tail; tail.pre = head; &#125; public int get(String key) &#123; DLinkedNode node = cache.get(key); if(node == null)&#123; return -1; // should raise exception here. &#125; // move the accessed node to the head; this.moveToHead(node); return node.value; &#125; public void set(String key, int value) &#123; DLinkedNode node = cache.get(key); if(node == null)&#123; DLinkedNode newNode = new DLinkedNode(); newNode.key = key; newNode.value = value; this.cache.put(key, newNode); this.addNode(newNode); ++count; if(count &gt; capacity)&#123; // pop the tail DLinkedNode tail = this.popTail(); this.cache.remove(tail.key); --count; &#125; &#125;else&#123; // update the value. node.value = value; this.moveToHead(node); &#125; &#125; /** * Always add the new node right after head; */ private void addNode(DLinkedNode node)&#123; node.pre = head; node.post = head.post; head.post.pre = node; head.post = node; &#125; /** * Remove an existing node from the linked list. */ private void removeNode(DLinkedNode node)&#123; DLinkedNode pre = node.pre; DLinkedNode post = node.post; pre.post = post; post.pre = pre; &#125; /** * Move certain node in between to the head. */ private void moveToHead(DLinkedNode node)&#123; this.removeNode(node); this.addNode(node); &#125; // pop the current tail. private DLinkedNode popTail()&#123; DLinkedNode res = tail.pre; this.removeNode(res); return res; &#125;&#125; Redis 如何实现？这个问题这么问肯定是有坑的，那就是redis肯定不是这样实现的。 Redis的LRU实现如果按照HashMap和双向链表实现，需要额外的存储存放 next 和 prev 指针，牺牲比较大的存储空间，显然是不划算的。所以Redis采用了一个近似的做法，就是随机取出若干个key，然后按照访问时间排序后，淘汰掉最不经常使用的，具体分析如下： 为了支持LRU，Redis 2.8.19中使用了一个全局的LRU时钟，server.lruclock，定义如下， 12#define REDIS_LRU_BITS 24unsigned lruclock:REDIS_LRU_BITS; /* Clock for LRU eviction */ 默认的LRU时钟的分辨率是1秒，可以通过改变REDIS_LRU_CLOCK_RESOLUTION宏的值来改变，Redis会在serverCron()中调用updateLRUClock定期的更新LRU时钟，更新的频率和hz参数有关，默认为100ms一次，如下， 1234567#define REDIS_LRU_CLOCK_MAX ((1&lt;&lt;REDIS_LRU_BITS)-1) /* Max value of obj-&gt;lru */#define REDIS_LRU_CLOCK_RESOLUTION 1 /* LRU clock resolution in seconds */void updateLRUClock(void) &#123; server.lruclock = (server.unixtime / REDIS_LRU_CLOCK_RESOLUTION) &amp; REDIS_LRU_CLOCK_MAX;&#125; server.unixtime是系统当前的unix时间戳，当 lruclock 的值超出REDIS_LRU_CLOCK_MAX时，会从头开始计算，所以在计算一个key的最长没有访问时间时，可能key本身保存的lru访问时间会比当前的lrulock还要大，这个时候需要计算额外时间，如下， 12345678910/* Given an object returns the min number of seconds the object was never * requested, using an approximated LRU algorithm. */unsigned long estimateObjectIdleTime(robj *o) &#123; if (server.lruclock &gt;= o-&gt;lru) &#123; return (server.lruclock - o-&gt;lru) * REDIS_LRU_CLOCK_RESOLUTION; &#125; else &#123; return ((REDIS_LRU_CLOCK_MAX - o-&gt;lru) + server.lruclock) * REDIS_LRU_CLOCK_RESOLUTION; &#125;&#125; Redis支持和LRU相关淘汰策略包括， volatile-lru 设置了过期时间的key参与近似的lru淘汰策略 allkeys-lru 所有的key均参与近似的lru淘汰策略 当进行LRU淘汰时，Redis按如下方式进行的， 123456789101112131415161718192021222324252627...... /* volatile-lru and allkeys-lru policy */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) &#123; for (k = 0; k &lt; server.maxmemory_samples; k++) &#123; sds thiskey; long thisval; robj *o; de = dictGetRandomKey(dict); thiskey = dictGetKey(de); /* When policy is volatile-lru we need an additional lookup * to locate the real key, as dict is set to db-&gt;expires. */ if (server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) de = dictFind(db-&gt;dict, thiskey); o = dictGetVal(de); thisval = estimateObjectIdleTime(o); /* Higher idle time is better candidate for deletion */ if (bestkey == NULL || thisval &gt; bestval) &#123; bestkey = thiskey; bestval = thisval; &#125; &#125; &#125; ...... Redis会基于server.maxmemory_samples配置选取固定数目的key，然后比较它们的lru访问时间，然后淘汰最近最久没有访问的key，maxmemory_samples的值越大，Redis的近似LRU算法就越接近于严格LRU算法，但是相应消耗也变高，对性能有一定影响，样本值默认为5。 总结看来，虽然一个简单的概念，在工业界的产品中，为了追求空间的利用率，也会采用权衡的实现方案。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>LRU</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的WeakHashMap]]></title>
    <url>%2F2019%2F06%2F21%2Fjava-WeakHashMap%2F</url>
    <content type="text"><![CDATA[什么是WeakHashMap？WeakHashMap 继承于AbstractMap，实现了Map接口。 1public class WeakHashMap&lt;K,V&gt;extends AbstractMap&lt;K,V&gt;implements Map&lt;K,V&gt; 和HashMap一样，WeakHashMap 也是一个散列表，它存储的内容也是键值对(key-value)映射，而且键和值都可以是null。 不过WeakHashMap的键是“弱键”。在 WeakHashMap 中，当某个键不再正常使用时，会被从WeakHashMap中被自动移除。更精确地说，对于一个给定的键，其映射的存在并不阻止垃圾回收器对该键的丢弃，这就使该键成为可终止的，被终止，然后被回收。某个键被终止时，它对应的键值对也就从映射中有效地移除了。 这个“弱键”的原理呢？大致上就是，通过WeakReference和ReferenceQueue实现的。 WeakHashMap的key是“弱键”，即是WeakReference类型的；ReferenceQueue是一个队列，它会保存被GC回收的“弱键”。实现步骤是： 新建WeakHashMap，将“键值对”添加到WeakHashMap中。实际上，WeakHashMap是通过数组table保存Entry(键值对)；每一个Entry实际上是一个单向链表，即Entry是键值对链表。 当某“弱键”不再被其它对象引用，并被GC回收时。在GC回收该“弱键”时，这个“弱键”也同时会被添加到ReferenceQueue(queue)队列中。 当下一次我们需要操作WeakHashMap时，会先同步table和queue。table中保存了全部的键值对，而queue中保存被GC回收的键值对；同步它们，就是删除table中被GC回收的键值对。 这就是“弱键”如何被自动从WeakHashMap中删除的步骤了。 和HashMap一样，WeakHashMap是不同步的。可以使用 Collections.synchronizedMap 方法来构造同步的 WeakHashMap。 在Java8中，当冲突的key变多时，HashMap引入了二叉树（红黑树）进行存储，而WeakHashMap则一直使用链表进行存储。 而WeakHashMap的特点，这里也有总结： 基于Map接口，是一种弱键相连，WeakHashMap里面的键会自动回收 支持 null值和null键。和HashMap有些相似 fast-fail机制 不允许重复 WeakHashMap经常用作缓存 Java里面引用分为4中类型,而在WeakHashMap则主要用到了WeakReference这个引用。 WeakHashMap数据结构1234567java.lang.Object ↳ java.util.AbstractMap&lt;K, V&gt; ↳ java.util.WeakHashMap&lt;K, V&gt;public class WeakHashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123;&#125; WeakHashMap与Map关系如下图：从图中可以看出： WeakHashMap继承于AbstractMap，并且实现了Map接口。 WeakHashMap是哈希表，但是它的键是”弱键”。WeakHashMap中保护几个重要的成员变量：table, size, threshold, loadFactor, modCount, queue。123456table是一个Entry[]数组类型，而Entry实际上就是一个单向链表。哈希表的&quot;key-value键值对&quot;都是存储在Entry数组中的。 size是map的大小，它是map保存的键值对的数量。 threshold是map的阈值，用于判断是否需要调整map的容量。threshold的值=&quot;容量*加载因子&quot;。loadFactor就是加载因子。 modCount是用来实现fail-fast机制的queue保存的是“已被GC清除”的“弱引用的键” 关于Entry&lt;K,V&gt;和HashMap一样，WeakHashMap也是用一个Entry实体来构造里面所有的元素的，但是这个Entry却和HashMap的不同，他是弱引用。 1private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; 如上，Entry还继承了WeakReference，所以Entry是个弱引用。何为弱引用呢？就是就是每当进行一次GC,你这个对象就会被清除，当然如果这个对象还存在着软引用或者强引用，就可能不会被清除。 ReferenceQueue queue作用queue是用来存放那些，被jvm清除的entry的引用，因为WeakHashMap使用的是弱引用，所以一旦gc，就会有key键被清除，所以会把entry加入到queue中。在WeakHashMap中加入queue的目的，就是为expungeStaleEntries所用。 12345678Entry(Object key, V value, ReferenceQueue&lt;Object&gt; queue, int hash, Entry&lt;K,V&gt; next) &#123; super(key, queue); this.value = value; this.hash = hash; this.next = next;&#125; 在构造每一个Entry时，都将它与queue绑定，从而一旦被jvm回收，那么这个Entry就会添加到queue中。 expungeStaleEntries方法这个方法里面就仅仅是释放value值。由前面的Entry的构造方法可知， super(key, queue); 传入父类的仅仅是key，所以经过仔细阅读jdk源码开始部分分析后，得出结论，在WeakHashMap中，有jvm回收的，仅仅是Entry的key部分，所以一旦jvm强制回收，那么这些key都会为null，再通过私有的expungeStaleEntries 方法，把value也制null，并且把size--。 首先看代码： 12345678910111213141516171819202122232425262728293031323334353637/** * 从ReferenceQueue中取出过期的entry，从WeakHashMap找到对应的entry，逐一删除 * 注意，只会把value置为null。 */private void expungeStaleEntries() &#123; for (Object x; (x = queue.poll()) != null; ) &#123; //遍历queue synchronized (queue) &#123; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x; int i = indexFor(e.hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; p = prev; while (p != null) &#123; //遍历table[i]所在链表 Entry&lt;K,V&gt; next = p.next; if (p == e) &#123; //queue里面有e，那就删了。 if (prev == e) //e就是当前的p.next table[i] = next; else prev.next = next; // Must not null out e.next; // stale entries may be in use by a HashIterator //置为null，帮助gc。只制null了value。 e.value = null; // Help GC //设置e的value，但是没看到设置e的key。 size--; break; &#125; prev = p; p = next; &#125; &#125; &#125;&#125; 上面代码逻辑为，当在table中找到queue中存在元素时，就把value制空，然后size--。所以在WeakHashMap中，就只有key被回收。下面看一个实例验证。 首先需要了解一点：expungeStaleEntries方法在哪些方面会被调用？经过阅读源码，发现expungeStaleEntries方法只在以下几个地方被调用： 1private Entry&lt;K,V&gt;[] getTable() 里面，而这个getTable则在下列方法被调用: 123456789101112`public V get(Object key)``Entry&lt;K,V&gt; getEntry(Object key)``public V put(K key, V value)``void resize(int newCapacity)``public V remove(Object key)``boolean removeMapping(Object o)``public boolean containsValue(Object value)``private boolean containsNullValue()``public void forEach(BiConsumer&lt;? super K, ? super V&gt; action)``public void replaceAll(BiFunction&lt;? super K, ? super V, ? extends V&gt; function)``public int size()``void resize(int newCapacity)` 应用场景tomcat的源码里，实现缓存时会用到WeakHashMap。 还有 ThreadLocal。 12345678910111213141516171819202122232425262728293031323334353637383940414243package org.apache.tomcat.util.collections;import java.util.Map;import java.util.WeakHashMap;import java.util.concurrent.ConcurrentHashMap;public final class ConcurrentCache&lt;K,V&gt; &#123; private final int size; private final Map&lt;K,V&gt; eden; private final Map&lt;K,V&gt; longterm; public ConcurrentCache(int size) &#123; this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); &#125; public V get(K k) &#123; V v = this.eden.get(k); if (v == null) &#123; synchronized (longterm) &#123; v = this.longterm.get(k); &#125; if (v != null) &#123; this.eden.put(k, v); &#125; &#125; return v; &#125; public void put(K k, V v) &#123; if (this.eden.size() &gt;= size) &#123; synchronized (longterm) &#123; this.longterm.putAll(this.eden); &#125; this.eden.clear(); &#125; this.eden.put(k, v); &#125;&#125; 源码中有eden和longterm的两个map，对jvm堆区有所了解的话，可以猜测出tomcat在这里是使用ConcurrentHashMap和WeakHashMap做了分代的缓存。在put方法里，在插入一个k-v时，先检查eden缓存的容量是不是超了。没有超就直接放入eden缓存，如果超了则锁定longterm将eden中所有的k-v都放入longterm。再将eden清空并插入k-v。在get方法中，也是优先从eden中找对应的v，如果没有则进入longterm缓存中查找，找到后就加入eden缓存并返回。经过这样的设计，相对常用的对象都能在eden缓存中找到，不常用（有可能被销毁的对象）的则进入longterm缓存。而longterm的key的实际对象没有其他引用指向它时，gc就会自动回收heap中该弱引用指向的实际对象，弱引用进入引用队列。longterm调用expungeStaleEntries()方法，遍历引用队列中的弱引用，并清除对应的Entry，不会造成内存空间的浪费。 参考：Java8中的WeakHashMapJava 集合系列13之 WeakHashMap详细介绍(源码解析)和使用示例WeakHashMap的使用场景]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>WeakHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java四种引用类型及其应用场景]]></title>
    <url>%2F2019%2F06%2F20%2Fjava-four-reference%2F</url>
    <content type="text"><![CDATA[java内存管理分为内存分配和内存回收，都不需要程序员负责，垃圾回收的机制主要是看对象是否有引用指向该对象。 java对象的引用包括强引用，软引用，弱引用，虚引用 Java中提供这四种引用类型主要有两个目的： 第一是可以让程序员通过代码的方式决定某些对象的生命周期； 第二是有利于JVM进行垃圾回收。 强引用 - Strong Reference功能：强引用不会被GC回收，并且在java.lang.ref里也没有实际的对应类型，平时工作接触的最多的就是强引用。一个对象如果具有强引用，那么垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。使用场景：我们平常大部分使用的场景都是使用了强引用，比如new创建对象，反射获得一个对象等。 1String str = new String(&quot;str&quot;); 这个str就是强引用。 软引用 - Soft Reference功能： 如果一个对象只具有软引用，则内存空间足够时，垃圾回收器就不会去回收它；如果内存空间不足时，就会回收这些对象的内存。软引用还可以和一个引用队列进行关联，如果这个软引用的对象被垃圾回收，那么VM就会将这个软引用加入到关联的队列中去。 使用场景： 软引用可用来实现内存敏感的高速缓存,比如网页缓存、图片缓存等。使用软引用能防止内存泄露，增强程序的健壮性。 SoftReference的特点是它的一个实例保存对一个Java对象的软引用， 该软引用的存在不妨碍垃圾收集线程对该Java对象的回收。也就是说，一旦SoftReference保存了对一个Java对象的软引用后，在垃圾线程对 这个Java对象回收前，SoftReference类所提供的get()方法返回Java对象的强引用。 另外，一旦垃圾线程回收该Java对象之 后，get()方法将返回null。 12MyObject aRef = new MyObject(); SoftReference aSoftRef=new SoftReference(aRef); 此时，对于这个MyObject对象，有两个引用路径，一个是来自SoftReference对象的软引用，一个来自变量aRef的强引用，所以这个MyObject对象是强可及对象。 1aRef = null; 此后，这个MyObject对象成为了软引用对象。如果垃圾收集线程进行内存垃圾收集，并不会因为有一个SoftReference对该对象的引用而始终保留该对象。Java虚拟机的垃圾收集线程对软可及对象和其他一般Java对象进行了区别对待:软可及对象的清理是由垃圾收集线程根据其特定算法按照内存需求决定的。也就是说，垃圾收集线程会在虚拟机抛出OutOfMemoryError之前回收软可及对象，而且虚拟机会尽可能优先回收长时间闲置不用的软可及对象，对那些刚刚构建的或刚刚使用过的“新”软可反对象会被虚拟机尽可能保留。在回收这些对象之前，我们可以通过: 1MyObject anotherRef=(MyObject)aSoftRef.get(); 重新获得对该实例的强引用。而回收之后，调用get()方法就只能得到null了。 使用ReferenceQueue清除失去了软引用对象的SoftReference： 作为一个Java对象，SoftReference对象除了具有保存软引用的特殊性之外，也具有Java对象的一般性。所以，当软可及对象被回收之后，虽然这个SoftReference对象的get()方法返回null,但这个SoftReference对象已经不再具有存在的价值，需要一个适当的清除机制，避免大量SoftReference对象带来的内存泄漏。在java.lang.ref包里还提供了ReferenceQueue。如果在创建SoftReference对象的时候，使用了一个ReferenceQueue对象作为参数提供给SoftReference的构造方法，如: 12ReferenceQueue queue = new ReferenceQueue(); SoftReference ref=new SoftReference(aMyObject, queue); 那么当这个SoftReference所软引用的MyObject被垃圾收集器回收的同时，ref所强引用的SoftReference对象被加入ReferenceQueue。也就是说，ReferenceQueue中保存的对象是Reference对象，而且是已经失去了它所软引用的对象的Reference对象。另外从ReferenceQueue这个名字也可以看出，它是一个队列，当我们调用它的poll()方法的时候，如果这个队列中不是空队列，那么将返回队列前面的那个Reference对象。 在任何时候，我们都可以调用ReferenceQueue的poll()方法来检查是否有它所关心的非强可及对象被回收。如果队列为空，将返回一个null,否则该方法返回队列中前面的一个Reference对象。利用这个方法，我们可以检查哪个SoftReference所软引用的对象已经被回收。于是我们可以把这些失去所软引用的对象的SoftReference对象清除掉。常用的方式为: 1234SoftReference ref = null; while ((ref = (EmployeeRef) q.poll()) != null) &#123; // 清除ref &#125; PS：图片编辑器，视频编辑器之类的软件可以使用这种思路。 弱引用 - Weak Reference功能： 被弱引用关联的对象，在垃圾回收时，如果这个对象只被弱引用关联（没有任何强引用关联它），那么这个对象就会被回收。弱引用和软引用的区别在于，只具有弱引用的对象拥有更短暂的生命周期，在垃圾回收器线程扫描它管辖的内存区域的过程中，一旦发现对象只具有弱引用，不管当前内存空间是否足够，都会回收它的内存。它比软引用的生命周期更短，和软引用相似，它同样可以和引用队列关联，如果被垃圾回收了，就会加入到这个关联队列中。**使用场景：WeakHashMap、ThreadLocal。 下面是使用示例： 1234567891011121314151617181920212223public class test &#123; public static void main(String[] args) &#123; WeakReference&lt;People&gt;reference=new WeakReference&lt;People&gt;(new People(&quot;zhouqian&quot;,20)); System.out.println(reference.get()); System.gc();//通知GVM回收资源 System.out.println(reference.get()); &#125; &#125; class People&#123; public String name; public int age; public People(String name,int age) &#123; this.name=name; this.age=age; &#125; @Override public String toString() &#123; return &quot;[name:&quot;+name+&quot;,age:&quot;+age+&quot;]&quot;; &#125; &#125; 输出结果： [name:zhouqian,age:20]null 第二个输出结果是null，这说明只要JVM进行垃圾回收，被弱引用关联的对象必定会被回收掉。不过要注意的是，这里所说的被弱引用关联的对象是指只有弱引用与之关联，如果存在强引用同时与之关联，则进行垃圾回收时也不会回收该对象（软引用也是如此）。 比如：将代码做一点小更改： 12345678910111213141516171819202122232425import java.lang.ref.WeakReference; public class test &#123; public static void main(String[] args) &#123; People people=new People(&quot;zhouqian&quot;,20); WeakReference&lt;People&gt;reference=new WeakReference&lt;People&gt;(people);//&lt;span style=&quot;color:#FF0000;&quot;&gt;关联强引用&lt;/span&gt; System.out.println(reference.get()); System.gc(); System.out.println(reference.get()); &#125; &#125; class People&#123; public String name; public int age; public People(String name,int age) &#123; this.name=name; this.age=age; &#125; @Override public String toString() &#123; return &quot;[name:&quot;+name+&quot;,age:&quot;+age+&quot;]&quot;; &#125; &#125;//结果发生了很大的变化 [name:zhouqian,age:20] [name:zhouqian,age:20] 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被JVM回收，这个软引用就会被加入到与之关联的引用队列中。 虚引用 - Phantom Reference功能： “虚引用”形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期，如果一个对象仅持有虚引用的话，那么它就和没有任何的引用一样，在任何时候都可能被垃圾回收器回收。 虚引用必须和引用队列联合使用，引用队列的作用和软弱引用一样。 虚引用的回收机制跟弱引用差不多，但是它被回收之前，会被放入ReferenceQueue中。其它引用是被JVM回收后才被传入ReferenceQueue中的。由于这个机制，所以虚引用大多被用于引用销毁前的处理工作。 虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。在java中用java.lang.ref.PhantomReference类表示。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。要注意的是，虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之 关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来获取一个对象的实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。虚引用和弱引用对关联对象的回收都不会产生影响，如果只有虚引用活着弱引用关联着对象，那么这个对象就会被回收。它们的不同之处在于弱引用的get方法，虚引用的get方法始终返回null,弱引用可以使用ReferenceQueue,虚引用必须配合ReferenceQueue使用。 使用场景：程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。还可以做对象销毁前的一些操作，比如说资源释放等。Object.finalize()虽然也可以做这类动作，但是这个方式即不安全又低效。 jdk中直接内存的回收就用到虚引用，由于jvm自动内存管理的范围是堆内存，而直接内存是在堆内存之外（其实是内存映射文件，自行去理解虚拟内存空间的相关概念），所以直接内存的分配和回收都是有Unsafe类去操作，java在申请一块直接内存之后，会在堆内存分配一个对象保存这个堆外内存的引用，这个对象被垃圾收集器管理，一旦这个对象被回收，相应的用户线程会收到通知并对直接内存进行清理工作。 1234String name = &quot;a&quot;;ReferenceQueue&lt;String&gt; prq = new ReferenceQueue&lt;&gt;();PhantomReference&lt;String&gt; nameRf = new PhantomReference&lt;&gt;(name, prq);System.out.println(prq.poll()); 参考：Java 四种引用介绍及使用场景java中的4种reference的差别和使用场景（含理论、代码和执行结果）Java中的四种引用介绍和使用场景软引用、弱引用、虚引用-他们的特点及应用场景Java的四种引用方式]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中ThreadLocal原理]]></title>
    <url>%2F2019%2F06%2F20%2Fjava-ThreadLocal%2F</url>
    <content type="text"><![CDATA[本文转发自技术世界，原文链接 http://www.jasongj.com/java/threadlocal/ ThreadLocal解决什么问题由于 ThreadLocal 支持范型，如 ThreadLocal&lt; StringBuilder &gt;，为表述方便，后文用 变量 代表 ThreadLocal 本身，而用 实例 代表具体类型（如 StringBuidler ）的实例。 不恰当的理解写这篇文章的一个原因在于，网上很多博客关于 ThreadLocal 的适用场景以及解决的问题，描述的并不清楚，甚至是错的。下面是常见的对于 ThreadLocal的介绍 ThreadLocal为解决多线程程序的并发问题提供了一种新的思路ThreadLocal的目的是为了解决多线程访问资源时的共享问题 还有很多文章在对比 ThreadLocal 与 synchronize 的异同。既然是作比较，那应该是认为这两者解决相同或类似的问题。 上面的描述，问题在于，ThreadLocal 并不解决多线程 共享 变量的问题。既然变量不共享，那就更谈不上同步的问题。 合理的理解ThreadLoal 变量，它的基本原理是，同一个 ThreadLocal 所包含的对象（对ThreadLocal&lt; String &gt;而言即为 String 类型变量），在不同的 Thread 中有不同的副本（实际是不同的实例，后文会详细阐述）。这里有几点需要注意 因为每个 Thread 内有自己的实例副本，且该副本只能由当前 Thread 使用。这是也是 ThreadLocal 命名的由来 既然每个 Thread 有自己的实例副本，且其它 Thread 不可访问，那就不存在多线程间共享的问题 既无共享，何来同步问题，又何来解决同步问题一说？ 那 ThreadLocal 到底解决了什么问题，又适用于什么样的场景？ This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID).Each thread holds an implicit reference to its copy of a thread-local variable as long as the thread is alive and the ThreadLocal instance is accessible; after a thread goes away, all of its copies of thread-local instances are subject to garbage collection (unless other references to these copies exist). 核心意思是 ThreadLocal 提供了线程本地的实例。它与普通变量的区别在于，每个使用该变量的线程都会初始化一个完全独立的实例副本。ThreadLocal 变量通常被private static修饰。当一个线程结束时，它所使用的所有 ThreadLocal 相对的实例副本都可被回收。 总的来说，ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也即变量在线程间隔离而在方法或类间共享的场景。后文会通过实例详细阐述该观点。另外，该场景下，并非必须使用 ThreadLocal ，其它方式完全可以实现同样的效果，只是 ThreadLocal 使得实现更简洁。 ThreadLocal用法实例代码下面通过如下代码说明 ThreadLocal 的使用方式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class ThreadLocalDemo &#123; public static void main(String[] args) throws InterruptedException &#123; int threads = 3; CountDownLatch countDownLatch = new CountDownLatch(threads); InnerClass innerClass = new InnerClass(); for(int i = 1; i &lt;= threads; i++) &#123; new Thread(() -&gt; &#123; for(int j = 0; j &lt; 4; j++) &#123; innerClass.add(String.valueOf(j)); innerClass.print(); &#125; innerClass.set(&quot;hello world&quot;); countDownLatch.countDown(); &#125;, &quot;thread - &quot; + i).start(); &#125; countDownLatch.await(); &#125; private static class InnerClass &#123; public void add(String newStr) &#123; StringBuilder str = Counter.counter.get(); Counter.counter.set(str.append(newStr)); &#125; public void print() &#123; System.out.printf(&quot;Thread name:%s , ThreadLocal hashcode:%s, Instance hashcode:%s, Value:%s\n&quot;, Thread.currentThread().getName(), Counter.counter.hashCode(), Counter.counter.get().hashCode(), Counter.counter.get().toString()); &#125; public void set(String words) &#123; Counter.counter.set(new StringBuilder(words)); System.out.printf(&quot;Set, Thread name:%s , ThreadLocal hashcode:%s, Instance hashcode:%s, Value:%s\n&quot;, Thread.currentThread().getName(), Counter.counter.hashCode(), Counter.counter.get().hashCode(), Counter.counter.get().toString()); &#125; &#125; private static class Counter &#123; private static ThreadLocal&lt;StringBuilder&gt; counter = new ThreadLocal&lt;StringBuilder&gt;() &#123; @Override protected StringBuilder initialValue() &#123; return new StringBuilder(); &#125; &#125;; &#125;&#125; 实例分析ThreadLocal本身支持范型。该例使用了 StringBuilder 类型的 ThreadLocal 变量。可通过 ThreadLocal 的 get() 方法读取 StringBuidler 实例，也可通过 set(T t) 方法设置 StringBuilder。 上述代码执行结果如下 123456789101112131415Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:418873098, Value:0Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1609588821, Value:0Thread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1780437710, Value:0Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1609588821, Value:01Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:418873098, Value:01Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1609588821, Value:012Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1609588821, Value:0123Set, Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1362597339, Value:hello worldThread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1780437710, Value:01Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:418873098, Value:012Thread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1780437710, Value:012Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:418873098, Value:0123Thread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1780437710, Value:0123Set, Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:482932940, Value:hello worldSet, Thread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1691922941, Value:hello world 从上面的输出可看出 从第1-3行输出可见，每个线程通过 ThreadLocal 的 get() 方法拿到的是不同的 StringBuilder 实例 第1-3行输出表明，每个线程所访问到的是同一个 ThreadLocal 变量 从7、12、13行输出以及第30行代码可见，虽然从代码上都是对 Counter 类的静态 counter 字段进行 get() 得到 StringBuilder 实例并追加字符串，但是这并不会将所有线程追加的字符串都放进同一个 StringBuilder 中，而是每个线程将字符串追加进各自的 StringBuidler 实例内 对比第1行与第15行输出并结合第38行代码可知，使用 set(T t) 方法后，ThreadLocal 变量所指向的 StringBuilder 实例被替换 ThreadLocal原理ThreadLocal维护线程与实例的映射既然每个访问 ThreadLocal 变量的线程都有自己的一个“本地”实例副本。一个可能的方案是 ThreadLocal 维护一个 Map，键是 Thread，值是它在该 Thread 内的实例。线程通过该 ThreadLocal 的 get() 方案获取实例时，只需要以线程为键，从 Map 中找出对应的实例即可。该方案如下图所示 ] 该方案可满足上文提到的每个线程内一个独立备份的要求。每个新线程访问该 ThreadLocal 时，需要向 Map 中添加一个映射，而每个线程结束时，应该清除该映射。这里就有两个问题： 增加线程与减少线程均需要写 Map，故需保证该 Map 线程安全。虽然从ConcurrentHashMap的演进看Java多线程核心技术一文介绍了几种实现线程安全 Map 的方式，但它或多或少都需要锁来保证线程的安全性 线程结束时，需要保证它所访问的所有 ThreadLocal 中对应的映射均删除，否则可能会引起内存泄漏。（后文会介绍避免内存泄漏的方法） 其中锁的问题，是 JDK 未采用该方案的一个原因。 Thread维护ThreadLocal与实例的映射上述方案中，出现锁的问题，原因在于多线程访问同一个 Map。如果该 Map 由 Thread 维护，从而使得每个 Thread 只访问自己的 Map，那就不存在多线程写的问题，也就不需要锁。该方案如下图所示。 ] 该方案虽然没有锁的问题，但是由于每个线程访问某 ThreadLocal 变量后，都会在自己的 Map 内维护该 ThreadLocal 变量与具体实例的映射，如果不删除这些引用（映射），则这些 ThreadLocal 不能被回收，可能会造成内存泄漏。后文会介绍 JDK 如何解决该问题。 ThreadLocal 在 JDK 8 中的实现ThreadLocalMap与内存泄漏该方案中，Map 由 ThreadLocal 类的静态内部类 ThreadLocalMap 提供。该类的实例维护某个 ThreadLocal 与具体实例的映射。与 HashMap 不同的是，ThreadLocalMap 的每个 Entry 都是一个对 键 的弱引用，这一点从super(k)可看出。另外，每个 Entry 都包含了一个对 值 的强引用。 123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 使用弱引用的原因在于，当没有强引用指向 ThreadLocal 变量时，它可被回收，从而避免上文所述 ThreadLocal 不能被回收而造成的内存泄漏的问题。 但是，这里又可能出现另外一种内存泄漏的问题。ThreadLocalMap 维护 ThreadLocal 变量与具体实例的映射，当 ThreadLocal 变量被回收后，该映射的键变为 null，该 Entry 无法被移除。从而使得实例被该 Entry 引用而无法被回收造成内存泄漏。 注：Entry虽然是弱引用，但它是 ThreadLocal 类型的弱引用（也即上文所述它是对 键 的弱引用），而非具体实例的的弱引用，所以无法避免具体实例相关的内存泄漏。 读取实例读取实例方法如下所示 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 读取实例时，线程首先通过getMap(t)方法获取自身的 ThreadLocalMap。从如下该方法的定义可见，该 ThreadLocalMap 的实例是 Thread 类的一个字段，即由 Thread 维护 ThreadLocal 对象与具体实例的映射，这一点与上文分析一致。 123ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 获取到 ThreadLocalMap 后，通过map.getEntry(this)(this 指的是 ThreadLocal类对象)方法获取该 ThreadLocal 在当前线程的 ThreadLocalMap 中对应的 Entry。该方法中的 this 即当前访问的 ThreadLocal 对象。 如果获取到的 Entry 不为 null，从 Entry 中取出值即为所需访问的本线程对应的实例。如果获取到的 Entry 为 null，则通过setInitialValue()方法设置该 ThreadLocal 变量在该线程中对应的具体实例的初始值。 设置初始值设置初始值方法如下 12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 该方法为 private 方法，无法被重载。 首先，通过initialValue()方法获取初始值。该方法为 public 方法，且默认返回 null。所以典型用法中常常重载该方法。上例中即在内部匿名类中将其重载。 然后拿到该线程对应的 ThreadLocalMap 对象，若该对象不为 null，则直接将该 ThreadLocal 对象与对应实例初始值的映射添加进该线程的 ThreadLocalMap中。若为 null，则先创建该 ThreadLocalMap 对象再将映射添加其中。 这里并不需要考虑 ThreadLocalMap 的线程安全问题。因为每个线程有且只有一个 ThreadLocalMap 对象，并且只有该线程自己可以访问它，其它线程不会访问该 ThreadLocalMap，也即该对象不会在多个线程中共享，也就不存在线程安全的问题。 设置实例除了通过initialValue()方法设置实例的初始值，还可通过 set 方法设置线程内实例的值，如下所示。 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 该方法先获取该线程的 ThreadLocalMap 对象，然后直接将 ThreadLocal 对象（即代码中的 this）与目标实例的映射添加进 ThreadLocalMap 中。当然，如果映射已经存在，就直接覆盖。另外，如果获取到的 ThreadLocalMap 为 null，则先创建该 ThreadLocalMap 对象。 防止内存泄漏对于已经不再被使用且已被回收的 ThreadLocal 对象，它在每个线程内对应的实例由于被线程的 ThreadLocalMap 的 Entry 强引用，无法被回收，可能会造成内存泄漏。 ThreadLocal对象是弱引用，可以被回收，但是ThreadLocalMap的Entry可能会存在内存泄漏问题. 针对该问题，ThreadLocalMap 的 set 方法中，通过 replaceStaleEntry 方法将所有键为 null 的 Entry 的值设置为 null，从而使得该值可被回收。另外，会在 rehash 方法中通过 expungeStaleEntry 方法将键和值为 null 的 Entry 设置为 null 从而使得该 Entry 可被回收。通过这种方式，ThreadLocal 可防止内存泄漏。 123456789101112131415161718192021private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 适用场景如上文所述，ThreadLocal 适用于如下两种场景 每个线程需要有自己单独的实例 实例需要在多个方法中共享，但不希望被多线程共享 对于第一点，每个线程拥有自己实例，实现它的方式很多。例如可以在线程内部构建一个单独的实例。ThreadLocal 可以以非常方便的形式满足该需求。 对于第二点，可以在满足第一点（每个线程有自己的实例）的条件下，通过方法间引用传递的形式实现。ThreadLocal 使得代码耦合度更低，且实现更优雅。 案例对于 Java Web 应用而言，Session 保存了很多信息。很多时候需要通过 Session 获取信息，有些时候又需要修改 Session 的信息。一方面，需要保证每个线程有自己单独的 Session 实例。另一方面，由于很多地方都需要操作 Session，存在多方法共享 Session 的需求。如果不使用 ThreadLocal，可以在每个线程内构建一个 Session实例，并将该实例在多个方法间传递，如下所示。 123456789101112131415161718192021222324252627282930313233343536public class SessionHandler &#123; @Data public static class Session &#123; private String id; private String user; private String status; &#125; public Session createSession() &#123; return new Session(); &#125; public String getUser(Session session) &#123; return session.getUser(); &#125; public String getStatus(Session session) &#123; return session.getStatus(); &#125; public void setStatus(Session session, String status) &#123; session.setStatus(status); &#125; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; SessionHandler handler = new SessionHandler(); Session session = handler.createSession(); handler.getStatus(session); handler.getUser(session); handler.setStatus(session, &quot;close&quot;); handler.getStatus(session); &#125;).start(); &#125;&#125; 该方法是可以实现需求的。但是每个需要使用 Session 的地方，都需要显式传递 Session 对象，方法间耦合度较高。 这里使用 ThreadLocal 重新实现该功能如下所示。 123456789101112131415161718192021222324252627282930313233public class SessionHandler &#123; public static ThreadLocal&lt;Session&gt; session = ThreadLocal.&lt;Session&gt;withInitial(() -&gt; new Session()); @Data public static class Session &#123; private String id; private String user; private String status; &#125; public String getUser() &#123; return session.get().getUser(); &#125; public String getStatus() &#123; return session.get().getStatus(); &#125; public void setStatus(String status) &#123; session.get().setStatus(status); &#125; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; SessionHandler handler = new SessionHandler(); handler.getStatus(); handler.getUser(); handler.setStatus(&quot;close&quot;); handler.getStatus(); &#125;).start(); &#125;&#125; 使用 ThreadLocal 改造后的代码，不再需要在各个方法间传递 Session 对象，并且也非常轻松的保证了每个线程拥有自己独立的实例。 如果单看其中某一点，替代方法很多。比如可通过在线程内创建局部变量可实现每个线程有自己的实例，使用静态变量可实现变量在方法间的共享。但如果要同时满足变量在线程间的隔离与方法间的共享，ThreadLocal再合适不过。 总结 ThreadLocal 并不解决线程间共享数据的问题 ThreadLocal 通过隐式的在不同线程内创建独立实例副本避免了实例线程安全的问题 每个线程持有一个 Map 并维护了 ThreadLocal 对象与具体实例的映射，该 Map 由于只被持有它的线程访问，故不存在线程安全以及锁的问题 ThreadLocalMap 的 Entry 对 ThreadLocal 的引用为弱引用，避免了 ThreadLocal 对象无法被回收的问题 ThreadLocalMap 的 set 方法通过调用 replaceStaleEntry 方法回收键为 null 的 Entry 对象的值（即为具体实例）以及 Entry 对象本身从而防止内存泄漏 ThreadLocal 适用于变量在线程间隔离且在方法间共享的场景]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java关键字synchronized]]></title>
    <url>%2F2019%2F06%2F20%2Fjava-synchronized%2F</url>
    <content type="text"><![CDATA[线程安全是并发编程中的重要关注点，应该注意到的是，造成线程安全问题的主要诱因有两点，一是存在共享数据(也称临界资源)，二是存在多条线程共同操作共享数据。因此为了解决这个问题，我们可能需要这样一个方案，当存在多个线程操作共享数据时，需要保证同一时刻有且只有一个线程在操作共享数据，其他线程必须等到该线程处理完数据后再进行，这种方式有个高大上的名称叫互斥锁，即能达到互斥访问目的的锁，也就是说当一个共享数据被当前正在访问的线程加上互斥锁后，在同一个时刻，其他线程只能处于等待的状态，直到当前线程处理完毕释放该锁。在 Java 中，关键字 synchronized可以保证在同一个时刻，只有一个线程可以执行某个方法或者某个代码块(主要是对方法或者代码块中存在共享数据的操作)，同时我们还应该注意到synchronized另外一个重要的作用，synchronized可保证一个线程的变化(主要是共享数据的变化)被其他线程所看到（保证可见性，完全可以替代Volatile功能），这点确实也是很重要的。 synchronized的三种应用方式synchronized关键字最主要有以下3种应用方式，下面分别介绍 12345修饰实例方法，作用于当前实例加锁，进入同步代码前要获得当前实例的锁修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 synchronized作用于实例方法所谓的实例对象锁就是用synchronized修饰实例对象中的实例方法，注意是实例方法不包括静态方法，如下 12345678910111213141516171819202122232425262728293031public class AccountingSync implements Runnable&#123; //共享资源(临界资源) static int i=0; /** * synchronized 修饰实例方法 */ public synchronized void increase()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; AccountingSync instance=new AccountingSync(); Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125; /** * 输出结果: * 2000000 */&#125; 上述代码中，我们开启两个线程操作同一个共享资源即变量i，由于i++;操作并不具备原子性，该操作是先读取值，然后写回一个新值，相当于原来的值加上1，分两步完成(应该是三步，读取，加1，写入)，如果第二个线程在第一个线程读取旧值和写回新值期间读取i的域值，那么第二个线程就会与第一个线程一起看到同一个值，并执行相同值的加1操作，这也就造成了线程安全失败，因此对于increase方法必须使用synchronized修饰，以便保证线程安全。此时我们应该注意到synchronized修饰的是实例方法increase，在这样的情况下，当前线程的锁便是实例对象instance，注意Java中的线程同步锁可以是任意对象。从代码执行结果来看确实是正确的，倘若我们没有使用synchronized关键字，其最终输出结果就很可能小于2000000，这便是synchronized关键字的作用。这里我们还需要意识到，当一个线程正在访问一个对象的 synchronized 实例方法，那么其他线程不能访问该对象的其他 synchronized 方法，毕竟一个对象只有一把锁，当一个线程获取了该对象的锁之后，其他线程无法获取该对象的锁，所以无法访问该对象的其他synchronized实例方法，但是其他线程还是可以访问该实例对象的其他非synchronized方法，当然如果是一个线程 A 需要访问实例对象 obj1 的 synchronized 方法 f1(当前对象锁是obj1)，另一个线程 B 需要访问实例对象 obj2 的 synchronized 方法 f2(当前对象锁是obj2)，这样是允许的，因为两个实例对象锁并不同相同，此时如果两个线程操作数据并非共享的，线程安全是有保障的，遗憾的是如果两个线程操作的是共享数据，那么线程安全就有可能无法保证了，如下代码将演示出该现象 123456789101112131415161718192021222324public class AccountingSyncBad implements Runnable&#123; static int i=0; public synchronized void increase()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //new新实例 Thread t1=new Thread(new AccountingSyncBad()); //new新实例 Thread t2=new Thread(new AccountingSyncBad()); t1.start(); t2.start(); //join含义:当前线程A等待thread线程终止之后才能从thread.join()返回 t1.join(); t2.join(); System.out.println(i); &#125;&#125; 上述代码与前面不同的是我们同时创建了两个新实例AccountingSyncBad，然后启动两个不同的线程对共享变量i进行操作，但很遗憾操作结果是1452317而不是期望结果2000000，因为上述代码犯了严重的错误，虽然我们使用synchronized修饰了increase方法，但却new了两个不同的实例对象，这也就意味着存在着两个不同的实例对象锁，因此t1和t2都会进入各自的对象锁，也就是说t1和t2线程使用的是不同的锁，因此线程安全是无法保证的。解决这种困境的的方式是将synchronized作用于静态的increase方法，这样的话，对象锁就当前类对象，由于无论创建多少个实例对象，但对于的类对象拥有只有一个，所有在这样的情况下对象锁就是唯一的。下面我们看看如何使用将synchronized作用于静态的increase方法。 synchronized作用于静态方法当synchronized作用于静态方法时，其锁就是当前类的class对象锁。由于静态成员不专属于任何一个实例对象，是类成员，因此通过class对象锁可以控制静态成员的并发操作。需要注意的是如果一个线程A调用一个实例对象的非static synchronized方法，而线程B需要调用这个实例对象所属类的静态 synchronized方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的class对象，而访问非静态 synchronized 方法占用的锁是当前实例对象锁，看如下代码 123456789101112131415161718192021222324252627282930313233343536public class AccountingSyncClass implements Runnable&#123; static int i=0; /** * 作用于静态方法,锁是当前class对象,也就是 * AccountingSyncClass类对应的class对象 */ public static synchronized void increase()&#123; i++; &#125; /** * 非静态,访问时锁不一样不会发生互斥 */ public synchronized void increase4Obj()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //new新实例 Thread t1=new Thread(new AccountingSyncClass()); //new心事了 Thread t2=new Thread(new AccountingSyncClass()); //启动线程 t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 由于synchronized关键字修饰的是静态increase方法，与修饰实例方法不同的是，其锁对象是当前类的class对象。注意代码中的increase4Obj方法是实例方法，其对象锁是当前实例对象，如果别的线程调用该方法，将不会产生互斥现象，毕竟锁对象不同，但我们应该意识到这种情况下可能会发现线程安全问题(操作了共享静态变量i)。 synchronized同步代码块除了使用关键字修饰实例方法和静态方法外，还可以修饰同步代码块，在某些情况下，我们编写的方法体可能比较大，同时存在一些比较耗时的操作，而需要同步的代码又只有一小部分，如果直接对整个方法进行同步操作，可能会得不偿失，此时我们可以使用同步代码块的方式对需要同步的代码进行包裹，这样就无需对整个方法进行同步操作了，同步代码块的使用示例如下： 123456789101112131415161718192021public class AccountingSync implements Runnable&#123; static AccountingSync instance=new AccountingSync(); static int i=0; @Override public void run() &#123; //省略其他耗时操作.... //使用同步代码块对变量i进行同步操作,锁对象为instance synchronized(instance)&#123; for(int j=0;j&lt;1000000;j++)&#123; i++; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 从代码看出，将synchronized作用于一个给定的实例对象instance，即当前实例对象就是锁对象，每次当线程进入synchronized包裹的代码块时就会要求当前线程持有instance实例对象锁，如果当前有其他线程正持有该对象锁，那么新到的线程就必须等待，这样也就保证了每次只有一个线程执行i++;操作。当然除了instance作为对象外，我们还可以使用this对象(代表当前实例)或者当前类的class对象作为锁，如下代码： 12345678910111213//this,当前实例对象锁synchronized(this)&#123; for(int j=0;j&lt;1000000;j++)&#123; i++; &#125;&#125;//class对象锁synchronized(AccountingSync.class)&#123; for(int j=0;j&lt;1000000;j++)&#123; i++; &#125;&#125; 了解完synchronized的基本含义及其使用方式后，下面我们将进一步深入理解synchronized的底层实现原理。 synchronized底层语义原理Java 虚拟机中的同步(Synchronization)基于进入和退出管程(Monitor)对象实现， 无论是显式同步(有明确的 monitorenter 和 monitorexit 指令,即同步代码块)还是隐式同步都是如此。在 Java 语言中，同步用的最多的地方可能是被 synchronized 修饰的同步方法。同步方法并不是由 monitorenter 和 monitorexit 指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的ACC_SYNCHRONIZED标志来隐式实现的，关于这点，稍后详细分析。下面先来了解一个概念Java对象头，这对深入理解synchronized实现原理非常关键。 理解Java对象头与Monitor在JVM中，对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充。如下： 实例变量：存放类的属性数据信息，包括父类的属性信息，如果是数组的实例部分还包括数组的长度，这部分内存按4字节对齐。 填充数据：由于虚拟机要求对象起始地址必须是8字节的整数倍。填充数据不是必须存在的，仅仅是为了字节对齐，这点了解即可。 而对于顶部，则是Java头对象，它是实现synchronized的锁对象的基础，这点我们重点分析它，一般而言，synchronized使用的锁对象是存储在Java对象头里的，jvm中采用2个字来存储对象头(如果对象是数组则会分配3个字，多出来的1个字记录的是数组长度)，其主要结构是由Mark Word 和Class Metadata Address 组成，其结构说明如下表： 虚拟机位数 头对象结构 说明 32/64bit Mark Word 存储对象的hashCode、锁信息或分代年龄或GC标志等信息 32/64bit Class Metadata Address 类型指针指向对象的类元数据，JVM通过这个指针确定该对象是哪个类的实例 其中Mark Word在默认情况下存储着对象的HashCode、分代年龄、锁标记位等. 以下是32位JVM的Mark Word默认存储结构 锁状态 25bit 4bit 1bit是否是偏向锁 2bit 锁标志位 无锁状态 对象HashCode 对象分代年龄 0 01 由于对象头的信息是与对象自身定义的数据没有关系的额外存储成本，因此考虑到JVM的空间效率，Mark Word 被设计成为一个非固定的数据结构，以便存储更多有效的数据，它会根据对象本身的状态复用自己的存储空间，如32位JVM下，除了上述列出的Mark Word默认存储结构外，还有如下可能变化的结构： 其中轻量级锁和偏向锁是Java 6对 synchronized 锁进行优化后新增加的，稍后我们会简要分析。这里我们主要分析一下重量级锁也就是通常说synchronized的对象锁，锁标识位为10，其中指针指向的是monitor对象（也称为管程或监视器锁）的起始地址。每个对象都存在着一个 monitor 与之关联，对象与其 monitor 之间的关系有存在多种实现方式，如monitor可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个 monitor 被某个线程持有后，它便处于锁定状态。在Java虚拟机(HotSpot)中，monitor是由ObjectMonitor实现的，其主要数据结构如下（位于HotSpot虚拟机源码ObjectMonitor.hpp文件，C++实现的） 123456789101112131415161718ObjectMonitor() &#123; _header = NULL; _count = 0; //记录个数 _waiters = 0, _recursions = 0; _object = NULL; _owner = NULL; _WaitSet = NULL; //处于wait状态的线程，会被加入到_WaitSet _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; //处于等待锁block状态的线程，会被加入到该列表 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; &#125; ObjectMonitor中有两个队列，_WaitSet 和 _EntryList，用来保存ObjectWaiter对象列表( 每个等待锁的线程都会被封装成ObjectWaiter对象)，_owner指向持有ObjectMonitor对象的线程，当多个线程同时访问一段同步代码时，首先会进入 _EntryList 集合，当线程获取到对象的monitor 后进入 _Owner 区域并把monitor中的owner变量设置为当前线程同时monitor中的计数器count加1，若线程调用 wait() 方法，将释放当前持有的monitor，owner变量恢复为null，count自减1，同时该线程进入 WaitSet集合中等待被唤醒。若当前线程执行完毕也将释放monitor(锁)并复位变量的值，以便其他线程进入获取monitor(锁)。如下图所示 由此看来，monitor对象存在于每个Java对象的对象头中(存储的指针的指向)，synchronized锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因，同时也是notify/notifyAll/wait等方法存在于顶级对象Object中的原因(关于这点稍后还会进行分析)，ok~，有了上述知识基础后，下面我们将进一步分析synchronized在字节码层面的具体语义实现。 synchronized代码块底层原理现在我们重新定义一个synchronized修饰的同步代码块，在代码块中操作共享变量i，如下 1234567891011public class SyncCodeBlock &#123; public int i; public void syncTask()&#123; //同步代码库 synchronized (this)&#123; i++; &#125; &#125;&#125; 编译上述代码并使用javap反编译后得到字节码如下(这里我们省略一部分没有必要的信息)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Classfile /Users/zejian/Downloads/Java8_Action/src/main/java/com/zejian/concurrencys/SyncCodeBlock.class Last modified 2017-6-2; size 426 bytes MD5 checksum c80bc322c87b312de760942820b4fed5 Compiled from &quot;SyncCodeBlock.java&quot;public class com.zejian.concurrencys.SyncCodeBlock minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: //........省略常量池中数据 //构造函数 public com.zejian.concurrencys.SyncCodeBlock(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 7: 0 //===========主要看看syncTask方法实现================ public void syncTask(); descriptor: ()V flags: ACC_PUBLIC Code: stack=3, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter //注意此处，进入同步方法 4: aload_0 5: dup 6: getfield #2 // Field i:I 9: iconst_1 10: iadd 11: putfield #2 // Field i:I 14: aload_1 15: monitorexit //注意此处，退出同步方法 16: goto 24 19: astore_2 20: aload_1 21: monitorexit //注意此处，退出同步方法 22: aload_2 23: athrow 24: return Exception table: //省略其他字节码.......&#125;SourceFile: &quot;SyncCodeBlock.java&quot; 我们主要关注字节码中的如下代码 1234563: monitorenter //进入同步方法//..........省略其他 15: monitorexit //退出同步方法16: goto 24//省略其他.......21: monitorexit //退出同步方法 从字节码中可知同步语句块的实现使用的是monitorenter 和 monitorexit 指令，其中monitorenter指令指向同步代码块的开始位置，monitorexit指令则指明同步代码块的结束位置，当执行monitorenter指令时，当前线程将试图获取 objectref(即对象锁) 所对应的 monitor 的持有权，当 objectref 的 monitor 的进入计数器为 0，那线程可以成功取得 monitor，并将计数器值设置为 1，取锁成功。如果当前线程已经拥有 objectref 的 monitor 的持有权，那它可以重入这个 monitor (关于重入性稍后会分析)，重入时计数器的值也会加 1。倘若其他线程已经拥有 objectref 的 monitor 的所有权，那当前线程将被阻塞，直到正在执行线程执行完毕，即monitorexit指令被执行，执行线程将释放 monitor(锁)并设置计数器值为0 ，其他线程将有机会持有 monitor 。值得注意的是编译器将会确保无论方法通过何种方式完成，方法中调用过的每条 monitorenter 指令都有执行其对应 monitorexit 指令，而无论这个方法是正常结束还是异常结束。为了保证在方法异常完成时 monitorenter 和 monitorexit 指令依然可以正确配对执行，编译器会自动产生一个异常处理器，这个异常处理器声明可处理所有的异常，它的目的就是用来执行 monitorexit 指令。从字节码中也可以看出多了一个monitorexit指令，它就是异常结束时被执行的释放monitor 的指令。 synchronized方法底层原理方法级的同步是隐式，即无需通过字节码指令来控制的，它实现在方法调用和返回操作之中。JVM可以从方法常量池中的方法表结构(method_info Structure) 中的 ACC_SYNCHRONIZED访问标志区分一个方法是否同步方法。当方法调用时，调用指令将会 检查方法的ACC_SYNCHRONIZED访问标志是否被设置，如果设置了，执行线程将先持有monitor（虚拟机规范中用的是管程一词）， 然后再执行方法，最后再方法完成(无论是正常完成还是非正常完成)时释放monitor。在方法执行期间，执行线程持有了monitor，其他任何线程都无法再获得同一个monitor。如果一个同步方法执行期间抛 出了异常，并且在方法内部无法处理此异常，那这个同步方法所持有的monitor将在异常抛到同步方法之外时自动释放。下面我们看看字节码层面如何实现： 12345678public class SyncMethod &#123; public int i; public synchronized void syncTask()&#123; i++; &#125;&#125; 使用javap反编译后的字节码如下： 123456789101112131415161718192021222324252627282930Classfile /Users/zejian/Downloads/Java8_Action/src/main/java/com/zejian/concurrencys/SyncMethod.class Last modified 2017-6-2; size 308 bytes MD5 checksum f34075a8c059ea65e4cc2fa610e0cd94 Compiled from &quot;SyncMethod.java&quot;public class com.zejian.concurrencys.SyncMethod minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool; //省略没必要的字节码 //==================syncTask方法====================== public synchronized void syncTask(); descriptor: ()V //方法标识ACC_PUBLIC代表public修饰，ACC_SYNCHRONIZED指明该方法为同步方法 flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=3, locals=1, args_size=1 0: aload_0 1: dup 2: getfield #2 // Field i:I 5: iconst_1 6: iadd 7: putfield #2 // Field i:I 10: return LineNumberTable: line 12: 0 line 13: 10&#125;SourceFile: &quot;SyncMethod.java&quot; 从字节码中可以看出，synchronized修饰的方法并没有monitorenter指令和monitorexit指令，取得代之的确实是ACC_SYNCHRONIZED标识，该标识指明了该方法是一个同步方法，JVM通过该ACC_SYNCHRONIZED访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。这便是synchronized锁在同步代码块和同步方法上实现的基本原理。同时我们还必须注意到的是在Java早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的Mutex Lock来实现的，而操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的synchronized效率低的原因。庆幸的是在Java 6之后Java官方对从JVM层面对synchronized较大优化，所以现在的synchronized锁效率也优化得很不错了，Java 6之后，为了减少获得锁和释放锁所带来的性能消耗，引入了轻量级锁和偏向锁，接下来我们将简单了解一下Java官方在JVM层面对synchronized锁的优化。 Java虚拟机对synchronized的优化锁的状态总共有四种，无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁，但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级，关于重量级锁，前面我们已详细分析过，下面我们将介绍偏向锁和轻量级锁以及JVM的其他优化手段，这里并不打算深入到每个锁的实现和转换过程更多地是阐述Java虚拟机所提供的每个锁的核心优化思想，毕竟涉及到具体过程比较繁琐，如需了解详细过程可以查阅《深入理解Java虚拟机原理》。 偏向锁偏向锁是Java 6之后加入的新锁，它是一种针对加锁操作的优化手段，经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了减少同一线程获取锁(会涉及到一些CAS操作,耗时)的代价而引入偏向锁。偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，当这个线程再次请求锁时，无需再做任何同步操作，即获取锁的过程，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。下面我们接着了解轻量级锁。 轻量级锁倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)，此时Mark Word 的结构也变为轻量级锁的结构。轻量级锁能够提升程序性能的依据是“对绝大部分的锁，在整个同步周期内都不存在竞争”，注意这是经验数据。需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合，就会导致轻量级锁膨胀为重量级锁。 自旋锁轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，因此自旋锁会假设在不久将来，当前的线程可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环(这也是称为自旋的原因)，一般不会太久，可能是50个循环或100循环，在经过若干次循环后，如果得到锁，就顺利进入临界区。如果还不能获得锁，那就会将线程在操作系统层面挂起，这就是自旋锁的优化方式，这种方式确实也是可以提升效率的。最后没办法也就只能升级为重量级锁了。 锁消除消除锁是虚拟机另外一种锁的优化，这种优化更彻底，Java虚拟机在JIT编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间，如下StringBuffer的append是一个同步方法，但是在add方法中的StringBuffer属于一个局部变量，并且不会被其他线程所使用，因此StringBuffer不可能存在共享资源竞争的情景，JVM会自动将其锁消除。 12345678910111213141516171819202122/** * Created by zejian on 2017/6/4. * Blog : http://blog.csdn.net/javazejian [原文地址,请尊重原创] * 消除StringBuffer同步锁 */public class StringBufferRemoveSync &#123; public void add(String str1, String str2) &#123; //StringBuffer是线程安全,由于sb只会在append方法中使用,不可能被其他线程引用 //因此sb属于不可能共享的资源,JVM会自动消除内部的锁 StringBuffer sb = new StringBuffer(); sb.append(str1).append(str2); &#125; public static void main(String[] args) &#123; StringBufferRemoveSync rmsync = new StringBufferRemoveSync(); for (int i = 0; i &lt; 10000000; i++) &#123; rmsync.add(&quot;abc&quot;, &quot;123&quot;); &#125; &#125;&#125; 关于synchronized 可能需要了解的关键点synchronized的可重入性从互斥锁的设计上来说，当一个线程试图操作一个由其他线程持有的对象锁的临界资源时，将会处于阻塞状态，但当一个线程再次请求自己持有对象锁的临界资源时，这种情况属于重入锁，请求将会成功，在java中synchronized是基于原子性的内部锁机制，是可重入的，因此在一个线程调用synchronized方法的同时在其方法体内部调用该对象另一个synchronized方法，也就是说一个线程得到一个对象锁后再次请求该对象锁，是允许的，这就是synchronized的可重入性。如下： 1234567891011121314151617181920212223242526272829public class AccountingSync implements Runnable&#123; static AccountingSync instance=new AccountingSync(); static int i=0; static int j=0; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; //this,当前实例对象锁 synchronized(this)&#123; i++; increase();//synchronized的可重入性 &#125; &#125; &#125; public synchronized void increase()&#123; j++; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 正如代码所演示的，在获取当前实例对象锁后进入synchronized代码块执行同步代码，并在代码块中调用了当前实例对象的另外一个synchronized方法，再次请求当前实例锁时，将被允许，进而执行方法体代码，这就是重入锁最直接的体现，需要特别注意另外一种情况，当子类继承父类时，子类也是可以通过可重入锁调用父类的同步方法。注意由于synchronized是基于monitor实现的，因此每次重入，monitor中的计数器仍会加1。 线程中断与synchronized线程中断正如中断二字所表达的意义，在线程运行(run方法)中间打断它，在Java中，提供了以下3个有关线程中断的方法 12345678//中断线程（实例方法）public void Thread.interrupt();//判断线程是否被中断（实例方法）public boolean Thread.isInterrupted();//判断是否被中断并清除当前中断状态（静态方法）public static boolean Thread.interrupted(); 当一个线程处于被阻塞状态或者试图执行一个阻塞操作时，使用Thread.interrupt()方式中断该线程，注意此时将会抛出一个InterruptedException的异常，同时中断状态将会被复位(由中断状态改为非中断状态)，如下代码将演示该过程： 12345678910111213141516171819202122232425262728293031public class InterruputSleepThread3 &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread() &#123; @Override public void run() &#123; //while在try中，通过异常中断就可以退出run循环 try &#123; while (true) &#123; //当前线程处于阻塞状态，异常必须捕捉处理，无法往外抛出 TimeUnit.SECONDS.sleep(2); &#125; &#125; catch (InterruptedException e) &#123; System.out.println(&quot;Interruted When Sleep&quot;); boolean interrupt = this.isInterrupted(); //中断状态被复位 System.out.println(&quot;interrupt:&quot;+interrupt); &#125; &#125; &#125;; t1.start(); TimeUnit.SECONDS.sleep(2); //中断处于阻塞状态的线程 t1.interrupt(); /** * 输出结果: Interruted When Sleep interrupt:false */ &#125;&#125; 如上述代码所示，我们创建一个线程，并在线程中调用了sleep方法从而使用线程进入阻塞状态，启动线程后，调用线程实例对象的interrupt方法中断阻塞异常，并抛出InterruptedException异常，此时中断状态也将被复位。这里有些人可能会诧异，为什么不用Thread.sleep(2000);而是用TimeUnit.SECONDS.sleep(2);其实原因很简单，前者使用时并没有明确的单位说明，而后者非常明确表达秒的单位，事实上后者的内部实现最终还是调用了Thread.sleep(2000);，但为了编写的代码语义更清晰，建议使用TimeUnit.SECONDS.sleep(2);的方式，注意TimeUnit是个枚举类型。ok~，除了阻塞中断的情景，我们还可能会遇到处于运行期且非阻塞的状态的线程，这种情况下，直接调用Thread.interrupt()中断线程是不会得到任响应的，如下代码，将无法中断非阻塞状态下的线程： 1234567891011121314151617181920212223public class InterruputThread &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread()&#123; @Override public void run()&#123; while(true)&#123; System.out.println(&quot;未被中断&quot;); &#125; &#125; &#125;; t1.start(); TimeUnit.SECONDS.sleep(2); t1.interrupt(); /** * 输出结果(无限执行): 未被中断 未被中断 未被中断 ...... */ &#125;&#125; 虽然我们调用了interrupt方法，但线程t1并未被中断，因为处于非阻塞状态的线程需要我们手动进行中断检测并结束程序，改进后代码如下： 123456789101112131415161718192021222324252627public class InterruputThread &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread()&#123; @Override public void run()&#123; while(true)&#123; //判断当前线程是否被中断 if (this.isInterrupted())&#123; System.out.println(&quot;线程中断&quot;); break; &#125; &#125; System.out.println(&quot;已跳出循环,线程中断!&quot;); &#125; &#125;; t1.start(); TimeUnit.SECONDS.sleep(2); t1.interrupt(); /** * 输出结果: 线程中断 已跳出循环,线程中断! */ &#125;&#125; 是的，我们在代码中使用了实例方法isInterrupted判断线程是否已被中断，如果被中断将跳出循环以此结束线程,注意非阻塞状态调用interrupt()并不会导致中断状态重置。综合所述，可以简单总结一下中断两种情况， 一种是当线程处于阻塞状态或者试图执行一个阻塞操作时，我们可以使用实例方法interrupt()进行线程中断，执行中断操作后将会抛出interruptException异常(该异常必须捕捉无法向外抛出)并将中断状态复位 另外一种是当线程处于运行状态时，我们也可调用实例方法interrupt()进行线程中断，但同时必须手动判断中断状态，并编写中断线程的代码(其实就是结束run方法体的代码)。 有时我们在编码时可能需要兼顾以上两种情况，那么就可以如下编写： 12345678910public void run()&#123; try &#123; //判断当前线程是否已中断,注意interrupted方法是静态的,执行后会对中断状态进行复位 while (!Thread.interrupted()) &#123; TimeUnit.SECONDS.sleep(2); &#125; &#125; catch (InterruptedException e) &#123; &#125;&#125; 中断与synchronized事实上线程的中断操作对于正在等待获取的锁对象的synchronized方法或者代码块并不起作用，也就是对于synchronized来说，如果一个线程在等待锁，那么结果只有两种，要么它获得这把锁继续执行，要么它就保存等待，即使调用中断线程的方法，也不会生效。演示代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Created by zejian on 2017/6/2. * Blog : http://blog.csdn.net/javazejian [原文地址,请尊重原创] */public class SynchronizedBlocked implements Runnable&#123; public synchronized void f() &#123; System.out.println(&quot;Trying to call f()&quot;); while(true) // Never releases lock Thread.yield(); // yield 让步，屈服，投降，让出CPU的意思，使当前线程从执行状态（运行状态）变为可执行态（就绪状态） &#125; /** * 在构造器中创建新线程并启动获取对象锁 */ public SynchronizedBlocked() &#123; //该线程已持有当前实例锁 new Thread() &#123; public void run() &#123; f(); // Lock acquired by this thread &#125; &#125;.start(); &#125; public void run() &#123; //中断判断 while (true) &#123; if (Thread.interrupted()) &#123; System.out.println(&quot;中断线程!!&quot;); break; &#125; else &#123; f(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; SynchronizedBlocked sync = new SynchronizedBlocked(); Thread t = new Thread(sync); //启动后调用f()方法,无法获取当前实例锁处于等待状态 t.start(); TimeUnit.SECONDS.sleep(1); //中断线程,无法生效 t.interrupt(); &#125;&#125; 我们在SynchronizedBlocked构造函数中创建一个新线程并启动获取调用f()获取到当前实例锁，由于SynchronizedBlocked自身也是线程，启动后在其run方法中也调用了f()，但由于对象锁被其他线程占用，导致t线程只能等到锁，此时我们调用了t.interrupt();但并不能中断线程。 等待唤醒机制与synchronized所谓等待唤醒机制本篇主要指的是notify/notifyAll和wait方法，在使用这3个方法时，必须处于synchronized代码块或者synchronized方法中，否则就会抛出IllegalMonitorStateException异常，这是因为调用这几个方法前必须拿到当前对象的监视器monitor对象，也就是说notify/notifyAll和wait方法依赖于monitor对象，在前面的分析中，我们知道monitor 存在于对象头的Mark Word 中(存储monitor引用指针)，而synchronized关键字可以获取 monitor ，这也就是为什么notify/notifyAll和wait方法必须在synchronized代码块或者synchronized方法调用的原因。 12345synchronized (obj) &#123; obj.wait(); obj.notify(); obj.notifyAll(); &#125; 需要特别理解的一点是，与sleep方法不同的是wait方法调用完成后，线程将被暂停，但wait方法将会释放当前持有的监视器锁(monitor)，直到有线程调用notify/notifyAll方法后方能继续执行，而sleep方法只让线程休眠并不释放锁。同时notify/notifyAll方法调用后，并不会马上释放监视器锁，而是在相应的synchronized(){}/synchronized方法执行结束后才自动释放锁。 本篇的主要参考资料：《Java编程思想》《深入理解Java虚拟机》《实战Java高并发程序设计》 作者：zejian_来源：CSDN原文：https://blog.csdn.net/javazejian/article/details/72828483版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java之AQS]]></title>
    <url>%2F2019%2F06%2F20%2Fjava-AQS%2F</url>
    <content type="text"><![CDATA[简单解释一下J.U.C，是JDK中提供的并发工具包,java.util.concurrent。里面提供了很多并发编程中很常用的实用工具类，比如atomic原子操作、比如lock同步锁、fork/join等。 概述谈到并发，不得不谈ReentrantLock；而谈到ReentrantLock，不得不AbstractQueuedSynchronizer（AQS）！ 类如其名，抽象的队列式的同步器，AQS定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch…。 它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。这里volatile是核心关键词，具体volatile的语义，在此不述。state的访问方式有三种: 123getState()setState()compareAndSetState() AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法： isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后续动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 从Lock作为切入点我想以lock作为切入点来讲解AQS，毕竟同步锁是解决线程安全问题的通用手段，也是我们工作中用得比较多的方式。 Lock APILock是一个接口，方法定义如下 12345void lock() // 如果锁可用就获得锁，如果锁不可用就阻塞直到锁释放void lockInterruptibly() // 和 lock()方法相似, 但阻塞的线程可中断，抛出 java.lang.InterruptedException异常boolean tryLock() // 非阻塞获取锁;尝试获取锁，如果成功返回trueboolean tryLock(long timeout, TimeUnit timeUnit) //带有超时时间的获取锁方法void unlock() // 释放锁 Lock的实现实现Lock接口的类有很多，以下为几个常见的锁实现 ReentrantLock：表示重入锁，它是唯一一个实现了Lock接口的类。重入锁指的是线程在获得锁之后，再次获取该锁不需要阻塞，而是直接关联一次计数器增加重入次数。 ReentrantReadWriteLock：重入读写锁，它实现了ReadWriteLock接口，在这个类中维护了两个锁，一个是ReadLock，一个是WriteLock，他们都分别实现了Lock接口。读写锁是一种适合读多写少的场景下解决线程安全问题的工具，基本原则是：读和读不互斥、读和写互斥、写和写互斥。也就是说涉及到影响数据变化的操作都会存在互斥。 StampedLock： stampedLock是JDK8引入的新的锁机制，可以简单认为是读写锁的一个改进版本，读写锁虽然通过分离读和写的功能使得读和读之间可以完全并发，但是读和写是有冲突的，如果大量的读线程存在，可能会引起写线程的饥饿。stampedLock是一种乐观的读策略，使得乐观锁完全不会阻塞写线程。 ReentrantLock的简单使用如何在实际应用中使用ReentrantLock呢？我们通过一个简单的demo来演示一下 1234567891011121314public class Demo &#123; private static int count=0; static Lock lock=new ReentrantLock(); public static void inc()&#123; lock.lock(); try &#123; Thread.sleep(1); count++; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally&#123; lock.unlock(); &#125; &#125; 这段代码主要做一件事，就是通过一个静态的incr()方法对共享变量count做连续递增，在没有加同步锁的情况下多线程访问这个方法一定会存在线程安全问题。所以用到了ReentrantLock来实现同步锁，并且在finally语句块中释放锁。那么我来引出一个问题，大家思考一下 多个线程通过lock竞争锁时，当竞争失败的锁是如何实现等待以及被唤醒的呢? 什么是AQSAQS全称为AbstractQueuedSynchronizer，它提供了一个FIFO队列，可以看成是一个用来实现同步锁以及其他涉及到同步功能的核心组件，常见的有:ReentrantLock、CountDownLatch等。AQS是一个抽象类，主要是通过继承的方式来使用，它本身没有实现任何的同步接口，仅仅是定义了同步状态的获取以及释放的方法来提供自定义的同步组件。可以这么说，只要搞懂了AQS，那么J.U.C中绝大部分的api都能轻松掌握。 AQS的两种功能从使用层面来说，AQS的功能分为两种：独占和共享 独占锁，每次只能有一个线程持有锁，比如前面给大家演示的ReentrantLock就是以独占方式实现的互斥锁 共享锁，允许多个线程同时获取锁，并发访问共享资源，比如ReentrantReadWriteLock ReentrantLock的类图仍然以ReentrantLock为例，来分析AQS在重入锁中的使用。毕竟单纯分析AQS没有太多的含义。先理解这个类图，可以方便我们理解AQS的原理 AQS的内部实现AQS的实现依赖内部的同步队列,也就是FIFO的双向队列（不是双端队列，双端队列是指允许两端都可以进行入队和出队操作的队列），如果当前线程竞争锁失败，那么AQS会把当前线程以及等待状态信息构造成一个Node加入到同步队列中，同时再阻塞该线程。当获取锁的线程释放锁以后，会从队列中唤醒一个阻塞的节点(线程)。 AQS队列内部维护的是一个FIFO的双向链表，这种结构的特点是每个数据结构都有两个指针，分别指向直接的后继节点和直接前驱节点。所以双向链表可以从任意一个节点开始很方便的访问前驱和后继。每个Node其实是对线程的封装，当线程争抢锁失败后会封装成Node加入到ASQ队列中去 Node类的组成如下 1234567891011121314151617181920212223242526272829303132333435363738static final class Node &#123; static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; //前驱节点 volatile Node next; //后继节点 volatile Thread thread;//当前线程 Node nextWaiter; //存储在condition队列中的后继节点 //是否为共享锁 final boolean isShared() &#123; return nextWaiter == SHARED; &#125; final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; //将线程构造成一个Node，添加到等待队列 Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; //这个方法会在Condition队列使用，后续单独写一篇文章分析condition Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125; &#125; 释放锁以及添加线程对于队列的变化添加节点当出现锁竞争以及释放锁的时候，AQS同步队列中的节点会发生变化，首先看一下添加节点的场景。这里会涉及到两个变化 新的线程封装成Node节点追加到同步队列中，设置prev节点以及修改当前节点的前置节点的next节点指向自己 通过CAS将tail重新指向新的尾部节点 释放锁移除节点head节点表示获取锁成功的节点，当头结点在释放同步状态时，会唤醒后继节点，如果后继节点获得锁成功，会把自己设置为头结点，节点的变化过程如下这个过程也是涉及到两个变化 修改head节点指向下一个获得锁的节点 新的获得锁的节点，将prev的指针指向null 这里有一个小的变化，就是设置head节点不需要用CAS，原因是设置head节点是由获得锁的线程来完成的，而同步锁只能由一个线程获得，所以不需要CAS保证，只需要把head节点设置为原首节点的后继节点，并且断开原head节点的next引用即可 synchronized 和 ReentrantLock 的区别共同点 都是用来协调多线程对共享对象、变量的访问 都是可重入锁，同一线程可以多次获得同一个锁 都保证了可见性和互斥性 不同点 ReentrantLock 显示的获得、释放锁，synchronized 隐式获得释放锁 synchronized的加锁和释放锁是自动的，ReetrantLock需要手动加锁和释放锁 ReentrantLock 可响应中断、可轮回，synchronized 是不可以响应中断的，为处理锁的不可用性提供了更高的灵活性 ReentrantLock 是 API 级别的，synchronized 是 JVM 级别的 Synchronized是Java语言的关键字，因此Synchronized的锁是原生语法层面的互斥，需要JVM来实现。具体是通过对象内部的一个叫做监视器锁（monitor）来实现的。ReentrantLock，字面意思可重入锁，它是JDK1.5之后提供的API层面的互斥锁，锁的功能主要由2个方法完成，即lock()和unlock()。 ReentrantLock 可以实现公平锁 ReentrantLock 通过 Condition 可以绑定多个条件 ReentrantLock可以同时绑定多个Condition对象，而synchronized中，锁对象的wait()和notify()或notifyAll()方法可以实现一个隐含的条件，如果要和多于一个条件关联时，只能再加一个额外的锁，而ReentrantLock只需要多次调用newCondition方法即可。 底层实现不一样， synchronized 是同步阻塞，使用的是悲观并发策略，lock 是同步非阻塞，采用的是乐观并发策略 ReetrantLock的tryLock可以设置超时机制 Lock 是一个接口，而 synchronized 是 Java 中的关键字，synchronized 是内置的语言实现。 synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，因此使用 Lock 时需要在 finally 块中释放锁。 Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，等待的线程会一直等待下去，不能够响应中断。 ReentrantLock在等待锁时可以使用lockInterruptibly()方法选择中断， 改为处理其他事情，而synchronized关键字，线程需要一直等待下去。同样的，tryLock()方法可以设置超时时间，用于在超时时间内一直获取不到锁时进行中断。 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 Lock 可以提高多个线程进行读操作的效率，既就是实现读写锁等。 AQS的源码分析清楚了AQS的基本架构以后，我们来分析一下AQS的源码，仍然以ReentrantLock为模型。 ReentrantLock的时序图调用ReentrantLock中的lock()方法，源码的调用过程我使用了时序图来展现从图上可以看出来，当锁获取失败时，会调用addWaiter()方法将当前线程封装成Node节点加入到AQS队列，基于这个思路，我们来分析AQS的源码实现 分析源码ReentrantLock.lock()123public void lock() &#123; sync.lock();&#125; 这个是获取锁的入口，调用sync这个类里面的方法，sync是什么呢？ 1abstract static class Sync extends AbstractQueuedSynchronizer sync是一个静态内部类，它继承了AQS这个抽象类，前面说过AQS是一个同步工具，主要用来实现同步控制。我们在利用这个工具的时候，会继承它来实现同步控制功能。通过进一步分析，发现Sync这个类有两个具体的实现，分别是NofairSync(非公平锁),FailSync(公平锁). 公平锁：表示所有线程严格按照FIFO来获取锁 非公平锁： 表示可以存在抢占锁的功能，也就是说不管当前队列上是否存在其他线程等待，新线程都有机会抢占锁 公平锁和非公平锁的实现上的差异，我会在文章后面做一个解释，接下来的分析仍然以非公平锁作为主要分析逻辑。 NonfairSync.lock123456final void lock() &#123; if (compareAndSetState(0, 1)) //通过cas操作来修改state状态，表示争抢锁的操作 setExclusiveOwnerThread(Thread.currentThread());//设置当前获得锁状态的线程 else acquire(1); //尝试去获取锁&#125; 这段代码简单解释一下 由于这里是非公平锁，所以调用lock方法时，先去通过cas去抢占锁 如果抢占锁成功，保存获得锁成功的当前线程 抢占锁失败，调用acquire来走锁竞争逻辑 compareAndSetStatecompareAndSetState的代码实现逻辑如下 12// See below for intrinsics setup to support thisreturn unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 123&gt; 这段代码其实逻辑很简单，就是通过cas乐观锁的方式来做比较并替换。上面这段代码的意思是，如果当前内存中的state的值和预期值expect相等，则替换为update。更新成功返回true，否则返回false.&gt; 这个操作是原子的，不会出现线程安全问题，这里面涉及到Unsafe这个类的操作，一级涉及到state这个属性的意义。&gt; 当state=0时，表示无锁状态 当state&gt;0时，表示已经有线程获得了锁，也就是state = 1，但是因为ReentrantLock允许重入，所以同一个线程多次获得同步锁的时候，state会递增，比如重入5次，那么state=5。 而在释放锁的时候，同样需要释放5次直到state=0其他线程才有资格获得锁 12&gt; private volatile int state;&gt; 需要注意的是：不同的AQS实现，state所表达的含义是不一样的。UnsafeUnsafe类是在sun.misc包下，不属于Java标准。但是很多Java的基础类库，包括一些被广泛使用的高性能开发库都是基于Unsafe类开发的，比如Netty、Hadoop、Kafka等；Unsafe可认为是Java中留下的后门，提供了一些低层次操作，如直接内存访问、线程调度等 12&gt; public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);&gt; 这个是一个native方法， 第一个参数为需要改变的对象，第二个为偏移量(即之前求出来的headOffset的值)，第三个参数为期待的值，第四个为更新后的值整个方法的作用是如果当前时刻的值等于预期值var4相等，则更新为新的期望值 var5，如果更新成功，则返回true，否则返回false； acquireacquire是AQS中的方法，如果CAS操作未能成功，说明state已经不为0，此时继续acquire(1)操作,这里大家思考一下，acquire方法中的1的参数是用来做什么呢？如果没猜中，往前面回顾一下state这个概念 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 这个方法的主要逻辑是 通过tryAcquire尝试获取独占锁，如果成功返回true，失败返回false 如果tryAcquire失败，则会通过addWaiter方法将当前线程封装成Node添加到AQS队列尾部 acquireQueued，将Node作为参数，通过自旋去尝试获取锁。 如果大家看过我写的Synchronized源码分析的文章，就应该能够明白自旋存在的意义 NonfairSync.tryAcquire这个方法的作用是尝试获取锁，如果成功返回true，不成功返回false它是重写AQS类中的tryAcquire方法，并且大家仔细看一下AQS中tryAcquire方法的定义，并没有实现，而是抛出异常。按照一般的思维模式，既然是一个不实现的模版方法，那应该定义成abstract，让子类来实现呀？大家想想为什么 123protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125; nonfairTryAcquiretryAcquire(1)在NonfairSync中的实现代码如下 1234567891011121314151617181920212223final boolean nonfairTryAcquire(int acquires) &#123; //获得当前执行的线程 final Thread current = Thread.currentThread(); int c = getState(); //获得state的值 if (c == 0) &#123; //state=0说明当前是无锁状态 //通过cas操作来替换state的值改为1，大家想想为什么要用cas呢？ //理由是，在多线程环境中，直接修改state=1会存在线程安全问题，你猜到了吗？ if (compareAndSetState(0, acquires)) &#123; //保存当前获得锁的线程 setExclusiveOwnerThread(current); return true; &#125; &#125; //这段逻辑就很简单了。如果是同一个线程来获得锁，则直接增加重入次数 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; //增加重入次数 if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 获取当前线程，判断当前的锁的状态 如果state=0表示当前是无锁状态，通过cas更新state状态的值 如果当前线程是属于重入，则增加重入次数 addWaiter当tryAcquire方法获取锁失败以后，则会先调用addWaiter将当前线程封装成Node，然后添加到AQS队列 12345678910111213141516private Node addWaiter(Node mode) &#123; //mode=Node.EXCLUSIVE //将当前线程封装成Node，并且mode为独占锁 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // tail是AQS的中表示同步队列队尾的属性，刚开始为null，所以进行enq(node)方法 Node pred = tail; if (pred != null) &#123; //tail不为空的情况，说明队列中存在节点数据 node.prev = pred; //将当前线程的Node的prev节点指向tail if (compareAndSetTail(pred, node)) &#123;//通过cas将node添加到AQS队列 pred.next = node;//cas成功，把旧的tail的next指针指向新的tail return node; &#125; &#125; enq(node); //tail=null，将node添加到同步队列中 return node; &#125; 将当前线程封装成Node 判断当前链表中的tail节点是否为空，如果不为空，则通过cas操作把当前线程的node添加到AQS队列 如果为空或者cas失败，调用enq将节点添加到AQS队列 enqenq就是通过自旋操作把当前节点加入到队列中 1234567891011121314151617181920private Node enq(final Node node) &#123; //自旋，不做过多解释，不清楚的关注公众号[架构师修炼宝典] for (;;) &#123; Node t = tail; //如果是第一次添加到队列，那么tail=null if (t == null) &#123; // Must initialize //CAS的方式创建一个空的Node作为头结点 if (compareAndSetHead(new Node())) //此时队列中只一个头结点，所以tail也指向它 tail = head; &#125; else &#123;//进行第二次循环时，tail不为null，进入else区域。将当前线程的Node结点的prev指向tail，然后使用CAS将tail指向Node node.prev = t; if (compareAndSetTail(t, node)) &#123;//t此时指向tail,所以可以CAS成功，将tail重新指向Node。此时t为更新前的tail的值，即指向空的头结点，t.next=node，就将头结点的后续结点指向Node，返回头结点 t.next = node; return t; &#125; &#125; &#125; &#125; 假如有两个线程t1,t2同时进入enq方法，t==null表示队列是首次使用，需要先初始化另外一个线程cas失败，则进入下次循环，通过cas操作将node添加到队尾 到目前为止，通过addwaiter方法构造了一个AQS队列，并且将线程添加到了队列的节点中 acquireQueued将添加到队列中的Node作为参数传入acquireQueued方法，这里面会做抢占锁的操作 1234567891011121314151617181920212223final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor();// 获取prev节点,若为null即刻抛出NullPointException if (p == head &amp;&amp; tryAcquire(arg)) &#123;// 如果前驱为head才有资格进行锁的抢夺 setHead(node); // 获取锁成功后就不需要再进行同步操作了,获取锁成功的线程作为新的head节点//凡是head节点,head.thread与head.prev永远为null, 但是head.next不为null p.next = null; // help GC failed = false; //获取锁成功 return interrupted; &#125;//如果获取锁失败，则根据节点的waitStatus决定是否需要挂起线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())// 若前面为true,则执行挂起,待下次唤醒的时候检测中断的标志 interrupted = true; &#125; &#125; finally &#123; if (failed) // 如果抛出异常则取消锁的获取,进行出队(sync queue)操作 cancelAcquire(node); &#125;&#125; 获取当前节点的prev节点 如果prev节点为head节点，那么它就有资格去争抢锁，调用tryAcquire抢占锁 抢占锁成功以后，把获得锁的节点设置为head，并且移除原来的初始化head节点 如果获得锁失败，则根据waitStatus决定是否需要挂起线程 最后，通过cancelAcquire取消获得锁的操作 前面的逻辑都很好理解，主要看一下shouldParkAfterFailedAcquire这个方法和parkAndCheckInterrupt的作用 shouldParkAfterFailedAcquire从上面的分析可以看出，只有队列的第二个节点可以有机会争用锁，如果成功获取锁，则此节点晋升为头节点。对于第三个及以后的节点，if (p == head)条件不成立，首先进行shouldParkAfterFailedAcquire(p, node)操作shouldParkAfterFailedAcquire方法是判断一个争用锁的线程是否应该被阻塞。它首先判断一个节点的前置节点的状态是否为Node.SIGNAL，如果是，是说明此节点已经将状态设置-如果锁释放，则应当通知它，所以它可以安全的阻塞了，返回true。 123456789101112131415161718192021private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; //前继节点的状态 if (ws == Node.SIGNAL)//如果是SIGNAL状态，意味着当前线程需要被unpark唤醒 return true;//如果前节点的状态大于0，即为CANCELLED状态时，则会从前节点开始逐步循环找到一个没有被“CANCELLED”节点设置为当前节点的前节点，返回false。在下次循环执行shouldParkAfterFailedAcquire时，返回true。这个操作实际是把队列中CANCELLED的节点剔除掉。 if (ws &gt; 0) &#123;// 如果前继节点是“取消”状态，则设置 “当前节点”的 “当前前继节点” 为 “‘原前继节点'的前继节点”。 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 如果前继节点为“0”或者“共享锁”状态，则设置前继节点为SIGNAL状态。 /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; parkAndCheckInterrupt如果shouldParkAfterFailedAcquire返回了true，则会执行：parkAndCheckInterrupt()方法，它是通过LockSupport.park(this)将当前线程挂起到WATING状态，它需要等待一个中断、unpark方法来唤醒它，通过这样一种FIFO的机制的等待，来实现了Lock的操作。 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; LockSupportLockSupport类是Java6引入的一个类，提供了基本的线程同步原语。LockSupport实际上是调用了Unsafe类里的函数，归结到Unsafe里，只有两个函数： 123&gt; public native void unpark(Thread jthread); &gt; public native void park(boolean isAbsolute, long time); &gt; unpark函数为线程提供“许可(permit)”，线程调用park函数则等待“许可”。这个有点像信号量，但是这个“许可”是不能叠加的，“许可”是一次性的。permit相当于0/1的开关，默认是0，调用一次unpark就加1变成了1.调用一次park会消费permit，又会变成0。 如果再调用一次park会阻塞，因为permit已经是0了。直到permit变成1.这时调用unpark会把permit设置为1.每个线程都有一个相关的permit，permit最多只有一个，重复调用unpark不会累积 锁的释放ReentrantLock.unlock加锁的过程分析完以后，再来分析一下释放锁的过程，调用release方法，这个方法里面做两件事， 1，释放锁 ； 2，唤醒park的线程 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; tryRelease这个动作可以认为就是一个设置锁状态的操作，而且是将状态减掉传入的参数值（参数是1），如果结果状态为0，就将排它锁的Owner设置为null，以使得其它的线程有机会进行执行。在排它锁中，加锁的时候状态会增加1（当然可以自己修改这个值），在解锁的时候减掉1，同一个锁，在可以重入后，可能会被叠加为2、3、4这些值，只有unlock()的次数与lock()的次数对应才会将Owner线程设置为空，而且也只有这种情况下才会返回true。 1234567891011121314protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; // 这里是将锁的数量减1 if (Thread.currentThread() != getExclusiveOwnerThread())// 如果释放的线程和获取锁的线程不是同一个，抛出非法监视器状态异常 throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; // 由于重入的关系，不是每次释放锁c都等于0， // 直到最后一次释放锁时，才会把当前线程释放 free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; unparkSuccessor在方法unparkSuccessor(Node)中，就意味着真正要释放锁了，它传入的是head节点（head节点是占用锁的节点），当前线程被释放之后，需要唤醒下一个节点的线程 123456789101112131415private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123;//判断后继节点是否为空或者是否是取消状态, s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) //然后从队列尾部向前遍历找到最前面的一个waitStatus小于0的节点, 至于为什么从尾部开始向前遍历，因为在doAcquireInterruptibly.cancelAcquire方法的处理过程中只设置了next的变化，没有设置prev的变化，在最后有这样一行代码：node.next = node，如果这时执行了unparkSuccessor方法，并且向后遍历的话，就成了死循环了，所以这时只有prev是稳定的 s = t; &#125;//内部首先会发生的动作是获取head节点的next节点，如果获取到的节点不为空，则直接通过：“LockSupport.unpark()”方法来释放对应的被挂起的线程，这样一来将会有一个节点唤醒后继续进入循环进一步尝试tryAcquire()方法来获取锁 if (s != null) LockSupport.unpark(s.thread); //释放许可&#125; 总结通过这篇文章基本将AQS队列的实现过程做了比较清晰的分析，主要是基于非公平锁的独占锁实现。在获得同步锁时，同步器维护一个同步队列，获取状态失败的线程都会被加入到队列中并在队列中进行自旋；移出队列（或停止自旋）的条件是前驱节点为头节点且成功获取了同步状态。在释放同步状态时，同步器调用tryRelease(int arg)方法释放同步状态，然后唤醒头节点的后继节点。 参考文章：深入分析AQS实现原理Java并发之AQS详解]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>AQS</tag>
        <tag>ReentrantLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis数据类型]]></title>
    <url>%2F2019%2F06%2F20%2Fredis-data-type%2F</url>
    <content type="text"><![CDATA[Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 String（字符串）string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。 string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。 实例1234redis 127.0.0.1:6379&gt; SET name &quot;runoob&quot;OKredis 127.0.0.1:6379&gt; GET name&quot;runoob&quot; 在以上实例中我们使用了 Redis 的 SET 和 GET 命令。键为 name，对应的值为 runoob。 注意：一个键最大能存储512MB。 Hash（哈希）Redis hash 是一个键值(key=&gt;value)对集合。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 实例DEL runoob 用于删除前面测试用过的 key，不然会报错：(error) WRONGTYPE Operation against a key holding the wrong kind of value 1234567redis 127.0.0.1:6379&gt; DEL runoobredis 127.0.0.1:6379&gt; HMSET myhash field1 &quot;Hello&quot; field2 &quot;World&quot;&quot;OK&quot;redis 127.0.0.1:6379&gt; HGET myhash field1&quot;Hello&quot;redis 127.0.0.1:6379&gt; HGET myhash field2&quot;World&quot; 实例中我们使用了 Redis HMSET, HGET 命令，HMSET 设置了两个 field=&gt;value 对, HGET 获取对应 field 对应的 value。 每个 hash 可以存储 232-1 键值对（40多亿）。 List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 实例123456789101112redis 127.0.0.1:6379&gt; DEL runoobredis 127.0.0.1:6379&gt; lpush runoob redis(integer) 1redis 127.0.0.1:6379&gt; lpush runoob mongodb(integer) 2redis 127.0.0.1:6379&gt; lpush runoob rabitmq(integer) 3redis 127.0.0.1:6379&gt; lrange runoob 0 101) &quot;rabitmq&quot;2) &quot;mongodb&quot;3) &quot;redis&quot;redis 127.0.0.1:6379&gt; 列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。 Set（集合）Redis的Set是string类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 sadd 命令添加一个 string 元素到 key 对应的 set 集合中，成功返回1，如果元素已经在集合中返回 0，如果 key 对应的 set 不存在则返回错误。 1sadd key member 实例1234567891011121314redis 127.0.0.1:6379&gt; DEL runoobredis 127.0.0.1:6379&gt; sadd runoob redis(integer) 1redis 127.0.0.1:6379&gt; sadd runoob mongodb(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 0redis 127.0.0.1:6379&gt; smembers runoob1) &quot;redis&quot;2) &quot;rabitmq&quot;3) &quot;mongodb&quot; 注意：以上实例中 rabitmq 添加了两次，但根据集合内元素的唯一性，第二次插入的元素将被忽略。 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令添加元素到集合，元素在集合中存在则更新对应score 1zadd key score member 实例12345678910111213redis 127.0.0.1:6379&gt; DEL runoobredis 127.0.0.1:6379&gt; zadd runoob 0 redis(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 mongodb(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 0redis 127.0.0.1:6379&gt; &gt; ZRANGEBYSCORE runoob 0 10001) &quot;mongodb&quot;2) &quot;rabitmq&quot;3) &quot;redis&quot; 参考文章：Redis 数据类型]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis持久化]]></title>
    <url>%2F2019%2F06%2F19%2Fredis-persistence%2F</url>
    <content type="text"><![CDATA[Redis持久化概述Redis是一种高级key-value数据库。它跟memcached类似，不过数据可以持久化，而且支持的数据类型很丰富。有字符串，链表，集合和有序集合和map。支持在服务器端计算集合的并，交和补集(difference)等，还支持多种排序功能。所以Redis也可以被看成是一个数据结构服务器。 Redis的所有数据都是保存在内存中，然后不定期的通过异步方式保存到磁盘上(这称为“半持久化模式”)；也可以把每一次数据变化都写入到一个append only file(aof)里面(这称为“全持久化模式”)。 由于Redis的数据都存放在内存中，如果没有配置持久化，redis重启后数据就全丢失了，于是需要开启redis的持久化功能，将数据保存到磁盘上，当redis重启后，可以从磁盘中恢复数据。redis提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化），另外一种是AOF（append only file）持久化（原理是将Reids的操作日志以追加的方式写入文件）。 持久化的功能：Redis是内存数据库，数据都是存储在内存中，为了避免进程退出导致数据的永久丢失，需要定期将Redis中的数据以某种形式(数据或命令)从内存保存到硬盘；当下次Redis重启时，利用持久化文件实现数据恢复。除此之外，为了进行灾难备份，可以将持久化文件拷贝到一个远程位置。 Redis持久化分为RDB持久化和AOF持久化：前者将当前数据保存到硬盘，后者则是将每次执行的写命令保存到硬盘（类似于MySQL的binlog）；由于AOF持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此AOF是目前主流的持久化方式，不过RDB持久化仍然有其用武之地。 Redis持久化的两种方式： RDB：在指定的时间间隔能对数据进行快照存储。 AOF：记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据。 持久化的配置为了使用持久化的功能，我们需要先知道该如何开启持久化的功能。 RDB的持久化配置12345678910111213141516171819# 时间策略save 900 1save 300 10save 60 10000# 文件名称dbfilename dump.rdb# 文件保存路径dir /home/work/app/redis/data/# 如果持久化出错，主进程是否停止写入stop-writes-on-bgsave-error yes# 是否压缩rdbcompression yes# 导入时是否检查rdbchecksum yes 配置其实非常简单，这里说一下持久化的时间策略具体是什么意思。 save 900 1 表示900s内如果有1条是写入命令，就触发产生一次快照，可以理解为就进行一次备份 save 300 10 表示300s内有10条写入，就产生快照 下面的类似，那么为什么需要配置这么多条规则呢？因为Redis每个时段的读写请求肯定不是均衡的，为了平衡性能与数据安全，我们可以自由定制什么情况下触发备份。所以这里就是根据自身Redis写入情况来进行合理配置。 stop-writes-on-bgsave-error yes 这个配置也是非常重要的一项配置，这是当备份进程出错时，主进程就停止接受新的写入操作，是为了保护持久化的数据一致性问题。如果自己的业务有完善的监控系统，可以禁止此项配置， 否则请开启。 关于压缩的配置 rdbcompression yes ，建议没有必要开启，毕竟Redis本身就属于CPU密集型服务器，再开启压缩会带来更多的CPU消耗，相比硬盘成本，CPU更值钱。 当然如果你想要禁用RDB配置，也是非常容易的，只需要在save的最后一行写上：`save “”` AOF的配置123456789101112131415161718192021# 是否开启aofappendonly yes# 文件名称appendfilename &quot;appendonly.aof&quot;# 同步方式appendfsync everysec# aof重写期间是否同步no-appendfsync-on-rewrite no# 重写触发配置auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# 加载aof时如果有错如何处理aof-load-truncated yes# 文件重写策略aof-rewrite-incremental-fsync yes 还是重点解释一些关键的配置： appendfsync everysec 它其实有三种模式: always：把每个写命令都立即同步到aof，很慢，但是很安全 everysec：每秒同步一次，是折中方案 no：redis不处理交给OS来处理，非常快，但是也最不安全 一般情况下都采用 everysec 配置，这样可以兼顾速度与安全，最多损失1s的数据。 aof-load-truncated yes 如果该配置启用，在加载时发现aof尾部不正确时，会向客户端写入一个log，但是会继续执行，如果设置为 no ，发现错误就会停止，必须修复后才能重新加载。 工作原理关于原理部分，我们主要来看RDB与AOF是如何完成持久化的，它们的过程是如何。 在介绍原理之前先说下Redis内部的定时任务机制，定时任务执行的频率可以在配置文件中通过 hz 10 来设置（这个配置表示1s内执行10次，也就是每100ms触发一次定时任务）。该值最大能够设置为：500，但是不建议超过：100，因为值越大说明执行频率越频繁越高，这会带来CPU的更多消耗，从而影响主进程读写性能。 定时任务使用的是Redis自己实现的 TimeEvent，它会定时去调用一些命令完成定时任务，这些任务可能会阻塞主进程导致Redis性能下降。因此我们在配置Redis时，一定要整体考虑一些会触发定时任务的配置，根据实际情况进行调整。 RDB的原理RDB持久化是将当前进程中的数据生成快照保存到硬盘(因此也称作快照持久化)，保存的文件后缀是rdb；当Redis重新启动时，可以读取快照文件恢复数据。 在Redis中RDB持久化的触发分为两种：自己手动触发与Redis定时触发。 手动触发save命令和bgsave命令都可以生成RDB文件。save命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在Redis服务器阻塞期间，服务器不能处理任何命令请求。 1save 而bgsave命令会创建一个子进程，由子进程来负责创建RDB文件，父进程(即Redis主进程)则继续处理请求。 1bgsave bgsave命令执行过程中，只有fork子进程时会阻塞服务器，而对于save命令，整个过程都会阻塞服务器，因此save已基本被废弃，线上环境要杜绝save的使用。 自动触发1234# 时间策略save 900 1save 300 10save 60 10000 其中save 900 1的含义是：当时间到900秒时，如果redis数据发生了至少1次变化，则执行bgsave；save 300 10和save 60 10000同理。当三个save条件满足任意一个时，都会引起bgsave的调用。 save m n的实现原理Redis的save m n，是通过serverCron函数、dirty计数器、和lastsave时间戳来实现的。 serverCron是Redis服务器的周期性操作函数，默认每隔100ms执行一次；该函数对服务器的状态进行维护，其中一项工作就是检查 save m n 配置的条件是否满足，如果满足就执行bgsave。 dirty计数器是Redis服务器维持的一个状态，记录了上一次执行bgsave/save命令后，服务器状态进行了多少次修改(包括增删改)；而当save/bgsave执行完成后，会将dirty重新置为0。 例如，如果Redis执行了set mykey helloworld，则dirty值会+1；如果执行了sadd myset v1 v2 v3，则dirty值会+3；注意dirty记录的是服务器进行了多少次修改，而不是客户端执行了多少修改数据的命令。 lastsave时间戳也是Redis服务器维持的一个状态，记录的是上一次成功执行save/bgsave的时间。save m n的原理如下： 123每隔100ms，执行serverCron函数；在serverCron函数中，遍历save m n配置的保存条件，只要有一个条件满足，就进行bgsave。对于每一个save m n条件，只有下面两条同时满足时才算满足： （1）当前时间-lastsave &gt; m （2）dirty &gt;= n 其他自动触发机制除了save m n 以外，还有一些其他情况会触发bgsave： 12在主从复制场景下，如果从节点执行全量复制操作，则主节点会执行bgsave命令，并将rdb文件发送给从节点 执行shutdown命令时，自动执行rdb持久化 自动触发的场景主要是有以下几点： 根据我们的 save m n 配置规则自动触发； 从节点全量复制时，主节点发送rdb文件给从节点完成复制操作，主节点会触发 bgsave； 执行 debug reload 时； 执行 shutdown时，如果没有开启aof，也会触发。bgsave执行流程图片中的5个步骤所进行的操作如下： Redis父进程首先判断：当前是否在执行save，或bgsave/bgrewriteaof（后面会详细介绍该命令）的子进程，如果在执行则bgsave命令直接返回。bgsave/bgrewriteaof 的子进程不能同时执行，主要是基于性能方面的考虑：两个并发的子进程同时执行大量的磁盘写操作，可能引起严重的性能问题。 父进程执行fork操作创建子进程，这个过程中父进程是阻塞的，Redis不能执行来自客户端的任何命令 父进程fork后，bgsave命令返回”Background saving started”信息并不再阻塞父进程，并可以响应其他命令 子进程创建RDB文件，根据父进程内存快照生成临时快照文件，完成后对原有文件进行原子替换 子进程发送信号给父进程表示完成，父进程更新统计信息 这里注意的是 fork 操作会阻塞，导致Redis读写性能下降。我们可以控制单个Redis实例的最大内存，来尽可能降低Redis在fork时的事件消耗。以及上面提到的自动触发的频率减少fork次数，或者使用手动触发，根据自己的机制来完成持久化。 RDB文件RDB文件是经过压缩的二进制文件，下面介绍关于RDB文件的一些细节。 RDB文件的存储路径既可以在启动前配置，也可以通过命令动态设定。 配置：dir配置指定目录，dbfilename指定文件名。默认是Redis根目录下的dump.rdb文件。 动态设定：Redis启动后也可以动态修改RDB存储路径，在磁盘损害或空间不足时非常有用；执行命令为config set dir {newdir}和config set dbfilename {newFileName} REDIS：常量，保存着”REDIS”5个字符。 db_version：RDB文件的版本号，注意不是Redis的版本号。 SELECTDB 0 pairs：表示一个完整的数据库(0号数据库)，同理SELECTDB 3 pairs表示完整的3号数据库；只有当数据库中有键值对时，RDB文件中才会有该数据库的信息(上图所示的Redis中只有0号和3号数据库有键值对)；如果Redis中所有的数据库都没有键值对，则这一部分直接省略。其中：SELECTDB是一个常量，代表后面跟着的是数据库号码；0和3是数据库号码；pairs则存储了具体的键值对信息，包括key、value值，及其数据类型、内部编码、过期时间、压缩信息等等。 EOF：常量，标志RDB文件正文内容结束。 check_sum：前面所有内容的校验和；Redis在载入RBD文件时，会计算前面的校验和并与check_sum值比较，判断文件是否损坏。Redis默认采用LZF算法对RDB文件进行压缩。虽然压缩耗时，但是可以大大减小RDB文件的体积，因此压缩默认开启；可以通过命令关闭：1config set rdbcompression no 需要注意的是，RDB文件的压缩并不是针对整个文件进行的，而是对数据库中的字符串进行的，且只有在字符串达到一定长度(20字节)时才会进行。 加载RDB文件的载入工作是在服务器启动时自动执行的，并没有专门的命令。但是由于AOF的优先级更高，因此当AOF开启时，Redis会优先载入AOF文件来恢复数据；只有当AOF关闭时，才会在Redis服务器启动时检测RDB文件，并自动载入。服务器载入RDB文件期间处于阻塞状态，直到载入完成为止。Redis载入RDB文件时，会对RDB文件进行校验，如果文件损坏，则日志中会打印错误，Redis启动失败。 RDB优势 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 RDB劣势 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 AOF的原理AOF的整个流程大体来看可以分为两步，一步是命令的实时写入（如果是 appendfsync everysec 配置，会有1s损耗），第二步是对aof文件的重写。 对于增量追加到文件这一步主要的流程是：命令写入=》追加到aof_buf =》同步到aof磁盘。那么这里为什么要先写入buf在同步到磁盘呢？如果实时写入磁盘会带来非常高的磁盘IO，影响整体性能。 aof重写是为了减少aof文件的大小，可以手动或者自动触发，关于自动触发的规则请看上面配置部分。fork的操作也是发生在重写这一步，也是这里会对主进程产生阻塞。 手动触发： bgrewriteaof，自动触发 就是根据配置规则来触发，当然自动触发的整体时间还跟Redis的定时任务频率有关系。 下面来看看重写的一个流程图： 对于上图有四个关键点补充一下： 在重写期间，由于主进程依然在响应命令，为了保证最终备份的完整性；因此它依然会写入旧的AOF file中，如果重写失败，能够保证数据不丢失。 为了把重写期间响应的写入信息也写入到新的文件中，因此也会为子进程保留一个buf，防止新写的file丢失数据。 重写是直接把当前内存的数据生成对应命令，并不需要读取老的AOF文件进行分析、命令合并。 AOF文件直接采用的文本协议，主要是兼容性好、追加方便、可读性高可认为修改修复。 不管是RDB还是AOF都是先写入一个临时文件，然后通过 rename 完成文件的替换工作。 AOF优势 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 AOF劣势 Redis会不断地将被执行的命令记录到AOF文件里面，所以随着Redis不断运行，AOF文件的体积也会不断增长。在极端情况下，体积不断增大的AOF文件甚至可能会用完硬盘的所有可用空间。对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 Redis在重启之后需要通过重新执行AOF文件记录的所有写命令来还原数据集，所以如果AOF文件的体积非常大，那么还原操作执行的时间就可能会非常长。 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。解决办法： 1231.为了解决AOF文件体积不断增大的问题，用户可以向Redis发送BGREWRITEAOF命令，这个命令会通过移除AOF文件中的冗余命令来重写（rewrite）AOF文件，使AOF文件的体积变得尽可能地小。BGREWRITEAOF的工作原理和BGSAVE创建快照的工作原理非常相似：Redis会创建一个子进程，然后由子进程负责对AOF文件进行重写。因为AOF文件重写也需要用到子进程，所以快照持久化因为创建子进程而导致的性能问题和内存占用问题，在AOF持久化中也同样存在。2.跟快照持久化可以通过设置save选项来自动执行BGSAVE一样，AOF持久化也可以通过设置auto-aof-rewrite-percentage选项和auto-aof-rewrite-min-size选项来自动执行BGREWRITEAOF。举个例子，假设用户对Redis设置了配置选项auto-aof-rewrite-percentage 100和auto-aof-rewrite-min-size 64mb，并且启动了AOF持久化，那么当AOF文件的体积大于64MB，并且AOF文件的体积比上一次重写之后的体积大了至少一倍（100%）的时候，Redis将执行BGREWRITEAOF命令。如果AOF重写执行得过于频繁的话，用户可以考虑将auto-aof-rewrite-percentage选项的值设置为100以上，这种做法可以让Redis在AOF文件的体积变得更大之后才执行重写操作，不过也会让Redis在启动时还原数据集所需的时间变得更长。 从持久化中恢复数据数据的备份、持久化做完了，我们如何从这些持久化文件中恢复数据呢？如果一台服务器上有既有RDB文件，又有AOF文件，该加载谁呢？ 其实想要从这些文件中恢复数据，只需要重新启动Redis即可。我们还是通过图来了解这个流程： 启动时会先检查AOF文件是否存在，如果不存在就尝试加载RDB。那么为什么会优先加载AOF呢？因为AOF保存的数据更完整，通过上面的分析我们知道AOF基本上最多损失1s的数据。 性能与实践通过上面的分析，我们都知道RDB的快照、AOF的重写都需要fork，这是一个重量级操作，会对Redis造成阻塞。因此为了不影响Redis主进程响应，我们需要尽可能降低阻塞。 降低fork的频率，比如可以手动来触发RDB生成快照与AOF重写； 控制Redis最大使用内存，防止fork耗时过长； 使用更牛逼的硬件； 合理配置Linux的内存分配策略，避免因为物理内存不足导致fork失败。 在线上我们到底该怎么做？我提供一些自己的实践经验。 如果Redis中的数据并不是特别敏感或者可以通过其它方式重写生成数据，可以关闭持久化，如果丢失数据可以通过其它途径补回； 自己制定策略定期检查Redis的情况，然后可以手动触发备份、重写数据； 单机如果部署多个实例，要防止多个机器同时运行持久化、重写操作，防止出现内存、CPU、IO资源竞争，让持久化变为串行； 可以加入主从机器，利用一台从机器进行备份处理，其它机器正常响应客户端的命令； RDB持久化与AOF持久化可以同时存在，配合使用。 参考文章：一起看懂Redis两种持久化方式的原理Redis重写/压缩AOF文件redis持久化的几种方式]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>持久化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消失的U盘内存]]></title>
    <url>%2F2019%2F06%2F19%2Fudisk-capacity-small%2F</url>
    <content type="text"><![CDATA[今天为了给mac book pro安装双系统ubuntu 16.04，用U盘制作安装启动盘。安装的过程还算顺利，唯一让人难受的地方进入安装界面之后 mbp的触摸板不能用，只能用有线的鼠标和键盘。安装好了之后发现32G容量的U盘只有2M多了。应该是制作安装启动盘的时候，把很大一部分内存隐藏了起来，所以导致U盘可见容量减小。 下面记录了找回U盘容量的过程： 右键我的电脑 -&gt; 管理 -&gt; 磁盘管理，发现U盘内存属于未分配的状态。 在黑色未分配区域点击右键选择新建简单卷 接下来一路 next 然后就发现消失的U盘内存回来了。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>U盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql分片、分区、分表、分库]]></title>
    <url>%2F2019%2F06%2F19%2Fmysql-partition%2F</url>
    <content type="text"><![CDATA[一、Scale Out（横向扩展）/Scale Up（纵向扩展）Mysql的扩展方案包括Scale Out和Scale Up两种。 Scale Out（横向扩展）Scale Out（横向扩展）：是指Application可以在水平方向上扩展。一般对数据中心的应用而言，Scale out指的是当添加更多的机器时，应用仍然可以很好的利用这些机器的资源来提升自己的效率从而达到很好的扩展性。 Scale Up（纵向扩展）Scale Up（纵向扩展）：是指Application可以在垂直方向上扩展。一般对单台机器而言，Scale Up指的是当某个计算节点（机器）添加更多的CPU Cores，存储设备，使用更大的内存时，应用可以很充分的利用这些资源来提升自己的效率从而达到很好的扩展性。 二、Sharding（属于横向扩展）Sharding 是把数据库横向扩展（Scale Out）到多个物理节点上的一种有效的方式，其主要目的是为突破单节点数据库服务器的 I/O 能力限制，解决数据库扩展性问题。Shard这个词的意思是“碎片”。如果将一个数据库当作一块大玻璃，将这块玻璃打碎，那么每一小块都称为数据库的碎片（Database Shard）。将整个数据库打碎的过程就叫做sharding，可以翻译为分片。 形式上，Sharding可以简单定义为将大数据库分布到多个物理节点上的一个分区方案。每一个分区包含数据库的某一部分，称为一个shard，分区方式可以是任意的，并不局限于传统的水平分区和垂直分区。一个shard可以包含多个表的内容甚至可以包含多个数据库实例中的内容。每个shard被放置在一个数据库服务器上。一个数据库服务器可以处理一个或多个shard的数据。系统中需要有服务器进行查询路由转发，负责将查询转发到包含该查询所访问数据的shard或shards节点上去执行。 Sharding 策略 哈希取模：hash(key) % N； 范围：可以是 ID 范围也可以是时间范围； 映射表：使用单独的一个数据库来存储映射关系。 MySql的Sharding策略包括垂直切分和水平切分两种。 垂直(纵向)拆分垂直(纵向)拆分：是指按功能模块拆分，以解决表与表之间的io竞争。即将表按照功能模块、关系密切程度划分出来，部署到不同的库上。例如，我们会建立定义数据库workDB、商品数据库payDB、用户数据库userDB、日志数据库logDB等，分别用于存储项目数据定义表、商品定义表、用户数据表、日志数据表等。 如userid,name,addr一个表，为了防止表过大，分成2个表。 123userid,nameuserid,addr 表结构设计垂直切分。常见的一些场景包括: 大字段的垂直切分。单独将大字段建在另外的表中，提高基础表的访问性能，原则上在性能关键的应用中应当避免数据库的大字段 按照使用用途垂直切分。例如企业物料属性，可以按照基本属性、销售属性、采购属性、生产制造属性、财务会计属性等用途垂直切分. 按照访问频率垂直切分。例如电子商务、Web 2.0系统中，如果用户属性设置非常多，可以将基本、使用频繁的属性和不常用的属性垂直切分开 水平(横向)拆分 水平(横向)拆分：将同一个表的数据进行分块保存到不同的数据库中，来解决单表中数据量增长出现的压力。这些数据库中的表结构完全相同。当一个表中的数据量过大时，我们可以把该表的数据按照某种规则，例如userID散列、按性别、按省，进行划分，然后存储到多个结构相同的表，和不同的库上。例如，我们的userDB中的用户数据表中，每一个表的数据量都很大，就可以把userDB切分为结构相同的多个userDB：part0DB、part1DB等，再将userDB上的用户数据表userTable，切分为很多userTable：userTable0、userTable1等，然后将这些表按照一定的规则存储到多个userDB上。 表结构设计水平切分。常见的一些场景包括: 比如在线电子商务网站，订单表数据量过大，按照年度、月度水平切分。 Web 2.0网站注册用户、在线活跃用户过多，按照用户ID范围等方式，将相关用户以及该用户紧密关联的表做水平切分。 例如论坛的置顶帖子，因为涉及到分页问题，每页都需要显示置顶贴，这种情况可以把置顶贴水平切分开来，避免取置顶帖子时从所有帖子的表中读取 三、分表和分区分表从表面意思说就是把一张表分成多个小表，把一张表按一定的规则分解成N个具有独立存储空间的实体表。系统读写时需要根据定义好的规则得到对应的字表明，然后操作它。 分区则是把一张表的数据分成N多个区块，这些区块可以在同一个磁盘上，也可以在不同的磁盘上，在逻辑上看最终只是一张表，但底层是由N个物理区块组成的，分区实现比较简单，数据库mysql、oracle等很容易就可支持。分区对业务透明，分区只不过把存放数据的文件分成了许多小块，根据一定的规则把数据文件(MYD)和索引文件（MYI）进行了分割，分区后的表呢，还是一张表。 分表和分区的区别： 实现方式上mysql的分表是真正的分表，一张表分成很多表后，每一个小表都是完整的一张表，都对应三个文件（MyISAM引擎：一个.MYD数据文件，.MYI索引文件，.frm表结构文件）。 数据处理上分表后数据都是存放在分表里，总表只是一个外壳，存取数据发生在一个一个的分表里面。分区则不存在分表的概念，分区只不过把存放数据的文件分成了许多小块，分区后的表还是一张表，数据处理还是由自己来完成。 提高性能上 （1）分表后，单表的并发能力提高了，磁盘I/O性能也提高了。并发能力为什么提高了呢，因为查询一次所花的时间变短了，如果出现高并发的话，总表可以根据不同的查询，将并发压力分到不同的小表里面。磁盘I/O性能高了，本来一个非常大的.MYD文件现在也分摊到各个小表的.MYD中去了。 （2）mysql提出了分区的概念，我觉得就想突破磁盘I/O瓶颈，想提高磁盘的读写能力，来增加mysql性能。 在这一点上，分区和分表的测重点不同，分表重点是存取数据时，如何提高mysql并发能力上；而分区呢，如何突破磁盘的读写能力，从而达到提高mysql性能的目的。 实现的难易度上分表的方法有很多，用merge来分表，是最简单的一种方式。这种方式和分区难易度差不多，并且对程序代码来说可以做到透明的。如果是用其他分表方式就比分区麻烦了。 分区实现是比较简单的，建立分区表，跟建平常的表没什么区别，并且对代码端来说是透明的。 分区的适用场景: 1231. 一张表的查询速度已经慢到影响使用的时候。2. 表中的数据是分段的3. 对数据的操作往往只涉及一部分数据，而不是所有的数据 1234567891011 CREATE TABLE sales ( id INT AUTO_INCREMENT, amount DOUBLE NOT NULL, order_day DATETIME NOT NULL, PRIMARY KEY(id, order_day) ) ENGINE=Innodb PARTITION BY RANGE(YEAR(order_day)) ( PARTITION p_2010 VALUES LESS THAN (2010), PARTITION p_2011 VALUES LESS THAN (2011), PARTITION p_2012 VALUES LESS THAN (2012), PARTITION p_catchall VALUES LESS THAN MAXVALUE); 分表的适用场景 121. 一张表的查询速度已经慢到影响使用的时候。2. 当频繁插入或者联合查询时，速度变慢。 分表的实现需要业务结合实现和迁移，较为复杂。 如何选择？应该使用哪一种方式来实施数据库分库分表，这要看数据库中数据量的瓶颈所在，并综合项目的业务类型进行考虑。 如果数据库是因为表太多而造成海量数据，并且项目的各项业务逻辑划分清晰、低耦合，那么规则简单明了、容易实施的垂直切分必是首选。 而如果数据库中的表并不多，但单表的数据量很大、或数据热度很高，这种情况之下就应该选择水平切分，水平切分比垂直切分要复杂一些，它将原本逻辑上属于一体的数据进行了物理分割，除了在分割时要对分割的粒度做好评估，考虑数据平均和负载平均，后期也将对项目人员及应用程序产生额外的数据管理负担。 在现实项目中，往往是这两种情况兼而有之，这就需要做出权衡，甚至既需要垂直切分，又需要水平切分。 四、分表和分库分表能够解决单表数据量过大带来的查询效率下降的问题，但是，却无法给数据库的并发处理能力带来质的提升。面对高并发的读写访问，当数据库master服务器无法承载写操作压力时，不管如何扩展slave服务器，此时都没有意义了。因此，我们必须换一种思路，对数据库进行拆分，从而提高数据库写入能力，这就是所谓的分库。 分表和分区都是基于同一个数据库里的数据分离技巧，对数据库性能有一定提升，但是随着业务数据量的增加，原来所有的数据都是在一个数据库上的，网络IO及文件IO都集中在一个数据库上的，因此CPU、内存、文件IO、网络IO都可能会成为系统瓶颈。当业务系统的数据容量接近或超过单台服务器的容量、QPS/TPS接近或超过单个数据库实例的处理极限等此时，往往是采用垂直和水平结合的数据拆分方法，把数据服务和数据存储分布到多台数据库服务器上。 与分表策略相似，分库可以采用通过一个关键字取模的方式，来对数据访问进行路由，如下图所示： 五、分库分表存在的问题事务问题在执行分库分表之后，由于数据存储到了不同的库上，数据库事务管理出现了困难。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价；如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库跨表的join问题在执行了分库分表之后，难以避免会将原本逻辑关联性很强的数据划分到不同的表、不同的库上，这时，表的关联操作将受到限制，我们无法join位于不同分库的表，也无法join分表粒度不同的表，结果原本一次查询能够完成的业务，可能需要多次查询才能完成。 额外的数据管理负担和数据运算压力。额外的数据管理负担，最显而易见的就是数据的定位问题和数据的增删改查的重复执行问题，这些都可以通过应用程序解决，但必然引起额外的逻辑运算，例如，对于一个记录用户成绩的用户数据表userTable，业务要求查出成绩最好的100位，在进行分表之前，只需一个order by语句就可以搞定，但是在进行分表之后，将需要n个order by语句，分别查出每一个分表的前100名用户数据，然后再对这些数据进行合并计算，才能得出结果。 六、分片（Sharding）和分区（Partition）sharding和partition的区别： 参考文章：mysql分片、分区、分表、分库Mysql分表和分区的区别、分库和分表区别阿里P8架构师谈：数据库分库分表、读写分离的原理实现，使用场景]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>分片</tag>
        <tag>分区</tag>
        <tag>分表</tag>
        <tag>分库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[桥梁模式bridge]]></title>
    <url>%2F2019%2F06%2F19%2Fbridge%2F</url>
    <content type="text"><![CDATA[桥梁模式的定义定义: 将抽象和实现解耦, 使得两者可以独立的变化 通俗的说, 就是一个类调用另一个类中的方法, 需要一个桥梁, 通过聚合的关系调用 其类图如下: 其中角色说明如下: 1234567Abstraction 抽象化角色: 它的主要职责是定义出该角色的行为, 同时保存一个对实现化角色的引用, 一般是抽象类Implementor 实现化角色: 接口或抽象类, 定义角色必须的行为和属性RefinedAbstraction 修正抽象化角色: 它引用实现化角色对抽象化角色进行修正ConcreteImplementor 具体实现化角色: 它实现接口或抽象类定义的方法和属性 桥梁模式实现抽象角色的部分实现是由实现角色完成的 实现化角色代码: 具体实现化角色代码: 抽象化角色代码: 具体抽象化角色代码: 场景类代码: 桥梁模式优点桥梁模式是一个很简单的模式, 它只是使用了类间的聚合关系、继承、覆写等常用功能, 但是它却提供了一个非常清晰、稳定的架构。 桥梁模式的优点: 123抽象和实现分离. 这是桥梁模式的主要特点, 它完全是为了解决继承的缺点而提出的设计模式. 在该模式下,实现可以不受抽象的约束,不用再绑定在一个固定的抽象层次上,具有优秀的扩充能力.实现细节对客户透明. 客户不用关心细节的实现, 它已经由抽象层通过聚合关系完成了封装 桥梁模式的使用场景:12345不希望或不适用使用继承的场景. 例如继承层次过滤、无法更细化设计颗粒等场景接口或抽象类不稳定的场景.重用性要求较高的场景. 设计的颗粒度越细,则被重用的可能性就越大, 而采用继承则受父类的限制, 不可能出现太细的颗粒度 使用桥梁模式主要考虑如何拆分抽象和实现,并不是一设计继承就要考虑使用该模式. 桥梁模式的意图还是对变化的封装, 尽量把可能变化的因素封装到最细、最小的逻辑单元中,避免风险扩散.因此在进行系统设计时,发现类的继承有N层时,可以考虑使用桥梁模式. 桥梁模式在Java应用中的一个非常典型的例子就是JDBC驱动器。JDBC为所有的关系型数据库提供一个通用的界面。一个应用系统动态地选择一个合适的驱动器，然后通过驱动器向数据库引擎发出指令。这个过程就是将抽象角色的行为委派给实现角色的过程。 抽象角色可以针对任何数据库引擎发出查询指令，因为抽象角色并不直接与数据库引擎打交道，JDBC驱动器负责这个底层的工作。由于JDBC驱动器的存在，应用系统可以不依赖于数据库引擎的细节而独立地演化；同时数据库引擎也可以独立于应用系统的细节而独立的演化。两个独立的等级结构如下图所示，左边是JDBC API的等级结构，右边是JDBC驱动器的等级结构。应用程序是建立在JDBC API的基础之上的。 应用系统作为一个等级结构，与JDBC驱动器这个等级结构是相对独立的，它们之间没有静态的强关联。应用系统通过委派与JDBC驱动器相互作用，这是一个桥梁模式的例子。 JDBC的这种架构，把抽象部分和具体部分分离开来，从而使得抽象部分和具体部分都可以独立地扩展。对于应用程序而言，只要选用不同的驱动，就可以让程序操作不同的数据库，而无需更改应用程序，从而实现在不同的数据库上移植；对于驱动程序而言，为数据库实现不同的驱动程序，并不会影响应用程序。 以上文章来自：23种设计模式之桥梁模式]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>桥接模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql之explain详解]]></title>
    <url>%2F2019%2F06%2F18%2Fmysql-explain%2F</url>
    <content type="text"><![CDATA[原文：MySQL 性能优化神器 Explain 使用分析 简介MySQL 提供了一个 EXPLAIN 命令, 它可以对 SELECT 语句进行分析, 并输出 SELECT 执行的详细信息, 以供开发人员针对性优化.EXPLAIN 命令用法十分简单, 在 SELECT 语句前加上 Explain 就可以了, 例如: 1EXPLAIN SELECT * from user_info WHERE id &lt; 300; 准备为了接下来方便演示 EXPLAIN 的使用, 首先我们需要建立两个测试用的表, 并添加相应的数据: 1234567891011121314151617181920CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB DEFAULT CHARSET = utf8INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15); 12345678910111213141516171819CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`)) ENGINE = InnoDB DEFAULT CHARSET = utf8INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出格式EXPLAIN 命令的输出内容大致如下: 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 各列的含义如下: id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type: SELECT 查询的类型. table: 查询的是哪个表 partitions: 匹配的分区 type: join 类型 possible_keys: 此次查询中可能选用的索引 key: 此次查询中确切使用到的索引. ref: 哪个字段或常数与 key 一起被使用 rows: 显示此查询一共扫描了多少行. 这个是一个估计值. filtered: 表示此查询条件所过滤的数据的百分比 extra: 额外的信息 接下来我们来重点看一下比较重要的几个字段. select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 最常见的查询类别应该是 SIMPLE 了, 比如当我们的查询没有子查询, 也没有 UNION 查询时, 那么通常就是 SIMPLE 类型, 例如: 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 如果我们使用了 UNION 查询, 那么 EXPLAIN 输出 的结果类似如下: 1234567891011mysql&gt; EXPLAIN (SELECT * FROM user_info WHERE id IN (1, 2, 3)) -&gt; UNION -&gt; (SELECT * FROM user_info WHERE id IN (3, 4, 5));+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+| 1 | PRIMARY | user_info | NULL | range | PRIMARY | PRIMARY | 8 | NULL | 3 | 100.00 | Using where || 2 | UNION | user_info | NULL | range | PRIMARY | PRIMARY | 8 | NULL | 3 | 100.00 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+3 rows in set, 1 warning (0.00 sec) table表示查询涉及的表或衍生表 typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. type 常用类型type 常用的取值有: system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可.例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的. 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如: 12345678910111213141516171819202122232425262728mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 314 ref: NULL rows: 9 filtered: 100.00 Extra: Using where; Using index*************************** 2. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: test.order_info.user_id rows: 1 filtered: 100.00 Extra: NULL2 rows in set, 1 warning (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询.例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678910111213141516171819202122232425262728mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: refpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 9 ref: const rows: 1 filtered: 100.00 Extra: Using index2 rows in set, 1 warning (0.01 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. 例如下面的例子就是一个范围查询: 1234567891011121314151617mysql&gt; EXPLAIN SELECT * -&gt; FROM user_info -&gt; WHERE id BETWEEN 2 AND 8 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: rangepossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: NULL rows: 7 filtered: 100.00 Extra: Using where1 row in set, 1 warning (0.00 sec) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据.index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index. 例如: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: indexpossible_keys: NULL key: name_index key_len: 152 ref: NULL rows: 10 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免.下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的. 123456789101112131415mysql&gt; EXPLAIN SELECT age FROM user_info WHERE age = 20 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 10 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) type 类型的性能比较通常来说, 不同的 type 类型的性能关系如下:ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; systemALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的.而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快.后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key此字段是 MySQL 在当前查询时所真正使用到的索引. key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的栗子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id &lt; 3 AND product_name = &apos;p1&apos; AND productor = &apos;WHH&apos; \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: rangepossible_keys: user_product_detail_index key: user_product_detail_index key_len: 9 ref: NULL rows: 5 filtered: 11.11 Extra: Using where; Using index1 row in set, 1 warning (0.00 sec) 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 不过此查询语句 WHERE user_id &lt; 3 AND product_name = &#39;p1&#39; AND productor = &#39;WHH&#39; 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT &#39;0&#39;, 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = &apos;p1&apos; \G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: refpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 161 ref: const,const rows: 2 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = &#39;p1&#39; 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 rowsrows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY product_name \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: NULL key: user_product_detail_index key_len: 253 ref: NULL rows: 9 filtered: 100.00 Extra: Using index; Using filesort1 row in set, 1 warning (0.00 sec) 我们的索引是 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort. 如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: NULL key: user_product_detail_index key_len: 253 ref: NULL rows: 9 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) Using index“覆盖索引扫描“, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java动态代理]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-dynamic-proxy%2F</url>
    <content type="text"><![CDATA[以下文章来自：java动态代理实现与原理详细分析 关于Java中的动态代理，我们首先需要了解的是一种常用的设计模式–代理模式，而对于代理，根据创建代理类的时间点，又可以分为静态代理和动态代理。 一、代理模式 代理模式是常用的java设计模式，他的特征是代理类与委托类有同样的接口，代理类主要负责为委托类预处理消息、过滤消息、把消息转发给委托类，以及事后处理消息等。代理类与委托类之间通常会存在关联关系，一个代理类的对象与一个委托类的对象关联，代理类的对象本身并不真正实现服务，而是通过调用委托类的对象的相关方法，来提供特定的服务。简单的说就是，我们在访问实际对象时，是通过代理对象来访问的，代理模式就是在访问实际对象时引入一定程度的间接性，因为这种间接性，可以附加多种用途。在后面我会解释这种间接性带来的好处。代理模式结构图（图片来自《大话设计模式》）： 二、静态代理静态代理静态代理：由程序员创建或特定工具自动生成源代码，也就是在编译时就已经将接口，被代理类，代理类等确定下来。在程序运行之前，代理类的.class文件就已经生成。 静态代理简单实现 根据上面代理模式的类图，来写一个简单的静态代理的例子。我这儿举一个比较粗糙的例子，假如一个班的同学要向老师交班费，但是都是通过班长把自己的钱转交给老师。这里，班长就是代理学生上交班费， 班长就是学生的代理。 ​ 首先，我们创建一个Person接口。这个接口就是学生（被代理类），和班长（代理类）的公共接口，他们都有上交班费的行为。这样，学生上交班费就可以让班长来代理执行。 12345678/** * 创建Person接口 * @author Gonjan */public interface Person &#123; //上交班费 void giveMoney();&#125; Student类实现Person接口。Student可以具体实施上交班费的动作。 1234567891011public class Student implements Person &#123; private String name; public Student(String name) &#123; this.name = name; &#125; @Override public void giveMoney() &#123; System.out.println(name + &quot;上交班费50元&quot;); &#125;&#125; StudentsProxy类，这个类也实现了Person接口，但是还另外持有一个学生类对象，由于实现了Peson接口，同时持有一个学生对象，那么他可以代理学生类对象执行上交班费（执行giveMoney()方法）行为。 123456789101112131415161718192021/** * 学生代理类，也实现了Person接口，保存一个学生实体，这样既可以代理学生产生行为 * @author Gonjan * */public class StudentsProxy implements Person&#123; //被代理的学生 Student stu; public StudentsProxy(Person stu) &#123; // 只代理学生对象 if(stu.getClass() == Student.class) &#123; this.stu = (Student)stu; &#125; &#125; //代理上交班费，调用被代理学生的上交班费行为 public void giveMoney() &#123; stu.giveMoney(); &#125;&#125; 下面测试一下，看如何使用代理模式： 123456789101112public class StaticProxyTest &#123; public static void main(String[] args) &#123; //被代理的学生张三，他的班费上交有代理对象monitor（班长）完成 Person zhangsan = new Student(&quot;张三&quot;); //生成代理对象，并将张三传给代理对象 Person monitor = new StudentsProxy(zhangsan); //班长代理上交班费 monitor.giveMoney(); &#125;&#125; 运行结果： 这里并没有直接通过张三（被代理对象）来执行上交班费的行为，而是通过班长（代理对象）来代理执行了。这就是代理模式。 代理模式最主要的就是有一个公共接口（Person），一个具体的类（Student），一个代理类（StudentsProxy）,代理类持有具体类的实例，代为执行具体类实例方法。上面说到，代理模式就是在访问实际对象时引入一定程度的间接性，因为这种间接性，可以附加多种用途。这里的间接性就是指不直接调用实际对象的方法，那么我们在代理过程中就可以加上一些其他用途。就这个例子来说，加入班长在帮张三上交班费之前想要先反映一下张三最近学习有很大进步，通过代理模式很轻松就能办到： 1234567891011121314151617public class StudentsProxy implements Person&#123; //被代理的学生 Student stu; public StudentsProxy(Person stu) &#123; // 只代理学生对象 if(stu.getClass() == Student.class) &#123; this.stu = (Student)stu; &#125; &#125; //代理上交班费，调用被代理学生的上交班费行为 public void giveMoney() &#123; System.out.println(&quot;张三最近学习有进步！&quot;); stu.giveMoney(); &#125;&#125; 运行结果： 可以看到，只需要在代理类中帮张三上交班费之前，执行其他操作就可以了。这种操作，也是使用代理模式的一个很大的优点。最直白的就是在Spring中的面向切面编程（AOP），我们能在一个切点之前执行一些操作，在一个切点之后执行一些操作，这个切点就是一个个方法。这些方法所在类肯定就是被代理了，在代理过程中切入了一些其他操作。 三、动态代理动态代理代理类在程序运行时创建的代理方式被成为动态代理。 我们上面静态代理的例子中，代理类(studentProxy)是自己定义好的，在程序运行之前就已经编译完成。然而动态代理，代理类并不是在Java代码中定义的，而是在运行时根据我们在Java代码中的“指示”动态生成的。相比于静态代理， 动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类中的方法。 比如说，想要在每个代理的方法前都加上一个处理方法： 12345public void giveMoney() &#123; //调用被代理方法前加入处理方法 beforeMethod(); stu.giveMoney(); &#125; 这里只有一个giveMoney方法，就写一次beforeMethod方法，但是如果出了giveMonney还有很多其他的方法，那就需要写很多次beforeMethod方法，麻烦。那看看下面动态代理如何实现。 动态代理简单实现在java的java.lang.reflect包下提供了一个Proxy类和一个InvocationHandler接口，通过这个类和这个接口可以生成JDK动态代理类和动态代理对象。 创建一个动态代理对象步骤，具体代码见后面： 创建一个InvocationHandler对象 12//创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new MyInvocationHandler&lt;Person&gt;(stu); 使用Proxy类的getProxyClass静态方法生成一个动态代理类stuProxyClass 1Class&lt;?&gt; stuProxyClass = Proxy.getProxyClass(Person.class.getClassLoader(), new Class&lt;?&gt;[] &#123;Person.class&#125;); 获得stuProxyClass 中一个带InvocationHandler参数的构造器constructor 1Constructor&lt;?&gt; constructor = PersonProxy.getConstructor(InvocationHandler.class); 通过构造器constructor来创建一个动态实例stuProxy 1Person stuProxy = (Person) cons.newInstance(stuHandler); 就此，一个动态代理对象就创建完毕，当然，上面四个步骤可以通过Proxy类的newProxyInstances方法来简化： 1234 //创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new MyInvocationHandler&lt;Person&gt;(stu);//创建一个代理对象stuProxy，代理对象的每个执行方法都会替换执行Invocation中的invoke方法 Person stuProxy= (Person) Proxy.newProxyInstance(Person.class.getClassLoader(), new Class&lt;?&gt;[]&#123;Person.class&#125;, stuHandler); 到这里肯定都会很疑惑，这动态代理到底是如何执行的，是如何通过代理对象来执行被代理对象的方法的，先不急，我们先看看一个简单的完整的动态代理的例子。还是上面静态代理的例子，班长需要帮学生代交班费。首先是定义一个Person接口: 12345678/** * 创建Person接口 * @author Gonjan */public interface Person &#123; //上交班费 void giveMoney();&#125; 创建需要被代理的实际类： 1234567891011121314151617public class Student implements Person &#123; private String name; public Student(String name) &#123; this.name = name; &#125; @Override public void giveMoney() &#123; try &#123; //假设数钱花了一秒时间 Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(name + &quot;上交班费50元&quot;); &#125;&#125; 再定义一个检测方法执行时间的工具类，在任何方法执行前先调用start方法，执行后调用finsh方法，就可以计算出该方法的运行时间，这也是一个最简单的方法执行时间检测工具。 1234567891011121314public class MonitorUtil &#123; private static ThreadLocal&lt;Long&gt; tl = new ThreadLocal&lt;&gt;(); public static void start() &#123; tl.set(System.currentTimeMillis()); &#125; //结束时打印耗时 public static void finish(String methodName) &#123; long finishTime = System.currentTimeMillis(); System.out.println(methodName + &quot;方法耗时&quot; + (finishTime - tl.get()) + &quot;ms&quot;); &#125;&#125; 创建StuInvocationHandler类，实现InvocationHandler接口，这个类中持有一个被代理对象的实例target。InvocationHandler中有一个invoke方法，所有执行代理对象的方法都会被替换成执行invoke方法。 再再invoke方法中执行被代理对象target的相应方法。当然，在代理过程中，我们在真正执行被代理对象的方法前加入自己其他处理。这也是Spring中的AOP实现的主要原理，这里还涉及到一个很重要的关于java反射方面的基础知识。 123456789101112131415161718192021222324public class StuInvocationHandler&lt;T&gt; implements InvocationHandler &#123; //invocationHandler持有的被代理对象 T target; public StuInvocationHandler(T target) &#123; this.target = target; &#125; /** * proxy:代表动态代理对象 * method：代表正在执行的方法 * args：代表调用目标方法时传入的实参 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;代理执行&quot; +method.getName() + &quot;方法&quot;); */ //代理过程中插入监测方法,计算该方法耗时 MonitorUtil.start(); Object result = method.invoke(target, args); MonitorUtil.finish(method.getName()); return result; &#125;&#125; 做完上面的工作后，我们就可以具体来创建动态代理对象了，上面简单介绍了如何创建动态代理对象，我们使用简化的方式创建动态代理对象： 12345678910111213141516public class ProxyTest &#123; public static void main(String[] args) &#123; //创建一个实例对象，这个对象是被代理的对象 Person zhangsan = new Student(&quot;张三&quot;); //创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new StuInvocationHandler&lt;Person&gt;(zhangsan); //创建一个代理对象stuProxy来代理zhangsan，代理对象的每个执行方法都会替换执行Invocation中的invoke方法 Person stuProxy = (Person) Proxy.newProxyInstance(Person.class.getClassLoader(), new Class&lt;?&gt;[]&#123;Person.class&#125;, stuHandler)； //代理执行上交班费的方法 stuProxy.giveMoney(); &#125;&#125; 我们执行这个ProxyTest类，先想一下，我们创建了一个需要被代理的学生张三，将zhangsan对象传给了stuHandler中，我们在创建代理对象stuProxy时，将stuHandler作为参数了的，上面也有说到所有执行代理对象的方法都会被替换成执行invoke方法，也就是说，最后执行的是StuInvocationHandler中的invoke方法。所以在看到下面的运行结果也就理所当然了。 运行结果： 上面说到，动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类中的方法。是因为所有被代理执行的方法，都是通过在InvocationHandler中的invoke方法调用的，所以我们只要在invoke方法中统一处理，就可以对所有被代理的方法进行相同的操作了。例如，这里的方法计时，所有的被代理对象执行的方法都会被计时，然而我只做了很少的代码量。 动态代理的过程，代理对象和被代理对象的关系不像静态代理那样一目了然，清晰明了。因为动态代理的过程中，我们并没有实际看到代理类，也没有很清晰地的看到代理类的具体样子，而且动态代理中被代理对象和代理对象是通过InvocationHandler来完成的代理过程的，其中具体是怎样操作的，为什么代理对象执行的方法都会通过InvocationHandler中的invoke方法来执行。带着这些问题，我们就需要对java动态代理的源码进行简要的分析，弄清楚其中缘由。 四、动态代理原理分析​ 1、Java动态代理创建出来的动态代理类 上面我们利用Proxy类的newProxyInstance方法创建了一个动态代理对象，查看该方法的源码，发现它只是封装了创建动态代理类的步骤(红色标准部分)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException&#123; Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); //红色部分 final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * Look up or generate the designated proxy class. */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams);//红色部分 final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; return cons.newInstance(new Object[]&#123;h&#125;); //红色部分 &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125;&#125; 其实，我们最应该关注的是Class&lt;?&gt; cl = getProxyClass0(loader, intfs);这句，这里产生了代理类，后面代码中的构造器也是通过这里产生的类来获得，可以看出，这个类的产生就是整个动态代理的关键，由于是动态生成的类文件，我这里不具体进入分析如何产生的这个类文件，只需要知道这个类文件时缓存在java虚拟机中的，我们可以通过下面的方法将其打印到文件里面，一睹真容： 123456789byte[] classFile = ProxyGenerator.generateProxyClass(&quot;$Proxy0&quot;, Student.class.getInterfaces());String path = &quot;G:/javacode/javase/Test/bin/proxy/StuProxy.class&quot;;try(FileOutputStream fos = new FileOutputStream(path)) &#123; fos.write(classFile); fos.flush(); System.out.println(&quot;代理类class文件写入成功&quot;);&#125; catch (Exception e) &#123; System.out.println(&quot;写文件错误&quot;);&#125; 对这个class文件进行反编译，我们看看jdk为我们生成了什么样的内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;import proxy.Person;public final class $Proxy0 extends Proxy implements Person&#123; private static Method m1; private static Method m2; private static Method m3; private static Method m0; /** *注意这里是生成代理类的构造方法，方法参数为InvocationHandler类型，看到这，是不是就有点明白 *为何代理对象调用方法都是执行InvocationHandler中的invoke方法，而InvocationHandler又持有一个 *被代理对象的实例，不禁会想难道是....？ 没错，就是你想的那样。 * *super(paramInvocationHandler)，是调用父类Proxy的构造方法。 *父类持有：protected InvocationHandler h; *Proxy构造方法： * protected Proxy(InvocationHandler h) &#123; * Objects.requireNonNull(h); * this.h = h; * &#125; * */ public $Proxy0(InvocationHandler paramInvocationHandler) throws &#123; super(paramInvocationHandler); &#125; //这个静态块本来是在最后的，我把它拿到前面来，方便描述 static &#123; try &#123; //看看这儿静态块儿里面有什么，是不是找到了giveMoney方法。请记住giveMoney通过反射得到的名字m3，其他的先不管 m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, new Class[] &#123; Class.forName(&quot;java.lang.Object&quot;) &#125;); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;, new Class[0]); m3 = Class.forName(&quot;proxy.Person&quot;).getMethod(&quot;giveMoney&quot;, new Class[0]); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;, new Class[0]); return; &#125; catch (NoSuchMethodException localNoSuchMethodException) &#123; throw new NoSuchMethodError(localNoSuchMethodException.getMessage()); &#125; catch (ClassNotFoundException localClassNotFoundException) &#123; throw new NoClassDefFoundError(localClassNotFoundException.getMessage()); &#125; &#125; /** * *这里调用代理对象的giveMoney方法，直接就调用了InvocationHandler中的invoke方法，并把m3传了进去。 *this.h.invoke(this, m3, null);这里简单，明了。 *来，再想想，代理对象持有一个InvocationHandler对象，InvocationHandler对象持有一个被代理的对象， *再联系到InvacationHandler中的invoke方法。嗯，就是这样。 */ public final void giveMoney() throws &#123; try &#123; this.h.invoke(this, m3, null); return; &#125; catch (Error|RuntimeException localError) &#123; throw localError; &#125; catch (Throwable localThrowable) &#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; //注意，这里为了节省篇幅，省去了toString，hashCode、equals方法的内容。原理和giveMoney方法一毛一样。&#125; jdk为我们的生成了一个叫$Proxy0（这个名字后面的0是编号，有多个代理类会一次递增）的代理类，这个类文件是放在内存中的，我们在创建代理对象时，就是通过反射获得这个类的构造方法，然后创建的代理实例。通过对这个生成的代理类源码的查看，我们很容易能看出，动态代理实现的具体过程。 我们可以把InvocationHandler看做一个中介类，中介类持有一个被代理对象，在invoke方法中调用了被代理对象的相应方法。通过聚合方式持有被代理对象的引用，把外部对invoke的调用最终都转为对被代理对象的调用。 代理类调用自己方法时，通过自身持有的中介类对象来调用中介类对象的invoke方法，从而达到代理执行被代理对象的方法。也就是说，动态代理通过中介类实现了具体的代理功能。 五、总结生成的代理类：$Proxy0 extends Proxy implements Person，我们看到代理类继承了Proxy类，所以也就决定了java动态代理只能对接口进行代理，Java的继承机制注定了这些动态代理类们无法实现对class的动态代理。 上面的动态代理的例子，其实就是AOP的一个简单实现了，在目标对象的方法执行之前和执行之后进行了处理，对方法耗时统计。Spring的AOP实现其实也是用了Proxy和InvocationHandler这两个东西的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql主从复制和读写分离]]></title>
    <url>%2F2019%2F06%2F18%2Fmysql-master-slave-replication%2F</url>
    <content type="text"><![CDATA[以下文章参考自：：MySql 主从复制及配置实现 Mysql 主从复制的原理和kafka的partition的replication机制很类似，原理互通，大概是因为这种做法确实可以保证分布式系统的可靠性。 一、什么是Mysql主从复制MySQL主从复制是其最重要的功能之一。主从复制是指一台服务器充当主数据库服务器，另一台或多台服务器充当从数据库服务器，主服务器中的数据自动复制到从服务器之中。对于多级复制，数据库服务器即可充当主机，也可充当从机。MySQL主从复制的基础是主服务器对数据库修改记录二进制日志(bin log)，从服务器通过主服务器的二进制日志自动执行更新。 二、Mysq主从复制的类型基于语句的复制：主服务器上面执行的语句在从服务器上面再执行一遍，在MySQL-3.23版本以后支持。 存在的问题：时间上可能不完全同步造成偏差，执行语句的用户也可能不是同一个用户。 基于行的复制：把主服务器上面改变后的内容直接复制过去，而不关心到底改变该内容是由哪条语句引发的，在MySQL-5.0版本以后引入。 存在的问题：比如一个工资表中有一万个用户，我们把每个用户的工资+1000，那么基于行的复制则要复制一万行的内容，由此造成的开销比较大，而基于语句的复制仅仅一条语句就可以了。 混合类型的复制：MySQL默认使用基于语句的复制，当基于语句的复制会引发问题的时候就会使用基于行的复制，MySQL会自动进行选择。 在MySQL主从复制架构中，读操作可以在所有的服务器上面进行，而写操作只能在主服务器上面进行。主从复制架构虽然给读操作提供了扩展，可如果写操作也比较多的话（多台从服务器还要从主服务器上面同步数据），单主模型的复制中主服务器势必会成为性能瓶颈。 三、主从复制的作用1、主数据库出现问题，可以切换到从数据库。2、可以进行数据库层面的读写分离。读写分离就是在主服务器上修改，数据会同步到从服务器，从服务器只能提供读取数据，不能写入，实现备份的同时也实现了数据库性能的优化，以及提升了服务器安全。 1234567891011121314151）基于程序代码内部实现在代码中根据select 、insert进行路由分类，这类方法也是目前生产环境下应用最广泛的。优点是性能较好，因为程序在代码中实现，不需要增加额外的硬件开支，缺点是需要开发人员来实现，运维人员无从下手。2）基于中间代理层实现代理一般介于应用服务器和数据库服务器之间，代理数据库服务器接收到应用服务器的请求后根据判断后转发到，后端数据库，有以下代表性的程序。（1）MySQL_proxy。MySQL_proxy是MySQL的一个开源项目，通过其自带的lua脚本进行sql判断。（2）Atlas。是由 Qihoo 360, Web平台部基础架构团队开发维护的一个基于MySQL协议的数据中间层项目。它是在MySQL-proxy 0.8.2版本的基础上，对其进行了优化，增加了一些新的功能特性。360内部使用Atlas运行的MySQL业务，每天承载的读写请求数达几十亿条。支持事物以及存储过程。（3）Amoeba。由阿里巴巴集团在职员工陈思儒使用java语言进行开发，阿里巴巴集团将其用户生产环境下，但是它并不支持事物以及存储过程。不是所有的应用都能够在基于程序代码中实现读写分离，像一些大型的java应用，如果在程序代码中实现读写分离对代码的改动就较大，所以，像这种应用一般会考虑使用代理层来实现。 3、可以在从数据库上进行日常备份 四、Mysql主从复制的工作原理如下图所示： [ 主服务器上面的任何修改都会保存在二进制日志Binary log里面，从服务器上面启动一个I/O thread（实际上就是一个主服务器的客户端进程），连接到主服务器上面请求读取二进制日志，然后把读取到的二进制日志写到本地的一个Realy log（中继日志）里面。从服务器上面开启一个SQL thread定时检查Realy log，如果发现有更改立即把更改的内容在本机上面执行一遍。 如果一主多从的话，这时主库既要负责写又要负责为几个从库提供二进制日志。此时可以稍做调整，将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从。或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。工作原理图如下： [ 实际上在老版本的MySQL主从复制中Slave端并不是两个进程完成的，而是由一个进程完成。但是后来发现这样做存在较大的风险和性能问题，主要如下： 123首先，一个进程会使复制bin-log日志和解析日志并在自身执行的过程成为一个串行的过程，性能受到了一定的限制，异步复制的延迟也会比较长。另外，Slave端从Master端获取bin-log过来之后，需要接着解析日志内容，然后在自身执行。在这个过程中，Master端可能又产生了大量变化并新增了大量的日志。如果在这个阶段Master端的存储出现了无法修复的错误，那么在这个阶段所产生的所有变更都将永远无法找回。如果在Slave端的压力比较大的时候，这个过程的时间可能会比较长。 为了提高复制的性能并解决存在的风险，后面版本的MySQL将Slave端的复制动作交由两个进程来完成。提出这个改进方案的人是Yahoo!的一位工程师“Jeremy Zawodny”。这样既解决了性能问题，又缩短了异步的延时时间，同时也减少了可能存在的数据丢失量。 当然，即使是换成了现在这样两个线程处理以后，同样也还是存在slave数据延时以及数据丢失的可能性的，毕竟这个复制是异步的。只要数据的更改不是在一个事务中，这些问题都是会存在的。如果要完全避免这些问题，就只能用MySQL的cluster来解决了。不过MySQL的cluster是内存数据库的解决方案，需要将所有数据都load到内存中，这样就对内存的要求就非常大了，对于一般的应用来说可实施性不是太大。 还有一点要提的是MySQL的复制过滤(Replication Filters)，复制过滤可以让你只复制服务器中的一部分数据。有两种复制过滤：在Master上过滤二进制日志中的事件；在Slave上过滤中继日志中的事件。如下： [ 配置Master的my.cnf文件(关键性的配置)/etc/my.cnf 1234567891011log-bin=mysql-binserver-id = 1binlog-do-db=icingabinlog-do-db=DB2 //如果备份多个数据库，重复设置这个选项即可binlog-do-db=DB3 //需要同步的数据库，如果没有本行，即表示同步所有的数据库binlog-ignore-db=mysql //被忽略的数据库 配置Slave的my.cnf文件(关键性的配置)/etc/my.cnf 12345678910111213141516171819log-bin=mysql-binserver-id=2master-host=10.1.68.110master-user=backupmaster-password=1234qwermaster-port=3306replicate-do-db=icingareplicate-do-db=DB2replicate-do-db=DB3 //需要同步的数据库，如果没有本行，即表示同步所有的数据库replicate-ignore-db=mysql //被忽略的数据库 网友说replicate-do-db的使用中可能会出些问题（http://blog.knowsky.com/19696…），自己没有亲自去测试。猜想binlog-do-db参数用于主服务器中，通过过滤Binary Log来过滤掉配置文件中不允许复制的数据库，也就是不向Binary Log中写入不允许复制数据的操作日志；而replicate-do-db用于从服务器中，通过过滤Relay Log来过滤掉不允许复制的数据库或表，也就是执行Relay Log中的动作时不执行那些不被允许的修改动作。这样的话，多个从数据库服务器的情况：有的从服务器既从主服务器中复制数据，又做为主服务器向另外的从服务器复制数据，那它的配置文件中应该可以同时存在binlog-do-db、replicate-do-db这两个参数才对。一切都是自己的预测，关于binlog-do-db、replicate-do-db的具体使用方法还得在实际开发中一点点摸索才可以。 网上有说，复制时忽略某些数据库或者表的操作最好不要在主服务器上面进行，因为主服务器忽略之后就不会再往二进制文件中写了，但是在从服务器上面虽然忽略了某些数据库但是主服务器上面的这些操作信息依然会被复制到从服务器上面的relay log里面，只是不会在从服务器上面执行而已。我想这个意思应该是建议在从服务器中设置replicate-do-db，而不要在主服务器上设置binlog-do-db。 另外，不管是黑名单（binlog-ignore-db、replicate-ignore-db）还是白名单（binlog-do-db、replicate-do-db）只写一个就行了，如果同时使用那么只有白名单生效。 五、Mysql主从复制的过程MySQL主从复制的两种情况：同步复制和异步复制，实际复制架构中大部分为异步复制。 复制的基本过程如下： Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容。 Master接收到来自Slave的IO进程的请求后，负责复制的IO进程会根据请求信息读取日志指定位置之后的日志信息，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置。 Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的 bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”。 Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行。 六、Mysql主从复制的具体配置复制通常用来创建主节点的副本，通过添加冗余节点来保证高可用性，当然复制也可以用于其他用途，例如在从节点上进行数据读、分析等等。在横向扩展的业务中，复制很容易实施，主要表现在在利用主节点进行写操作，多个从节点进行读操作，MySQL复制的异步性是指：事物首先在主节点上提交，然后复制给从节点并在从节点上应用，这样意味着在同一个时间点主从上的数据可能不一致。异步复制的好处在于它比同步复制要快，如果对数据的一致性要求很高，还是采用同步复制较好。 最简单的复制模式就是一主一从的复制模式了，这样一个简单的架构只需要三个步骤即可完成： （1）建立一个主节点，开启binlog，设置服务器id； （2）建立一个从节点，设置服务器id； （3）将从节点连接到主节点上。 下面我们开始操作，以MySQL 5.5为例，操作系统Ubuntu12.10，Master 10.1.6.159 Slave 10.1.6.191。 1apt-get install mysql-server Master机器Master上面开启binlog日志，并且设置一个唯一的服务器id，在局域网内这个id必须唯一。二进制的binlog日志记录master上的所有数据库改变，这个日志会被复制到从节点上，并且在从节点上回放。修改my.cnf文件，在mysqld模块下修改如下内容： 123[mysqld]server-id = 1log_bin = /var/log/mysql/mysql-bin.log log_bin设置二进制日志所产生文件的基本名称，二进制日志由一系列文件组成，log_bin的值是可选项，如果没有为log_bin设置值，则默认值是：主机名-bin。如果随便修改主机名，则binlog日志的名称也会被改变的。server-id是用来唯一标识一个服务器的，每个服务器的server-id都不一样。这样slave连接到master后，会请求master将所有的binlog传递给它，然后将这些binlog在slave上回放。为了防止权限混乱，一般都是建立一个单独用于复制的账户。 binlog是复制过程的关键，它记录了数据库的所有改变，通常即将执行完毕的语句会在binlog日志的末尾写入一条记录，binlog只记录改变数据库的语句，对于不改变数据库的语句则不进行记录。这种情况叫做基于语句的复制，前面提到过还有一种情况是基于行的复制，两种模式各有各的优缺点。 Slave机器slave机器和master一样，需要一个唯一的server-id。 12[mysqld]server-id = 2 连接Slave到Master 在Master和Slave都配置好后，只需要把slave只想master即可 123change master to master_host=&apos;10.1.6.159&apos;,master_port=3306,master_user=&apos;rep&apos;,master_password=&apos;123456&apos;;start slave; 接下来在master上做一些针对改变数据库的操作，来观察slave的变化情况。在修改完my.cnf配置重启数据库后，就开始记录binlog了。可以在/var/log/mysql目录下看到一个mysql-bin.000001文件，而且还有一个mysql-bin.index文件，这个mysql-bin.index文件是什么？这个文件保存了所有的binlog文件列表，但是我们在配置文件中并没有设置改值，这个可以通过log_bin_index进行设置，如果没有设置改值，则默认值和log_bin一样。在master上执行show binlog events命令，可以看到第一个binlog文件的内容。 注意：上面的sql语句是从头开始复制第一个binlog，如果想从某个位置开始复制binlog，就需要在change master to时指定要开始的binlog文件名和语句在文件中的起点位置，参数如下：master_log_file和master_log_pos。 1234567891011121314151617181920212223mysql&gt; show binlog events\G*************************** 1. row *************************** Log_name: mysql-bin.000001 Pos: 4 Event_type: Format_desc Server_id: 1End_log_pos: 107 Info: Server ver: 5.5.28-0ubuntu0.12.10.2-log, Binlog ver: 4*************************** 2. row *************************** Log_name: mysql-bin.000001 Pos: 107 Event_type: Query Server_id: 1End_log_pos: 181 Info: create user rep*************************** 3. row *************************** Log_name: mysql-bin.000001 Pos: 181 Event_type: Query Server_id: 1End_log_pos: 316 Info: grant replication slave on *.* to rep identified by &apos;123456&apos;3 rows in set (0.00 sec) Log_name 是二进制日志文件的名称，一个事件不能横跨两个文件 Pos 这是该事件在文件中的开始位置 Event_type 事件的类型，事件类型是给slave传递信息的基本方法，每个新的binlog都已Format_desc类型开始，以Rotate类型结束 Server_id 创建该事件的服务器id End_log_pos 该事件的结束位置，也是下一个事件的开始位置，因此事件范围为Pos~End_log_pos-1 Info 事件信息的可读文本，不同的事件有不同的信息 示例 在master的test库中创建一个rep表，并插入一条记录。 123create table rep(name var);insert into rep values (&quot;guol&quot;);flush logs; flush logs命令强制轮转日志，生成一个新的二进制日志，可以通过show binlog events in ‘xxx’来查看该二进制日志。可以通过show master status查看当前正在写入的binlog文件。这样就会在slave上执行相应的改变操作。 上面就是最简单的主从复制模式，不过有时候随着时间的推进，binlog会变得非常庞大，如果新增加一台slave，从头开始复制master的binlog文件是非常耗时的，所以我们可以从一个指定的位置开始复制binlog日志，可以通过其他方法把以前的binlog文件进行快速复制，例如copy物理文件。在change master to中有两个参数可以实现该功能，master_log_file和master_log_pos，通过这两个参数指定binlog文件及其位置。我们可以从master上复制也可以从slave上复制，假如我们是从master上复制，具体操作过程如下： （1）为了防止在操作过程中数据更新，导致数据不一致，所以需要先刷新数据并锁定数据库：flush tables with read lock。 （2）检查当前的binlog文件及其位置：show master status。 1234567mysql&gt; show master status\G*************************** 1. row ***************************File: mysql-bin.000003Position: 107Binlog_Do_DB:Binlog_Ignore_DB:1 row in set (0.00 sec) （3）通过mysqldump命令创建数据库的逻辑备分：mysqldump –all-databases -hlocalhost -p &gt;back.sql。 （4）有了master的逻辑备份后，对数据库进行解锁：unlock tables。 （5）把back.sql复制到新的slave上，执行：mysql -hlocalhost -p 把master的逻辑备份插入slave的数据库中。 （6）现在可以把新的slave连接到master上了，只需要在change master to中多设置两个参数master_log_file=’mysql-bin.000003’和master_log_pos=’107’即可，然后启动slave：start slave，这样slave就可以接着107的位置进行复制了。 123change master to master_host=&apos;10.1.6.159&apos;,master_port=3306,master_user=&apos;rep&apos;,master_password=&apos;123456&apos;,master_log_file=&apos;mysql-bin.000003&apos;,master_log_pos=&apos;107&apos;;start slave; 有时候master并不能让你锁住表进行复制，因为可能跑一些不间断的服务，如果这时master已经有了一个slave，我们则可以通过这个slave进行再次扩展一个新的slave。原理同在master上进行复制差不多，关键在于找到binlog的位置，你在复制的同时可能该slave也在和master进行同步，操作如下： （1）为了防止数据变动，还是需要停止slave的同步：stop slave。 （2）然后刷新表，并用mysqldump逻辑备份数据库。 （3）使用show slave status查看slave的相关信息，记录下两个字段的值Relay_Master_Log_File和Exec_Master_Log_Pos，这个用来确定从后面哪里开始复制。 （4）对slave解锁，把备份的逻辑数据库导入新的slave的数据库中，然后设置change master to，这一步和复制master一样。 七、深入了解Mysql主从配置一主多从由一个master和一个slave组成复制系统是最简单的情况。Slave之间并不相互通信，只能与master进行通信。在实际应用场景中，MySQL复制90%以上都是一个Master复制到一个或者多个Slave的架构模式，主要用于读压力比较大的应用的数据库端廉价扩展解决方案。 [ 在上图中，是我们开始时提到的一主多从的情况，这时主库既要负责写又要负责为几个从库提供二进制日志。这种情况将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从，或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。 主主复制[ 上图中，Master-Master复制的两台服务器，既是master，又是另一台服务器的slave。这样，任何一方所做的变更，都会通过复制应用到另外一方的数据库中。在这种复制架构中，各自上运行的不是同一db，比如左边的是db1,右边的是db2，db1的从在右边反之db2的从在左边，两者互为主从，再辅助一些监控的服务还可以实现一定程度上的高可以用。 主动—被动模式的Master-Master(Master-Master in Active-Passive Mode)[ 上图中，这是由master-master结构变化而来的，它避免了M-M的缺点，实际上，这是一种具有容错和高可用性的系统。它的不同点在于其中只有一个节点在提供读写服务，另外一个节点时刻准备着，当主节点一旦故障马上接替服务。比如通过corosync+pacemaker+drbd+MySQL就可以提供这样一组高可用服务，主备模式下再跟着slave服务器，也可以实现读写分离。 带从服务器的Master-Master结构(Master-Master with Slaves)[ 这种结构的优点就是提供了冗余。在地理上分布的复制结构，它不存在单一节点故障问题，而且还可以将读密集型的请求放到slave上。 MySQL-5.5支持半同步复制 早前的MySQL复制只能是基于异步来实现，从MySQL-5.5开始，支持半自动复制。在以前的异步（asynchronous）复制中，主库在执行完一些事务后，是不会管备库的进度的。如果备库处于落后，而更不幸的是主库此时又出现Crash（例如宕机），这时备库中的数据就是不完整的。简而言之，在主库发生故障的时候，我们无法使用备库来继续提供数据一致的服务了。Semisynchronous Replication(半同步复制)则一定程度上保证提交的事务已经传给了至少一个备库。Semi synchronous中，仅仅保证事务的已经传递到备库上，但是并不确保已经在备库上执行完成了。 此外，还有一种情况会导致主备数据不一致。在某个session中，主库上提交一个事务后，会等待事务传递给至少一个备库，如果在这个等待过程中主库Crash，那么也可能备库和主库不一致，这是很致命的。如果主备网络故障或者备库挂了，主库在事务提交后等待10秒（rpl_semi_sync_master_timeout的默认值）后，就会继续。这时，主库就会变回原来的异步状态。 MySQL在加载并开启Semi-sync插件后，每一个事务需等待备库接收日志后才返回给客户端。如果做的是小事务，两台主机的延迟又较小，则Semi-sync可以实现在性能很小损失的情况下的零数据丢失。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中clone常见的三种方式]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-clone-threeMethod%2F</url>
    <content type="text"><![CDATA[在 JAVA 中克隆一个对象常见的有三种形式 : 通过自己写一个克隆方法，里面 new 一个同样的对象来进行 get、set 依次赋值实现深度克隆（很繁琐且易出错）； 通过实现 Cloneable 接口并重写 Object 类的 clone() 方法（分为深浅两种方式）； 通过实现 Serializable 接口并用对象的序列化和反序列化来实现真正的深度克隆； 下面介绍第二、第三种方法。 Cloneable 接口实现克隆Cloneable 接口实现浅克隆12345678910111213141516171819202122232425262728public class People implements Cloneable &#123; private String name = &quot;ilt&quot;; private Hand hand = new Hand(); public Hand getHand() &#123; return hand; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub return super.clone(); &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; People p1 = new People(); People p2 = (People) p1.clone(); System.out.println(&quot;第一个对象的hash值:&quot;+p1.hashCode()); System.out.println(&quot;第二个对象的hash值:&quot;+p2.hashCode()); System.out.println(&quot;分割线-----------&quot;); System.out.println(&quot;p1中的hand对象的hash值:&quot;+p1.getHand().hashCode()); System.out.println(&quot;p2中的hand对象的hash值:&quot;+p2.getHand().hashCode()); &#125;&#125;class Hand implements Cloneable &#123;&#125; 上面代码输出的结果如下，根据hash值相等能确定两个对象是否相等的原则，发现p1和p2不等，但p1中的hand对象与p2中的hand对象是相等的。Cloneable 接口实现克隆是先在内存中开辟一块和原始对象一样的空间，然后原样拷贝原始对象中的内容，对基本数据类型就是值复制，而对非基本类型变量保存的仅仅是对象的引用，所以会导致 clone 后的非基本类型变量和原始对象中相应的变量指向的是同一个对象。 12345第一个对象的hash值:1408448235第二个对象的hash值:77244764分割线-----------p1中的hand对象的hash值:1172625760p2中的hand对象的hash值:1172625760 Cloneable 接口实现深克隆在浅度克隆的基础上对于要克隆对象中的非基本数据类型的属性对应的类也实现克隆，这样对于非基本数据类型的属性复制的不是一份引用。 123456789101112131415161718192021222324252627282930313233343536373839public class People implements Cloneable &#123; private String name = &quot;ilt&quot;; private Hand hand = new Hand(); public Hand getHand() &#123; return hand; &#125; public void setHand(Hand hand) &#123; this.hand = hand; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub People p2 = (People) super.clone(); p2.setHand((Hand) hand.clone()); return p2; &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; People p1 = new People(); People p2 = (People) p1.clone(); System.out.println(&quot;第一个对象的hash值:&quot; + p1.hashCode()); System.out.println(&quot;第二个对象的hash值:&quot; + p2.hashCode()); System.out.println(&quot;分割线-----------&quot;); System.out.println(&quot;p1中的hand对象的hash值:&quot; + p1.getHand().hashCode()); System.out.println(&quot;p2中的hand对象的hash值:&quot; + p2.getHand().hashCode()); &#125;&#125;class Hand implements Cloneable &#123; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub return super.clone(); &#125;&#125; 结果如下，证明已经进行深克隆 12345第一个对象的hash值:1172625760第二个对象的hash值:863719801分割线-----------p1中的hand对象的hash值:1696725334p2中的hand对象的hash值:427340025 序列化与反序列化实现深克隆对象序列化操作可以将对象的状态转换成字节流传输或者存储再生，我们可以借用这一特点实现对象的深度克隆，特别是当我们的对象嵌套非常复杂且想实现深度克隆时如果使用序列化方式会大大减少代码量。 1234567891011121314151617181920212223242526272829public class TestClone implements Serializable&#123; private static final long serialVersionUID = 1L; public String name = &quot;ilt&quot;; public static void main(String[] args) throws Exception &#123; TestClone t1 = new TestClone(); byte[] b = ObjectUtil.objectToBytes(t1);//序列化 TestClone t2 = (TestClone) ObjectUtil.bytesToObject(b);//反序列化 System.out.println(&quot;t1对象的name：&quot;+t1.name); System.out.println(&quot;t2对象的name：&quot;+t2.name); System.out.println(&quot;分割线-------------&quot;); System.out.println(&quot;t1对象的hash值为：&quot;+t1.hashCode()); System.out.println(&quot;t2对象的hash值为：&quot;+t2.hashCode()); System.out.println(&quot;分割线-------------&quot;); System.out.println(&quot;t1中的obj对象的hash值为：&quot;+t1.obj.hashCode()); System.out.println(&quot;t2中的obj对象的hash值为：&quot;+t2.obj.hashCode()); &#125; class Bean implements Serializable&#123; private static final long serialVersionUID = 1L; &#125;&#125; 结果如下，证明对象的属性被深克隆下来了 12345678t1对象的name：iltt2对象的name：ilt分割线-------------t1对象的hash值为：1847546936t2对象的hash值为：812610706分割线-------------t1中的obj对象的hash值为：1164730192t2中的obj对象的hash值为：1699624469 作者：youngerTree来源：CSDN原文：https://blog.csdn.net/syilt/article/details/78482927版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>clone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中volatile关键字的作用]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-volatile%2F</url>
    <content type="text"><![CDATA[以下文章来源于：Java并发：volatile内存可见性和指令重排 volatile两大作用1、保证内存可见性 2、防止指令重排 此外需注意volatile并不保证操作的原子性。 （一）内存可见性1 概念JVM内存模型：主内存和线程独立的工作内存 Java内存模型规定，对于多个线程共享的变量，存储在主内存当中，每个线程都有自己独立的工作内存（比如CPU的寄存器），线程只能访问自己的工作内存，不可以访问其它线程的工作内存。 工作内存中保存了主内存共享变量的副本，线程要操作这些共享变量，只能通过操作工作内存中的副本来实现，操作完毕之后再同步回到主内存当中。 如何保证多个线程操作主内存的数据完整性是一个难题，Java内存模型也规定了工作内存与主内存之间交互的协议，定义了8种原子操作： 123456789101112131415(1) lock:将主内存中的变量锁定，为一个线程所独占(2) unclock:将lock加的锁定解除，此时其它的线程可以有机会访问此变量(3) read:将主内存中的变量值读到工作内存当中(4) load:将read读取的值保存到工作内存中的变量副本中。(5) use:将值传递给线程的代码执行引擎(6) assign:将执行引擎处理返回的值重新赋值给变量副本(7) store:将变量副本的值存储到主内存中。(8) write:将store存储的值写入到主内存的共享变量当中。 通过上面Java内存模型的概述，我们会注意到这么一个问题，每个线程在获取锁之后会在自己的工作内存来操作共享变量，操作完成之后将工作内存中的副本回写到主内存，并且在其它线程从主内存将变量同步回自己的工作内存之前，共享变量的改变对其是不可见的。即其他线程的本地内存中的变量已经是过时的，并不是更新后的值。 2 内存可见性带来的问题很多时候我们需要一个线程对共享变量的改动，其它线程也需要立即得知这个改动该怎么办呢？下面举两个例子说明内存可见性的重要性： 例子1有一个全局的状态变量open: 1`boolean` `open=``true``;` 这个变量用来描述对一个资源的打开关闭状态，true表示打开，false表示关闭，假设有一个线程A,在执行一些操作后将open修改为false: 123//线程Aresource.close();open = false; 线程B随时关注open的状态，当open为true的时候通过访问资源来进行一些操作: 1234//线程Bwhile(open) &#123;doSomethingWithResource(resource);&#125; 当A把资源关闭的时候，open变量对线程B是不可见的，如果此时open变量的改动尚未同步到线程B的工作内存中,那么线程B就会用一个已经关闭了的资源去做一些操作，因此产生错误。 例子2下面是一个通过布尔标志判断线程是否结束的例子： 12345678910111213141516171819202122232425public class CancelThreadTest &#123; publicstatic void main(String[] args) throws Exception&#123; PrimeGeneratorgen = new PrimeGenerator(); newThread(gen).start(); try &#123; Thread.sleep(3000); &#125;finally&#123; gen.cancel(); &#125; &#125;&#125; class PrimeGenerator implements Runnable&#123; privateboolean cancelled; @Override publicvoid run() &#123; while(!cancelled) &#123; System.out.println(&quot;Running...&quot;); //doingsomething here... &#125; &#125; publicvoid cancel()&#123;cancelled = true;&#125;&#125; 主线程中设置PrimeGenerator线程的是否取消标识，PrimeGenerator线程检测到这个标识后就会结束线程，由于主线程修改cancelled变量的内存可见性，主线程修改cancelled标识后并不马上同步回主内存，所以PrimeGenerator线程结束的时间难以把控（最终是一定会同步回主内存，让PrimeGenerator线程结束）。 如果PrimeGenerator线程执行一些比较关键的操作，主线程希望能够及时终止它，这时将cenceled用volatile关键字修饰就是必要的。 特别注意：上面演示这个并不是正确的取消线程的方法，因为一旦PrimeGenerator线程中包含BolckingQueue.put()等阻塞方法，那么将可能永远不会去检查cancelled标识，导致线程永远不会退出。正确的方法参见另外一篇关于如何正确终止线程的方法。 3 提供内存可见性volatile保证可见性的原理是在每次访问变量时都会进行一次刷新，因此每次访问都是主内存中最新的版本。所以volatile关键字的作用之一就是保证变量修改的实时可见性。 针对上面的例子1： 要求一个线程对open的改变，其他的线程能够立即可见，Java为此提供了volatile关键字，在声明open变量的时候加入volatile关键字就可以保证open的内存可见性，即open的改变对所有的线程都是立即可见的。 针对上面的例子2： 将cancelled标志设置的volatile保证主线程针对cancelled标识的修改能够让PrimeGenerator线程立马看到。 备注：也可以通过提供synchronized同步的open变量的Get/Set方法解决此内存可见性问题，因为要Get变量open，必须等Set方完全释放锁之后。后面将介绍到两者的区别。 （二）指令重排1 概念指令重排序是JVM为了优化指令，提高程序运行效率，在不影响单线程程序执行结果的前提下，尽可能地提高并行度。编译器、处理器也遵循这样一个目标。注意是单线程。多线程的情况下指令重排序就会给程序员带来问题。 不同的指令间可能存在数据依赖。比如下面计算圆的面积的语句： 123double r = 2.3d;//(1)double pi =3.1415926; //(2)double area = pi* r * r; //(3) area的计算依赖于r与pi两个变量的赋值指令。而r与pi无依赖关系。 as-if-serial语义是指：不管如何重排序（编译器与处理器为了提高并行度），（单线程）程序的结果不能被改变。这是编译器、Runtime、处理器必须遵守的语义。 虽然，（1） – happensbefore -&gt; （2）,（2） – happens before -&gt; （3），但是计算顺序(1)(2)(3)与(2)(1)(3) 对于r、pi、area变量的结果并无区别。编译器、Runtime在优化时可以根据情况重排序（1）与（2），而丝毫不影响程序的结果。 指令重排序包括编译器重排序和运行时重排序。 2 指令重排带来的问题如果一个操作不是原子的，就会给JVM留下重排的机会。下面看几个例子： 例子1：A线程指令重排导致B线程出错对于在同一个线程内，这样的改变是不会对逻辑产生影响的，但是在多线程的情况下指令重排序会带来问题。看下面这个情景: 在线程A中: 12context = loadContext();inited = true; 在线程B中: 1234while(!inited )&#123; //根据线程A中对inited变量的修改决定是否使用context变量 sleep(100);&#125;doSomethingwithconfig(context); 假设线程A中发生了指令重排序: 12inited = true;context = loadContext(); 那么B中很可能就会拿到一个尚未初始化或尚未初始化完成的context,从而引发程序错误。 例子2：指令重排导致单例模式失效我们都知道一个经典的懒加载方式的双重判断单例模式： 1234567891011121314public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance == null) &#123; synchronzied(Singleton.class) &#123; if(instance == null) &#123; &lt;strong&gt;instance = new Singleton(); //非原子操作 &#125; &#125; &#125; return instance; &#125;&#125; 看似简单的一段赋值语句：instance= new Singleton()，但是很不幸它并不是一个原子操作，其实际上可以抽象为下面几条JVM指令： 123memory =allocate(); //1：分配对象的内存空间 ctorInstance(memory); //2：初始化对象 instance =memory; //3：设置instance指向刚分配的内存地址 上面操作2依赖于操作1，但是操作3并不依赖于操作2，所以JVM是可以针对它们进行指令的优化重排序的，经过重排序后如下： 123memory =allocate(); //1：分配对象的内存空间 instance =memory; //3：instance指向刚分配的内存地址，此时对象还未初始化ctorInstance(memory); //2：初始化对象 可以看到指令重排之后，instance指向分配好的内存放在了前面，而这段内存的初始化被排在了后面。 在线程A执行这段赋值语句，在初始化分配对象之前就已经将其赋值给instance引用，恰好另一个线程进入方法判断instance引用不为null，然后就将其返回使用，导致出错。 3 防止指令重排除了前面内存可见性中讲到的volatile关键字可以保证变量修改的可见性之外，还有另一个重要的作用：在JDK1.5之后，可以使用volatile变量禁止指令重排序。 解决方案：例子1中的inited和例子2中的instance以关键字volatile修饰之后，就会阻止JVM对其相关代码进行指令重排，这样就能够按照既定的顺序指执行。 volatile关键字通过提供“内存屏障”的方式来防止指令被重排序，为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 （三）总结volatile是轻量级同步机制相对于synchronized块的代码锁，volatile应该是提供了一个轻量级的针对共享变量的锁，当我们在多个线程间使用共享变量进行通信的时候需要考虑将共享变量用volatile来修饰。 volatile是一种稍弱的同步机制，在访问volatile变量时不会执行加锁操作，也就不会执行线程阻塞，因此volatilei变量是一种比synchronized关键字更轻量级的同步机制。 volatile使用建议使用建议：在两个或者更多的线程需要访问的成员变量上使用volatile。当要访问的变量已在synchronized代码块中，或者为常量时，没必要使用volatile。 由于使用volatile屏蔽掉了JVM中必要的代码优化，所以在效率上比较低，因此一定在必要时才使用此关键字。 volatile和synchronized区别1、volatile不会进行加锁操作： volatile变量是一种稍弱的同步机制，在访问volatile变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile变量是一种比synchronized关键字更轻量级的同步机制。 2、volatile变量作用类似于同步变量读写操作： 从内存可见性的角度看，写入volatile变量相当于退出同步代码块，而读取volatile变量相当于进入同步代码块。 3、volatile不如synchronized安全： 在代码中如果过度依赖volatile变量来控制状态的可见性，通常会比使用锁的代码更脆弱，也更难以理解。仅当volatile变量能简化代码的实现以及对同步策略的验证时，才应该使用它。一般来说，用同步机制会更安全些。 4、volatile无法同时保证内存可见性和原子性： 加锁机制（即同步机制）既可以确保可见性又可以确保原子性，而volatile变量只能确保可见性，原因是声明为volatile的简单变量如果当前值与该变量以前的值相关，那么volatile关键字不起作用，也就是说如下的表达式都不是原子操作：“count++”、“count = count+1”。 当且仅当满足以下所有条件时，才应该使用volatile变量： 1231、对变量的写入操作不依赖变量的当前值，或者你能确保只有单个线程更新变量的值。2、该变量没有包含在具有其他变量的不变式中。 总结：在需要同步的时候，第一选择应该是synchronized关键字，这是最安全的方式，尝试其他任何方式都是有风险的。尤其在、jdK1.5之后，对synchronized同步机制做了很多优化，如：自适应的自旋锁、锁粗化、锁消除、轻量级锁等，使得它的性能明显有了很大的提升。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式]]></title>
    <url>%2F2019%2F06%2F17%2Fjava-design-pattern%2F</url>
    <content type="text"><![CDATA[以下文章来自于：Java 设计模式 一直想写一篇介绍设计模式的文章，让读者可以很快看完，而且一看就懂，看懂就会用，同时不会将各个模式搞混。自认为本文还是写得不错的???，花了不少心思来写这文章和做图，力求让读者真的能看着简单同时有所收获。 设计模式是对大家实际工作中写的各种代码进行高层次抽象的总结，其中最出名的当属 1234Gang of Four ( GoF ) 的分类了，他们将设计模式分类为 23 种经典的模式，根据用途我们又可以分为三大类，分别为创建型模式、结构型模式和行为型模式。是的，我不善于扯这些有的没的，还是少点废话吧~ 有一些重要的设计原则在开篇和大家分享下，这些原则将贯通全文： 面向接口编程，而不是面向实现。这个很重要，也是优雅的、可扩展的代码的第一步，这就不需要多说了吧。 职责单一原则。每个类都应该只有一个单一的功能，并且该功能应该由这个类完全封装起来。 对修改关闭，对扩展开放。对修改关闭是说，我们辛辛苦苦加班写出来的代码，该实现的功能和该修复的 bug 都完成了，别人可不能说改就改；对扩展开放就比较好理解了，也就是说在我们写好的代码基础上，很容易实现扩展。 创建型模式创建型模式的作用就是创建对象，说到创建一个对象，最熟悉的就是 new 一个对象，然后 set 相关属性。但是，在很多场景下，我们需要给客户端提供更加友好的创建对象的方式，尤其是那种我们定义了类，但是需要提供给其他开发者用的时候。 简单工厂模式和名字一样简单，非常简单，直接上代码吧： 12345678910111213141516public class FoodFactory &#123; public static Food makeFood(String name) &#123; if (name.equals("noodle")) &#123; Food noodle = new LanZhouNoodle(); noodle.addSpicy("more"); return noodle; &#125; else if (name.equals("chicken")) &#123; Food chicken = new HuangMenChicken(); chicken.addCondiment("potato"); return chicken; &#125; else &#123; return null; &#125; &#125;&#125; 其中，LanZhouNoodle 和 HuangMenChicken 都继承自 Food。 简单地说，简单工厂模式通常就是这样，一个工厂类 XxxFactory，里面有一个静态方法，根据我们不同的参数，返回不同的派生自同一个父类（或实现同一接口）的实例对象。 我们强调职责单一原则，一个类只提供一种功能，FoodFactory 的功能就是只要负责生产各种 Food。 工厂模式简单工厂模式很简单，如果它能满足我们的需要，我觉得就不要折腾了。之所以需要引入工厂模式，是因为我们往往需要使用两个或两个以上的工厂。 1234567891011121314151617181920212223242526272829public interface FoodFactory &#123; Food makeFood(String name);&#125;public class ChineseFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new ChineseFoodA(); &#125; else if (name.equals("B")) &#123; return new ChineseFoodB(); &#125; else &#123; return null; &#125; &#125;&#125;public class AmericanFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new AmericanFoodA(); &#125; else if (name.equals("B")) &#123; return new AmericanFoodB(); &#125; else &#123; return null; &#125; &#125;&#125; 其中，ChineseFoodA、ChineseFoodB、AmericanFoodA、AmericanFoodB 都派生自 Food。 客户端调用： 12345678public class APP &#123; public static void main(String[] args) &#123; // 先选择一个具体的工厂 FoodFactory factory = new ChineseFoodFactory(); // 由第一步的工厂产生具体的对象，不同的工厂造出不一样的对象 Food food = factory.makeFood("A"); &#125;&#125; 虽然都是调用 makeFood(“A”) 制作 A 类食物，但是，不同的工厂生产出来的完全不一样。 第一步，我们需要选取合适的工厂，然后第二步基本上和简单工厂一样。 核心在于，我们需要在第一步选好我们需要的工厂。比如，我们有 LogFactory 接口，实现类有 FileLogFactory 和 KafkaLogFactory，分别对应将日志写入文件和写入 Kafka 中，显然，我们客户端第一步就需要决定到底要实例化 FileLogFactory 还是 KafkaLogFactory，这将决定之后的所有的操作。 虽然简单，不过我也把所有的构件都画到一张图上，这样读者看着比较清晰： 抽象工厂模式当涉及到产品族的时候，就需要引入抽象工厂模式了。 一个经典的例子是造一台电脑。我们先不引入抽象工厂模式，看看怎么实现。 因为电脑是由许多的构件组成的，我们将 CPU 和主板进行抽象，然后 CPU 由 CPUFactory 生产，主板由 MainBoardFactory 生产，然后，我们再将 CPU 和主板搭配起来组合在一起，如下图： 这个时候的客户端调用是这样的： 12345678910// 得到 Intel 的 CPUCPUFactory intelCPUFactory = new IntelCPUFactory();CPU cpu = intelCPUFactory.makeCPU();// 得到 AMD 的主板MainBoardFactory mainBoardFactory = new AmdMainBoardFactory();MainBoard mainBoard = mainBoardFactory.make();// 组装 CPU 和主板Computer computer = new Computer(cpu, mainBoard); 单独看 CPU 工厂和主板工厂，它们分别是前面我们说的工厂模式。这种方式也容易扩展，因为要给电脑加硬盘的话，只需要加一个 HardDiskFactory 和相应的实现即可，不需要修改现有的工厂。 但是，这种方式有一个问题，那就是如果 Intel 家产的 CPU 和 AMD 产的主板不能兼容使用，那么这代码就容易出错，因为客户端并不知道它们不兼容，也就会错误地出现随意组合。 下面就是我们要说的产品族的概念，它代表了组成某个产品的一系列附件的集合： 当涉及到这种产品族的问题的时候，就需要抽象工厂模式来支持了。我们不再定义 CPU 工厂、主板工厂、硬盘工厂、显示屏工厂等等，我们直接定义电脑工厂，每个电脑工厂负责生产所有的设备，这样能保证肯定不存在兼容问题。 这个时候，对于客户端来说，不再需要单独挑选 CPU厂商、主板厂商、硬盘厂商等，直接选择一家品牌工厂，品牌工厂会负责生产所有的东西，而且能保证肯定是兼容可用的。 12345678910111213public static void main(String[] args) &#123; // 第一步就要选定一个“大厂” ComputerFactory cf = new AmdFactory(); // 从这个大厂造 CPU CPU cpu = cf.makeCPU(); // 从这个大厂造主板 MainBoard board = cf.makeMainBoard(); // 从这个大厂造硬盘 HardDisk hardDisk = cf.makeHardDisk(); // 将同一个厂子出来的 CPU、主板、硬盘组装在一起 Computer result = new Computer(cpu, board, hardDisk);&#125; 当然，抽象工厂的问题也是显而易见的，比如我们要加个显示器，就需要修改所有的工厂，给所有的工厂都加上制造显示器的方法。这有点违反了对修改关闭，对扩展开放这个设计原则。 单例模式单例模式用得最多，错得最多。 饿汉模式最简单： 1234567891011121314public class Singleton &#123; // 首先，将 new Singleton() 堵死 private Singleton() &#123;&#125;; // 创建私有静态实例，意味着这个类第一次使用的时候就会进行创建 // 这个代码还可以放在静态代码块中 private static Singleton instance = new Singleton(); public static Singleton getInstance() &#123; return instance; &#125; // 瞎写一个静态方法。这里想说的是，如果我们只是要调用 Singleton.getDate(...)， // 本来是不想要生成 Singleton 实例的，不过没办法，已经生成了 public static Date getDate(String mode) &#123;return new Date();&#125;&#125; 很多人都能说出饿汉模式的缺点(浪费内存空间)，可是我觉得生产过程中，很少碰到这种情况：你定义了一个单例的类，不需要其实例，可是你却把一个或几个你会用到的静态方法塞到这个类中。 饱汉模式最容易出错： 1234567891011121314151617181920public class Singleton &#123; // 首先，也是先堵死 new Singleton() 这条路 private Singleton() &#123;&#125; // 和饿汉模式相比，这边不需要先实例化出来，注意这里的 volatile，它是必须的 private static volatile Singleton instance = null; public static Singleton getInstance() &#123; if (instance == null) &#123; // 加锁 synchronized (Singleton.class) &#123; // 这一次判断也是必须的，不然会有并发问题 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; volatile 关键字可以保证 instance 的内存可见性和防止指令重排序，使得 instance 在多线程环境下依然可以正确的被初始化。详情请移步这里：java中volatile关键字的作用。 双重检查，指的是两次检查 instance 是否为 null。 volatile 在这里是需要的，希望能引起读者的关注。 很多人不知道怎么写，直接就在 getInstance() 方法签名上加上 synchronized，这就不多说了，性能太差。 嵌套类最经典，以后大家就用它吧： 1234567891011public class Singleton3 &#123; private Singleton3() &#123;&#125; // 主要是使用了 嵌套类可以访问外部类的静态属性和静态方法 的特性 private static class Holder &#123; private static Singleton3 instance = new Singleton3(); &#125; public static Singleton3 getInstance() &#123; return Holder.instance; &#125;&#125; 注意，很多人都会把这个嵌套类说成是静态内部类，严格地说，内部类和嵌套类是不一样的，它们能访问的外部类权限也是不一样的。 最后，一定有人跳出来说用枚举实现单例，是的没错，枚举类很特殊，它在类加载的时候会初始化里面的所有的实例，而且 JVM 保证了它们不会再被实例化，所以它天生就是单例的。不说了，读者自己看着办吧，不建议使用。 建造者模式经常碰见的 XxxBuilder 的类，通常都是建造者模式的产物。建造者模式其实有很多的变种，但是对于客户端来说，我们的使用通常都是一个模式的： 12Food food = new FoodBuilder().a().b().c().build();Food food = Food.builder().a().b().c().build(); 套路就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 来一个中规中矩的建造者模式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class User &#123; // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) &#123; this.name = name; this.password = password; this.nickName = nickName; this.age = age; &#125; // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() &#123; return new UserBuilder(); &#125; public static class UserBuilder &#123; // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() &#123; &#125; // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) &#123; this.name = name; return this; &#125; public UserBuilder password(String password) &#123; this.password = password; return this; &#125; public UserBuilder nickName(String nickName) &#123; this.nickName = nickName; return this; &#125; public UserBuilder age(int age) &#123; this.age = age; return this; &#125; // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() &#123; if (name == null || password == null) &#123; throw new RuntimeException("用户名和密码必填"); &#125; if (age &lt;= 0 || age &gt;= 150) &#123; throw new RuntimeException("年龄不合法"); &#125; // 还可以做赋予”默认值“的功能 if (nickName == null) &#123; nickName = name; &#125; return new User(name, password, nickName, age); &#125; &#125;&#125; 核心是：先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。 看看客户端的调用： 123456789public class APP &#123; public static void main(String[] args) &#123; User d = User.builder() .name("foo") .password("pAss12345") .age(25) .build(); &#125;&#125; 说实话，建造者模式的链式写法很吸引人，但是，多写了很多“无用”的 builder 的代码，感觉这个模式没什么用。不过，当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 题外话，强烈建议读者使用 lombok，用了 lombok 以后，上面的一大堆代码会变成如下这样: 1234567@Builderclass User &#123; private String name; private String password; private String nickName; private int age;&#125; 怎么样，省下来的时间是不是又可以干点别的了。 当然，如果你只是想要链式写法，不想要建造者模式，有个很简单的办法，User 的 getter 方法不变，所有的 setter 方法都让其 return this 就可以了，然后就可以像下面这样调用： 1User user = new User().setName("").setPassword("").setAge(20); 原型模式这是我要说的创建型模式的最后一个设计模式了。 原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 1protected native Object clone() throws CloneNotSupportedException; java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 原型模式了解到这里我觉得就够了，各种变着法子说这种代码或那种代码是原型模式，没什么意义。 创建型模式总结（简、工、抽、单、建、原）创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 123456简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式;单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源；建造者模式专门对付属性很多的那种类，为了让代码更优美；原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 理解代理这个词，这个模式其实就简单了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken("1kg"); f.setSpicy("1g"); f.setSalt("3g"); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle("500g"); f.setSalt("5g"); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println("我们马上要开始制作鸡肉了"); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println("鸡肉制作完成啦，加点胡椒粉"); // 增强 food.addCondiment("pepper"); return food; &#125; public Food makeNoodle() &#123; System.out.println("准备制作拉面~"); Food food = foodService.makeNoodle(); System.out.println("制作完成啦") return food; &#125;&#125; 客户端调用，注意，我们要用代理来实例化接口： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Apache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 12345678910public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125; 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 1234567891011121314151617181920212223242526public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125; 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 1234567891011public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125; 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 123456789101112131415161718public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println("咕咕叫"); &#125; public void fly() &#123; System.out.println("鸡也会飞哦"); &#125;&#125; 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 123456789101112131415161718192021// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125; 客户端调用很简单了： 1234567public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125; 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 123public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125; 然后是一系列实现类： 123456789101112131415161718public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125; 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 12345678public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 定义抽象类的子类： 123456789101112131415161718192021222324252627// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125; 最后，我们来看客户端演示： 1234567public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125; 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 本节引用了这里的例子，并对其进行了修改。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。 首先，定义饮料抽象基类： 123456public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125; 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 1234567891011121314151617public class BlackTea extends Beverage &#123; public String getDescription() &#123; return "红茶"; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return "绿茶"; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 1234// 调料public abstract class Condiment extends Beverage &#123;&#125; 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 1234567891011121314151617181920212223242526272829public class Lemon extends Condiment &#123; private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + ", 加柠檬"; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + ", 加芒果"; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类 看客户端调用： 12345678910public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + " 价格：￥" + beverage.cost()); //"绿茶, 加柠檬, 加芒果 价格：￥16"&#125; 如果我们需要芒果珍珠双份柠檬红茶： 1Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea())))); 是不是很变态？ 看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 1InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream(""))); 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 123LineNumberInputStream is = new LineNumberInputStream( new BufferedInputStream( new FileInputStream(""))); 所以说嘛，要找到纯的严格符合设计模式的代码还是比较难的。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 123public interface Shape &#123; void draw();&#125; 定义几个实现类： 123456789101112131415public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println("Circle::draw()"); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println("Rectangle::draw()"); &#125;&#125; 客户端调用： 123456789public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125; 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： 12345678910111213141516171819202122232425public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 看看现在客户端怎么调用： 12345678public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125; 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 1234567891011121314151617181920212223242526272829public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return ("Employee :[ Name : " + name + ", dept : " + dept + ", salary :" + salary+" ]"); &#125; &#125; 通常，这种类需要定义 add(node)、remove(node)、getChildren() 这些方法。 这说的其实就是组合模式，这种简单的模式我就不做过多介绍了，相信各位读者也不喜欢看我写废话。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是共享元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 这种简单的代码我就不演示了。 结构型模式总结(代、适、桥、装、门、组、享)前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。读者是否可以分别把这几个模式说清楚了呢？在说到这些模式的时候，心中是否有一个清晰的图或处理流程在脑海里呢？ 1234567代理模式是做方法增强的;适配器模式是把鸡包装成鸭这种用来适配接口的;桥梁模式做到了很好的解耦（将接口与实现解耦，分开）;装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景；门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可；组合模式用于描述具有层次结构的数据；享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。 行为型模式行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式策略模式太常用了，所以把它放到最前面进行介绍。它比较简单，我就不废话，直接用代码说事吧。 下面设计的场景是，我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。 首先，先定义一个策略接口： 123public interface Strategy &#123; public void draw(int radius, int x, int y);&#125; 然后我们定义具体的几个策略： 123456789101112131415161718public class RedPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125; 使用策略的类： 1234567891011public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeDraw(int radius, int x, int y)&#123; return strategy.draw(radius, x, y); &#125;&#125; 客户端演示： 1234public static void main(String[] args) &#123; Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);&#125; 放到一张图上，让大家看得清晰些： 这个时候，大家有没有联想到结构型模式中的桥梁模式，它们其实非常相似，我把桥梁模式的图拿过来大家对比下： 要我说的话，它们非常相似，桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式观察者模式对于我们来说，真是再简单不过了。无外乎两个操作，观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 首先，需要定义主题，每个主题需要持有观察者列表的引用，用于在数据变更的时候通知各个观察者： 1234567891011121314151617181920212223242526public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; // 通知观察者们 public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125; 定义观察者接口： 1234public abstract class Observer &#123; protected Subject subject; public abstract void update();&#125; 其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 我们来定义具体的几个观察者类： 123456789101112131415161718192021222324252627282930public class BinaryObserver extends Observer &#123; // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) &#123; this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); &#125; // 该方法由主题类在数据变更的时候进行调用 @Override public void update() &#123; String result = Integer.toBinaryString(subject.getState()); System.out.println("订阅的数据发生变化，新的数据处理为二进制值为：" + result); &#125;&#125;public class HexaObserver extends Observer &#123; public HexaObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println("订阅的数据发生变化，新的数据处理为十六进制值为：" + result); &#125;&#125; 客户端使用也非常简单： 12345678910public static void main(String[] args) &#123; // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);&#125; output: 12订阅的数据发生变化，新的数据处理为二进制值为：1011订阅的数据发生变化，新的数据处理为十六进制值为：B 当然，jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。 实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 如果产品给你这个需求的话，我想大部分人一开始肯定想的就是，用一个 List 来存放所有的规则，然后 foreach 执行一下每个规则就好了。不过，读者也先别急，看看责任链模式和我们说的这个有什么不一样？ 首先，我们要定义流程上节点的基类： 123456789101112131415public abstract class RuleHandler &#123; // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) &#123; this.successor = successor; &#125; public RuleHandler getSuccessor() &#123; return successor; &#125;&#125;复制代码 接下来，我们需要定义具体的每个节点了。 校验用户是否是新用户： 123456789101112131415public class NewUserRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; if (context.isNewUser()) &#123; // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException(&quot;该活动仅限新用户参与&quot;); &#125; &#125;&#125;复制代码 校验用户所在地区是否可以参与： 12345678910111213public class LocationRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException(&quot;非常抱歉，您所在的地区无法参与本次活动&quot;); &#125; &#125;&#125;复制代码 校验奖品是否已领完： 12345678910111213public class LimitRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(userInfo); &#125; &#125; else &#123; throw new RuntimeException(&quot;您来得太晚了，奖品被领完了&quot;); &#125; &#125;&#125;复制代码 客户端： 12345678910public static void main(String[] args) &#123; RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);&#125;复制代码 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 至于它和我们前面说的用一个 List 存放需要执行的规则的做法有什么异同，留给读者自己琢磨吧。 模板方法模式在含有继承结构的代码中，模板方法模式是非常常用的，这也是在开源代码中大量被使用的。 通常会有一个抽象类： 12345678910111213141516public abstract class AbstractTemplate &#123; // 这就是模板方法 public void templateMethod()&#123; init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 &#125; protected void init() &#123; System.out.println(&quot;init 抽象层已经实现，子类也可以选择覆写&quot;); &#125; // 留给子类实现 protected abstract void apply(); protected void end() &#123; &#125;&#125;复制代码 模板方法中调用了 3 个方法，其中 apply() 是抽象方法，子类必须实现它，其实模板方法中有几个抽象方法完全是自由的，我们也可以将三个方法都设置为抽象方法，让子类来实现。也就是说，模板方法只负责定义第一步应该要做什么，第二步应该做什么，第三步应该做什么，至于怎么做，由子类来实现。 我们写一个实现类： 123456789public class ConcreteTemplate extends AbstractTemplate &#123; public void apply() &#123; System.out.println(&quot;子类实现抽象方法 apply&quot;); &#125; public void end() &#123; System.out.println(&quot;我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了&quot;); &#125;&#125;复制代码 客户端调用演示： 123456public static void main(String[] args) &#123; AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();&#125;复制代码 代码其实很简单，基本上看到就懂了，关键是要学会用到自己的代码中。 状态模式废话我就不说了，我们说一个简单的例子。商品库存中心有个最基本的需求是减库存和补库存，我们看看怎么用状态模式来写。 核心在于，我们的关注点不再是 Context 是该进行哪种操作，而是关注在这个 Context 会有哪些操作。 定义状态接口： 1234public interface State &#123; public void doAction(Context context);&#125;复制代码 定义减库存的状态： 1234567891011121314public class DeductState implements State &#123; public void doAction(Context context) &#123; System.out.println(&quot;商品卖出，准备减库存&quot;); context.setState(this); //... 执行减库存的具体操作 &#125; public String toString()&#123; return &quot;Deduct State&quot;; &#125;&#125;复制代码 定义补库存状态： 123456789101112public class RevertState implements State &#123; public void doAction(Context context) &#123; System.out.println(&quot;给此商品补库存&quot;); context.setState(this); //... 执行加库存的具体操作 &#125; public String toString() &#123; return &quot;Revert State&quot;; &#125;&#125;复制代码 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 123456789101112131415public class Context &#123; private State state; private String name; public Context(String name) &#123; this.name = name; &#125; public void setState(State state) &#123; this.state = state; &#125; public void getState() &#123; return this.state; &#125;&#125;复制代码 我们来看下客户端调用，大家就一清二楚了： 12345678910111213141516public static void main(String[] args) &#123; // 我们需要操作的是 iPhone X Context context = new Context(&quot;iPhone X&quot;); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();&#125;复制代码 读者可能会发现，在上面这个例子中，如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。 不过，商品库存这个例子毕竟只是个例，我们还有很多实例是需要知道当前 context 处于什么状态的。 行为型模式总结行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限，而且本文篇幅也挺大了，我就不进行介绍了。 总结学习设计模式的目的是为了让我们的代码更加的优雅、易维护、易扩展。 作者：JavaDoop 链接：https://juejin.im/post/5bc96afff265da0aa94a4493 来源：掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是restful？]]></title>
    <url>%2F2019%2F06%2F17%2Fwhat-is-resuful%2F</url>
    <content type="text"><![CDATA[以下文章来源于：如何给老婆解释什么是RESTful 老婆经常喜欢翻看我订阅的技术杂志，她总能从她的视角提出很多有趣的问题。 一个悠闲的周日下午，她午觉醒来，又习惯性的抓起这个月的杂志，饶有兴趣地看了起来。 果不其然，看着看着，她又对我发难了，“Restful是什么呀，老公？是restaurant的形容词吗，突然就觉得好饿了啊……” 作为一个合格的程序员，我一直把能够将一项技术讲给老婆听，并且能给她讲懂，作为我已经掌握了这项技术的标准。 如果我直接回答说，“REST就是Representational State Transfer的缩写呀，翻译为中文就是‘表述性状态转移’”，那她今晚肯定得罚我跪键盘。我必须找个合适的机会，把Restful的来龙去脉给她形象的描述一遍。 “走，咱们去楼下咖啡厅吃个下午茶吧”，我对老婆说。 “一个芝士蛋糕，一杯拿铁，两条吸管，谢谢”，我对前台的服务员说，然后我们找了个角落坐了下来。 Level 0 - 面向前台“刚才我们向前台点了一杯拿铁，这个过程可以用这段文字来描述”，说着，我在纸上写下了这段JSON，虽然她不知道什么叫JSON，但理解这段文字对于英语专业8级的她，实在再简单不过。 12345&#123; &quot;addOrder&quot;: &#123; &quot;orderName&quot;: &quot;latte&quot; &#125;&#125; “我们通过这段文字，告诉前台，新增一笔订单，订单是一杯拿铁咖啡”，接着，前台给我们返回这么一串回复： 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “订单ID？还是订单编号？” “恩恩，就是订单编号” “那我们就等着前台喊‘订单123456的客户可以取餐了’，然后就可以开吃了！” “哈哈，你真聪明，不过，在这之前，假设我们有一张会员卡，我们想查询一下这张会员卡的余额，这时候，要向前台发起另一个询问”，我继续在纸上写着： 12345&#123; &quot;queryBalance&quot;: &#123; &quot;cardId&quot;: &quot;886333&quot; &#125;&#125; “查询卡号为886333的卡的余额？” “真棒！接着，查询的结果返回来了” 123&#123; &quot;balance&quot;: &quot;0&quot;&#125; “切，没钱……” “哈哈，没钱，现在我们要跟前台说，这杯咖啡不要了”，我在纸上写到： 12345&#123; &quot;deleteOrder&quot;: &#123; &quot;orderId&quot;: &quot;123456&quot; &#125;&#125; “哼，这就把订单取消啦？” Level 1 - 面向资源“现在这家咖啡店越做越大，来喝咖啡的人越来越多，单靠前台显然是不行的，店主决定进行分工，每个资源都有专人负责，我们可以直接面向资源操作。” “面向资源？” “是的，比如还是下单，请求的内容不变，但是我们多了一条消息”，我在纸上画出这次的模型： 1234567/orders&#123; &quot;addOrder&quot;: &#123; &quot;orderName&quot;: &quot;latte&quot; &#125;&#125; “多了一个斜杠和orders？这是什么意思？” “这个表示我们这个请求是发给哪个资源的，订单是一种资源，我们可以理解为是咖啡厅专门管理订单的人，他可以帮我们处理所有有关订单的操作，包括新增订单、修改订单、取消订单等操作” “Soga…” “接着还是会返回订单的编号给我们” 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “下面，我们还是要查询会员卡余额，这次请求的资源变成了cards” 1234567/cards&#123; &quot;queryBalance&quot;: &#123; &quot;cardId&quot;: &quot;886333&quot; &#125;&#125; “接下来是取消订单” “这个我会”，说着，她抢走我手上的笔，在纸上写了起来： 1234567/orders&#123; &quot;deleteOrder&quot;: &#123; &quot;orderId&quot;: &quot;123456&quot; &#125;&#125; Level 2 - 打上标签“接下来，店主还想继续优化他的咖啡厅的服务流程，他发现负责处理订单的员工，每次都要去订单内容里面看是新增订单还是删除订单，还是其他的什么操作，十分不方便，于是规定，所有新增资源的请求，都在请求上面写上大大的‘POST’，表示这是一笔新增资源的请求” “其他种类的请求，比如查询类的，用‘GET’表示，删除类的，用‘DELETE’表示” “还有修改类的，修改分为两种，第一种，如果这个修改，无论发送多少次，最后一次修改后的资源，总是和第一次修改后的一样，比如将拿铁改为猫屎，那么用‘PUT’表示；第二种，如果这个修改，每次修改都会让这个资源和前一次的不一样，比如是加一杯咖啡，那么这种请求用‘PATCH’或者‘POST’表示”，一口气讲了这么多，发现她有点似懂非懂。 “来，我们再来重复上面那个过程，来一杯拿铁”，我边说边画着： 12345POST /orders&#123; &quot;orderName&quot;: &quot;latte&quot;&#125; “请求的内容简洁多啦，不用告诉店员是addOrder，看到POST就知道是新增”，她听的很认真，理解的也很透彻。 “恩恩，返回的内容还是一样” 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “接着是查询会员卡余额，这次也简化了很多” 12345GET /cards&#123; &quot;cardId&quot;: &quot;886333&quot;&#125; “这个请求我们还可以进一步优化为这样” 1GET /cards/886333 “Soga，直接把要查询的卡号写在后面了” “没错，接着，取消订单” 1DELETE /orders/123456 Level 3 - 完美服务“忽然有一天，有个顾客抱怨说，他买了咖啡后，不知道要怎么取消订单，咖啡厅一个店员回了一句，你不会看我们的宣传单吗，上面不写着： 1DELETE /orders/&#123;orderId&#125; 顾客反问道，谁会去看那个啊，店员不服，又说到，你瞎了啊你……据说后面两人吵着吵着还打了起来…” “噗，真是悲剧…” “有了这次教训，店长决定，顾客下了单之后，不仅给他们返回订单的编号，还给顾客返回所有可以对这个订单做的操作，比如告诉用户如何删除订单。现在，我们还是发出请求，请求内容和上一次一样” 12345POST /orders&#123; &quot;orderName&quot;: &quot;latte&quot;&#125; “但是这次返回时多了些内容” 1234567&#123; &quot;orderId&quot;: &quot;123456&quot;, &quot;link&quot;: &#123; &quot;rel&quot;: &quot;cancel&quot;, &quot;url&quot;: &quot;/order/123456&quot; &#125;&#125; “这次返回时多了一项link信息，里面包含了一个rel属性和url属性，rel是relationship的意思，这里的关系是cancel，url则告诉你如何执行这个cancel操作，接着你就可以这样子来取消订单啦” 1DELETE /orders/123456 “哈哈，这服务真是贴心，以后再也不用担心店员和顾客打起来了” “订单123456的客户可以取餐了”，伴随着咖啡厅的广播，我们吃起了下午茶，一杯拿铁，两支吸管…… 对程序员的话用了大白话，给老婆讲明白了RESTful的来龙去脉，当然，我还是有些话想说的，只是怕老婆听完一脸懵逼，没给她说： 1、 上面讲的Level0~Level3，来自Leonard Richardson提出的Richardson Maturity Model： Level0和Level1最大的区别，就是Level1拥有了Restful的第一个特征——面向资源，这对构建可伸缩、分布式的架构是至关重要的。同时，如果把Level0的数据格式换成Xml，那么其实就是SOAP，SOAP的特点是关注行为和处理，和面向资源的RESTful有很大的不同。 Level0和Level1，其实都很挫，他们都只是把HTTP当做一个传输的通道，没有把HTTP当做一种传输协议。 Level2，真正将HTTP作为了一种传输协议，最直观的一点就是Level2使用了HTTP动词，GET/PUT/POST/DELETE/PATCH….,这些都是HTTP的规范，规范的作用自然是重大的，用户看到一个POST请求，就知道它不是幂等的，使用时要小心，看到PUT，就知道他是幂等的，调用多几次都不会造成问题，当然，这些的前提都是API的设计者和开发者也遵循这一套规范，确保自己提供的PUT接口是幂等的。 Level3，关于这一层，有一个古怪的名词，叫HATEOAS（Hypertext As The Engine Of Application State），中文翻译为“将超媒体格式作为应用状态的引擎”，核心思想就是每个资源都有它的状态，不同状态下，可对它进行的操作不一样。理解了这一层，再来看看REST的全称，Representational State Transfer，中文翻译为“表述性状态转移”，是不是好理解多了？ Level3的Restful API，给使用者带来了很大的便利，使用者只需要知道如何获取资源的入口，之后的每个URI都可以通过请求获得，无法获得就说明无法执行那个请求。 现在绝大多数的RESTful接口都做到了Level2的层次，做到Level3的比较少。当然，这个模型并不是一种规范，只是用来理解Restful的工具。所以，做到了Level2，也就是面向资源和使用Http动词，就已经很Restful了。Restful本身也不是一种规范，我比较倾向于用“风格“来形容它。如果你想深入了解Level3，可以阅读《Rest in Practice》第五章。 2、 我跟老婆讲的时候，用的数据格式是JSON，但是要强调一点，Restful对数据格式没有限制，就算你用的是XML或者其他格式，只要符合上面提到的几个特征，也算Restful。]]></content>
      <categories>
        <category>Restful</category>
      </categories>
      <tags>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出理解基于 Kafka 和 ZooKeeper 的分布式消息队列]]></title>
    <url>%2F2019%2F06%2F17%2Fkafka-zookeeper%2F</url>
    <content type="text"><![CDATA[消息队列中间件是分布式系统中重要的组件，主要解决应用耦合(使消息的生产者和消费者解耦)，异步消息（消息的生产和消费是异步的），流量削峰等问题。实现高性能，高可用，可伸缩和最终一致性架构，是大型分布式系统不可缺少的中间件。 本场 Chat 主要内容： Kafka 的架构解读； Kafka 为什么要将 Topic 进行分区； Kafka 高可靠性实现基础解读； Kafka 复制原理和同步方式； Leader 选举机制，及如何确保新选举出的 Leader 是优选； 同步副本 ISR； Kafka 数据可靠性和持久性保证； 深入解读 HW 机制； Kafka 架构中 ZooKeeper 以怎样的形式存在； 全程解析：Producer -&gt; kafka -&gt; consumer。 相关内容链接： 《分布式中间件实践之路》 《Python 快速入门实战教程》 《分布式锁的最佳实践之：基于 Etcd 的分布式锁》 《基于 Redis 的分布式锁实现及踩坑案例》 《一个高可靠性商用 Redis 集群方案介绍》 1. Kafka 总体架构基于 Kafka-ZooKeeper 的分布式消息队列系统总体架构如下： 如上图所示，一个典型的 Kafka 体系架构包括若干 Producer（消息生产者），若干 broker（作为 Kafka 节点的服务器），若干 Consumer（Group），以及一个 ZooKeeper 集群。Kafka通过 ZooKeeper 管理集群配置、选举 Leader 以及在 consumer group 发生变化时进行 Rebalance（即消费者负载均衡，在下一课介绍）。Producer 使用 push（推）模式将消息发布到 broker，Consumer 使用 pull（拉）模式从 broker 订阅并消费消息。 上图仅描摹了一个总体架构，并没有对作为 Kafka 节点的 broker 进行深入刻画，事实上，它的内部细节相当复杂，如下图所示，Kafka 节点涉及 Topic、Partition 两个重要概念。 在 Kafka 架构中，有几个术语： Producer：生产者，即消息发送者，push 消息到 Kafka 集群中的 broker（就是 server）中； Broker：Kafka 集群由多个 Kafka 实例（server） 组成，每个实例构成一个 broker，说白了就是服务器； Topic：producer 向 kafka 集群 push 的消息会被归于某一类别，即Topic，这本质上只是一个逻辑概念，面向的对象是 producer 和 consumer，producer 只需要关注将消息 push 到哪一个 Topic 中，而 consumer 只需要关心自己订阅了哪个 Topic； Partition：每一个 Topic 又被分为多个 Partitions，即物理分区；出于负载均衡的考虑，同一个 Topic 的 Partitions 分别存储于 Kafka 集群的多个 broker 上；而为了提高可靠性，这些 Partitions 可以由 Kafka 机制中的 replicas 来设置备份的数量；如上面的框架图所示，每个 partition 都存在两个备份； Consumer：消费者，从 Kafka 集群的 broker 中 pull 消息、消费消息； Consumer group：high-level consumer API 中，每个 consumer 都属于一个 consumer-group，每条消息只能被 consumer-group 中的一个 Consumer 消费，但可以被多个 consumer-group 消费； replicas：partition 的副本，保障 partition 的高可用； leader：replicas 中的一个角色， producer 和 consumer 只跟 leader 交互； follower：replicas 中的一个角色，从 leader 中复制数据，作为副本，一旦 leader 挂掉，会从它的 followers 中选举出一个新的 leader 继续提供服务； controller：Kafka 集群中的其中一个服务器，用来进行 leader election 以及 各种 failover(故障转移)； ZooKeeper：Kafka 通过 ZooKeeper 来存储集群的 meta 信息等，文中将详述。 1.1 Topic &amp; Partition一个 topic 可以认为是一类消息，每个 topic 将被分成多个 partition，每个 partition 在存储层面是 append log 文件。任何发布到此 partition 的消息都会被追加到log文件的尾部，每条消息在文件中的位置称为 offset（偏移量），offset 为一个 long 型的数字，它唯一标记一条消息。 Kafka 机制中，producer push 来的消息是追加（append）到 partition 中的，这是一种顺序写磁盘的机制，效率远高于随机写内存，如下示意图： 1.2 Kafka 为什么要将 Topic 进行分区？1简而言之：负载均衡 + 水平扩展。 前已述及，Topic 只是逻辑概念，面向的是 producer 和 consumer；而 Partition 则是物理概念。可以想象，如果 Topic 不进行分区，而将 Topic 内的消息存储于一个 broker，那么关于该 Topic 的所有读写请求都将由这一个 broker 处理，吞吐量很容易陷入瓶颈，这显然是不符合高吞吐量应用场景的。有了 Partition 概念以后，假设一个 Topic 被分为 10 个 Partitions，Kafka 会根据一定的算法将 10 个 Partition 尽可能均匀的分布到不同的 broker（服务器）上，当 producer 发布消息时，producer 客户端可以采用 random、key-hash 及 轮询 等算法选定目标 partition，若不指定，Kafka 也将根据一定算法将其置于某一分区上。Partiton 机制可以极大的提高吞吐量，并且使得系统具备良好的水平扩展能力。 在创建 topic 时可以在 $KAFKA_HOME/config/server.properties 中指定这个 partition 的数量（如下所示），当然可以在 topic 创建之后去修改 partition 的数量。 1234# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=3 在发送一条消息时，可以指定这个消息的 key，producer 根据这个 key 和 partition 机制来判断这个消息发送到哪个partition。partition 机制可以通过指定 producer 的 partition.class 这一参数来指定（即支持自定义），该 class 必须实现 kafka.producer.Partitioner 接口。 有关 topic 与 partition 的更多细节，可以参考下面的“Kafka 文件存储机制”这一节。 2. Kafka 高可靠性实现基础解读谈及可靠性，最常规、最有效的策略就是 “副本（replication）机制” ，Kafka 实现高可靠性同样采用了该策略。通过调节副本相关参数，可使 Kafka 在性能和可靠性之间取得平衡。本节先从 Kafka 文件存储机制入手，从最底层了解 Kafka 的存储细节，进而对消息的存储有个微观的认知。之后通过介绍 Kafka 的复制原理和同步方式来阐述宏观层面的概念。最后介绍 ISR，HW 和 leader 选举。 2.1 Kafka 文件存储机制Kafka 中消息是以 topic 进行分类的，生产者通过 topic 向 Kafka broker 发送消息，消费者通过 topic 读取数据。然而 topic 在物理层面又能以 partition 为分组，一个 topic 可以分成若干个 partition。事实上，partition 并不是最终的存储粒度，partition 还可以细分为 segment，一个 partition 物理上由多个 segment 组成，那么这些 segment 又是什么呢？ 为了便于说明问题，假设这里只有一个 Kafka 集群，且这个集群只有一个 Kafka broker，即只有一台物理机。在这个 Kafka broker 中配置 log.dirs=/tmp/kafka-logs，以此来设置 Kafka 消息文件存储目录；与此同时，通过命令创建一个 topic：mytopic_test，partition 的数量配置为 4（创建 topic 的命令请见上一课）。之后，可以在 /tmp/kafka-logs 目录中可以看到生成了 4 个目录： 1234drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-0drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-1drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-2drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-3 在 Kafka 文件存储中，同一个 topic 下有多个不同的 partition，每个 partiton 为一个目录，partition 的名称规则为：topic 名称 + 有序序号，第一个序号从 0 开始计，最大的序号为 partition 数量减 1，partition 是实际物理上的概念，而 topic 是逻辑上的概念。 问题 1：为什么不能以 partition 作为存储单位？ 上面提到 partition 还可以细分为 segment，这个 segment 又是什么？如果就以 partition 为最小存储单位，可以想象，当 Kafka producer 不断发送消息，必然会引起 partition 文件的无限扩张，将对消息文件的维护以及已消费的消息的清理带来严重的影响，因此，需以 segment 为单位将 partition 进一步细分。每个 partition（目录）相当于一个巨型文件被平均分配到多个大小相等的 segment（段）数据文件中（每个 segment 文件中消息数量不一定相等）这种特性也方便 old segment 的删除，即方便已被消费的消息的清理，提高磁盘的利用率。每个 partition 只需要支持顺序读写就行，segment 的文件生命周期由服务端配置参数（log.segment.bytes，log.roll.{ms,hours} 等若干参数）决定。 问题 2：segment 的工作原理是怎样的？ segment 文件由两部分组成，分别为 “.index” 文件和 “.log” 文件，分别表示为 segment 索引文件和数据文件。这两个文件的命令规则为：partition 全局的第一个 segment 从 0 开始，后续每个 segment 文件名为上一个 segment 文件最后一条消息的 offset 值，数值大小为 64 位，20 位数字字符长度，没有数字用 0 填充，如下： 12345600000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.log 以上面的 segment 文件为例，展示出 segment：00000000000000170410 的 “.index” 文件和 “.log” 文件的对应的关系，如下图： 如上图，“.index” 索引文件存储大量的元数据，“.log” 数据文件存储大量的消息，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。其中以 “.index” 索引文件中的元数据 [3, 348] 为例，在 “.log” 数据文件表示第 3 个消息，即在全局 partition 中表示 170410+3=170413 个消息，该消息的物理偏移地址为 348。 问题 3：如何从 partition 中通过 offset 查找 message 呢？ 以上图为例，读取 offset=170418 的消息，首先查找 segment 文件，其中 00000000000000000000.index 为最开始的文件，第二个文件为 00000000000000170410.index（起始偏移为 170410+1=170411），而第三个文件为 00000000000000239430.index（起始偏移为 239430+1=239431），所以这个 offset=170418 就落到了第二个文件之中。其它后续文件可以依次类推，以其偏移量命名并排列这些文件，然后根据二分查找法就可以快速定位到具体文件位置。其次根据 00000000000000170410.index 文件中的 [8,1325] 定位到 00000000000000170410.log 文件中的 1325 的位置进行读取。 要是读取 offset=170418 的消息，从 00000000000000170410.log 文件中的 1325 的位置进行读取，那么，如何确定何时读完本条消息呢？（否则就读到下一条消息的内容了） 这个问题由消息的物理结构解决，消息都具有固定的物理结构，包括：offset（8 Bytes）、消息体的大小（4 Bytes）、crc32（4 Bytes）、magic（1 Byte）、attributes（1 Byte）、key length（4 Bytes）、key（K Bytes）、payload（N Bytes）等等字段，可以确定一条消息的大小，即读取到哪里截止。 2.2 复制原理和同步方式Kafka 中 topic 的每个 partition 有一个预写式的日志文件，虽然 partition 可以继续细分为若干个 segment 文件，但是对于上层应用来说，仍然可以将 partition 看成最小的存储单元（一个有多个 segment 文件拼接的 “巨型” 文件），每个 partition 都由一些列有序的、不可变的消息组成，这些消息被连续的追加到 partition 中。 上图中有两个新名词：HW 和 LEO。这里先介绍下 LEO，LogEndOffset 的缩写，表示每个 partition 的 log 最后一条 Message 的位置。HW 是 HighWatermark 的缩写，是指 consumer 能够看到的此 partition 的位置，这个涉及到多副本的概念，这里先提及一下，下文再详述。 言归正传，为了提高消息的可靠性，Kafka 每个 topic 的 partition 有 N 个副本（replicas），其中 N（大于等于 1）是 topic 的复制因子（replica fator）的个数。Kafka 通过多副本机制实现故障自动转移，当 Kafka 集群中出现 broker 失效时，副本机制可保证服务可用。对于任何一个 partition，它的 N 个 replicas 中，其中一个 replica 为 leader，其他都为 follower，leader 负责处理 partition 的所有读写请求，follower 则负责被动地去复制 leader 上的数据。如下图所示，Kafka 集群中有 4 个 broker，某 topic 有 3 个 partition，且复制因子即副本个数也为 3： 如果 leader 所在的 broker 发生故障或宕机，对应 partition 将因无 leader 而不能处理客户端请求，这时副本的作用就体现出来了：一个新 leader 将从 follower 中被选举出来并继续处理客户端的请求。 如何确保新选举出的 leader 是优选呢？ 一个 partition 有多个副本（replicas），为了提高可靠性，这些副本分散在不同的 broker 上，由于带宽、读写性能、网络延迟等因素，同一时刻，这些副本的状态通常是不一致的：即 followers 与 leader 的状态不一致。那么，如何保证新选举出的 leader 是优选呢？ Kafka 机制中，leader 将负责维护和跟踪一个 ISR（In-Sync Replicas）列表，即同步副本队列，这个列表里面的副本与 leader 保持同步，状态一致。如果新的 leader 从 ISR 列表中的副本中选出，那么就可以保证新 leader 为优选。当然，这不是唯一的策略，下文将继续解读。 2.3 同步副本 ISR上一节中讲到了同步副本队列 ISR（In-Sync Replicas）。虽然副本极大的增强了可用性，但是副本数量对 Kafka 的吞吐率有一定影响。默认情况下 Kafka 的 replica 数量为 1，即每个 partition 都只有唯一的 leader，无 follower，没有容灾能力。为了确保消息的可靠性，生产环境中，通常将其值（由 broker 的参数 offsets.topic.replication.factor 指定）大小设置为大于 1，比如 3。 所有的副本（replicas）统称为 Assigned Replicas，即 AR。ISR 是 AR 中的一个子集，由 leader 维护 ISR 列表，follower 从 leader 同步数据有一些延迟（由参数 replica.lag.time.max.ms 设置超时阈值），超过阈值的 follower 将被剔除出 ISR， 存入 OSR（Outof-Sync Replicas）列表，新加入的 follower 也会先存放在 OSR 中。AR=ISR+OSR。 1注：ISR中包括：leader + 与leader保持同步的followers。 上面一节还涉及到一个概念，即 HW。HW 俗称高水位，HighWatermark 的缩写，取一个 partition 对应的 ISR 中最小的 LEO 作为 HW，consumer 最多只能消费到 HW 所在的位置。另外每个 replica 都有 HW，leader 和 follower 各自负责更新自己的 HW 的状态。对于 leader 新写入的消息，consumer 不能立刻消费，leader 会等待该消息被所有 ISR 中的 replicas 同步后更新 HW，此时消息才能被 consumer 消费。这样就保证了如果 leader 所在的 broker 失效，该消息仍然可以从新选举的 leader 中获取。对于来自内部 broker 的读取请求，没有 HW 的限制。 下图详细的说明了当 producer 生产消息至 broker 后，ISR 以及 HW 和 LEO 的流转过程： 由此可见，Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 都复制完，这条消息才会被 commit，这种复制方式受限于复制最慢的 follower，会极大的影响吞吐率。而异步复制方式下，follower 异步的从 leader 复制数据，数据只要被 leader 写入 log 就被认为已经 commit，这种情况下如果 follower 都还没有复制完，落后于 leader 时，突然 leader 宕机，则会丢失数据，降低可靠性。而 Kafka 使用 ISR 的策略则在可靠性和吞吐率方面取得了较好的平衡。 Kafka 的 ISR 的管理最终都会反馈到 ZooKeeper 节点上，具体位置为： 1/brokers/topics/[topic]/partitions/[partition]/state 目前，有两个地方会对这个 ZooKeeper 的节点进行维护。 Controller 来维护：Kafka 集群中的其中一个 Broker 会被选举为 Controller，主要负责 Partition 管理和副本状态管理，也会执行类似于重分配 partition 之类的管理任务。在符合某些特定条件下，Controller 下的 LeaderSelector 会选举新的 leader，ISR 和新的 leader_epoch 及 controller_epoch 写入 ZooKeeper 的相关节点中。同时发起 LeaderAndIsrRequest 通知所有的 replicas。 leader 来维护：leader 有单独的线程定期检测 ISR 中 follower 是否脱离 ISR，如果发现 ISR 变化，则会将新的 ISR 的信息返回到 ZooKeeper 的相关节点中。 2.4 数据可靠性和持久性保证当 producer 向 leader 发送数据时，可以通过 request.required.acks 参数来设置数据可靠性的级别： 1. request.required.acks = 1 这是默认情况，即：producer 发送数据到 leader，leader 写本地日志成功，返回客户端成功；此时 ISR 中的其它副本还没有来得及拉取该消息，如果此时 leader 宕机了，那么此次发送的消息就会丢失。 2. request.required.acks = 0 producer 不停向leader发送数据，而不需要 leader 反馈成功消息，这种情况下数据传输效率最高，但是数据可靠性确是最低的。可能在发送过程中丢失数据，可能在 leader 宕机时丢失数据。 3. request.required.acks = -1（all） producer 发送数据给 leader，leader 收到数据后要等到 ISR 列表中的所有副本都同步数据完成后（强一致性），才向生产者返回成功消息，如果一直收不到成功消息，则认为发送数据失败会自动重发数据。这是可靠性最高的方案，当然，性能也会受到一定影响。 *注意：参数 min.insync.replicas * 如果要提高数据的可靠性，在设置 request.required.acks=-1 的同时，还需参数 min.insync.replicas 配合，如此才能发挥最大的功效。min.insync.replicas 这个参数用于设定 ISR 中的最小副本数，默认值为1，当且仅当 request.required.acks 参数设置为-1时，此参数才生效。当 ISR 中的副本数少于 min.insync.replicas 配置的数量时，客户端会返回异常：org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。不难理解，如果 min.insync.replicas 设置为 2，当 ISR 中实际副本数为 1 时（只有leader），将无法保证可靠性，此时拒绝客户端的写请求以防止消息丢失。 2.5 深入解读 HW 机制考虑这样一种场景：acks=-1，部分 ISR 副本完成同步，此时leader挂掉，如下图所示：follower1 同步了消息 4、5，follower2 同步了消息 4，与此同时 follower2 被选举为 leader，那么此时 follower1 中的多出的消息 5 该做如何处理呢？ 这里就需要 HW 的协同配合了。如前所述，一个 partition 中的 ISR 列表中，leader 的 HW 是所有 ISR 列表里副本中最小的那个的 LEO。类似于木桶原理，水位取决于最低那块短板。 如上图，某个 topic 的某 partition 有三个副本，分别为 A、B、C。A 作为 leader 肯定是 LEO 最高，B 紧随其后，C 机器由于配置比较低，网络比较差，故而同步最慢。这个时候 A 机器宕机，这时候如果 B 成为 leader，假如没有 HW，在 A 重新恢复之后会做同步（makeFollower) 操作，在宕机时 log 文件之后直接做追加操作，而假如 B 的 LEO 已经达到了 A 的 LEO，会产生数据不一致的情况，所以使用 HW 来避免这种情况。 A 在做同步操作的时候，先将 log 文件截断到之前自己的 HW 的位置，即 3，之后再从 B 中拉取消息进行同步。 如果失败的 follower 恢复过来，它首先将自己的 log 文件截断到上次 checkpointed 时刻的 HW 的位置，之后再从 leader 中同步消息。leader 挂掉会重新选举，新的 leader 会发送 “指令” 让其余的 follower 截断至自身的 HW 的位置然后再拉取新的消息。 当 ISR 中的个副本的 LEO 不一致时，如果此时 leader 挂掉，选举新的 leader 时并不是按照 LEO 的高低进行选举，而是按照 ISR 中的顺序选举。 2.6 Leader 选举为了保证可靠性，对于任意一条消息，只有它被 ISR 中的所有 follower 都从 leader 复制过去才会被认为已提交，并返回信息给 producer。如此，可以避免因部分数据被写进 leader，而尚未被任何 follower 复制就宕机的情况下而造成数据丢失。对于 producer 而言，它可以选择是否等待消息 commit，这可以通过参数 request.required.acks 来设置。这种机制可以确保：只要 ISR 中有一个或者以上的 follower，一条被 commit 的消息就不会丢失。 问题 1：如何在保证可靠性的前提下避免吞吐量下降？ 有一个很重要的问题是当 leader 宕机了，怎样在 follower 中选举出新的 leader，因为 follower 可能落后很多或者直接 crash 了，所以必须确保选择 “最新” 的 follower 作为新的 leader。一个基本的原则就是，如果 leader 挂掉，新的 leader 必须拥有原来的 leader 已经 commit 的所有消息，这不就是 ISR 中副本的特征吗？ 但是，存在一个问题，ISR 列表维持多大的规模合适呢？换言之，leader 在一个消息被 commit 前需要等待多少个 follower 确认呢？等待 follower 的数量越多，与 leader 保持同步的 follower 就越多，可靠性就越高，但这也会造成吞吐率的下降。 少数服从多数的选举原则 一种常用的选举 leader 的策略是 “少数服从多数” ，不过，Kafka 并不是采用这种方式。这种模式下，如果有 2f+1 个副本，那么在 commit 之前必须保证有 f+1 个 replica 复制完消息，同时为了保证能正确选举出新的 leader，失败的副本数不能超过 f 个。这种方式有个很大的优势，系统的延迟取决于最快的几台机器，也就是说比如副本数为 3，那么延迟就取决于最快的那个 follower 而不是最慢的那个。 “少数服从多数” 的策略也有一些劣势，为了保证 leader 选举的正常进行，它所能容忍的失败的 follower 数比较少，如果要容忍 1 个 follower 挂掉，那么至少要 3 个以上的副本，如果要容忍 2 个 follower 挂掉，必须要有 5 个以上的副本。也就是说，在生产环境下为了保证较高的容错率，必须要有大量的副本，而大量的副本又会在大数据量下导致性能的急剧下降。这种算法更多用在 ZooKeeper 这种共享集群配置的系统中，而很少在需要大量数据的系统中使用。 Kafka 选举 leader 的策略是怎样的？ 实际上，leader 选举的算法非常多，比如 ZooKeeper 的 Zab、Raft 以及 Viewstamped Replication。而 Kafka 所使用的 leader 选举算法更像是微软的 PacificA 算法。 Kafka 在 ZooKeeper 中为每一个 partition 动态的维护了一个 ISR，这个 ISR 里的所有 replica 都与 leader 保持同步，只有 ISR 里的成员才能有被选为 leader 的可能（通过参数配置：unclean.leader.election.enable=false）。在这种模式下，对于 f+1 个副本，一个 Kafka topic 能在保证不丢失已经 commit 消息的前提下容忍 f 个副本的失败，在大多数使用场景下，这种模式是十分有利的。事实上，对于任意一条消息，只有它被 ISR 中的所有 follower 都从 leader 复制过去才会被认为已提交，并返回信息给 producer，从而保证可靠性。但与 “少数服从多数” 策略不同的是，Kafka ISR 列表中副本的数量不需要超过副本总数的一半，即不需要满足 “多数派” 原则，通常，ISR 列表副本数大于等于 2 即可，如此，便在可靠性和吞吐量方面取得平衡。 极端情况下的 leader 选举策略 前已述及，当 ISR 中至少有一个 follower 时（ISR 包括 leader），Kafka 可以确保已经 commit 的消息不丢失，但如果某一个 partition 的所有 replica 都挂了，自然就无法保证数据不丢失了。这种情况下如何进行 leader 选举呢？通常有两种方案： 等待 ISR 中任意一个 replica 恢复过来，并且选它作为 leader； 选择第一个恢复过来的 replica（并不一定是在 ISR 中）作为leader。（默认） 如何选择呢？这就需要在可用性和一致性当中作出抉择。如果一定要等待 ISR 中的 replica 恢复过来，不可用的时间就可能会相对较长。而且如果 ISR 中所有的 replica 都无法恢复了，或者数据丢失了，这个 partition 将永远不可用。 选择第一个恢复过来的 replica 作为 leader，如果这个 replica 不是 ISR 中的 replica，那么，它可能并不具备所有已经 commit 的消息，从而造成消息丢失。默认情况下，Kafka 采用第二种策略，即 unclean.leader.election.enable=true，也可以将此参数设置为 false 来启用第一种策略。 unclean.leader.election.enable 这个参数对于 leader 的选举、系统的可用性以及数据的可靠性都有至关重要的影响。生产环境中应慎重权衡。 3. Kafka 架构中 ZooKeeper 以怎样的形式存在？ZooKeeper 是一个分布式的、开放源码的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现。分布式应用程序可以基于它实现统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等工作。在基于 Kafka 的分布式消息队列中，ZooKeeper 的作用有：broker 注册、topic 注册、producer 和 consumer 负载均衡、维护 partition 与 consumer 的关系、记录消息消费的进度以及 consumer 注册等。 3.1 broker 在 ZooKeeper 中的注册 为了记录 broker 的注册信息，在 ZooKeeper 上，专门创建了属于 Kafka 的一个节点，其路径为 /brokers； Kafka 的每个 broker 启动时，都会到 ZooKeeper 中进行注册，告诉 ZooKeeper 其 broker.id，在整个集群中，broker.id 应该全局唯一，并在 ZooKeeper 上创建其属于自己的节点，其节点路径为 /brokers/ids/{broker.id}； 创建完节点后，Kafka 会将该 broker 的 broker.name 及端口号记录到该节点； 另外，该 broker 节点属性为临时节点，当 broker 会话失效时，ZooKeeper 会删除该节点，这样，我们就可以很方便的监控到broker 节点的变化，及时调整负载均衡等。 3.2 Topic 在 ZooKeeper 中的注册在 Kafka 中，所有 topic 与 broker 的对应关系都由 ZooKeeper 进行维护，在 ZooKeeper 中，建立专门的节点来记录这些信息，其节点路径为 /brokers/topics/{topic_name}。 前面说过，为了保障数据的可靠性，每个 Topic 的 Partitions 实际上是存在备份的，并且备份的数量由 Kafka 机制中的 replicas 来控制。那么问题来了：如下图所示，假设某个 TopicA 被分为 2 个 Partitions，并且存在两个备份，由于这 2 个 Partitions（1-2）被分布在不同的 broker 上，同一个 partiton 与其备份不能（也不应该）存储于同一个 broker 上。以 Partition1 为例，假设它被存储于 broker2，其对应的备份分别存储于 broker1 和 broker4，有了备份，可靠性得到保障，但数据一致性却是个问题。 为了保障数据的一致性，ZooKeeper 机制得以引入。基于 ZooKeeper，Kafka 为每一个 partition 找一个节点作为 leader，其余备份作为 follower；接续上图的例子，就 TopicA 的 partition1 而言，如果位于 broker2（Kafka 节点）上的 partition1 为 leader，那么位于 broker1 和 broker4 上面的 partition1 就充当 follower，则有下图： 基于上图的架构，当 producer push 的消息写入 partition（分区) 时，作为 leader 的 broker（Kafka 节点） 会将消息写入自己的分区，同时还会将此消息复制到各个 follower，实现同步。如果，某个follower 挂掉，leader 会再找一个替代并同步消息；如果 leader 挂了，follower 们会选举出一个新的 leader 替代，继续业务，这些都是由 ZooKeeper 完成的。 3.3 consumer 在 ZooKeeper 中的注册注册新的消费者分组 当新的消费者组注册到 ZooKeeper 中时，ZooKeeper 会创建专用的节点来保存相关信息，其节点路径为 ls/consumers/{group_id}，其节点下有三个子节点，分别为 [ids, owners, offsets]。 ids 节点：记录该消费组中当前正在消费的消费者； owners 节点：记录该消费组消费的 topic 信息； offsets 节点：记录每个 topic 的每个分区的 offset。 注册新的消费者 当新的消费者注册到 Kafka 中时，会在 /consumers/{group_id}/ids 节点下创建临时子节点，并记录相关信息。 监听消费者分组中消费者的变化 每个消费者都要关注其所属消费者组中消费者数目的变化，即监听 /consumers/{group_id}/ids 下子节点的变化。一旦发现消费者新增或减少，就会触发消费者的负载均衡。 3.4 Producers 负载均衡对于同一个 topic 的不同 partition，Kafka会尽力将这些 partition 分布到不同的 broker 服务器上，这种均衡策略实际上是基于 ZooKeeper 实现的。在一个 broker 启动时，会首先完成 broker 的注册过程，并注册一些诸如 “有哪些可订阅的 topic” 之类的元数据信息。producers 启动后也要到 ZooKeeper 下注册，创建一个临时节点来监听 broker 服务器列表的变化。由于在 ZooKeeper 下 broker 创建的也是临时节点，当 brokers 发生变化时，producers 可以得到相关的通知，从改变自己的 broker list。其它的诸如 topic 的变化以及broker 和 topic 的关系变化，也是通过 ZooKeeper 的这种 Watcher 监听实现的。 在生产中，必须指定 topic；但是对于 partition，有两种指定方式： 明确指定 partition(0-N)，则数据被发送到指定 partition； 设置为 RD_KAFKA_PARTITION_UA，则 Kafka 会回调 partitioner 进行均衡选取，partitioner 方法需要自己实现。可以轮询或者传入 key 进行 hash。未实现则采用默认的随机方法 rd_kafka_msg_partitioner_random 随机选择。 3.5 Consumer 负载均衡Kafka 保证同一 consumer group 中只有一个 consumer 可消费某条消息，实际上，Kafka 保证的是稳定状态下每一个 consumer 实例只会消费某一个或多个特定的数据，而某个 partition 的数据只会被某一个特定的 consumer 实例所消费。这样设计的劣势是无法让同一个 consumer group 里的 consumer 均匀消费数据，优势是每个 consumer 不用都跟大量的 broker 通信，减少通信开销，同时也降低了分配难度，实现也更简单。另外，因为同一个 partition 里的数据是有序的，这种设计可以保证每个 partition 里的数据也是有序被消费。 consumer 数量不等于 partition 数量 如果某 consumer group 中 consumer 数量少于 partition 数量，则至少有一个 consumer 会消费多个 partition 的数据；如果 consumer 的数量与 partition 数量相同，则正好一个 consumer 消费一个 partition 的数据，而如果 consumer 的数量多于 partition 的数量时，会有部分 consumer 无法消费该 topic 下任何一条消息。 借助 ZooKeeper 实现负载均衡 关于负载均衡，对于某些低级别的 API，consumer 消费时必须指定 topic 和 partition，这显然不是一种友好的均衡策略。基于高级别的 API，consumer 消费时只需制定 topic，借助 ZooKeeper 可以根据 partition 的数量和 consumer 的数量做到均衡的动态配置。 consumers 在启动时会到 ZooKeeper 下以自己的 conusmer-id 创建临时节点 /consumer/[group-id]/ids/[conusmer-id]，并对 /consumer/[group-id]/ids 注册监听事件，当消费者发生变化时，同一 group 的其余消费者会得到通知。当然，消费者还要监听 broker 列表的变化。librdkafka 通常会将 partition 进行排序后，根据消费者列表，进行轮流的分配。 3.6 记录消费进度 Offset在 consumer 对指定消息 partition 的消息进行消费的过程中，需要定时地将 partition 消息的消费进度 Offset 记录到 ZooKeeper上，以便在该 consumer 进行重启或者其它 consumer 重新接管该消息分区的消息消费权后，能够从之前的进度开始继续进行消息消费。Offset 在 ZooKeeper 中由一个专门节点进行记录，其节点路径为： 12#节点内容就是Offset的值。/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id] PS：Kafka 已推荐将 consumer 的 Offset 信息保存在 Kafka 内部的 topic 中，即： 1__consumer_offsets(/brokers/topics/__consumer_offsets) 并且默认提供了 kafka_consumer_groups.sh 脚本供用户查看consumer 信息（命令：sh kafka-consumer-groups.sh –bootstrap-server * –describe –group *）。在当前版本中，offset 存储方式要么存储在本地文件中，要么存储在 broker 端，具体的存储方式取决 offset.store.method 的配置，默认是存储在 broker 端。 3.7 记录 Partition 与 Consumer 的关系consumer group 下有多个 consumer（消费者），对于每个消费者组（consumer group），Kafka都会为其分配一个全局唯一的 group ID，group 内部的所有消费者共享该 ID。订阅的 topic 下的每个分区只能分配给某个 group 下的一个consumer（当然该分区还可以被分配给其它 group）。同时，Kafka 为每个消费者分配一个 consumer ID，通常采用 hostname:UUID 形式表示。 在Kafka中，规定了每个 partition 只能被同组的一个消费者进行消费，因此，需要在 ZooKeeper 上记录下 partition 与 consumer 之间的关系，每个 consumer 一旦确定了对一个 partition 的消费权力，需要将其 consumer ID 写入到 ZooKeeper 对应消息分区的临时节点上，例如： 1/consumers/[group_id]/owners/[topic]/[broker_id-partition_id] 其中，[broker_id-partition_id] 就是一个消息分区的标识，节点内容就是该消息分区 消费者的 consumer ID。 4. 全程解析（Producer-kafka-consumer）4.1 producer 发布消息producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。 其路由机制为： 指定了 patition，则直接使用； 未指定 patition 但指定 key，通过对 key 进行 hash 选出一个 patition； patition 和 key 都未指定，使用轮询选出一个 patition。 写入流程： producer 先从 ZooKeeper 的 “/brokers/…/state” 节点找到该 partition 的leader； producer 将消息发送给该 leader； leader 将消息写入本地 log； followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK； leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK； 4.2 Broker 存储消息物理上把 topic 分成一个或多个 patition，每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件） 4.3 Consumer 消费消息high-level consumer API 提供了 consumer group 的语义，一个消息只能被 group 内的一个 consumer 所消费，且 consumer 消费消息时不关注 offset，最后一个 offset 由 ZooKeeper 保存（下次消费时，该group 中的consumer将从offset记录的位置开始消费）。 注意： 如果消费线程大于 patition 数量，则有些线程将收不到消息； 如果 patition 数量大于消费线程数，则有些线程多收到多个 patition 的消息； 如果一个线程消费多个 patition，则无法保证你收到的消息的顺序，而一个 patition 内的消息是有序的。 consumer 采用 pull 模式从 broker 中读取数据。 push 模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。 对于 Kafka 而言，pull 模式更合适，它可简化 broker 的设计，consumer 可自主控制消费消息的速率，同时 consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 上述文章参考自：深入浅出理解基于 Kafka 和 ZooKeeper 的分布式消息队列]]></content>
      <categories>
        <category>MQ</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中sleep和wait的区别]]></title>
    <url>%2F2019%2F06%2F17%2Fjava-sleep-wait%2F</url>
    <content type="text"><![CDATA[sleep() &amp; wait() sleep()方法 wait()方法 sleep()使当前线程进入停滞状态（阻塞当前线程，即会发生上下文的切换，会有很大的开销），让出CUP的使用，目的是不让当前线程独自霸占该进程所获的CPU资源，以留一定时间给其他线程执行的机会 wait()方法是Object类里的方法；当一个线程执行到wait()方法时，它就进入到一个和该对象相关的等待池中，同时失去（释放）了对象的机锁（暂时失去机锁，wait(long timeout)超时时间到后还需要返还对象锁）；其他线程可以访问 sleep()是Thread类的static(静态)的方法；因此它不能改变对象的锁，所以当在一个synchronized块中调用sleep()方法是，线程虽然休眠了，但是对象的机锁并没有被释放，其他线程无法获取对象锁（即使睡着也持有对象锁） wait()使用notify或者notifyAlll或者指定睡眠时间来唤醒当前等待池中的线程 在sleep()休眠时间期满后，该线程不一定会立即执行，这是因为其它线程可能正在运行而且没有被调度为放弃执行，除非此线程具有更高的优先级 wait()必须放在synchronized block中，否则会在program runtime时抛出”java.lang.IllegalMonitorStateException“异常 ## 区别 1sleep（）和wait()这两个函数被调用之后线程都应该放弃执行权，不同的是sleep（）不释放锁而wait（）的话是释放锁。直白的意思是一个线程调用Sleep（）之后进入了阻塞状态，它的意思就是当sleep()状态超时，线程重新转入可运行(runnable)状态。而Wait（）在释放执行权之后也把锁释放了,通过notify()或者notifyAll()或者指定睡眠时间来唤醒后，它要运行的话还是要和其他的线程去竞争锁，之后才可以获得执行权。 1234所以sleep()和wait()方法的最大区别是： sleep()睡眠时，保持对象锁，仍然占有该锁； 而wait()睡眠时，释放对象锁。 但是wait()和sleep()都可以通过interrupt()方法打断线程的暂停状态，从而使线程立刻抛出InterruptedException（但不建议使用该方法）。 Java中sleep方法的几个注意点 Thread.sleep()方法用来暂停线程的执行，将CPU放给线程调度器。 Thread.sleep()方法是一个静态方法，它暂停的是当前执行的线程。 Java有两种sleep方法，一个只有一个毫秒参数，另一个有毫秒和纳秒两个参数。 与wait方法不同，sleep方法不会释放锁。 如果其他的线程中断了一个休眠的线程，sleep方法会抛出Interrupted Exception。 休眠的线程在唤醒之后不保证能获取到CPU，它会先进入就绪态，与其他线程竞争CPU。 有一个易错的地方，当调用t.sleep()的时候，会暂停线程t。这是不对的，因为Thread.sleep是一个静态方法，它会使当前线程而不是线程t进入休眠状态。 wait方法必须正在同步环境下使用，比如synchronized方法或者同步代码块。如果你不在同步条件下使用，会抛出IllegalMonitorStateException异常。另外，sleep方法不需要再同步条件下调用，你可以任意正常的使用。 wait方法用于和定义于Object类的，而sleep方法操作于当前线程，定义在java.lang.Thread类里面。 参考自： Java sleep和wait的区别 在阻塞式io中，如果一个线程在等待io操作，那么cpu还会分配时间片给该线程吗？]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>sleep</tag>
        <tag>wait</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reactor线程模型]]></title>
    <url>%2F2019%2F06%2F17%2FReactor-thread-model%2F</url>
    <content type="text"><![CDATA[Reactor是什么? The reactor design_pattern is an event_handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to the associated request handlers. from wiki 通过wiki中的定义我们可以发现Reactor的重点 事件驱动 可以处理一个或多个输入源 通过多路复用将请求的事件分发给对应的处理器处理 根据大神Doug Lea 在 《Scalable IO in Java 》中的介绍，Reacotr模型主要分为三个角色 Reactor：把IO事件分配给对应的handler处理 Acceptor：处理客户端连接事件 Handler：处理非阻塞的任务 为什么使用Reactor？ 传统阻塞IO模型的不足 每个连接都需要独立线程处理，当并发数大时，创建线程数多，占用资源 采用阻塞IO模型，连接建立后，若当前线程没有数据可读，线程会阻塞（阻塞会有切换进程或线程上下文的开销）在读操作上，造成资源浪费 针对传统阻塞IO模型的两个问题，可以采用如下的方案 基于池化思想，避免为每个连接创建线程，连接完成后将业务处理交给线程池处理 基于IO复用模型，多个连接共用同一个阻塞对象，不用等待所有的连接。遍历到有新数据可以处理时，操作系统会通知程序，线程跳出阻塞状态，进行业务逻辑处理 Reactor线程模型的思想就是基于IO复用和线程池（线程复用）的结合 Reactor线程模型分类根据Reactor的数量和处理资源的线程数量的不同，分为三类： 单Reactor单线程模型 单Reactor多线程模型 多Reactor多线程模型 单Reactor单线程模型 这种模型在Reactor中处理事件，并分发事件，如果是连接事件交给acceptor处理，如果是读写事件和业务处理就交给handler处理，但始终只有一个线程执行所有的事情 该线程模型的不足 仅用一个线程处理请求，对于多核资源机器来说是有点浪费的 当处理读写任务的线程负载过高后，处理速度下降，事件会堆积，严重的会超时，可能导致客户端重新发送请求，性能越来越差 单线程也会有可靠性的问题 针对上面的种种不足，就有了下面的线程模型 单Reactor多线程模型 这种模型和第一种模型到的主要区别是把业务处理从之前的单一线程脱离出来，换成线程池处理，也就是Reactor线程只处理连接事件和读写事件，业务处理交给线程池处理，充分利用多核机器的资源、提高性能并且增加可靠性 该线程模型的不足 Reactor线程承担所有的事件，例如监听和响应，高并发场景下单线程存在性能问题 多Reactor多线程模型 这种模型下和第二种模型相比是把Reactor线程拆分了mainReactor和subReactor两个部分，mainReactor只处理连接事件，读写事件交给subReactor来处理。业务逻辑还是由线程池来处理 mainRactor只处理连接事件，用一个线程来处理就好。处理读写事件的subReactor个数一般和CPU数量相等，一个subReactor对应一个线程，业务逻辑由线程池处理 这种模型使各个模块职责单一，降低耦合度，性能和稳定性都有提高 这种模型在许多项目中广泛应用，比如Netty的主从线程模型等 Reactor三种模式形象比喻餐厅一般有接待员和服务员，接待员负责在门口接待顾客，服务员负责全程服务顾客 Reactor的三种线程模型可以用接待员和服务员类比 单Reactor单线程模型：接待员和服务员是同一个人，一直为顾客服务。客流量较少适合 单Reactor多线程模型：一个接待员，多个服务员。客流量大，一个人忙不过来，由专门的接待员在门口接待顾客，然后安排好桌子后，由一个服务员一直服务，一般每个服务员负责一片中的几张桌子 多Reactor多线程模型：多个接待员，多个服务员。这种就是客流量太大了，一个接待员忙不过来了 参考资料 《Scalable IO in Java》 -Doug Lea 【关注公众号，回复“Doug Lea” 获取该pdf】 1文章来源微信公众号：每天晒白牙]]></content>
      <categories>
        <category>IO模型</category>
      </categories>
      <tags>
        <tag>Reactor</tag>
        <tag>IO模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux grep命令详解]]></title>
    <url>%2F2019%2F06%2F16%2Flinux-grep%2F</url>
    <content type="text"><![CDATA[一篇讲解grep很好的文章，以便日后查阅。 文章来源：grep命令详解]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中永久代的垃圾回收]]></title>
    <url>%2F2019%2F06%2F16%2Fjava-perm-GC%2F</url>
    <content type="text"><![CDATA[今天面试被问到jvm中永久代会发生垃圾回收吗？ 首先，关于永久代的内容可以看这个：[jvm中方法区和永久代的关系] 垃圾回收不会出现在永久代，但是如果永久代满了会触发完全垃圾回收（Full GC）。 Hotspot的永久代是在方法区，主要存储的是类加载信息，静态变量以及常量，方法（字节码）等等，可以进行常量池回收和类型卸载。 如果这个常量在其它任何对象都没被引用，则可以被回收。 而类型卸载有点复杂，有以下三点要求： 该类型的所有实例都已经被回收 该类型的ClassLoader已经被回收 该类型的java.lang.Class没有在任何地方被引用，该类型不能在任何地方以反射的方式实例化一个对象。在java8中，已经取消了永久代，但是引入了一个元数据区的navite内存区。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>永久代</tag>
        <tag>metaspace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2019%2F06%2F16%2Fmarkdown-syntax%2F</url>
    <content type="text"><![CDATA[前言学习一下Markdown的语法，以便更好地组织文章结构。在此记录，以便日后的查阅。 主要内容 Markdown是什么？谁创造了它？为什么要使用它？怎么使用？谁在用？尝试一下 正文1. Markdown是什么？Markdown是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，并最终以HTML格式发布。Markdown也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。 2. 谁创造了它？它由Aaron Swartz和John Gruber共同设计，Aaron Swartz就是那位于去年（2013年1月11日）自杀,有着开挂一般人生经历的程序员。维基百科对他的介绍是：软件工程师、作家、政治组织者、互联网活动家、维基百科人。 他有着足以让你跪拜的人生经历： 14岁参与RSS 1.0规格标准的制订。 2004年入读斯坦福，之后退学。 2005年创建Infogami，之后与Reddit合并成为其合伙人。 2010年创立求进会（Demand Progress），积极参与禁止网络盗版法案（SOPA）活动，最终该提案被撤回。 2011年7月19日，因被控从MIT和JSTOR下载480万篇学术论文并以免费形式上传于网络被捕。 2013年1月自杀身亡。 天才都有早逝的归途。 3. 为什么要使用它？ 它是易读（看起来舒服）、易写（语法简单）、易更改纯文本。处处体现着极简主义的影子。 兼容HTML，可以转换为HTML格式发布。 跨平台使用。 越来越多的网站支持Markdown。 更方便清晰地组织你的电子邮件。（Markdown-here, Airmail） 摆脱Word（我不是认真的）。 4. 怎么使用？如果不算扩展，Markdown的语法绝对简单到让你爱不释手。 Markdown语法主要分为如下几大部分： 标题，段落，区块引用，代码区块，强调，列表，分割线，链接，图片，反斜杠 \，符号’`’。 4.1 标题两种形式：1）使用=和-标记一级和二级标题。 一级标题=========二级标题--------- 效果： 一级标题二级标题 2）使用#，可表示1-6级标题。 一级标题二级标题三级标题四级标题五级标题六级标题 效果： 一级标题二级标题三级标题四级标题五级标题六级标题 4.2 段落段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用两个以上空格加上回车（引用中换行省略回车）。 4.3 区块引用在段落的每行或者只在第一行使用符号&gt;,还可使用多个嵌套引用，如： &gt; 区块引用&gt;&gt; 嵌套引用 效果： 区块引用 嵌套引用 4.4 代码区块代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如普通段落： void main(){ printf(“Hello, Markdown.”);} 代码区块： 1234void main()&#123; printf(&quot;Hello, Markdown.&quot;);&#125; 注意:需要和普通段落之间存在空行。 4.5 强调在强调内容两侧分别加上*或者_，如： 斜体，斜体粗体，粗体 效果： 斜体，斜体粗体，粗体 4.6 列表使用·、+、或-标记无序列表，如： -（+） 第一项 -（+） 第二项 - （+*）第三项 注意：标记有一个_空格或制表符_。若不在引用区块中，必须和前方段落之间存在空行。 效果： 第一项 第二项 第三项 有序列表的标记方式是将上述的符号换成数字,并辅以.，如： 1 . 第一项2 . 第二项3 . 第三项 效果： 第一项 第二项 第三项 4.7 分割线分割线最常使用就是三个或以上*，还可以使用-和_。 4.8 链接链接可以由两种形式生成：行内式和参考式。 行内式： younghz的Markdown库。 效果： younghz的Markdown库。 参考式： younghz的Markdown库1younghz的Markdown库2 效果： younghz的Markdown库1younghz的Markdown库2 注意：上述的[1]:https:://github.com/younghz/Markdown &quot;Markdown&quot;不出现在区块中。 4.9 图片添加图片的形式和链接相似，只需在链接的基础上前方加一个！。 4.10 反斜杠\相当于反转义作用。使符号成为普通符号。 4.11 符号’`’起到标记作用。如： 12&gt; ctrl+a&gt; 效果： 12&gt; ctrl+a&gt; 4.12 上标、下标markdown是支持HTML语法的，所以可以用HTML的语法表示下标、下标等。 上标： &lt;sup&gt;xxx&lt;/sup&gt;语法：2&lt;sup&gt;32&lt;/sup&gt; - 1效果：232 - 1 下标： &lt;sub&gt;xxx&lt;/sub&gt;语法：a=log&lt;sub&gt;2&lt;/sub&gt;b效果：a=log2b 5. 谁在用？Markdown的使用者： GitHub 简书 Stack Overflow Apollo Moodle Reddit 等等 6. 尝试一下 Chrome下的插件诸如stackedit与markdown-here等非常方便，也不用担心平台受限。 在线的dillinger.io评价也不错 Windowns下的MarkdownPad也用过，不过免费版的体验不是很好。 Mac下的Mou是国人贡献的，口碑很好。 Linux下的ReText不错。 当然，最终境界永远都是笔下是语法，心中格式化 :)。 注意：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。 虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber] (http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/ )。 以上基本是所有traditonal markdown的语法。 其它：列表的使用(非traditonal markdown)： 用|表示表格纵向边界，表头和表内容用-隔开，并可用:进行对齐设置，两边都有:则表示居中，若不加:则默认左对齐。 代码库 链接 MarkDown https://github.com/younghz/Markdown MarkDownCopy https://github.com/younghz/Markdown 关于其它扩展语法可参见具体工具的使用说明。 1以上内容来源自：https://github.com/younghz/Markdown]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm中方法区和永久代的关系]]></title>
    <url>%2F2019%2F06%2F16%2Fjvm-MethodArea-PERM%2F</url>
    <content type="text"><![CDATA[前言知道有方法区，知道里面存放的是什么东西。知道有永久代，也知道它在jdk1.7和jdk1.8的区别。但是对他们的关系有点稀里糊涂。。。 什么是方法区？方法区（Method Area）是jvm规范里面的运行时数据区的一个组成部分，jvm规范中的运行时数据区还包含了：pc寄存器、虚拟机栈、堆、方法区、运行时常量池、本地方法栈，还应该有堆外内存。 方法区存储什么？主要用来存储class、运行时常量池、字段、方法、代码、JIT代码等。 注意： 运行时数据区跟内存不是一个概念。 方法区是运行时数据区的一部分。 方法区是jvm规范中的一部分，并不是实际的实现，切忌将规范跟实现混为一谈。 永久代（Perm区）永久代又叫Perm区，只存在于hotspot jvm中，并且只存在于jdk7和之前的版本中，jdk8中已经彻底移除了永久代，jdk8中引入了一个新的内存区域叫metaspace。 并不是所有的jvm中都有永久代，ibm的j9，oracle的JRocket都没有永久代。 永久代是实现层面的东西。 永久代里面存的东西基本上就是方法区规定的那些东西。 因此，我们可以说，永久代是方法区的一种实现，当然，在hotspot jdk8中metaspace可以看成是方法区的一种实现。 下面我们来看下hotspot jdk8中移除了永久代以后的内存结构： 结论 方法区是规范层面的东西，规定了这一个区域要存放哪些东西 永久代（Hotspot虚拟机特有的概念）或者是metaspace是对方法区的不同实现，是实现层面的东西。 1234作者：若鱼1919链接：https://www.imooc.com/article/47149来源：慕课网本文原创发布于慕课网 ，转载请注明出处，谢谢合作]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>永久代</tag>
        <tag>metaspace</tag>
        <tag>jvm</tag>
        <tag>方法区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产者-消费者问题详解]]></title>
    <url>%2F2019%2F06%2F15%2Fproducer-consumer%2F</url>
    <content type="text"><![CDATA[文章参考自：生产者-消费者问题详解 一、明确定义要理解生产消费者问题，首先应弄清PV操作的含义：PV操作是由P操作原语和V操作原语组成（原语是不可中断的过程），对信号量进行操作，具体定义如下： P（S）：①将信号量S的值减1，即S=S-1； ​ ②如果S&gt;=0，则该进程继续执行；否则该进程置为等待状态，排入等待队列。 1解读：P操作可以理解为申请资源。P操作每次申请一个资源（S = S - 1，可用资源个数减少1），如果可用资源的个数大于等于0（S&gt;=0），那么说明本次申请资源操作成功，继续执行后续程序。否则说明没有足够的资源供该进程使用，该进程置为等待状态，加入等待该资源的等待队列。 V（S）：①将信号量S的值加1，即S=S+1； ​ ②如果S&gt;0，则该进程继续执行；否则释放队列中第一个等待信号量的进程。 1解读：V操作可以理解为释放资源。V操作每次释放一个资源（S = S + 1，可用资源个数增加1），如果可用资源的个数大于0（S &gt; 0），说明本次释放资源操作成功，继续执行后续程序。否则释放该资源的等待队列中第一个等待信号量的进程。 这只是书本的定义，对于这部分内容，老师先不要急于解释上面的程序流程，而是应该让学生首先知道P操作与V操作到底有什么作用。 P操作相当于申请资源，而V操作相当于释放资源。所以要学生记住以下几个关键字： 123P操作-----&gt;申请资源V操作-----&gt;释放资源 二、形象启发为此举两个生活中的例子： 例一：在公共电话厅打电话 公共电话厅里有多个电话，如某人要打电话，首先要进行申请，看是否有电话空闲，若有，则可以使用电话，如果电话亭里所有电话都有人正在使用，那后来的人只有排队等候。当某人用完电话后，则有空电话腾出，正在排队的第一个人就可以使用电话。这就相当于PV操作： 123某人要打电话，首先要进行申请，相当于执行一次P操作，申请一个可用资源（电话）；某人用完电话，则有空电话腾出，相当于执行一次V操作，释放一个可用资源（电话）。 三、分层解剖在理解了PV操作的的含义后，就必须讲解利用PV操作可以实现进程的两种情况：互斥和同步。根据互斥和同步不同的特点，就有利用PV操作实现互斥与同步相对固定的结构模式。这里就不详细讲解了。但生产者-消费者问题是一个有代表性的进程同步问题。但是如果我们将问题细分成三种情况进行讲解，理解难度将大大降低。 1）一个生产者，一个消费者，公用一个缓冲区。可以作以下比喻：将一个生产者比喻为一个生产厂家，如伊利牛奶厂家，而一个消费者，比喻是学生小明，而一个缓冲区则比喻成一间好又多(类似于商店)。第一种情况，可以理解成伊利牛奶生产厂家生产一盒牛奶，把它放在好又多一分店进行销售，而小明则可以从那里买到这盒牛奶。只有当厂家把牛奶放在商店里面后，小明才可以从商店里买到牛奶。所以很明显这是最简单的同步问题。 解题如下： 定义两个同步信号量： 123empty——表示缓冲区是否为空，初值为1。full——表示缓冲区中是否为满，初值为0。 生产者进程 123456while(TRUE)&#123; 生产一个产品; P(empty); 产品送往Buffer; V(full);&#125; 生产者行为： 生成一个商品 拿到 empty 同步信号量，执行P操作（empty -= 1，empty现在等于0，表示不为空），表示缓冲区有数据 产品运送Buffer 拿到 full 同步信号量，执行V操作（full += 1，full现在等于1，表示满），表示缓冲区有数据 消费者进程 123456while(TRUE)&#123; P(full); 从Buffer取出一个产品; V(empty); 消费该产品;&#125; 消费者行为： 拿到 full 同步信号量，执行P操作（full -= 1，full现在等于0，表示不满），表示缓冲区无数据 从Buffer取出一个产品 拿到 empty 同步信号量，执行V操作（empty += 1，empty现在等于1，表示空），表示缓冲区无数据 消费该产品 2）一个生产者，一个消费者，公用n个环形缓冲区。第二种情况可以理解为伊利牛奶生产厂家可以生产好多牛奶，并将它们放在多个好又多分店进行销售，而小明可以从任一间好又多分店中购买到牛奶。同样，只有当厂家把牛奶放在某一分店里，小明才可以从这间分店中买到牛奶。不同于第一种情况的是，第二种情况有N个分店（即N个缓冲区形成一个环形缓冲区），所以要利用指针，要求厂家必须按一定的顺序将商品依次放到每一个分店中。缓冲区的指向则通过模运算得到。 解题如下： 定义两个同步信号量： 123empty——表示缓冲区是否为空，初值为n。full——表示缓冲区中是否为满，初值为0。 设缓冲区的编号为1～n-1，定义两个指针in和out，分别是生产者进程和消费者进程使用的指针，指向下一个可用的缓冲区。 生产者进程 1234567while(TRUE)&#123; 生产一个产品; P(empty); 产品送往buffer（in）； in=(in+1)mod n； V(full);&#125; 消费者进程 1234567while(TRUE)&#123; P(full); 从buffer（out）中取出产品； out=(out+1)mod n； V(empty); 消费该产品;&#125; 3）一组生产者，一组消费者，公用n个环形缓冲区第三种情况，可以理解成有多间牛奶生产厂家，如蒙牛，达能，光明等，消费者也不只小明一人，有许许多多消费者。不同的牛奶生产厂家生产的商品可以放在不同的好又多分店中销售，而不同的消费者可以去不同的分店中购买。当某一分店已放满某个厂家的商品时，下一个厂家只能把商品放在下一间分店。所以在这种情况中，生产者与消费者存在同步关系，而且各个生产者之间、各个消费者之间存在互斥关系,他们必须互斥地访问缓冲区。（得好好揣摩这句话） 解题如下： 定义四个信号量： 1234567empty——表示缓冲区是否为空，初值为n。full——表示缓冲区中是否为满，初值为0。mutex1——生产者之间的互斥信号量，初值为1。mutex2——消费者之间的互斥信号量，初值为1。 设缓冲区的编号为1～n-1，定义两个指针in和out，分别是生产者进程和消费者进程使用的指针，指向下一个可用的缓冲区。 生产者进程 123456789while(TRUE)&#123; 生产一个产品; P(empty); //生成产品，那么缓冲区一定不为空 P(mutex1)；//同一时间只能有一个生产者生成商品 产品送往buffer（in）； in=(in+1)mod n； V(mutex1); V(full);&#125; 消费者进程 12345678while(TRUE)&#123; P(full); P(mutex2)； 从buffer（out）中取出产品； out=(out+1)mod n； V（mutex2）； V(empty);&#125;]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>生产者&amp;消费者</tag>
        <tag>PV操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitignore]]></title>
    <url>%2F2019%2F06%2F15%2Fgitignore%2F</url>
    <content type="text"><![CDATA[下面文章参考自：Git 忽略提交 .gitignore 前言在使用Git的过程中，有的文件比如日志，临时文件，编译的中间文件等不需要提交到代码仓库，这时就要设置相应的忽略规则，来忽略这些文件的提交。 规则 作用 /mtk 过滤整个文件夹 *.zip 过滤所有.zip文件 /mtk/do.c 过滤某个具体文件 !/mtk/one.txt 追踪（不过滤）某个具体文件 注意：如果你创建.gitignore文件之前就push了某一文件，那么即使你在.gitignore文件中写入过滤该文件的规则，该规则也不会起作用，git仍然会对该文件进行版本管理。 配置语法以斜杠“/”开头表示目录； 以星号“*”通配多个字符； 以问号“?”通配单个字符 以方括号“[]”包含单个字符的匹配列表； 以叹号“!”表示不忽略(跟踪)匹配到的文件或目录。 注意： git 对于 .gitignore配置文件是按行从上到下进行规则匹配的 Git 忽略文件提交的方法有三种方法可以实现忽略Git中不想提交的文件。 在Git项目中定义 .gitignore 文件这种方式通过在项目的某个文件夹下定义 .gitignore 文件，在该文件中定义相应的忽略规则，来管理当前文件夹下的文件的Git提交行为。 .gitignore 文件是可以提交到共有仓库中，这就为该项目下的所有开发者都共享一套定义好的忽略规则。 在 .gitingore 文件中，遵循相应的语法，在每一行指定一个忽略规则。如： 123*.log*.temp/vendor 在Git项目的设置中指定排除文件这种方式只是临时指定该项目的行为，需要编辑当前项目下的 .git/info/exclude 文件，然后将需要忽略提交的文件写入其中。 需要注意的是，这种方式指定的忽略文件的根目录是项目根目录。 定义Git全局的 .gitignore 文件除了可以在项目中定义 .gitignore 文件外，还可以设置全局的 git .gitignore 文件来管理所有Git项目的行为。这种方式在不同的项目开发者之间是不共享的，是属于项目之上Git应用级别的行为。 这种方式也需要创建相应的 .gitignore 文件，可以放在任意位置。然后在使用以下命令配置Git： 1git config --global core.excludesfile ~/.gitignore Git 忽略规则详细的忽略规则可以参考官方英文文档 Git 忽略规则优先级在 .gitingore 文件中，每一行指定一个忽略规则，Git 检查忽略规则的时候有多个来源，它的优先级如下（由高到低）： 从命令行中读取可用的忽略规则 当前目录定义的规则 父级目录定义的规则，依次地推 $GIT_DIR/info/exclude 文件中定义的规则 core.excludesfile中定义的全局规则 Git 忽略规则匹配语法在 .gitignore 文件中，每一行的忽略规则的语法如下： 空格不匹配任意文件，可作为分隔符，可用反斜杠转义 # 开头的模式标识注释，可以使用反斜杠进行转义 ! 开头的模式标识否定，该文件将会再次被包含，如果排除了该文件的父级目录，则使用 ! 也不会再次被包含。可以使用反斜杠进行转义 / 结束的模式只匹配文件夹以及在该文件夹路径下的内容，但是不匹配该文件 / 开始的模式匹配项目跟目录 如果一个模式不包含斜杠，则它匹配相对于当前 .gitignore 文件路径的内容，如果该模式不在 .gitignore 文件中，则相对于项目根目录 **匹配多级目录，可在开始，中间，结束 ?通用匹配单个字符 []通用匹配单个字符列表 常用匹配示例： bin/: 忽略当前路径下的bin文件夹，该文件夹下的所有内容都会被忽略，不忽略 bin 文件 /bin: 忽略根目录下的bin文件 /*.c: 忽略 cat.c，不忽略 build/cat.c debug/*.obj: 忽略 debug/io.obj，不忽略 debug/common/io.obj 和 tools/debug/io.obj **/foo: 忽略/foo, a/foo, a/b/foo等 a/**/b: 忽略a/b, a/x/b, a/x/y/b等 !/bin/run.sh: 不忽略 bin 目录下的 run.sh 文件 *.log: 忽略所有 .log 文件 config.php: 忽略当前路径的 config.php 文件 .gitignore规则不生效.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。 解决方法就是先把本地缓存删除（改变成未track状态），然后再提交: 123git rm -r --cached .git add .git commit -m &apos;update .gitignore&apos; 作者：王伟desire链接：https://www.jianshu.com/p/74bd0ceb6182来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js css 404]]></title>
    <url>%2F2019%2F06%2F15%2Fjs-404%2F</url>
    <content type="text"><![CDATA[问题描述本地调试Hexo的过程中，无意间打开google浏览器的开发者模式，然后发现很多有关 css、js404的错误： 问题分析首先打开Hexo所使用的主题所在目录（我用的是next6，点击这里下载），依次进入source/lib目录下，发现并没有 canvas-nest等文件夹，所以也并没有canvas-nest.min.js等文件，问题锁定。 问题解决打开Hexo所用主题所在目录，找到主题配置文件_config.yml，将其打开，然后搜索canvas_nest，结果如下图： canvas_nest下面 enable 属性为 true，说明开启了canvas_nest，但是并没有对应的lib支持，所以需要安装对应的lib。 安装方法切换到Hexo主题根目录下，我的是：D:\work\hexo\changsk\themes\next6。 看上图，写明了lib的下载地址：Dependencies: https://github.com/theme-next/theme-next-canvas-nest 只需下载即可，在这里利用 git clone，执行以下命令： 1git clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest note：source/lib/canvas-nest 表示下载的lib存放的位置，next6的lib资源文件都放在source/lib下，canvas-nest文件夹的名称要和报错信息里面文件夹名称一致，不然还是会报404错误，找不到该资源文件。 当然也可以手动进行下载，然后放到正确的位置。执行完成后，可以发现lib下面多了个文件夹，里面有前端所需要的canvas-nest.min.js然后执行 123hexo cleanhexo ghexo s 进行本地调试，发现canvas-nest.min.js 404错误已经消失，其他类似的错误都可以通过这种方式解决。 后记执行git clone *之后，下载目录下面会有 *.git文件夹，最好把它删掉。 因为如果进行hexo源文件备份的话，会把整个hexo源文件push到github仓库，包括theme文件夹，因为theme文件夹/source/lib/的一些lib是通过*git clone的方式获取的。这种方式下载的lib会在文件夹下面生成.git。那么进行hexo备份的时候，会发现有多个.git**文件夹存在，就会报错，详情看这里]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>404</tag>
        <tag>css</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 创建分支]]></title>
    <url>%2F2019%2F06%2F12%2Fgit-command%2F</url>
    <content type="text"><![CDATA[搭建好本地 Hexo ，然后链接到了 github pages，也绑定了域名changsk.top 。但是 github 博客仓库里面中的文件和本地文件不一样，有些差异，这是因为本地文件经 hexo g 命令生成静态页面后，然后经hexo d deploy（部署）到github上，所以github仓库是没有本地hexo源文件。如果某一天不小心把本地仓库文件删了，或者换了电脑等原因，致使hexo源文件丢失，那么会造成一定的损失。所以可以把本地 hexo 源文件也同步到 github 上面。方法是在原仓库另创建一个分支，专门用于同步本地 Hexo 源文件。 执行以下命令的前提：当前主机已经可以通过SSH连接到 github 博客仓库（即本机生成的SSH KEY放入到 github 博客仓库当中去） 新建 git 仓库首先新建一个文件夹，比如起名为 changsk_backup ，在此文件夹内打开Git Bash，输入命令： 1git init # 在当前目录创建新的 Git 仓库 可以看到在当前文件夹里面会生成隐藏文件夹 .git，表示当前文件夹是一个git仓库 添加远程仓库1git remote add origin https://github.com/Tkzccsk/changsk.git https://github.com/Tkzccsk/changsk.git 是博客仓库的地址，获取方式是登录GitHub，找到自己的博客的仓库。远程库的名字就是origin，这是Git默认的叫法，可以起其他名。 查看远程仓库的名称 1git remote 下载远程仓库将GitHub上的博客仓库完全下载下来 1git pull origin master # 将远程仓库 origin 的 master 分支 pull 到本地仓库 创建新分支创建并切换到一个新分支（原来的分支名为master），输入命令： 1git checkout -b changsk changsk 为新分支名上述命令相当于两条命令 12git branch changsk # 创建分支git checkout changsk # 切换分支 上传本地hexo源文件到github博客仓库的changsk分支这样就在我们的博客仓库中新建了一个名为changsk的分支，接下来把生成的.git文件复制到本地 hexo 仓库中去，现在changsk_backup就没有用了，可以删了。接下来我们把 Hexo 本地博客仓库源文件中上传到GitHub的博客仓库的changsk分支中。 123git add .git commit -m &quot;backup&quot;git push origin changsk git add . ： 使用它会把工作时的所有变化提交到暂存区，包括文件内容修改(modified)以及新文件(new)，但不包括被删除的文件git commit -m “backup” ： 主要是将暂存区里的改动给提交到本地的版本库。-m 表示使用消息参数， “backup” 为 -m 的内容，用来表示这次提交的简要说明。git push origin changsk ： 将本地仓库的代码推送到changsk分支。 进入到GitHub的博客仓库中，可以发现出现了一个新的分支changsk，并且里面是我们的博客原件。最好在上传备份之前写一份README.md 文件，作为一项说明（因为这是GitHub建议的）。通过以上方式，我们就完成了备份啦，下一次更新了博客，首先执行 1hexo clean &amp; hexo d -g 注意：部署本地 Hexo 到 github 用的是 master 分支(__config.yml中的声明)生成及部署hexo，然后执行 123git add .git commit -m &quot;backup&quot;git push origin changsk 进行本地 Hexo 源文件的备份。参考 ：https://blog.csdn.net/qq_34229391/article/details/82251852]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[changes not staged for commit]]></title>
    <url>%2F2019%2F06%2F12%2Fchanges-not-staged-for-commit%2F</url>
    <content type="text"><![CDATA[在使用 git 的过程中，要把当前文件夹下的内容 push 到 changsk 分支，先执行 1git add . 然后执行 1git commit -m &quot;backup&quot; 最后执行 push 操作，将当前本地 git 仓库提交到 origin 远程仓库的 changsk 分支 1git push origin changsk # changsk 是我创建的一个分支 然后就收到报错，报错信息如下：大概意思是： 1更新（push操作）被拒绝，因为github远程仓库changsk分支的有些内容在本地仓库没有。 然后我想起了我昨天在 github 仓库 changsk 分支里面创建了一个 README.md，所以本地仓库是没有的，造成了远方仓库和本地仓库的不一致（精确来说是远方仓库有，但是本地仓库没有），所以 push 之前应该先把远程仓库的内容pull（拉取）到本地，然后才可以进行push。 所以先应该执行 1git pull origin changsk 将远程仓库 origin 的 chagnsk 分支和当前本地 git 仓库进行合并，使它们保持一致然后执行 1git add . 然后执行 1git commit -m &quot;backup&quot; 又收到一个不同的错误经过网上查阅，大部分人都说是因为没有执行 git add .，但我显然不是这个问题。原因是我要 push 的本地仓库里面还有另外的clone过来的git仓库，我查看文件夹，就像报错信息里面说的， themes/next（Hexo 的一个主题，也是本网站使用的主题） 里面是我 git clone 下来的一个仓库。解决办法是删除 themes/next 文件夹里面的隐藏文件夹 .git然后再执行就没有问题了。 1git commit -m &quot;backup&quot; 最后执行 push 操作 1git push origin changsk 问题解决。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CRLF CR LF]]></title>
    <url>%2F2019%2F06%2F12%2FCRLF-CR-LF%2F</url>
    <content type="text"><![CDATA[deploy 本地 Hexo 到 github pages 的时候，遇到了两陌生的单词：LF和CRLF ，本着不求甚解的态度，去网上找了相关的资料，特此记录，以便日后查看。其实前几天在安装 Hexo 的过程中有过一个设置LF和CRLF 的转换的选项，当时没在意。 名词解释CR：Carriage Return，对应ASCII中转义字符\r，表示回车LF：Linefeed，对应ASCII中转义字符\n，表示换行CRLF：Carriage Return &amp; Linefeed，\r\n，表示回车并换行 Windows操作系统采用两个字符来进行换行，即CRLF Unix/Linux/Mac OS X操作系统采用单个字符LF来进行换行 野史据野史记载，在很久以前的机械打字机时代，CR和LF分别具有不同的作用：LF会将打印纸张上移一行位置，但是保持当前打字的水平位置不变；CR则会将“Carriage”（打字机上的滚动托架）滚回到打印纸张的最左侧，但是保持当前打字的垂直位置不变，即还是在同一行。当CR和LF组合使用时，则会将打印纸张上移一行，且下一个打字位置将回到该行的最左侧，也就是我们今天所理解的换行操作。 随着时间的推移，机械打字机渐渐地退出了历史舞台，当初的纸张变成了今天的显示器，打字机的按键也演变为了如今的键盘。在操作系统出现的年代，受限于内存和软盘空间的不足，一些操作系统的设计者决定采用单个字符来表示换行符，如Unix的LF、MacIntosh的CR。他们的意图都是为了进行换行操作，只是当初并没有一个国际标准，所以才有这样字符上的不同。参考：https://www.jianshu.com/p/b03ad01acd69]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error JAVA_HOME is incorrectly set]]></title>
    <url>%2F2019%2F06%2F11%2FError_JAVA_HOME_is%20_incorrectly_set%2F</url>
    <content type="text"><![CDATA[在安装、调试、运行Hexo的过程中，出现了以下错误： Error: JAVA_HOME is incorrectly set. Please update D:\software\software\hadoop3\hadoop-3.0.2\etc\hadoop\hadoop-env.cmd 然后找到对应的目录，打开hadoop-env.cmd，发现其中的 JAVA_HOME 是这样的： 然后打开 terminal，查询 java 版本 以及 JAVA_HOME 环境变量： 发现 JAVA_HOME 已正确配置。那么问题究竟出在哪里？经网上查阅，因为Program Files中存在空格，所以出现错误，只需要用PROGRA~1代替Program Files即可。如图： 或者也可以将jdk装到其他不存在空格的目录下。]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo 的常用命令]]></title>
    <url>%2F2019%2F06%2F11%2Fhexo-command%2F</url>
    <content type="text"><![CDATA[常用 hexo 命令hexo new “postName” #新建文章hexo new page “pageName” #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help #查看帮助hexo version #查看Hexo的版本 缩写hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 组合命令：hexo s -g #生成并本地预览hexo d -g #生成并上传hexo clear # 删除无用 tags 和 categorieshexo clean &amp; hexo d -g # 清除缓存 生成静态页面并发布 给文章添加标签和分类1234567title: hexo 的常用命令date: 2019-06-11 11:43:56tags: - hexo # 文章标签- aaa- bbbcategories: hexo # 该文章类别为 categories\hexo 效果图：]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
