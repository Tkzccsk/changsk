<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mysql分片、分区、分表、分库]]></title>
    <url>%2F2019%2F06%2F19%2Fmysql-partition%2F</url>
    <content type="text"><![CDATA[一、Scale Out（横向扩展）/Scale Up（纵向扩展）Mysql的扩展方案包括Scale Out和Scale Up两种。 Scale Out（横向扩展）Scale Out（横向扩展）：是指Application可以在水平方向上扩展。一般对数据中心的应用而言，Scale out指的是当添加更多的机器时，应用仍然可以很好的利用这些机器的资源来提升自己的效率从而达到很好的扩展性。 Scale Up（纵向扩展）Scale Up（纵向扩展）：是指Application可以在垂直方向上扩展。一般对单台机器而言，Scale Up指的是当某个计算节点（机器）添加更多的CPU Cores，存储设备，使用更大的内存时，应用可以很充分的利用这些资源来提升自己的效率从而达到很好的扩展性。 二、Sharding（属于横向扩展）Sharding 是把数据库横向扩展（Scale Out）到多个物理节点上的一种有效的方式，其主要目的是为突破单节点数据库服务器的 I/O 能力限制，解决数据库扩展性问题。Shard这个词的意思是“碎片”。如果将一个数据库当作一块大玻璃，将这块玻璃打碎，那么每一小块都称为数据库的碎片（Database Shard）。将整个数据库打碎的过程就叫做sharding，可以翻译为分片。 形式上，Sharding可以简单定义为将大数据库分布到多个物理节点上的一个分区方案。每一个分区包含数据库的某一部分，称为一个shard，分区方式可以是任意的，并不局限于传统的水平分区和垂直分区。一个shard可以包含多个表的内容甚至可以包含多个数据库实例中的内容。每个shard被放置在一个数据库服务器上。一个数据库服务器可以处理一个或多个shard的数据。系统中需要有服务器进行查询路由转发，负责将查询转发到包含该查询所访问数据的shard或shards节点上去执行。 Sharding 策略 哈希取模：hash(key) % N； 范围：可以是 ID 范围也可以是时间范围； 映射表：使用单独的一个数据库来存储映射关系。 MySql的Sharding策略包括垂直切分和水平切分两种。 垂直(纵向)拆分垂直(纵向)拆分：是指按功能模块拆分，以解决表与表之间的io竞争。即将表按照功能模块、关系密切程度划分出来，部署到不同的库上。例如，我们会建立定义数据库workDB、商品数据库payDB、用户数据库userDB、日志数据库logDB等，分别用于存储项目数据定义表、商品定义表、用户数据表、日志数据表等。 如userid,name,addr一个表，为了防止表过大，分成2个表。 123userid,nameuserid,addr 表结构设计垂直切分。常见的一些场景包括: 大字段的垂直切分。单独将大字段建在另外的表中，提高基础表的访问性能，原则上在性能关键的应用中应当避免数据库的大字段 按照使用用途垂直切分。例如企业物料属性，可以按照基本属性、销售属性、采购属性、生产制造属性、财务会计属性等用途垂直切分. 按照访问频率垂直切分。例如电子商务、Web 2.0系统中，如果用户属性设置非常多，可以将基本、使用频繁的属性和不常用的属性垂直切分开 水平(横向)拆分 水平(横向)拆分：将同一个表的数据进行分块保存到不同的数据库中，来解决单表中数据量增长出现的压力。这些数据库中的表结构完全相同。当一个表中的数据量过大时，我们可以把该表的数据按照某种规则，例如userID散列、按性别、按省，进行划分，然后存储到多个结构相同的表，和不同的库上。例如，我们的userDB中的用户数据表中，每一个表的数据量都很大，就可以把userDB切分为结构相同的多个userDB：part0DB、part1DB等，再将userDB上的用户数据表userTable，切分为很多userTable：userTable0、userTable1等，然后将这些表按照一定的规则存储到多个userDB上。 表结构设计水平切分。常见的一些场景包括: 比如在线电子商务网站，订单表数据量过大，按照年度、月度水平切分。 Web 2.0网站注册用户、在线活跃用户过多，按照用户ID范围等方式，将相关用户以及该用户紧密关联的表做水平切分。 例如论坛的置顶帖子，因为涉及到分页问题，每页都需要显示置顶贴，这种情况可以把置顶贴水平切分开来，避免取置顶帖子时从所有帖子的表中读取 三、分表和分区分表从表面意思说就是把一张表分成多个小表，把一张表按一定的规则分解成N个具有独立存储空间的实体表。系统读写时需要根据定义好的规则得到对应的字表明，然后操作它。 分区则是把一张表的数据分成N多个区块，这些区块可以在同一个磁盘上，也可以在不同的磁盘上，在逻辑上看最终只是一张表，但底层是由N个物理区块组成的，分区实现比较简单，数据库mysql、oracle等很容易就可支持。分区对业务透明，分区只不过把存放数据的文件分成了许多小块，根据一定的规则把数据文件(MYD)和索引文件（MYI）进行了分割，分区后的表呢，还是一张表。 分表和分区的区别： 实现方式上mysql的分表是真正的分表，一张表分成很多表后，每一个小表都是完整的一张表，都对应三个文件（MyISAM引擎：一个.MYD数据文件，.MYI索引文件，.frm表结构文件）。 数据处理上分表后数据都是存放在分表里，总表只是一个外壳，存取数据发生在一个一个的分表里面。分区则不存在分表的概念，分区只不过把存放数据的文件分成了许多小块，分区后的表还是一张表，数据处理还是由自己来完成。 提高性能上 （1）分表后，单表的并发能力提高了，磁盘I/O性能也提高了。并发能力为什么提高了呢，因为查询一次所花的时间变短了，如果出现高并发的话，总表可以根据不同的查询，将并发压力分到不同的小表里面。磁盘I/O性能高了，本来一个非常大的.MYD文件现在也分摊到各个小表的.MYD中去了。 （2）mysql提出了分区的概念，我觉得就想突破磁盘I/O瓶颈，想提高磁盘的读写能力，来增加mysql性能。 在这一点上，分区和分表的测重点不同，分表重点是存取数据时，如何提高mysql并发能力上；而分区呢，如何突破磁盘的读写能力，从而达到提高mysql性能的目的。 实现的难易度上分表的方法有很多，用merge来分表，是最简单的一种方式。这种方式和分区难易度差不多，并且对程序代码来说可以做到透明的。如果是用其他分表方式就比分区麻烦了。 分区实现是比较简单的，建立分区表，跟建平常的表没什么区别，并且对代码端来说是透明的。 分区的适用场景: 1231. 一张表的查询速度已经慢到影响使用的时候。2. 表中的数据是分段的3. 对数据的操作往往只涉及一部分数据，而不是所有的数据 1234567891011 CREATE TABLE sales ( id INT AUTO_INCREMENT, amount DOUBLE NOT NULL, order_day DATETIME NOT NULL, PRIMARY KEY(id, order_day) ) ENGINE=Innodb PARTITION BY RANGE(YEAR(order_day)) ( PARTITION p_2010 VALUES LESS THAN (2010), PARTITION p_2011 VALUES LESS THAN (2011), PARTITION p_2012 VALUES LESS THAN (2012), PARTITION p_catchall VALUES LESS THAN MAXVALUE); 分表的适用场景 121. 一张表的查询速度已经慢到影响使用的时候。2. 当频繁插入或者联合查询时，速度变慢。 分表的实现需要业务结合实现和迁移，较为复杂。 如何选择？应该使用哪一种方式来实施数据库分库分表，这要看数据库中数据量的瓶颈所在，并综合项目的业务类型进行考虑。 如果数据库是因为表太多而造成海量数据，并且项目的各项业务逻辑划分清晰、低耦合，那么规则简单明了、容易实施的垂直切分必是首选。 而如果数据库中的表并不多，但单表的数据量很大、或数据热度很高，这种情况之下就应该选择水平切分，水平切分比垂直切分要复杂一些，它将原本逻辑上属于一体的数据进行了物理分割，除了在分割时要对分割的粒度做好评估，考虑数据平均和负载平均，后期也将对项目人员及应用程序产生额外的数据管理负担。 在现实项目中，往往是这两种情况兼而有之，这就需要做出权衡，甚至既需要垂直切分，又需要水平切分。 四、分表和分库分表能够解决单表数据量过大带来的查询效率下降的问题，但是，却无法给数据库的并发处理能力带来质的提升。面对高并发的读写访问，当数据库master服务器无法承载写操作压力时，不管如何扩展slave服务器，此时都没有意义了。因此，我们必须换一种思路，对数据库进行拆分，从而提高数据库写入能力，这就是所谓的分库。 分表和分区都是基于同一个数据库里的数据分离技巧，对数据库性能有一定提升，但是随着业务数据量的增加，原来所有的数据都是在一个数据库上的，网络IO及文件IO都集中在一个数据库上的，因此CPU、内存、文件IO、网络IO都可能会成为系统瓶颈。当业务系统的数据容量接近或超过单台服务器的容量、QPS/TPS接近或超过单个数据库实例的处理极限等此时，往往是采用垂直和水平结合的数据拆分方法，把数据服务和数据存储分布到多台数据库服务器上。 与分表策略相似，分库可以采用通过一个关键字取模的方式，来对数据访问进行路由，如下图所示： 五、分库分表存在的问题事务问题在执行分库分表之后，由于数据存储到了不同的库上，数据库事务管理出现了困难。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价；如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库跨表的join问题在执行了分库分表之后，难以避免会将原本逻辑关联性很强的数据划分到不同的表、不同的库上，这时，表的关联操作将受到限制，我们无法join位于不同分库的表，也无法join分表粒度不同的表，结果原本一次查询能够完成的业务，可能需要多次查询才能完成。 额外的数据管理负担和数据运算压力。额外的数据管理负担，最显而易见的就是数据的定位问题和数据的增删改查的重复执行问题，这些都可以通过应用程序解决，但必然引起额外的逻辑运算，例如，对于一个记录用户成绩的用户数据表userTable，业务要求查出成绩最好的100位，在进行分表之前，只需一个order by语句就可以搞定，但是在进行分表之后，将需要n个order by语句，分别查出每一个分表的前100名用户数据，然后再对这些数据进行合并计算，才能得出结果。 六、分片（Sharding）和分区（Partition）sharding和partition的区别： 参考文章：mysql分片、分区、分表、分库Mysql分表和分区的区别、分库和分表区别阿里P8架构师谈：数据库分库分表、读写分离的原理实现，使用场景]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>分片</tag>
        <tag>分区</tag>
        <tag>分表</tag>
        <tag>分库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[桥梁模式bridge]]></title>
    <url>%2F2019%2F06%2F19%2Fbridge%2F</url>
    <content type="text"><![CDATA[桥梁模式的定义定义: 将抽象和实现解耦, 使得两者可以独立的变化 通俗的说, 就是一个类调用另一个类中的方法, 需要一个桥梁, 通过聚合的关系调用 其类图如下: 其中角色说明如下: 1234567Abstraction 抽象化角色: 它的主要职责是定义出该角色的行为, 同时保存一个对实现化角色的引用, 一般是抽象类Implementor 实现化角色: 接口或抽象类, 定义角色必须的行为和属性RefinedAbstraction 修正抽象化角色: 它引用实现化角色对抽象化角色进行修正ConcreteImplementor 具体实现化角色: 它实现接口或抽象类定义的方法和属性 桥梁模式实现抽象角色的部分实现是由实现角色完成的 实现化角色代码: 具体实现化角色代码: 抽象化角色代码: 具体抽象化角色代码: 场景类代码: 桥梁模式优点桥梁模式是一个很简单的模式, 它只是使用了类间的聚合关系、继承、覆写等常用功能, 但是它却提供了一个非常清晰、稳定的架构。 桥梁模式的优点: 123抽象和实现分离. 这是桥梁模式的主要特点, 它完全是为了解决继承的缺点而提出的设计模式. 在该模式下,实现可以不受抽象的约束,不用再绑定在一个固定的抽象层次上,具有优秀的扩充能力.实现细节对客户透明. 客户不用关心细节的实现, 它已经由抽象层通过聚合关系完成了封装 桥梁模式的使用场景:12345不希望或不适用使用继承的场景. 例如继承层次过滤、无法更细化设计颗粒等场景接口或抽象类不稳定的场景.重用性要求较高的场景. 设计的颗粒度越细,则被重用的可能性就越大, 而采用继承则受父类的限制, 不可能出现太细的颗粒度 使用桥梁模式主要考虑如何拆分抽象和实现,并不是一设计继承就要考虑使用该模式. 桥梁模式的意图还是对变化的封装, 尽量把可能变化的因素封装到最细、最小的逻辑单元中,避免风险扩散.因此在进行系统设计时,发现类的继承有N层时,可以考虑使用桥梁模式. 桥梁模式在Java应用中的一个非常典型的例子就是JDBC驱动器。JDBC为所有的关系型数据库提供一个通用的界面。一个应用系统动态地选择一个合适的驱动器，然后通过驱动器向数据库引擎发出指令。这个过程就是将抽象角色的行为委派给实现角色的过程。 抽象角色可以针对任何数据库引擎发出查询指令，因为抽象角色并不直接与数据库引擎打交道，JDBC驱动器负责这个底层的工作。由于JDBC驱动器的存在，应用系统可以不依赖于数据库引擎的细节而独立地演化；同时数据库引擎也可以独立于应用系统的细节而独立的演化。两个独立的等级结构如下图所示，左边是JDBC API的等级结构，右边是JDBC驱动器的等级结构。应用程序是建立在JDBC API的基础之上的。 应用系统作为一个等级结构，与JDBC驱动器这个等级结构是相对独立的，它们之间没有静态的强关联。应用系统通过委派与JDBC驱动器相互作用，这是一个桥梁模式的例子。 JDBC的这种架构，把抽象部分和具体部分分离开来，从而使得抽象部分和具体部分都可以独立地扩展。对于应用程序而言，只要选用不同的驱动，就可以让程序操作不同的数据库，而无需更改应用程序，从而实现在不同的数据库上移植；对于驱动程序而言，为数据库实现不同的驱动程序，并不会影响应用程序。 以上文章来自：23种设计模式之桥梁模式]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>桥接模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql之explain详解]]></title>
    <url>%2F2019%2F06%2F18%2Fmysql-explain%2F</url>
    <content type="text"><![CDATA[原文：MySQL 性能优化神器 Explain 使用分析 简介MySQL 提供了一个 EXPLAIN 命令, 它可以对 SELECT 语句进行分析, 并输出 SELECT 执行的详细信息, 以供开发人员针对性优化.EXPLAIN 命令用法十分简单, 在 SELECT 语句前加上 Explain 就可以了, 例如: 1EXPLAIN SELECT * from user_info WHERE id &lt; 300; 准备为了接下来方便演示 EXPLAIN 的使用, 首先我们需要建立两个测试用的表, 并添加相应的数据: 1234567891011121314151617181920CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT '', `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB DEFAULT CHARSET = utf8INSERT INTO user_info (name, age) VALUES ('xys', 20);INSERT INTO user_info (name, age) VALUES ('a', 21);INSERT INTO user_info (name, age) VALUES ('b', 23);INSERT INTO user_info (name, age) VALUES ('c', 50);INSERT INTO user_info (name, age) VALUES ('d', 15);INSERT INTO user_info (name, age) VALUES ('e', 20);INSERT INTO user_info (name, age) VALUES ('f', 21);INSERT INTO user_info (name, age) VALUES ('g', 23);INSERT INTO user_info (name, age) VALUES ('h', 50);INSERT INTO user_info (name, age) VALUES ('i', 15); 12345678910111213141516171819CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT '', `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`)) ENGINE = InnoDB DEFAULT CHARSET = utf8INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p1', 'WHH');INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p2', 'WL');INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p1', 'DX');INSERT INTO order_info (user_id, product_name, productor) VALUES (2, 'p1', 'WHH');INSERT INTO order_info (user_id, product_name, productor) VALUES (2, 'p5', 'WL');INSERT INTO order_info (user_id, product_name, productor) VALUES (3, 'p3', 'MA');INSERT INTO order_info (user_id, product_name, productor) VALUES (4, 'p1', 'WHH');INSERT INTO order_info (user_id, product_name, productor) VALUES (6, 'p1', 'WHH');INSERT INTO order_info (user_id, product_name, productor) VALUES (9, 'p8', 'TE'); EXPLAIN 输出格式EXPLAIN 命令的输出内容大致如下: 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 各列的含义如下: id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type: SELECT 查询的类型. table: 查询的是哪个表 partitions: 匹配的分区 type: join 类型 possible_keys: 此次查询中可能选用的索引 key: 此次查询中确切使用到的索引. ref: 哪个字段或常数与 key 一起被使用 rows: 显示此查询一共扫描了多少行. 这个是一个估计值. filtered: 表示此查询条件所过滤的数据的百分比 extra: 额外的信息 接下来我们来重点看一下比较重要的几个字段. select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 最常见的查询类别应该是 SIMPLE 了, 比如当我们的查询没有子查询, 也没有 UNION 查询时, 那么通常就是 SIMPLE 类型, 例如: 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 如果我们使用了 UNION 查询, 那么 EXPLAIN 输出 的结果类似如下: 1234567891011mysql&gt; EXPLAIN (SELECT * FROM user_info WHERE id IN (1, 2, 3)) -&gt; UNION -&gt; (SELECT * FROM user_info WHERE id IN (3, 4, 5));+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+| 1 | PRIMARY | user_info | NULL | range | PRIMARY | PRIMARY | 8 | NULL | 3 | 100.00 | Using where || 2 | UNION | user_info | NULL | range | PRIMARY | PRIMARY | 8 | NULL | 3 | 100.00 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+3 rows in set, 1 warning (0.00 sec) table表示查询涉及的表或衍生表 typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. type 常用类型type 常用的取值有: system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可.例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的. 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如: 12345678910111213141516171819202122232425262728mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 314 ref: NULL rows: 9 filtered: 100.00 Extra: Using where; Using index*************************** 2. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: test.order_info.user_id rows: 1 filtered: 100.00 Extra: NULL2 rows in set, 1 warning (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询.例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678910111213141516171819202122232425262728mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: refpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 9 ref: const rows: 1 filtered: 100.00 Extra: Using index2 rows in set, 1 warning (0.01 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. 例如下面的例子就是一个范围查询: 1234567891011121314151617mysql&gt; EXPLAIN SELECT * -&gt; FROM user_info -&gt; WHERE id BETWEEN 2 AND 8 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: rangepossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: NULL rows: 7 filtered: 100.00 Extra: Using where1 row in set, 1 warning (0.00 sec) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据.index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index. 例如: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: indexpossible_keys: NULL key: name_index key_len: 152 ref: NULL rows: 10 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免.下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的. 123456789101112131415mysql&gt; EXPLAIN SELECT age FROM user_info WHERE age = 20 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 10 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) type 类型的性能比较通常来说, 不同的 type 类型的性能关系如下:ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; systemALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的.而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快.后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key此字段是 MySQL 在当前查询时所真正使用到的索引. key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的栗子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id &lt; 3 AND product_name = 'p1' AND productor = 'WHH' \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: rangepossible_keys: user_product_detail_index key: user_product_detail_index key_len: 9 ref: NULL rows: 5 filtered: 11.11 Extra: Using where; Using index1 row in set, 1 warning (0.00 sec) 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 不过此查询语句 WHERE user_id &lt; 3 AND product_name = &#39;p1&#39; AND productor = &#39;WHH&#39; 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT &#39;0&#39;, 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = 'p1' \G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: refpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 161 ref: const,const rows: 2 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = &#39;p1&#39; 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 rowsrows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY product_name \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: NULL key: user_product_detail_index key_len: 253 ref: NULL rows: 9 filtered: 100.00 Extra: Using index; Using filesort1 row in set, 1 warning (0.00 sec) 我们的索引是 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort. 如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: NULL key: user_product_detail_index key_len: 253 ref: NULL rows: 9 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java动态代理]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-dynamic-proxy%2F</url>
    <content type="text"><![CDATA[以下文章来自：java动态代理实现与原理详细分析 关于Java中的动态代理，我们首先需要了解的是一种常用的设计模式–代理模式，而对于代理，根据创建代理类的时间点，又可以分为静态代理和动态代理。 一、代理模式 代理模式是常用的java设计模式，他的特征是代理类与委托类有同样的接口，代理类主要负责为委托类预处理消息、过滤消息、把消息转发给委托类，以及事后处理消息等。代理类与委托类之间通常会存在关联关系，一个代理类的对象与一个委托类的对象关联，代理类的对象本身并不真正实现服务，而是通过调用委托类的对象的相关方法，来提供特定的服务。简单的说就是，我们在访问实际对象时，是通过代理对象来访问的，代理模式就是在访问实际对象时引入一定程度的间接性，因为这种间接性，可以附加多种用途。在后面我会解释这种间接性带来的好处。代理模式结构图（图片来自《大话设计模式》）： 二、静态代理静态代理静态代理：由程序员创建或特定工具自动生成源代码，也就是在编译时就已经将接口，被代理类，代理类等确定下来。在程序运行之前，代理类的.class文件就已经生成。 静态代理简单实现 根据上面代理模式的类图，来写一个简单的静态代理的例子。我这儿举一个比较粗糙的例子，假如一个班的同学要向老师交班费，但是都是通过班长把自己的钱转交给老师。这里，班长就是代理学生上交班费， 班长就是学生的代理。 ​ 首先，我们创建一个Person接口。这个接口就是学生（被代理类），和班长（代理类）的公共接口，他们都有上交班费的行为。这样，学生上交班费就可以让班长来代理执行。 12345678/** * 创建Person接口 * @author Gonjan */public interface Person &#123; //上交班费 void giveMoney();&#125; Student类实现Person接口。Student可以具体实施上交班费的动作。 1234567891011public class Student implements Person &#123; private String name; public Student(String name) &#123; this.name = name; &#125; @Override public void giveMoney() &#123; System.out.println(name + "上交班费50元"); &#125;&#125; StudentsProxy类，这个类也实现了Person接口，但是还另外持有一个学生类对象，由于实现了Peson接口，同时持有一个学生对象，那么他可以代理学生类对象执行上交班费（执行giveMoney()方法）行为。 123456789101112131415161718192021/** * 学生代理类，也实现了Person接口，保存一个学生实体，这样既可以代理学生产生行为 * @author Gonjan * */public class StudentsProxy implements Person&#123; //被代理的学生 Student stu; public StudentsProxy(Person stu) &#123; // 只代理学生对象 if(stu.getClass() == Student.class) &#123; this.stu = (Student)stu; &#125; &#125; //代理上交班费，调用被代理学生的上交班费行为 public void giveMoney() &#123; stu.giveMoney(); &#125;&#125; 下面测试一下，看如何使用代理模式： 123456789101112public class StaticProxyTest &#123; public static void main(String[] args) &#123; //被代理的学生张三，他的班费上交有代理对象monitor（班长）完成 Person zhangsan = new Student("张三"); //生成代理对象，并将张三传给代理对象 Person monitor = new StudentsProxy(zhangsan); //班长代理上交班费 monitor.giveMoney(); &#125;&#125; 运行结果： 这里并没有直接通过张三（被代理对象）来执行上交班费的行为，而是通过班长（代理对象）来代理执行了。这就是代理模式。 代理模式最主要的就是有一个公共接口（Person），一个具体的类（Student），一个代理类（StudentsProxy）,代理类持有具体类的实例，代为执行具体类实例方法。上面说到，代理模式就是在访问实际对象时引入一定程度的间接性，因为这种间接性，可以附加多种用途。这里的间接性就是指不直接调用实际对象的方法，那么我们在代理过程中就可以加上一些其他用途。就这个例子来说，加入班长在帮张三上交班费之前想要先反映一下张三最近学习有很大进步，通过代理模式很轻松就能办到： 1234567891011121314151617public class StudentsProxy implements Person&#123; //被代理的学生 Student stu; public StudentsProxy(Person stu) &#123; // 只代理学生对象 if(stu.getClass() == Student.class) &#123; this.stu = (Student)stu; &#125; &#125; //代理上交班费，调用被代理学生的上交班费行为 public void giveMoney() &#123; System.out.println("张三最近学习有进步！"); stu.giveMoney(); &#125;&#125; 运行结果： 可以看到，只需要在代理类中帮张三上交班费之前，执行其他操作就可以了。这种操作，也是使用代理模式的一个很大的优点。最直白的就是在Spring中的面向切面编程（AOP），我们能在一个切点之前执行一些操作，在一个切点之后执行一些操作，这个切点就是一个个方法。这些方法所在类肯定就是被代理了，在代理过程中切入了一些其他操作。 三、动态代理动态代理代理类在程序运行时创建的代理方式被成为动态代理。 我们上面静态代理的例子中，代理类(studentProxy)是自己定义好的，在程序运行之前就已经编译完成。然而动态代理，代理类并不是在Java代码中定义的，而是在运行时根据我们在Java代码中的“指示”动态生成的。相比于静态代理， 动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类中的方法。 比如说，想要在每个代理的方法前都加上一个处理方法： 12345public void giveMoney() &#123; //调用被代理方法前加入处理方法 beforeMethod(); stu.giveMoney(); &#125; 这里只有一个giveMoney方法，就写一次beforeMethod方法，但是如果出了giveMonney还有很多其他的方法，那就需要写很多次beforeMethod方法，麻烦。那看看下面动态代理如何实现。 动态代理简单实现在java的java.lang.reflect包下提供了一个Proxy类和一个InvocationHandler接口，通过这个类和这个接口可以生成JDK动态代理类和动态代理对象。 创建一个动态代理对象步骤，具体代码见后面： 创建一个InvocationHandler对象 12//创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new MyInvocationHandler&lt;Person&gt;(stu); 使用Proxy类的getProxyClass静态方法生成一个动态代理类stuProxyClass 1Class&lt;?&gt; stuProxyClass = Proxy.getProxyClass(Person.class.getClassLoader(), new Class&lt;?&gt;[] &#123;Person.class&#125;); 获得stuProxyClass 中一个带InvocationHandler参数的构造器constructor 1Constructor&lt;?&gt; constructor = PersonProxy.getConstructor(InvocationHandler.class); 通过构造器constructor来创建一个动态实例stuProxy 1Person stuProxy = (Person) cons.newInstance(stuHandler); 就此，一个动态代理对象就创建完毕，当然，上面四个步骤可以通过Proxy类的newProxyInstances方法来简化： 1234 //创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new MyInvocationHandler&lt;Person&gt;(stu);//创建一个代理对象stuProxy，代理对象的每个执行方法都会替换执行Invocation中的invoke方法 Person stuProxy= (Person) Proxy.newProxyInstance(Person.class.getClassLoader(), new Class&lt;?&gt;[]&#123;Person.class&#125;, stuHandler); 到这里肯定都会很疑惑，这动态代理到底是如何执行的，是如何通过代理对象来执行被代理对象的方法的，先不急，我们先看看一个简单的完整的动态代理的例子。还是上面静态代理的例子，班长需要帮学生代交班费。首先是定义一个Person接口: 12345678/** * 创建Person接口 * @author Gonjan */public interface Person &#123; //上交班费 void giveMoney();&#125; 创建需要被代理的实际类： 1234567891011121314151617public class Student implements Person &#123; private String name; public Student(String name) &#123; this.name = name; &#125; @Override public void giveMoney() &#123; try &#123; //假设数钱花了一秒时间 Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(name + "上交班费50元"); &#125;&#125; 再定义一个检测方法执行时间的工具类，在任何方法执行前先调用start方法，执行后调用finsh方法，就可以计算出该方法的运行时间，这也是一个最简单的方法执行时间检测工具。 1234567891011121314public class MonitorUtil &#123; private static ThreadLocal&lt;Long&gt; tl = new ThreadLocal&lt;&gt;(); public static void start() &#123; tl.set(System.currentTimeMillis()); &#125; //结束时打印耗时 public static void finish(String methodName) &#123; long finishTime = System.currentTimeMillis(); System.out.println(methodName + "方法耗时" + (finishTime - tl.get()) + "ms"); &#125;&#125; 创建StuInvocationHandler类，实现InvocationHandler接口，这个类中持有一个被代理对象的实例target。InvocationHandler中有一个invoke方法，所有执行代理对象的方法都会被替换成执行invoke方法。 再再invoke方法中执行被代理对象target的相应方法。当然，在代理过程中，我们在真正执行被代理对象的方法前加入自己其他处理。这也是Spring中的AOP实现的主要原理，这里还涉及到一个很重要的关于java反射方面的基础知识。 123456789101112131415161718192021222324public class StuInvocationHandler&lt;T&gt; implements InvocationHandler &#123; //invocationHandler持有的被代理对象 T target; public StuInvocationHandler(T target) &#123; this.target = target; &#125; /** * proxy:代表动态代理对象 * method：代表正在执行的方法 * args：代表调用目标方法时传入的实参 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("代理执行" +method.getName() + "方法"); */ //代理过程中插入监测方法,计算该方法耗时 MonitorUtil.start(); Object result = method.invoke(target, args); MonitorUtil.finish(method.getName()); return result; &#125;&#125; 做完上面的工作后，我们就可以具体来创建动态代理对象了，上面简单介绍了如何创建动态代理对象，我们使用简化的方式创建动态代理对象： 12345678910111213141516public class ProxyTest &#123; public static void main(String[] args) &#123; //创建一个实例对象，这个对象是被代理的对象 Person zhangsan = new Student("张三"); //创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new StuInvocationHandler&lt;Person&gt;(zhangsan); //创建一个代理对象stuProxy来代理zhangsan，代理对象的每个执行方法都会替换执行Invocation中的invoke方法 Person stuProxy = (Person) Proxy.newProxyInstance(Person.class.getClassLoader(), new Class&lt;?&gt;[]&#123;Person.class&#125;, stuHandler)； //代理执行上交班费的方法 stuProxy.giveMoney(); &#125;&#125; 我们执行这个ProxyTest类，先想一下，我们创建了一个需要被代理的学生张三，将zhangsan对象传给了stuHandler中，我们在创建代理对象stuProxy时，将stuHandler作为参数了的，上面也有说到所有执行代理对象的方法都会被替换成执行invoke方法，也就是说，最后执行的是StuInvocationHandler中的invoke方法。所以在看到下面的运行结果也就理所当然了。 运行结果： 上面说到，动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类中的方法。是因为所有被代理执行的方法，都是通过在InvocationHandler中的invoke方法调用的，所以我们只要在invoke方法中统一处理，就可以对所有被代理的方法进行相同的操作了。例如，这里的方法计时，所有的被代理对象执行的方法都会被计时，然而我只做了很少的代码量。 动态代理的过程，代理对象和被代理对象的关系不像静态代理那样一目了然，清晰明了。因为动态代理的过程中，我们并没有实际看到代理类，也没有很清晰地的看到代理类的具体样子，而且动态代理中被代理对象和代理对象是通过InvocationHandler来完成的代理过程的，其中具体是怎样操作的，为什么代理对象执行的方法都会通过InvocationHandler中的invoke方法来执行。带着这些问题，我们就需要对java动态代理的源码进行简要的分析，弄清楚其中缘由。 四、动态代理原理分析​ 1、Java动态代理创建出来的动态代理类 上面我们利用Proxy类的newProxyInstance方法创建了一个动态代理对象，查看该方法的源码，发现它只是封装了创建动态代理类的步骤(红色标准部分)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException&#123; Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); //红色部分 final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * Look up or generate the designated proxy class. */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams);//红色部分 final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; return cons.newInstance(new Object[]&#123;h&#125;); //红色部分 &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125;&#125; 其实，我们最应该关注的是Class&lt;?&gt; cl = getProxyClass0(loader, intfs);这句，这里产生了代理类，后面代码中的构造器也是通过这里产生的类来获得，可以看出，这个类的产生就是整个动态代理的关键，由于是动态生成的类文件，我这里不具体进入分析如何产生的这个类文件，只需要知道这个类文件时缓存在java虚拟机中的，我们可以通过下面的方法将其打印到文件里面，一睹真容： 123456789byte[] classFile = ProxyGenerator.generateProxyClass("$Proxy0", Student.class.getInterfaces());String path = "G:/javacode/javase/Test/bin/proxy/StuProxy.class";try(FileOutputStream fos = new FileOutputStream(path)) &#123; fos.write(classFile); fos.flush(); System.out.println("代理类class文件写入成功");&#125; catch (Exception e) &#123; System.out.println("写文件错误");&#125; 对这个class文件进行反编译，我们看看jdk为我们生成了什么样的内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;import proxy.Person;public final class $Proxy0 extends Proxy implements Person&#123; private static Method m1; private static Method m2; private static Method m3; private static Method m0; /** *注意这里是生成代理类的构造方法，方法参数为InvocationHandler类型，看到这，是不是就有点明白 *为何代理对象调用方法都是执行InvocationHandler中的invoke方法，而InvocationHandler又持有一个 *被代理对象的实例，不禁会想难道是....？ 没错，就是你想的那样。 * *super(paramInvocationHandler)，是调用父类Proxy的构造方法。 *父类持有：protected InvocationHandler h; *Proxy构造方法： * protected Proxy(InvocationHandler h) &#123; * Objects.requireNonNull(h); * this.h = h; * &#125; * */ public $Proxy0(InvocationHandler paramInvocationHandler) throws &#123; super(paramInvocationHandler); &#125; //这个静态块本来是在最后的，我把它拿到前面来，方便描述 static &#123; try &#123; //看看这儿静态块儿里面有什么，是不是找到了giveMoney方法。请记住giveMoney通过反射得到的名字m3，其他的先不管 m1 = Class.forName("java.lang.Object").getMethod("equals", new Class[] &#123; Class.forName("java.lang.Object") &#125;); m2 = Class.forName("java.lang.Object").getMethod("toString", new Class[0]); m3 = Class.forName("proxy.Person").getMethod("giveMoney", new Class[0]); m0 = Class.forName("java.lang.Object").getMethod("hashCode", new Class[0]); return; &#125; catch (NoSuchMethodException localNoSuchMethodException) &#123; throw new NoSuchMethodError(localNoSuchMethodException.getMessage()); &#125; catch (ClassNotFoundException localClassNotFoundException) &#123; throw new NoClassDefFoundError(localClassNotFoundException.getMessage()); &#125; &#125; /** * *这里调用代理对象的giveMoney方法，直接就调用了InvocationHandler中的invoke方法，并把m3传了进去。 *this.h.invoke(this, m3, null);这里简单，明了。 *来，再想想，代理对象持有一个InvocationHandler对象，InvocationHandler对象持有一个被代理的对象， *再联系到InvacationHandler中的invoke方法。嗯，就是这样。 */ public final void giveMoney() throws &#123; try &#123; this.h.invoke(this, m3, null); return; &#125; catch (Error|RuntimeException localError) &#123; throw localError; &#125; catch (Throwable localThrowable) &#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; //注意，这里为了节省篇幅，省去了toString，hashCode、equals方法的内容。原理和giveMoney方法一毛一样。&#125; jdk为我们的生成了一个叫$Proxy0（这个名字后面的0是编号，有多个代理类会一次递增）的代理类，这个类文件是放在内存中的，我们在创建代理对象时，就是通过反射获得这个类的构造方法，然后创建的代理实例。通过对这个生成的代理类源码的查看，我们很容易能看出，动态代理实现的具体过程。 我们可以把InvocationHandler看做一个中介类，中介类持有一个被代理对象，在invoke方法中调用了被代理对象的相应方法。通过聚合方式持有被代理对象的引用，把外部对invoke的调用最终都转为对被代理对象的调用。 代理类调用自己方法时，通过自身持有的中介类对象来调用中介类对象的invoke方法，从而达到代理执行被代理对象的方法。也就是说，动态代理通过中介类实现了具体的代理功能。 五、总结生成的代理类：$Proxy0 extends Proxy implements Person，我们看到代理类继承了Proxy类，所以也就决定了java动态代理只能对接口进行代理，Java的继承机制注定了这些动态代理类们无法实现对class的动态代理。 上面的动态代理的例子，其实就是AOP的一个简单实现了，在目标对象的方法执行之前和执行之后进行了处理，对方法耗时统计。Spring的AOP实现其实也是用了Proxy和InvocationHandler这两个东西的。]]></content>
      <categories>
        <category>动态代理</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql主从复制和读写分离]]></title>
    <url>%2F2019%2F06%2F18%2Fmysql-master-slave-replication%2F</url>
    <content type="text"><![CDATA[以下文章参考自：：MySql 主从复制及配置实现 Mysql 主从复制的原理和kafka的partition的replication机制很类似，原理互通，大概是因为这种做法确实可以保证分布式系统的可靠性。 一、什么是Mysql主从复制MySQL主从复制是其最重要的功能之一。主从复制是指一台服务器充当主数据库服务器，另一台或多台服务器充当从数据库服务器，主服务器中的数据自动复制到从服务器之中。对于多级复制，数据库服务器即可充当主机，也可充当从机。MySQL主从复制的基础是主服务器对数据库修改记录二进制日志(bin log)，从服务器通过主服务器的二进制日志自动执行更新。 二、Mysq主从复制的类型基于语句的复制：主服务器上面执行的语句在从服务器上面再执行一遍，在MySQL-3.23版本以后支持。 存在的问题：时间上可能不完全同步造成偏差，执行语句的用户也可能不是同一个用户。 基于行的复制：把主服务器上面改变后的内容直接复制过去，而不关心到底改变该内容是由哪条语句引发的，在MySQL-5.0版本以后引入。 存在的问题：比如一个工资表中有一万个用户，我们把每个用户的工资+1000，那么基于行的复制则要复制一万行的内容，由此造成的开销比较大，而基于语句的复制仅仅一条语句就可以了。 混合类型的复制：MySQL默认使用基于语句的复制，当基于语句的复制会引发问题的时候就会使用基于行的复制，MySQL会自动进行选择。 在MySQL主从复制架构中，读操作可以在所有的服务器上面进行，而写操作只能在主服务器上面进行。主从复制架构虽然给读操作提供了扩展，可如果写操作也比较多的话（多台从服务器还要从主服务器上面同步数据），单主模型的复制中主服务器势必会成为性能瓶颈。 三、主从复制的作用1、主数据库出现问题，可以切换到从数据库。2、可以进行数据库层面的读写分离。读写分离就是在主服务器上修改，数据会同步到从服务器，从服务器只能提供读取数据，不能写入，实现备份的同时也实现了数据库性能的优化，以及提升了服务器安全。 1234567891011121314151）基于程序代码内部实现在代码中根据select 、insert进行路由分类，这类方法也是目前生产环境下应用最广泛的。优点是性能较好，因为程序在代码中实现，不需要增加额外的硬件开支，缺点是需要开发人员来实现，运维人员无从下手。2）基于中间代理层实现代理一般介于应用服务器和数据库服务器之间，代理数据库服务器接收到应用服务器的请求后根据判断后转发到，后端数据库，有以下代表性的程序。（1）MySQL_proxy。MySQL_proxy是MySQL的一个开源项目，通过其自带的lua脚本进行sql判断。（2）Atlas。是由 Qihoo 360, Web平台部基础架构团队开发维护的一个基于MySQL协议的数据中间层项目。它是在MySQL-proxy 0.8.2版本的基础上，对其进行了优化，增加了一些新的功能特性。360内部使用Atlas运行的MySQL业务，每天承载的读写请求数达几十亿条。支持事物以及存储过程。（3）Amoeba。由阿里巴巴集团在职员工陈思儒使用java语言进行开发，阿里巴巴集团将其用户生产环境下，但是它并不支持事物以及存储过程。不是所有的应用都能够在基于程序代码中实现读写分离，像一些大型的java应用，如果在程序代码中实现读写分离对代码的改动就较大，所以，像这种应用一般会考虑使用代理层来实现。 3、可以在从数据库上进行日常备份 四、Mysql主从复制的工作原理如下图所示： [ 主服务器上面的任何修改都会保存在二进制日志Binary log里面，从服务器上面启动一个I/O thread（实际上就是一个主服务器的客户端进程），连接到主服务器上面请求读取二进制日志，然后把读取到的二进制日志写到本地的一个Realy log（中继日志）里面。从服务器上面开启一个SQL thread定时检查Realy log，如果发现有更改立即把更改的内容在本机上面执行一遍。 如果一主多从的话，这时主库既要负责写又要负责为几个从库提供二进制日志。此时可以稍做调整，将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从。或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。工作原理图如下： [ 实际上在老版本的MySQL主从复制中Slave端并不是两个进程完成的，而是由一个进程完成。但是后来发现这样做存在较大的风险和性能问题，主要如下： 123首先，一个进程会使复制bin-log日志和解析日志并在自身执行的过程成为一个串行的过程，性能受到了一定的限制，异步复制的延迟也会比较长。另外，Slave端从Master端获取bin-log过来之后，需要接着解析日志内容，然后在自身执行。在这个过程中，Master端可能又产生了大量变化并新增了大量的日志。如果在这个阶段Master端的存储出现了无法修复的错误，那么在这个阶段所产生的所有变更都将永远无法找回。如果在Slave端的压力比较大的时候，这个过程的时间可能会比较长。 为了提高复制的性能并解决存在的风险，后面版本的MySQL将Slave端的复制动作交由两个进程来完成。提出这个改进方案的人是Yahoo!的一位工程师“Jeremy Zawodny”。这样既解决了性能问题，又缩短了异步的延时时间，同时也减少了可能存在的数据丢失量。 当然，即使是换成了现在这样两个线程处理以后，同样也还是存在slave数据延时以及数据丢失的可能性的，毕竟这个复制是异步的。只要数据的更改不是在一个事务中，这些问题都是会存在的。如果要完全避免这些问题，就只能用MySQL的cluster来解决了。不过MySQL的cluster是内存数据库的解决方案，需要将所有数据都load到内存中，这样就对内存的要求就非常大了，对于一般的应用来说可实施性不是太大。 还有一点要提的是MySQL的复制过滤(Replication Filters)，复制过滤可以让你只复制服务器中的一部分数据。有两种复制过滤：在Master上过滤二进制日志中的事件；在Slave上过滤中继日志中的事件。如下： [ 配置Master的my.cnf文件(关键性的配置)/etc/my.cnf 1234567891011log-bin=mysql-binserver-id = 1binlog-do-db=icingabinlog-do-db=DB2 //如果备份多个数据库，重复设置这个选项即可binlog-do-db=DB3 //需要同步的数据库，如果没有本行，即表示同步所有的数据库binlog-ignore-db=mysql //被忽略的数据库 配置Slave的my.cnf文件(关键性的配置)/etc/my.cnf 12345678910111213141516171819log-bin=mysql-binserver-id=2master-host=10.1.68.110master-user=backupmaster-password=1234qwermaster-port=3306replicate-do-db=icingareplicate-do-db=DB2replicate-do-db=DB3 //需要同步的数据库，如果没有本行，即表示同步所有的数据库replicate-ignore-db=mysql //被忽略的数据库 网友说replicate-do-db的使用中可能会出些问题（http://blog.knowsky.com/19696…），自己没有亲自去测试。猜想binlog-do-db参数用于主服务器中，通过过滤Binary Log来过滤掉配置文件中不允许复制的数据库，也就是不向Binary Log中写入不允许复制数据的操作日志；而replicate-do-db用于从服务器中，通过过滤Relay Log来过滤掉不允许复制的数据库或表，也就是执行Relay Log中的动作时不执行那些不被允许的修改动作。这样的话，多个从数据库服务器的情况：有的从服务器既从主服务器中复制数据，又做为主服务器向另外的从服务器复制数据，那它的配置文件中应该可以同时存在binlog-do-db、replicate-do-db这两个参数才对。一切都是自己的预测，关于binlog-do-db、replicate-do-db的具体使用方法还得在实际开发中一点点摸索才可以。 网上有说，复制时忽略某些数据库或者表的操作最好不要在主服务器上面进行，因为主服务器忽略之后就不会再往二进制文件中写了，但是在从服务器上面虽然忽略了某些数据库但是主服务器上面的这些操作信息依然会被复制到从服务器上面的relay log里面，只是不会在从服务器上面执行而已。我想这个意思应该是建议在从服务器中设置replicate-do-db，而不要在主服务器上设置binlog-do-db。 另外，不管是黑名单（binlog-ignore-db、replicate-ignore-db）还是白名单（binlog-do-db、replicate-do-db）只写一个就行了，如果同时使用那么只有白名单生效。 五、Mysql主从复制的过程MySQL主从复制的两种情况：同步复制和异步复制，实际复制架构中大部分为异步复制。 复制的基本过程如下： Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容。 Master接收到来自Slave的IO进程的请求后，负责复制的IO进程会根据请求信息读取日志指定位置之后的日志信息，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置。 Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的 bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”。 Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行。 六、Mysql主从复制的具体配置复制通常用来创建主节点的副本，通过添加冗余节点来保证高可用性，当然复制也可以用于其他用途，例如在从节点上进行数据读、分析等等。在横向扩展的业务中，复制很容易实施，主要表现在在利用主节点进行写操作，多个从节点进行读操作，MySQL复制的异步性是指：事物首先在主节点上提交，然后复制给从节点并在从节点上应用，这样意味着在同一个时间点主从上的数据可能不一致。异步复制的好处在于它比同步复制要快，如果对数据的一致性要求很高，还是采用同步复制较好。 最简单的复制模式就是一主一从的复制模式了，这样一个简单的架构只需要三个步骤即可完成： （1）建立一个主节点，开启binlog，设置服务器id； （2）建立一个从节点，设置服务器id； （3）将从节点连接到主节点上。 下面我们开始操作，以MySQL 5.5为例，操作系统Ubuntu12.10，Master 10.1.6.159 Slave 10.1.6.191。 1apt-get install mysql-server Master机器Master上面开启binlog日志，并且设置一个唯一的服务器id，在局域网内这个id必须唯一。二进制的binlog日志记录master上的所有数据库改变，这个日志会被复制到从节点上，并且在从节点上回放。修改my.cnf文件，在mysqld模块下修改如下内容： 123[mysqld]server-id = 1log_bin = /var/log/mysql/mysql-bin.log log_bin设置二进制日志所产生文件的基本名称，二进制日志由一系列文件组成，log_bin的值是可选项，如果没有为log_bin设置值，则默认值是：主机名-bin。如果随便修改主机名，则binlog日志的名称也会被改变的。server-id是用来唯一标识一个服务器的，每个服务器的server-id都不一样。这样slave连接到master后，会请求master将所有的binlog传递给它，然后将这些binlog在slave上回放。为了防止权限混乱，一般都是建立一个单独用于复制的账户。 binlog是复制过程的关键，它记录了数据库的所有改变，通常即将执行完毕的语句会在binlog日志的末尾写入一条记录，binlog只记录改变数据库的语句，对于不改变数据库的语句则不进行记录。这种情况叫做基于语句的复制，前面提到过还有一种情况是基于行的复制，两种模式各有各的优缺点。 Slave机器slave机器和master一样，需要一个唯一的server-id。 12[mysqld]server-id = 2 连接Slave到Master 在Master和Slave都配置好后，只需要把slave只想master即可 123change master to master_host='10.1.6.159',master_port=3306,master_user='rep',master_password='123456';start slave; 接下来在master上做一些针对改变数据库的操作，来观察slave的变化情况。在修改完my.cnf配置重启数据库后，就开始记录binlog了。可以在/var/log/mysql目录下看到一个mysql-bin.000001文件，而且还有一个mysql-bin.index文件，这个mysql-bin.index文件是什么？这个文件保存了所有的binlog文件列表，但是我们在配置文件中并没有设置改值，这个可以通过log_bin_index进行设置，如果没有设置改值，则默认值和log_bin一样。在master上执行show binlog events命令，可以看到第一个binlog文件的内容。 注意：上面的sql语句是从头开始复制第一个binlog，如果想从某个位置开始复制binlog，就需要在change master to时指定要开始的binlog文件名和语句在文件中的起点位置，参数如下：master_log_file和master_log_pos。 1234567891011121314151617181920212223mysql&gt; show binlog events\G*************************** 1. row *************************** Log_name: mysql-bin.000001 Pos: 4 Event_type: Format_desc Server_id: 1End_log_pos: 107 Info: Server ver: 5.5.28-0ubuntu0.12.10.2-log, Binlog ver: 4*************************** 2. row *************************** Log_name: mysql-bin.000001 Pos: 107 Event_type: Query Server_id: 1End_log_pos: 181 Info: create user rep*************************** 3. row *************************** Log_name: mysql-bin.000001 Pos: 181 Event_type: Query Server_id: 1End_log_pos: 316 Info: grant replication slave on *.* to rep identified by '123456'3 rows in set (0.00 sec) Log_name 是二进制日志文件的名称，一个事件不能横跨两个文件 Pos 这是该事件在文件中的开始位置 Event_type 事件的类型，事件类型是给slave传递信息的基本方法，每个新的binlog都已Format_desc类型开始，以Rotate类型结束 Server_id 创建该事件的服务器id End_log_pos 该事件的结束位置，也是下一个事件的开始位置，因此事件范围为Pos~End_log_pos-1 Info 事件信息的可读文本，不同的事件有不同的信息 示例 在master的test库中创建一个rep表，并插入一条记录。 123create table rep(name var);insert into rep values ("guol");flush logs; flush logs命令强制轮转日志，生成一个新的二进制日志，可以通过show binlog events in ‘xxx’来查看该二进制日志。可以通过show master status查看当前正在写入的binlog文件。这样就会在slave上执行相应的改变操作。 上面就是最简单的主从复制模式，不过有时候随着时间的推进，binlog会变得非常庞大，如果新增加一台slave，从头开始复制master的binlog文件是非常耗时的，所以我们可以从一个指定的位置开始复制binlog日志，可以通过其他方法把以前的binlog文件进行快速复制，例如copy物理文件。在change master to中有两个参数可以实现该功能，master_log_file和master_log_pos，通过这两个参数指定binlog文件及其位置。我们可以从master上复制也可以从slave上复制，假如我们是从master上复制，具体操作过程如下： （1）为了防止在操作过程中数据更新，导致数据不一致，所以需要先刷新数据并锁定数据库：flush tables with read lock。 （2）检查当前的binlog文件及其位置：show master status。 1234567mysql&gt; show master status\G*************************** 1. row ***************************File: mysql-bin.000003Position: 107Binlog_Do_DB:Binlog_Ignore_DB:1 row in set (0.00 sec) （3）通过mysqldump命令创建数据库的逻辑备分：mysqldump –all-databases -hlocalhost -p &gt;back.sql。 （4）有了master的逻辑备份后，对数据库进行解锁：unlock tables。 （5）把back.sql复制到新的slave上，执行：mysql -hlocalhost -p 把master的逻辑备份插入slave的数据库中。 （6）现在可以把新的slave连接到master上了，只需要在change master to中多设置两个参数master_log_file=’mysql-bin.000003’和master_log_pos=’107’即可，然后启动slave：start slave，这样slave就可以接着107的位置进行复制了。 123change master to master_host='10.1.6.159',master_port=3306,master_user='rep',master_password='123456',master_log_file='mysql-bin.000003',master_log_pos='107';start slave; 有时候master并不能让你锁住表进行复制，因为可能跑一些不间断的服务，如果这时master已经有了一个slave，我们则可以通过这个slave进行再次扩展一个新的slave。原理同在master上进行复制差不多，关键在于找到binlog的位置，你在复制的同时可能该slave也在和master进行同步，操作如下： （1）为了防止数据变动，还是需要停止slave的同步：stop slave。 （2）然后刷新表，并用mysqldump逻辑备份数据库。 （3）使用show slave status查看slave的相关信息，记录下两个字段的值Relay_Master_Log_File和Exec_Master_Log_Pos，这个用来确定从后面哪里开始复制。 （4）对slave解锁，把备份的逻辑数据库导入新的slave的数据库中，然后设置change master to，这一步和复制master一样。 七、深入了解Mysql主从配置一主多从由一个master和一个slave组成复制系统是最简单的情况。Slave之间并不相互通信，只能与master进行通信。在实际应用场景中，MySQL复制90%以上都是一个Master复制到一个或者多个Slave的架构模式，主要用于读压力比较大的应用的数据库端廉价扩展解决方案。 [ 在上图中，是我们开始时提到的一主多从的情况，这时主库既要负责写又要负责为几个从库提供二进制日志。这种情况将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从，或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。 主主复制[ 上图中，Master-Master复制的两台服务器，既是master，又是另一台服务器的slave。这样，任何一方所做的变更，都会通过复制应用到另外一方的数据库中。在这种复制架构中，各自上运行的不是同一db，比如左边的是db1,右边的是db2，db1的从在右边反之db2的从在左边，两者互为主从，再辅助一些监控的服务还可以实现一定程度上的高可以用。 主动—被动模式的Master-Master(Master-Master in Active-Passive Mode)[ 上图中，这是由master-master结构变化而来的，它避免了M-M的缺点，实际上，这是一种具有容错和高可用性的系统。它的不同点在于其中只有一个节点在提供读写服务，另外一个节点时刻准备着，当主节点一旦故障马上接替服务。比如通过corosync+pacemaker+drbd+MySQL就可以提供这样一组高可用服务，主备模式下再跟着slave服务器，也可以实现读写分离。 带从服务器的Master-Master结构(Master-Master with Slaves)[ 这种结构的优点就是提供了冗余。在地理上分布的复制结构，它不存在单一节点故障问题，而且还可以将读密集型的请求放到slave上。 MySQL-5.5支持半同步复制 早前的MySQL复制只能是基于异步来实现，从MySQL-5.5开始，支持半自动复制。在以前的异步（asynchronous）复制中，主库在执行完一些事务后，是不会管备库的进度的。如果备库处于落后，而更不幸的是主库此时又出现Crash（例如宕机），这时备库中的数据就是不完整的。简而言之，在主库发生故障的时候，我们无法使用备库来继续提供数据一致的服务了。Semisynchronous Replication(半同步复制)则一定程度上保证提交的事务已经传给了至少一个备库。Semi synchronous中，仅仅保证事务的已经传递到备库上，但是并不确保已经在备库上执行完成了。 此外，还有一种情况会导致主备数据不一致。在某个session中，主库上提交一个事务后，会等待事务传递给至少一个备库，如果在这个等待过程中主库Crash，那么也可能备库和主库不一致，这是很致命的。如果主备网络故障或者备库挂了，主库在事务提交后等待10秒（rpl_semi_sync_master_timeout的默认值）后，就会继续。这时，主库就会变回原来的异步状态。 MySQL在加载并开启Semi-sync插件后，每一个事务需等待备库接收日志后才返回给客户端。如果做的是小事务，两台主机的延迟又较小，则Semi-sync可以实现在性能很小损失的情况下的零数据丢失。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中clone常见的三种方式]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-clone-threeMethod%2F</url>
    <content type="text"><![CDATA[在 JAVA 中克隆一个对象常见的有三种形式 : 通过自己写一个克隆方法，里面 new 一个同样的对象来进行 get、set 依次赋值实现深度克隆（很繁琐且易出错）； 通过实现 Cloneable 接口并重写 Object 类的 clone() 方法（分为深浅两种方式）； 通过实现 Serializable 接口并用对象的序列化和反序列化来实现真正的深度克隆； 下面介绍第二、第三种方法。 Cloneable 接口实现克隆Cloneable 接口实现浅克隆12345678910111213141516171819202122232425262728public class People implements Cloneable &#123; private String name = "ilt"; private Hand hand = new Hand(); public Hand getHand() &#123; return hand; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub return super.clone(); &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; People p1 = new People(); People p2 = (People) p1.clone(); System.out.println("第一个对象的hash值:"+p1.hashCode()); System.out.println("第二个对象的hash值:"+p2.hashCode()); System.out.println("分割线-----------"); System.out.println("p1中的hand对象的hash值:"+p1.getHand().hashCode()); System.out.println("p2中的hand对象的hash值:"+p2.getHand().hashCode()); &#125;&#125;class Hand implements Cloneable &#123;&#125; 上面代码输出的结果如下，根据hash值相等能确定两个对象是否相等的原则，发现p1和p2不等，但p1中的hand对象与p2中的hand对象是相等的。Cloneable 接口实现克隆是先在内存中开辟一块和原始对象一样的空间，然后原样拷贝原始对象中的内容，对基本数据类型就是值复制，而对非基本类型变量保存的仅仅是对象的引用，所以会导致 clone 后的非基本类型变量和原始对象中相应的变量指向的是同一个对象。 12345第一个对象的hash值:1408448235第二个对象的hash值:77244764分割线-----------p1中的hand对象的hash值:1172625760p2中的hand对象的hash值:1172625760 Cloneable 接口实现深克隆在浅度克隆的基础上对于要克隆对象中的非基本数据类型的属性对应的类也实现克隆，这样对于非基本数据类型的属性复制的不是一份引用。 123456789101112131415161718192021222324252627282930313233343536373839public class People implements Cloneable &#123; private String name = "ilt"; private Hand hand = new Hand(); public Hand getHand() &#123; return hand; &#125; public void setHand(Hand hand) &#123; this.hand = hand; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub People p2 = (People) super.clone(); p2.setHand((Hand) hand.clone()); return p2; &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; People p1 = new People(); People p2 = (People) p1.clone(); System.out.println("第一个对象的hash值:" + p1.hashCode()); System.out.println("第二个对象的hash值:" + p2.hashCode()); System.out.println("分割线-----------"); System.out.println("p1中的hand对象的hash值:" + p1.getHand().hashCode()); System.out.println("p2中的hand对象的hash值:" + p2.getHand().hashCode()); &#125;&#125;class Hand implements Cloneable &#123; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub return super.clone(); &#125;&#125; 结果如下，证明已经进行深克隆 12345第一个对象的hash值:1172625760第二个对象的hash值:863719801分割线-----------p1中的hand对象的hash值:1696725334p2中的hand对象的hash值:427340025 序列化与反序列化实现深克隆对象序列化操作可以将对象的状态转换成字节流传输或者存储再生，我们可以借用这一特点实现对象的深度克隆，特别是当我们的对象嵌套非常复杂且想实现深度克隆时如果使用序列化方式会大大减少代码量。 1234567891011121314151617181920212223242526272829public class TestClone implements Serializable&#123; private static final long serialVersionUID = 1L; public String name = "ilt"; public static void main(String[] args) throws Exception &#123; TestClone t1 = new TestClone(); byte[] b = ObjectUtil.objectToBytes(t1);//序列化 TestClone t2 = (TestClone) ObjectUtil.bytesToObject(b);//反序列化 System.out.println("t1对象的name："+t1.name); System.out.println("t2对象的name："+t2.name); System.out.println("分割线-------------"); System.out.println("t1对象的hash值为："+t1.hashCode()); System.out.println("t2对象的hash值为："+t2.hashCode()); System.out.println("分割线-------------"); System.out.println("t1中的obj对象的hash值为："+t1.obj.hashCode()); System.out.println("t2中的obj对象的hash值为："+t2.obj.hashCode()); &#125; class Bean implements Serializable&#123; private static final long serialVersionUID = 1L; &#125;&#125; 结果如下，证明对象的属性被深克隆下来了 12345678t1对象的name：iltt2对象的name：ilt分割线-------------t1对象的hash值为：1847546936t2对象的hash值为：812610706分割线-------------t1中的obj对象的hash值为：1164730192t2中的obj对象的hash值为：1699624469 作者：youngerTree来源：CSDN原文：https://blog.csdn.net/syilt/article/details/78482927版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>clone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中volatile关键字的作用]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-volatile%2F</url>
    <content type="text"><![CDATA[以下文章来源于：Java并发：volatile内存可见性和指令重排 volatile两大作用1、保证内存可见性 2、防止指令重排 此外需注意volatile并不保证操作的原子性。 （一）内存可见性1 概念JVM内存模型：主内存和线程独立的工作内存 Java内存模型规定，对于多个线程共享的变量，存储在主内存当中，每个线程都有自己独立的工作内存（比如CPU的寄存器），线程只能访问自己的工作内存，不可以访问其它线程的工作内存。 工作内存中保存了主内存共享变量的副本，线程要操作这些共享变量，只能通过操作工作内存中的副本来实现，操作完毕之后再同步回到主内存当中。 如何保证多个线程操作主内存的数据完整性是一个难题，Java内存模型也规定了工作内存与主内存之间交互的协议，定义了8种原子操作： 123456789101112131415(1) lock:将主内存中的变量锁定，为一个线程所独占(2) unclock:将lock加的锁定解除，此时其它的线程可以有机会访问此变量(3) read:将主内存中的变量值读到工作内存当中(4) load:将read读取的值保存到工作内存中的变量副本中。(5) use:将值传递给线程的代码执行引擎(6) assign:将执行引擎处理返回的值重新赋值给变量副本(7) store:将变量副本的值存储到主内存中。(8) write:将store存储的值写入到主内存的共享变量当中。 通过上面Java内存模型的概述，我们会注意到这么一个问题，每个线程在获取锁之后会在自己的工作内存来操作共享变量，操作完成之后将工作内存中的副本回写到主内存，并且在其它线程从主内存将变量同步回自己的工作内存之前，共享变量的改变对其是不可见的。即其他线程的本地内存中的变量已经是过时的，并不是更新后的值。 2 内存可见性带来的问题很多时候我们需要一个线程对共享变量的改动，其它线程也需要立即得知这个改动该怎么办呢？下面举两个例子说明内存可见性的重要性： 例子1有一个全局的状态变量open: 1`boolean` `open=``true``;` 这个变量用来描述对一个资源的打开关闭状态，true表示打开，false表示关闭，假设有一个线程A,在执行一些操作后将open修改为false: 123//线程Aresource.close();open = false; 线程B随时关注open的状态，当open为true的时候通过访问资源来进行一些操作: 1234//线程Bwhile(open) &#123;doSomethingWithResource(resource);&#125; 当A把资源关闭的时候，open变量对线程B是不可见的，如果此时open变量的改动尚未同步到线程B的工作内存中,那么线程B就会用一个已经关闭了的资源去做一些操作，因此产生错误。 例子2下面是一个通过布尔标志判断线程是否结束的例子： 12345678910111213141516171819202122232425public class CancelThreadTest &#123; publicstatic void main(String[] args) throws Exception&#123; PrimeGeneratorgen = new PrimeGenerator(); newThread(gen).start(); try &#123; Thread.sleep(3000); &#125;finally&#123; gen.cancel(); &#125; &#125;&#125; class PrimeGenerator implements Runnable&#123; privateboolean cancelled; @Override publicvoid run() &#123; while(!cancelled) &#123; System.out.println("Running..."); //doingsomething here... &#125; &#125; publicvoid cancel()&#123;cancelled = true;&#125;&#125; 主线程中设置PrimeGenerator线程的是否取消标识，PrimeGenerator线程检测到这个标识后就会结束线程，由于主线程修改cancelled变量的内存可见性，主线程修改cancelled标识后并不马上同步回主内存，所以PrimeGenerator线程结束的时间难以把控（最终是一定会同步回主内存，让PrimeGenerator线程结束）。 如果PrimeGenerator线程执行一些比较关键的操作，主线程希望能够及时终止它，这时将cenceled用volatile关键字修饰就是必要的。 特别注意：上面演示这个并不是正确的取消线程的方法，因为一旦PrimeGenerator线程中包含BolckingQueue.put()等阻塞方法，那么将可能永远不会去检查cancelled标识，导致线程永远不会退出。正确的方法参见另外一篇关于如何正确终止线程的方法。 3 提供内存可见性volatile保证可见性的原理是在每次访问变量时都会进行一次刷新，因此每次访问都是主内存中最新的版本。所以volatile关键字的作用之一就是保证变量修改的实时可见性。 针对上面的例子1： 要求一个线程对open的改变，其他的线程能够立即可见，Java为此提供了volatile关键字，在声明open变量的时候加入volatile关键字就可以保证open的内存可见性，即open的改变对所有的线程都是立即可见的。 针对上面的例子2： 将cancelled标志设置的volatile保证主线程针对cancelled标识的修改能够让PrimeGenerator线程立马看到。 备注：也可以通过提供synchronized同步的open变量的Get/Set方法解决此内存可见性问题，因为要Get变量open，必须等Set方完全释放锁之后。后面将介绍到两者的区别。 （二）指令重排1 概念指令重排序是JVM为了优化指令，提高程序运行效率，在不影响单线程程序执行结果的前提下，尽可能地提高并行度。编译器、处理器也遵循这样一个目标。注意是单线程。多线程的情况下指令重排序就会给程序员带来问题。 不同的指令间可能存在数据依赖。比如下面计算圆的面积的语句： 123double r = 2.3d;//(1)double pi =3.1415926; //(2)double area = pi* r * r; //(3) area的计算依赖于r与pi两个变量的赋值指令。而r与pi无依赖关系。 as-if-serial语义是指：不管如何重排序（编译器与处理器为了提高并行度），（单线程）程序的结果不能被改变。这是编译器、Runtime、处理器必须遵守的语义。 虽然，（1） – happensbefore -&gt; （2）,（2） – happens before -&gt; （3），但是计算顺序(1)(2)(3)与(2)(1)(3) 对于r、pi、area变量的结果并无区别。编译器、Runtime在优化时可以根据情况重排序（1）与（2），而丝毫不影响程序的结果。 指令重排序包括编译器重排序和运行时重排序。 2 指令重排带来的问题如果一个操作不是原子的，就会给JVM留下重排的机会。下面看几个例子： 例子1：A线程指令重排导致B线程出错对于在同一个线程内，这样的改变是不会对逻辑产生影响的，但是在多线程的情况下指令重排序会带来问题。看下面这个情景: 在线程A中: 12context = loadContext();inited = true; 在线程B中: 1234while(!inited )&#123; //根据线程A中对inited变量的修改决定是否使用context变量 sleep(100);&#125;doSomethingwithconfig(context); 假设线程A中发生了指令重排序: 12inited = true;context = loadContext(); 那么B中很可能就会拿到一个尚未初始化或尚未初始化完成的context,从而引发程序错误。 例子2：指令重排导致单例模式失效我们都知道一个经典的懒加载方式的双重判断单例模式： 1234567891011121314public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance == null) &#123; synchronzied(Singleton.class) &#123; if(instance == null) &#123; &lt;strong&gt;instance = new Singleton(); //非原子操作 &#125; &#125; &#125; return instance; &#125;&#125; 看似简单的一段赋值语句：instance= new Singleton()，但是很不幸它并不是一个原子操作，其实际上可以抽象为下面几条JVM指令： 123memory =allocate(); //1：分配对象的内存空间 ctorInstance(memory); //2：初始化对象 instance =memory; //3：设置instance指向刚分配的内存地址 上面操作2依赖于操作1，但是操作3并不依赖于操作2，所以JVM是可以针对它们进行指令的优化重排序的，经过重排序后如下： 123memory =allocate(); //1：分配对象的内存空间 instance =memory; //3：instance指向刚分配的内存地址，此时对象还未初始化ctorInstance(memory); //2：初始化对象 可以看到指令重排之后，instance指向分配好的内存放在了前面，而这段内存的初始化被排在了后面。 在线程A执行这段赋值语句，在初始化分配对象之前就已经将其赋值给instance引用，恰好另一个线程进入方法判断instance引用不为null，然后就将其返回使用，导致出错。 3 防止指令重排除了前面内存可见性中讲到的volatile关键字可以保证变量修改的可见性之外，还有另一个重要的作用：在JDK1.5之后，可以使用volatile变量禁止指令重排序。 解决方案：例子1中的inited和例子2中的instance以关键字volatile修饰之后，就会阻止JVM对其相关代码进行指令重排，这样就能够按照既定的顺序指执行。 volatile关键字通过提供“内存屏障”的方式来防止指令被重排序，为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 （三）总结volatile是轻量级同步机制相对于synchronized块的代码锁，volatile应该是提供了一个轻量级的针对共享变量的锁，当我们在多个线程间使用共享变量进行通信的时候需要考虑将共享变量用volatile来修饰。 volatile是一种稍弱的同步机制，在访问volatile变量时不会执行加锁操作，也就不会执行线程阻塞，因此volatilei变量是一种比synchronized关键字更轻量级的同步机制。 volatile使用建议使用建议：在两个或者更多的线程需要访问的成员变量上使用volatile。当要访问的变量已在synchronized代码块中，或者为常量时，没必要使用volatile。 由于使用volatile屏蔽掉了JVM中必要的代码优化，所以在效率上比较低，因此一定在必要时才使用此关键字。 volatile和synchronized区别1、volatile不会进行加锁操作： volatile变量是一种稍弱的同步机制，在访问volatile变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile变量是一种比synchronized关键字更轻量级的同步机制。 2、volatile变量作用类似于同步变量读写操作： 从内存可见性的角度看，写入volatile变量相当于退出同步代码块，而读取volatile变量相当于进入同步代码块。 3、volatile不如synchronized安全： 在代码中如果过度依赖volatile变量来控制状态的可见性，通常会比使用锁的代码更脆弱，也更难以理解。仅当volatile变量能简化代码的实现以及对同步策略的验证时，才应该使用它。一般来说，用同步机制会更安全些。 4、volatile无法同时保证内存可见性和原子性： 加锁机制（即同步机制）既可以确保可见性又可以确保原子性，而volatile变量只能确保可见性，原因是声明为volatile的简单变量如果当前值与该变量以前的值相关，那么volatile关键字不起作用，也就是说如下的表达式都不是原子操作：“count++”、“count = count+1”。 当且仅当满足以下所有条件时，才应该使用volatile变量： 1231、对变量的写入操作不依赖变量的当前值，或者你能确保只有单个线程更新变量的值。2、该变量没有包含在具有其他变量的不变式中。 总结：在需要同步的时候，第一选择应该是synchronized关键字，这是最安全的方式，尝试其他任何方式都是有风险的。尤其在、jdK1.5之后，对synchronized同步机制做了很多优化，如：自适应的自旋锁、锁粗化、锁消除、轻量级锁等，使得它的性能明显有了很大的提升。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式]]></title>
    <url>%2F2019%2F06%2F17%2Fjava-design-pattern%2F</url>
    <content type="text"><![CDATA[以下文章来自于：Java 设计模式 一直想写一篇介绍设计模式的文章，让读者可以很快看完，而且一看就懂，看懂就会用，同时不会将各个模式搞混。自认为本文还是写得不错的???，花了不少心思来写这文章和做图，力求让读者真的能看着简单同时有所收获。 设计模式是对大家实际工作中写的各种代码进行高层次抽象的总结，其中最出名的当属 1234Gang of Four ( GoF ) 的分类了，他们将设计模式分类为 23 种经典的模式，根据用途我们又可以分为三大类，分别为创建型模式、结构型模式和行为型模式。是的，我不善于扯这些有的没的，还是少点废话吧~ 有一些重要的设计原则在开篇和大家分享下，这些原则将贯通全文： 面向接口编程，而不是面向实现。这个很重要，也是优雅的、可扩展的代码的第一步，这就不需要多说了吧。 职责单一原则。每个类都应该只有一个单一的功能，并且该功能应该由这个类完全封装起来。 对修改关闭，对扩展开放。对修改关闭是说，我们辛辛苦苦加班写出来的代码，该实现的功能和该修复的 bug 都完成了，别人可不能说改就改；对扩展开放就比较好理解了，也就是说在我们写好的代码基础上，很容易实现扩展。 创建型模式创建型模式的作用就是创建对象，说到创建一个对象，最熟悉的就是 new 一个对象，然后 set 相关属性。但是，在很多场景下，我们需要给客户端提供更加友好的创建对象的方式，尤其是那种我们定义了类，但是需要提供给其他开发者用的时候。 简单工厂模式和名字一样简单，非常简单，直接上代码吧： 1234567891011121314151617public class FoodFactory &#123; public static Food makeFood(String name) &#123; if (name.equals("noodle")) &#123; Food noodle = new LanZhouNoodle(); noodle.addSpicy("more"); return noodle; &#125; else if (name.equals("chicken")) &#123; Food chicken = new HuangMenChicken(); chicken.addCondiment("potato"); return chicken; &#125; else &#123; return null; &#125; &#125;&#125;复制代码 其中，LanZhouNoodle 和 HuangMenChicken 都继承自 Food。 简单地说，简单工厂模式通常就是这样，一个工厂类 XxxFactory，里面有一个静态方法，根据我们不同的参数，返回不同的派生自同一个父类（或实现同一接口）的实例对象。 我们强调职责单一原则，一个类只提供一种功能，FoodFactory 的功能就是只要负责生产各种 Food。 工厂模式简单工厂模式很简单，如果它能满足我们的需要，我觉得就不要折腾了。之所以需要引入工厂模式，是因为我们往往需要使用两个或两个以上的工厂。 123456789101112131415161718192021222324252627282930public interface FoodFactory &#123; Food makeFood(String name);&#125;public class ChineseFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new ChineseFoodA(); &#125; else if (name.equals("B")) &#123; return new ChineseFoodB(); &#125; else &#123; return null; &#125; &#125;&#125;public class AmericanFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new AmericanFoodA(); &#125; else if (name.equals("B")) &#123; return new AmericanFoodB(); &#125; else &#123; return null; &#125; &#125;&#125;复制代码 其中，ChineseFoodA、ChineseFoodB、AmericanFoodA、AmericanFoodB 都派生自 Food。 客户端调用： 123456789public class APP &#123; public static void main(String[] args) &#123; // 先选择一个具体的工厂 FoodFactory factory = new ChineseFoodFactory(); // 由第一步的工厂产生具体的对象，不同的工厂造出不一样的对象 Food food = factory.makeFood("A"); &#125;&#125;复制代码 虽然都是调用 makeFood(“A”) 制作 A 类食物，但是，不同的工厂生产出来的完全不一样。 第一步，我们需要选取合适的工厂，然后第二步基本上和简单工厂一样。 核心在于，我们需要在第一步选好我们需要的工厂。比如，我们有 LogFactory 接口，实现类有 FileLogFactory 和 KafkaLogFactory，分别对应将日志写入文件和写入 Kafka 中，显然，我们客户端第一步就需要决定到底要实例化 FileLogFactory 还是 KafkaLogFactory，这将决定之后的所有的操作。 虽然简单，不过我也把所有的构件都画到一张图上，这样读者看着比较清晰： 抽象工厂模式当涉及到产品族的时候，就需要引入抽象工厂模式了。 一个经典的例子是造一台电脑。我们先不引入抽象工厂模式，看看怎么实现。 因为电脑是由许多的构件组成的，我们将 CPU 和主板进行抽象，然后 CPU 由 CPUFactory 生产，主板由 MainBoardFactory 生产，然后，我们再将 CPU 和主板搭配起来组合在一起，如下图： 这个时候的客户端调用是这样的： 1234567891011// 得到 Intel 的 CPUCPUFactory intelCPUFactory = new IntelCPUFactory();CPU cpu = intelCPUFactory.makeCPU();// 得到 AMD 的主板MainBoardFactory mainBoardFactory = new AmdMainBoardFactory();MainBoard mainBoard = mainBoardFactory.make();// 组装 CPU 和主板Computer computer = new Computer(cpu, mainBoard);复制代码 单独看 CPU 工厂和主板工厂，它们分别是前面我们说的工厂模式。这种方式也容易扩展，因为要给电脑加硬盘的话，只需要加一个 HardDiskFactory 和相应的实现即可，不需要修改现有的工厂。 但是，这种方式有一个问题，那就是如果 Intel 家产的 CPU 和 AMD 产的主板不能兼容使用，那么这代码就容易出错，因为客户端并不知道它们不兼容，也就会错误地出现随意组合。 下面就是我们要说的产品族的概念，它代表了组成某个产品的一系列附件的集合： 当涉及到这种产品族的问题的时候，就需要抽象工厂模式来支持了。我们不再定义 CPU 工厂、主板工厂、硬盘工厂、显示屏工厂等等，我们直接定义电脑工厂，每个电脑工厂负责生产所有的设备，这样能保证肯定不存在兼容问题。 这个时候，对于客户端来说，不再需要单独挑选 CPU厂商、主板厂商、硬盘厂商等，直接选择一家品牌工厂，品牌工厂会负责生产所有的东西，而且能保证肯定是兼容可用的。 1234567891011121314public static void main(String[] args) &#123; // 第一步就要选定一个“大厂” ComputerFactory cf = new AmdFactory(); // 从这个大厂造 CPU CPU cpu = cf.makeCPU(); // 从这个大厂造主板 MainBoard board = cf.makeMainBoard(); // 从这个大厂造硬盘 HardDisk hardDisk = cf.makeHardDisk(); // 将同一个厂子出来的 CPU、主板、硬盘组装在一起 Computer result = new Computer(cpu, board, hardDisk);&#125;复制代码 当然，抽象工厂的问题也是显而易见的，比如我们要加个显示器，就需要修改所有的工厂，给所有的工厂都加上制造显示器的方法。这有点违反了对修改关闭，对扩展开放这个设计原则。 单例模式单例模式用得最多，错得最多。 饿汉模式最简单： 123456789101112131415public class Singleton &#123; // 首先，将 new Singleton() 堵死 private Singleton() &#123;&#125;; // 创建私有静态实例，意味着这个类第一次使用的时候就会进行创建 // 这个代码还可以放在静态代码块中 private static Singleton instance = new Singleton(); public static Singleton getInstance() &#123; return instance; &#125; // 瞎写一个静态方法。这里想说的是，如果我们只是要调用 Singleton.getDate(...)， // 本来是不想要生成 Singleton 实例的，不过没办法，已经生成了 public static Date getDate(String mode) &#123;return new Date();&#125;&#125;复制代码 很多人都能说出饿汉模式的缺点(浪费内存空间)，可是我觉得生产过程中，很少碰到这种情况：你定义了一个单例的类，不需要其实例，可是你却把一个或几个你会用到的静态方法塞到这个类中。 饱汉模式最容易出错： 123456789101112131415161718192021public class Singleton &#123; // 首先，也是先堵死 new Singleton() 这条路 private Singleton() &#123;&#125; // 和饿汉模式相比，这边不需要先实例化出来，注意这里的 volatile，它是必须的 private static volatile Singleton instance = null; public static Singleton getInstance() &#123; if (instance == null) &#123; // 加锁 synchronized (Singleton.class) &#123; // 这一次判断也是必须的，不然会有并发问题 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125;复制代码 volatile 关键字可以保证 instance 的内存可见性和防止指令重排序，使得 instance 在多线程环境下依然可以正确的被初始化。详情请移步这里：java中volatile关键字的作用。 双重检查，指的是两次检查 instance 是否为 null。 volatile 在这里是需要的，希望能引起读者的关注。 很多人不知道怎么写，直接就在 getInstance() 方法签名上加上 synchronized，这就不多说了，性能太差。 嵌套类最经典，以后大家就用它吧： 123456789101112public class Singleton3 &#123; private Singleton3() &#123;&#125; // 主要是使用了 嵌套类可以访问外部类的静态属性和静态方法 的特性 private static class Holder &#123; private static Singleton3 instance = new Singleton3(); &#125; public static Singleton3 getInstance() &#123; return Holder.instance; &#125;&#125;复制代码 注意，很多人都会把这个嵌套类说成是静态内部类，严格地说，内部类和嵌套类是不一样的，它们能访问的外部类权限也是不一样的。 最后，一定有人跳出来说用枚举实现单例，是的没错，枚举类很特殊，它在类加载的时候会初始化里面的所有的实例，而且 JVM 保证了它们不会再被实例化，所以它天生就是单例的。不说了，读者自己看着办吧，不建议使用。 建造者模式经常碰见的 XxxBuilder 的类，通常都是建造者模式的产物。建造者模式其实有很多的变种，但是对于客户端来说，我们的使用通常都是一个模式的： 123Food food = new FoodBuilder().a().b().c().build();Food food = Food.builder().a().b().c().build();复制代码 套路就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 来一个中规中矩的建造者模式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class User &#123; // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) &#123; this.name = name; this.password = password; this.nickName = nickName; this.age = age; &#125; // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() &#123; return new UserBuilder(); &#125; public static class UserBuilder &#123; // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() &#123; &#125; // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) &#123; this.name = name; return this; &#125; public UserBuilder password(String password) &#123; this.password = password; return this; &#125; public UserBuilder nickName(String nickName) &#123; this.nickName = nickName; return this; &#125; public UserBuilder age(int age) &#123; this.age = age; return this; &#125; // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() &#123; if (name == null || password == null) &#123; throw new RuntimeException("用户名和密码必填"); &#125; if (age &lt;= 0 || age &gt;= 150) &#123; throw new RuntimeException("年龄不合法"); &#125; // 还可以做赋予”默认值“的功能 if (nickName == null) &#123; nickName = name; &#125; return new User(name, password, nickName, age); &#125; &#125;&#125;复制代码 核心是：先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。 看看客户端的调用： 12345678910public class APP &#123; public static void main(String[] args) &#123; User d = User.builder() .name("foo") .password("pAss12345") .age(25) .build(); &#125;&#125;复制代码 说实话，建造者模式的链式写法很吸引人，但是，多写了很多“无用”的 builder 的代码，感觉这个模式没什么用。不过，当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 题外话，强烈建议读者使用 lombok，用了 lombok 以后，上面的一大堆代码会变成如下这样: 12345678@Builderclass User &#123; private String name; private String password; private String nickName; private int age;&#125;复制代码 怎么样，省下来的时间是不是又可以干点别的了。 当然，如果你只是想要链式写法，不想要建造者模式，有个很简单的办法，User 的 getter 方法不变，所有的 setter 方法都让其 return this 就可以了，然后就可以像下面这样调用： 12User user = new User().setName("").setPassword("").setAge(20);复制代码 原型模式这是我要说的创建型模式的最后一个设计模式了。 原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 12protected native Object clone() throws CloneNotSupportedException;复制代码 java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 原型模式了解到这里我觉得就够了，各种变着法子说这种代码或那种代码是原型模式，没什么意义。 创建型模式总结创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 123456简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式;单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源；建造者模式专门对付属性很多的那种类，为了让代码更优美；原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 理解代理这个词，这个模式其实就简单了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken("1kg"); f.setSpicy("1g"); f.setSalt("3g"); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle("500g"); f.setSalt("5g"); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println("我们马上要开始制作鸡肉了"); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println("鸡肉制作完成啦，加点胡椒粉"); // 增强 food.addCondiment("pepper"); return food; &#125; public Food makeNoodle() &#123; System.out.println("准备制作拉面~"); Food food = foodService.makeNoodle(); System.out.println("制作完成啦") return food; &#125;&#125;复制代码 客户端调用，注意，我们要用代理来实例化接口： 1234// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken();复制代码 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 1234567891011public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125;复制代码 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 123456789101112131415161718192021222324252627public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125;复制代码 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 123456789101112public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125;复制代码 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 12345678910111213141516171819public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println("咕咕叫"); &#125; public void fly() &#123; System.out.println("鸡也会飞哦"); &#125;&#125;复制代码 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 12345678910111213141516171819202122// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125;复制代码 客户端调用很简单了： 12345678public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125;复制代码 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 1234public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125;复制代码 然后是一系列实现类： 12345678910111213141516171819public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;复制代码 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 123456789public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125;复制代码 定义抽象类的子类： 12345678910111213141516171819202122232425262728// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125;复制代码 最后，我们来看客户端演示： 12345678public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125;复制代码 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 本节引用了这里的例子，并对其进行了修改。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。 首先，定义饮料抽象基类： 1234567public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125;复制代码 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 123456789101112131415161718public class BlackTea extends Beverage &#123; public String getDescription() &#123; return "红茶"; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return "绿茶"; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略复制代码 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 12345// 调料public abstract class Condiment extends Beverage &#123;&#125;复制代码 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 123456789101112131415161718192021222324252627282930public class Lemon extends Condiment &#123; private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + ", 加柠檬"; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + ", 加芒果"; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类复制代码 看客户端调用： 1234567891011public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + " 价格：￥" + beverage.cost()); //"绿茶, 加柠檬, 加芒果 价格：￥16"&#125;复制代码 如果我们需要芒果珍珠双份柠檬红茶： 12Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea()))));复制代码 是不是很变态？ 看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 12InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream("")));复制代码 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 1234LineNumberInputStream is = new LineNumberInputStream( new BufferedInputStream( new FileInputStream("")));复制代码 所以说嘛，要找到纯的严格符合设计模式的代码还是比较难的。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 1234public interface Shape &#123; void draw();&#125;复制代码 定义几个实现类： 12345678910111213141516public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println("Circle::draw()"); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println("Rectangle::draw()"); &#125;&#125;复制代码 客户端调用： 12345678910public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125;复制代码 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： 1234567891011121314151617181920212223242526public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125;复制代码 看看现在客户端怎么调用： 123456789public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125;复制代码 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 123456789101112131415161718192021222324252627282930public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return ("Employee :[ Name : " + name + ", dept : " + dept + ", salary :" + salary+" ]"); &#125; &#125;复制代码 通常，这种类需要定义 add(node)、remove(node)、getChildren() 这些方法。 这说的其实就是组合模式，这种简单的模式我就不做过多介绍了，相信各位读者也不喜欢看我写废话。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是共享元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 这种简单的代码我就不演示了。 结构型模式总结前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。读者是否可以分别把这几个模式说清楚了呢？在说到这些模式的时候，心中是否有一个清晰的图或处理流程在脑海里呢？ 1234567代理模式是做方法增强的;适配器模式是把鸡包装成鸭这种用来适配接口的;桥梁模式做到了很好的解耦;装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景；门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可；组合模式用于描述具有层次结构的数据；享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。 行为型模式行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式策略模式太常用了，所以把它放到最前面进行介绍。它比较简单，我就不废话，直接用代码说事吧。 下面设计的场景是，我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。 首先，先定义一个策略接口： 1234public interface Strategy &#123; public void draw(int radius, int x, int y);&#125;复制代码 然后我们定义具体的几个策略： 12345678910111213141516171819public class RedPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;复制代码 使用策略的类： 123456789101112public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeDraw(int radius, int x, int y)&#123; return strategy.draw(radius, x, y); &#125;&#125;复制代码 客户端演示： 12345public static void main(String[] args) &#123; Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);&#125;复制代码 放到一张图上，让大家看得清晰些： 这个时候，大家有没有联想到结构型模式中的桥梁模式，它们其实非常相似，我把桥梁模式的图拿过来大家对比下： 要我说的话，它们非常相似，桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式观察者模式对于我们来说，真是再简单不过了。无外乎两个操作，观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 首先，需要定义主题，每个主题需要持有观察者列表的引用，用于在数据变更的时候通知各个观察者： 123456789101112131415161718192021222324252627public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; // 通知观察者们 public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125;复制代码 定义观察者接口： 12345public abstract class Observer &#123; protected Subject subject; public abstract void update();&#125;复制代码 其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 我们来定义具体的几个观察者类： 12345678910111213141516171819202122232425262728293031public class BinaryObserver extends Observer &#123; // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) &#123; this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); &#125; // 该方法由主题类在数据变更的时候进行调用 @Override public void update() &#123; String result = Integer.toBinaryString(subject.getState()); System.out.println("订阅的数据发生变化，新的数据处理为二进制值为：" + result); &#125;&#125;public class HexaObserver extends Observer &#123; public HexaObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println("订阅的数据发生变化，新的数据处理为十六进制值为：" + result); &#125;&#125;复制代码 客户端使用也非常简单： 1234567891011public static void main(String[] args) &#123; // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);&#125;复制代码 output: 123订阅的数据发生变化，新的数据处理为二进制值为：1011订阅的数据发生变化，新的数据处理为十六进制值为：B复制代码 当然，jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。 实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 如果产品给你这个需求的话，我想大部分人一开始肯定想的就是，用一个 List 来存放所有的规则，然后 foreach 执行一下每个规则就好了。不过，读者也先别急，看看责任链模式和我们说的这个有什么不一样？ 首先，我们要定义流程上节点的基类： 123456789101112131415public abstract class RuleHandler &#123; // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) &#123; this.successor = successor; &#125; public RuleHandler getSuccessor() &#123; return successor; &#125;&#125;复制代码 接下来，我们需要定义具体的每个节点了。 校验用户是否是新用户： 123456789101112131415public class NewUserRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; if (context.isNewUser()) &#123; // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException("该活动仅限新用户参与"); &#125; &#125;&#125;复制代码 校验用户所在地区是否可以参与： 12345678910111213public class LocationRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException("非常抱歉，您所在的地区无法参与本次活动"); &#125; &#125;&#125;复制代码 校验奖品是否已领完： 12345678910111213public class LimitRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(userInfo); &#125; &#125; else &#123; throw new RuntimeException("您来得太晚了，奖品被领完了"); &#125; &#125;&#125;复制代码 客户端： 12345678910public static void main(String[] args) &#123; RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);&#125;复制代码 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 至于它和我们前面说的用一个 List 存放需要执行的规则的做法有什么异同，留给读者自己琢磨吧。 模板方法模式在含有继承结构的代码中，模板方法模式是非常常用的，这也是在开源代码中大量被使用的。 通常会有一个抽象类： 12345678910111213141516public abstract class AbstractTemplate &#123; // 这就是模板方法 public void templateMethod()&#123; init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 &#125; protected void init() &#123; System.out.println("init 抽象层已经实现，子类也可以选择覆写"); &#125; // 留给子类实现 protected abstract void apply(); protected void end() &#123; &#125;&#125;复制代码 模板方法中调用了 3 个方法，其中 apply() 是抽象方法，子类必须实现它，其实模板方法中有几个抽象方法完全是自由的，我们也可以将三个方法都设置为抽象方法，让子类来实现。也就是说，模板方法只负责定义第一步应该要做什么，第二步应该做什么，第三步应该做什么，至于怎么做，由子类来实现。 我们写一个实现类： 123456789public class ConcreteTemplate extends AbstractTemplate &#123; public void apply() &#123; System.out.println("子类实现抽象方法 apply"); &#125; public void end() &#123; System.out.println("我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了"); &#125;&#125;复制代码 客户端调用演示： 123456public static void main(String[] args) &#123; AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();&#125;复制代码 代码其实很简单，基本上看到就懂了，关键是要学会用到自己的代码中。 状态模式update: 2017-10-19 废话我就不说了，我们说一个简单的例子。商品库存中心有个最基本的需求是减库存和补库存，我们看看怎么用状态模式来写。 核心在于，我们的关注点不再是 Context 是该进行哪种操作，而是关注在这个 Context 会有哪些操作。 定义状态接口： 1234public interface State &#123; public void doAction(Context context);&#125;复制代码 定义减库存的状态： 1234567891011121314public class DeductState implements State &#123; public void doAction(Context context) &#123; System.out.println("商品卖出，准备减库存"); context.setState(this); //... 执行减库存的具体操作 &#125; public String toString()&#123; return "Deduct State"; &#125;&#125;复制代码 定义补库存状态： 123456789101112public class RevertState implements State &#123; public void doAction(Context context) &#123; System.out.println("给此商品补库存"); context.setState(this); //... 执行加库存的具体操作 &#125; public String toString() &#123; return "Revert State"; &#125;&#125;复制代码 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 123456789101112131415public class Context &#123; private State state; private String name; public Context(String name) &#123; this.name = name; &#125; public void setState(State state) &#123; this.state = state; &#125; public void getState() &#123; return this.state; &#125;&#125;复制代码 我们来看下客户端调用，大家就一清二楚了： 12345678910111213141516public static void main(String[] args) &#123; // 我们需要操作的是 iPhone X Context context = new Context("iPhone X"); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();&#125;复制代码 读者可能会发现，在上面这个例子中，如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。 不过，商品库存这个例子毕竟只是个例，我们还有很多实例是需要知道当前 context 处于什么状态的。 行为型模式总结行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限，而且本文篇幅也挺大了，我就不进行介绍了。 总结学习设计模式的目的是为了让我们的代码更加的优雅、易维护、易扩展。这次整理这篇文章，让我重新审视了一下各个设计模式，对我自己而言收获还是挺大的。我想，文章的最大收益者一般都是作者本人，为了写一篇文章，需要巩固自己的知识，需要寻找各种资料，而且，自己写过的才最容易记住，也算是我给读者的建议吧。 作者：JavaDoop 链接：https://juejin.im/post/5bc96afff265da0aa94a4493 来源：掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是restful？]]></title>
    <url>%2F2019%2F06%2F17%2Fwhat-is-resuful%2F</url>
    <content type="text"><![CDATA[以下文章来源于：如何给老婆解释什么是RESTful 老婆经常喜欢翻看我订阅的技术杂志，她总能从她的视角提出很多有趣的问题。 一个悠闲的周日下午，她午觉醒来，又习惯性的抓起这个月的杂志，饶有兴趣地看了起来。 果不其然，看着看着，她又对我发难了，“Restful是什么呀，老公？是restaurant的形容词吗，突然就觉得好饿了啊……” 作为一个合格的程序员，我一直把能够将一项技术讲给老婆听，并且能给她讲懂，作为我已经掌握了这项技术的标准。 如果我直接回答说，“REST就是Representational State Transfer的缩写呀，翻译为中文就是‘表述性状态转移’”，那她今晚肯定得罚我跪键盘。我必须找个合适的机会，把Restful的来龙去脉给她形象的描述一遍。 “走，咱们去楼下咖啡厅吃个下午茶吧”，我对老婆说。 “一个芝士蛋糕，一杯拿铁，两条吸管，谢谢”，我对前台的服务员说，然后我们找了个角落坐了下来。 Level 0 - 面向前台“刚才我们向前台点了一杯拿铁，这个过程可以用这段文字来描述”，说着，我在纸上写下了这段JSON，虽然她不知道什么叫JSON，但理解这段文字对于英语专业8级的她，实在再简单不过。 12345&#123; &quot;addOrder&quot;: &#123; &quot;orderName&quot;: &quot;latte&quot; &#125;&#125; “我们通过这段文字，告诉前台，新增一笔订单，订单是一杯拿铁咖啡”，接着，前台给我们返回这么一串回复： 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “订单ID？还是订单编号？” “恩恩，就是订单编号” “那我们就等着前台喊‘订单123456的客户可以取餐了’，然后就可以开吃了！” “哈哈，你真聪明，不过，在这之前，假设我们有一张会员卡，我们想查询一下这张会员卡的余额，这时候，要向前台发起另一个询问”，我继续在纸上写着： 12345&#123; &quot;queryBalance&quot;: &#123; &quot;cardId&quot;: &quot;886333&quot; &#125;&#125; “查询卡号为886333的卡的余额？” “真棒！接着，查询的结果返回来了” 123&#123; &quot;balance&quot;: &quot;0&quot;&#125; “切，没钱……” “哈哈，没钱，现在我们要跟前台说，这杯咖啡不要了”，我在纸上写到： 12345&#123; &quot;deleteOrder&quot;: &#123; &quot;orderId&quot;: &quot;123456&quot; &#125;&#125; “哼，这就把订单取消啦？” Level 1 - 面向资源“现在这家咖啡店越做越大，来喝咖啡的人越来越多，单靠前台显然是不行的，店主决定进行分工，每个资源都有专人负责，我们可以直接面向资源操作。” “面向资源？” “是的，比如还是下单，请求的内容不变，但是我们多了一条消息”，我在纸上画出这次的模型： 1234567/orders&#123; &quot;addOrder&quot;: &#123; &quot;orderName&quot;: &quot;latte&quot; &#125;&#125; “多了一个斜杠和orders？这是什么意思？” “这个表示我们这个请求是发给哪个资源的，订单是一种资源，我们可以理解为是咖啡厅专门管理订单的人，他可以帮我们处理所有有关订单的操作，包括新增订单、修改订单、取消订单等操作” “Soga…” “接着还是会返回订单的编号给我们” 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “下面，我们还是要查询会员卡余额，这次请求的资源变成了cards” 1234567/cards&#123; &quot;queryBalance&quot;: &#123; &quot;cardId&quot;: &quot;886333&quot; &#125;&#125; “接下来是取消订单” “这个我会”，说着，她抢走我手上的笔，在纸上写了起来： 1234567/orders&#123; &quot;deleteOrder&quot;: &#123; &quot;orderId&quot;: &quot;123456&quot; &#125;&#125; Level 2 - 打上标签“接下来，店主还想继续优化他的咖啡厅的服务流程，他发现负责处理订单的员工，每次都要去订单内容里面看是新增订单还是删除订单，还是其他的什么操作，十分不方便，于是规定，所有新增资源的请求，都在请求上面写上大大的‘POST’，表示这是一笔新增资源的请求” “其他种类的请求，比如查询类的，用‘GET’表示，删除类的，用‘DELETE’表示” “还有修改类的，修改分为两种，第一种，如果这个修改，无论发送多少次，最后一次修改后的资源，总是和第一次修改后的一样，比如将拿铁改为猫屎，那么用‘PUT’表示；第二种，如果这个修改，每次修改都会让这个资源和前一次的不一样，比如是加一杯咖啡，那么这种请求用‘PATCH’或者‘POST’表示”，一口气讲了这么多，发现她有点似懂非懂。 “来，我们再来重复上面那个过程，来一杯拿铁”，我边说边画着： 12345POST /orders&#123; &quot;orderName&quot;: &quot;latte&quot;&#125; “请求的内容简洁多啦，不用告诉店员是addOrder，看到POST就知道是新增”，她听的很认真，理解的也很透彻。 “恩恩，返回的内容还是一样” 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “接着是查询会员卡余额，这次也简化了很多” 12345GET /cards&#123; &quot;cardId&quot;: &quot;886333&quot;&#125; “这个请求我们还可以进一步优化为这样” 1GET /cards/886333 “Soga，直接把要查询的卡号写在后面了” “没错，接着，取消订单” 1DELETE /orders/123456 Level 3 - 完美服务“忽然有一天，有个顾客抱怨说，他买了咖啡后，不知道要怎么取消订单，咖啡厅一个店员回了一句，你不会看我们的宣传单吗，上面不写着： 1DELETE /orders/&#123;orderId&#125; 顾客反问道，谁会去看那个啊，店员不服，又说到，你瞎了啊你……据说后面两人吵着吵着还打了起来…” “噗，真是悲剧…” “有了这次教训，店长决定，顾客下了单之后，不仅给他们返回订单的编号，还给顾客返回所有可以对这个订单做的操作，比如告诉用户如何删除订单。现在，我们还是发出请求，请求内容和上一次一样” 12345POST /orders&#123; &quot;orderName&quot;: &quot;latte&quot;&#125; “但是这次返回时多了些内容” 1234567&#123; &quot;orderId&quot;: &quot;123456&quot;, &quot;link&quot;: &#123; &quot;rel&quot;: &quot;cancel&quot;, &quot;url&quot;: &quot;/order/123456&quot; &#125;&#125; “这次返回时多了一项link信息，里面包含了一个rel属性和url属性，rel是relationship的意思，这里的关系是cancel，url则告诉你如何执行这个cancel操作，接着你就可以这样子来取消订单啦” 1DELETE /orders/123456 “哈哈，这服务真是贴心，以后再也不用担心店员和顾客打起来了” “订单123456的客户可以取餐了”，伴随着咖啡厅的广播，我们吃起了下午茶，一杯拿铁，两支吸管…… 对程序员的话用了大白话，给老婆讲明白了RESTful的来龙去脉，当然，我还是有些话想说的，只是怕老婆听完一脸懵逼，没给她说： 1、 上面讲的Level0~Level3，来自Leonard Richardson提出的Richardson Maturity Model： Level0和Level1最大的区别，就是Level1拥有了Restful的第一个特征——面向资源，这对构建可伸缩、分布式的架构是至关重要的。同时，如果把Level0的数据格式换成Xml，那么其实就是SOAP，SOAP的特点是关注行为和处理，和面向资源的RESTful有很大的不同。 Level0和Level1，其实都很挫，他们都只是把HTTP当做一个传输的通道，没有把HTTP当做一种传输协议。 Level2，真正将HTTP作为了一种传输协议，最直观的一点就是Level2使用了HTTP动词，GET/PUT/POST/DELETE/PATCH….,这些都是HTTP的规范，规范的作用自然是重大的，用户看到一个POST请求，就知道它不是幂等的，使用时要小心，看到PUT，就知道他是幂等的，调用多几次都不会造成问题，当然，这些的前提都是API的设计者和开发者也遵循这一套规范，确保自己提供的PUT接口是幂等的。 Level3，关于这一层，有一个古怪的名词，叫HATEOAS（Hypertext As The Engine Of Application State），中文翻译为“将超媒体格式作为应用状态的引擎”，核心思想就是每个资源都有它的状态，不同状态下，可对它进行的操作不一样。理解了这一层，再来看看REST的全称，Representational State Transfer，中文翻译为“表述性状态转移”，是不是好理解多了？ Level3的Restful API，给使用者带来了很大的便利，使用者只需要知道如何获取资源的入口，之后的每个URI都可以通过请求获得，无法获得就说明无法执行那个请求。 现在绝大多数的RESTful接口都做到了Level2的层次，做到Level3的比较少。当然，这个模型并不是一种规范，只是用来理解Restful的工具。所以，做到了Level2，也就是面向资源和使用Http动词，就已经很Restful了。Restful本身也不是一种规范，我比较倾向于用“风格“来形容它。如果你想深入了解Level3，可以阅读《Rest in Practice》第五章。 2、 我跟老婆讲的时候，用的数据格式是JSON，但是要强调一点，Restful对数据格式没有限制，就算你用的是XML或者其他格式，只要符合上面提到的几个特征，也算Restful。]]></content>
      <categories>
        <category>Restful</category>
      </categories>
      <tags>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出理解基于 Kafka 和 ZooKeeper 的分布式消息队列]]></title>
    <url>%2F2019%2F06%2F17%2Fkafka-zookeeper%2F</url>
    <content type="text"><![CDATA[消息队列中间件是分布式系统中重要的组件，主要解决应用耦合(使消息的生产者和消费者解耦)，异步消息（消息的生产和消费是异步的），流量削峰等问题。实现高性能，高可用，可伸缩和最终一致性架构，是大型分布式系统不可缺少的中间件。 本场 Chat 主要内容： Kafka 的架构解读； Kafka 为什么要将 Topic 进行分区； Kafka 高可靠性实现基础解读； Kafka 复制原理和同步方式； Leader 选举机制，及如何确保新选举出的 Leader 是优选； 同步副本 ISR； Kafka 数据可靠性和持久性保证； 深入解读 HW 机制； Kafka 架构中 ZooKeeper 以怎样的形式存在； 全程解析：Producer -&gt; kafka -&gt; consumer。 相关内容链接： 《分布式中间件实践之路》 《Python 快速入门实战教程》 《分布式锁的最佳实践之：基于 Etcd 的分布式锁》 《基于 Redis 的分布式锁实现及踩坑案例》 《一个高可靠性商用 Redis 集群方案介绍》 1. Kafka 总体架构基于 Kafka-ZooKeeper 的分布式消息队列系统总体架构如下： 如上图所示，一个典型的 Kafka 体系架构包括若干 Producer（消息生产者），若干 broker（作为 Kafka 节点的服务器），若干 Consumer（Group），以及一个 ZooKeeper 集群。Kafka通过 ZooKeeper 管理集群配置、选举 Leader 以及在 consumer group 发生变化时进行 Rebalance（即消费者负载均衡，在下一课介绍）。Producer 使用 push（推）模式将消息发布到 broker，Consumer 使用 pull（拉）模式从 broker 订阅并消费消息。 上图仅描摹了一个总体架构，并没有对作为 Kafka 节点的 broker 进行深入刻画，事实上，它的内部细节相当复杂，如下图所示，Kafka 节点涉及 Topic、Partition 两个重要概念。 在 Kafka 架构中，有几个术语： Producer：生产者，即消息发送者，push 消息到 Kafka 集群中的 broker（就是 server）中； Broker：Kafka 集群由多个 Kafka 实例（server） 组成，每个实例构成一个 broker，说白了就是服务器； Topic：producer 向 kafka 集群 push 的消息会被归于某一类别，即Topic，这本质上只是一个逻辑概念，面向的对象是 producer 和 consumer，producer 只需要关注将消息 push 到哪一个 Topic 中，而 consumer 只需要关心自己订阅了哪个 Topic； Partition：每一个 Topic 又被分为多个 Partitions，即物理分区；出于负载均衡的考虑，同一个 Topic 的 Partitions 分别存储于 Kafka 集群的多个 broker 上；而为了提高可靠性，这些 Partitions 可以由 Kafka 机制中的 replicas 来设置备份的数量；如上面的框架图所示，每个 partition 都存在两个备份； Consumer：消费者，从 Kafka 集群的 broker 中 pull 消息、消费消息； Consumer group：high-level consumer API 中，每个 consumer 都属于一个 consumer-group，每条消息只能被 consumer-group 中的一个 Consumer 消费，但可以被多个 consumer-group 消费； replicas：partition 的副本，保障 partition 的高可用； leader：replicas 中的一个角色， producer 和 consumer 只跟 leader 交互； follower：replicas 中的一个角色，从 leader 中复制数据，作为副本，一旦 leader 挂掉，会从它的 followers 中选举出一个新的 leader 继续提供服务； controller：Kafka 集群中的其中一个服务器，用来进行 leader election 以及 各种 failover(故障转移)； ZooKeeper：Kafka 通过 ZooKeeper 来存储集群的 meta 信息等，文中将详述。 1.1 Topic &amp; Partition一个 topic 可以认为是一类消息，每个 topic 将被分成多个 partition，每个 partition 在存储层面是 append log 文件。任何发布到此 partition 的消息都会被追加到log文件的尾部，每条消息在文件中的位置称为 offset（偏移量），offset 为一个 long 型的数字，它唯一标记一条消息。 Kafka 机制中，producer push 来的消息是追加（append）到 partition 中的，这是一种顺序写磁盘的机制，效率远高于随机写内存，如下示意图： 1.2 Kafka 为什么要将 Topic 进行分区？1简而言之：负载均衡 + 水平扩展。 前已述及，Topic 只是逻辑概念，面向的是 producer 和 consumer；而 Partition 则是物理概念。可以想象，如果 Topic 不进行分区，而将 Topic 内的消息存储于一个 broker，那么关于该 Topic 的所有读写请求都将由这一个 broker 处理，吞吐量很容易陷入瓶颈，这显然是不符合高吞吐量应用场景的。有了 Partition 概念以后，假设一个 Topic 被分为 10 个 Partitions，Kafka 会根据一定的算法将 10 个 Partition 尽可能均匀的分布到不同的 broker（服务器）上，当 producer 发布消息时，producer 客户端可以采用 random、key-hash 及 轮询 等算法选定目标 partition，若不指定，Kafka 也将根据一定算法将其置于某一分区上。Partiton 机制可以极大的提高吞吐量，并且使得系统具备良好的水平扩展能力。 在创建 topic 时可以在 $KAFKA_HOME/config/server.properties 中指定这个 partition 的数量（如下所示），当然可以在 topic 创建之后去修改 partition 的数量。 1234# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=3 在发送一条消息时，可以指定这个消息的 key，producer 根据这个 key 和 partition 机制来判断这个消息发送到哪个partition。partition 机制可以通过指定 producer 的 partition.class 这一参数来指定（即支持自定义），该 class 必须实现 kafka.producer.Partitioner 接口。 有关 topic 与 partition 的更多细节，可以参考下面的“Kafka 文件存储机制”这一节。 2. Kafka 高可靠性实现基础解读谈及可靠性，最常规、最有效的策略就是 “副本（replication）机制” ，Kafka 实现高可靠性同样采用了该策略。通过调节副本相关参数，可使 Kafka 在性能和可靠性之间取得平衡。本节先从 Kafka 文件存储机制入手，从最底层了解 Kafka 的存储细节，进而对消息的存储有个微观的认知。之后通过介绍 Kafka 的复制原理和同步方式来阐述宏观层面的概念。最后介绍 ISR，HW 和 leader 选举。 2.1 Kafka 文件存储机制Kafka 中消息是以 topic 进行分类的，生产者通过 topic 向 Kafka broker 发送消息，消费者通过 topic 读取数据。然而 topic 在物理层面又能以 partition 为分组，一个 topic 可以分成若干个 partition。事实上，partition 并不是最终的存储粒度，partition 还可以细分为 segment，一个 partition 物理上由多个 segment 组成，那么这些 segment 又是什么呢？ 为了便于说明问题，假设这里只有一个 Kafka 集群，且这个集群只有一个 Kafka broker，即只有一台物理机。在这个 Kafka broker 中配置 log.dirs=/tmp/kafka-logs，以此来设置 Kafka 消息文件存储目录；与此同时，通过命令创建一个 topic：mytopic_test，partition 的数量配置为 4（创建 topic 的命令请见上一课）。之后，可以在 /tmp/kafka-logs 目录中可以看到生成了 4 个目录： 1234drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-0drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-1drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-2drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-3 在 Kafka 文件存储中，同一个 topic 下有多个不同的 partition，每个 partiton 为一个目录，partition 的名称规则为：topic 名称 + 有序序号，第一个序号从 0 开始计，最大的序号为 partition 数量减 1，partition 是实际物理上的概念，而 topic 是逻辑上的概念。 问题 1：为什么不能以 partition 作为存储单位？ 上面提到 partition 还可以细分为 segment，这个 segment 又是什么？如果就以 partition 为最小存储单位，可以想象，当 Kafka producer 不断发送消息，必然会引起 partition 文件的无限扩张，将对消息文件的维护以及已消费的消息的清理带来严重的影响，因此，需以 segment 为单位将 partition 进一步细分。每个 partition（目录）相当于一个巨型文件被平均分配到多个大小相等的 segment（段）数据文件中（每个 segment 文件中消息数量不一定相等）这种特性也方便 old segment 的删除，即方便已被消费的消息的清理，提高磁盘的利用率。每个 partition 只需要支持顺序读写就行，segment 的文件生命周期由服务端配置参数（log.segment.bytes，log.roll.{ms,hours} 等若干参数）决定。 问题 2：segment 的工作原理是怎样的？ segment 文件由两部分组成，分别为 “.index” 文件和 “.log” 文件，分别表示为 segment 索引文件和数据文件。这两个文件的命令规则为：partition 全局的第一个 segment 从 0 开始，后续每个 segment 文件名为上一个 segment 文件最后一条消息的 offset 值，数值大小为 64 位，20 位数字字符长度，没有数字用 0 填充，如下： 12345600000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.log 以上面的 segment 文件为例，展示出 segment：00000000000000170410 的 “.index” 文件和 “.log” 文件的对应的关系，如下图： 如上图，“.index” 索引文件存储大量的元数据，“.log” 数据文件存储大量的消息，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。其中以 “.index” 索引文件中的元数据 [3, 348] 为例，在 “.log” 数据文件表示第 3 个消息，即在全局 partition 中表示 170410+3=170413 个消息，该消息的物理偏移地址为 348。 问题 3：如何从 partition 中通过 offset 查找 message 呢？ 以上图为例，读取 offset=170418 的消息，首先查找 segment 文件，其中 00000000000000000000.index 为最开始的文件，第二个文件为 00000000000000170410.index（起始偏移为 170410+1=170411），而第三个文件为 00000000000000239430.index（起始偏移为 239430+1=239431），所以这个 offset=170418 就落到了第二个文件之中。其它后续文件可以依次类推，以其偏移量命名并排列这些文件，然后根据二分查找法就可以快速定位到具体文件位置。其次根据 00000000000000170410.index 文件中的 [8,1325] 定位到 00000000000000170410.log 文件中的 1325 的位置进行读取。 要是读取 offset=170418 的消息，从 00000000000000170410.log 文件中的 1325 的位置进行读取，那么，如何确定何时读完本条消息呢？（否则就读到下一条消息的内容了） 这个问题由消息的物理结构解决，消息都具有固定的物理结构，包括：offset（8 Bytes）、消息体的大小（4 Bytes）、crc32（4 Bytes）、magic（1 Byte）、attributes（1 Byte）、key length（4 Bytes）、key（K Bytes）、payload（N Bytes）等等字段，可以确定一条消息的大小，即读取到哪里截止。 2.2 复制原理和同步方式Kafka 中 topic 的每个 partition 有一个预写式的日志文件，虽然 partition 可以继续细分为若干个 segment 文件，但是对于上层应用来说，仍然可以将 partition 看成最小的存储单元（一个有多个 segment 文件拼接的 “巨型” 文件），每个 partition 都由一些列有序的、不可变的消息组成，这些消息被连续的追加到 partition 中。 上图中有两个新名词：HW 和 LEO。这里先介绍下 LEO，LogEndOffset 的缩写，表示每个 partition 的 log 最后一条 Message 的位置。HW 是 HighWatermark 的缩写，是指 consumer 能够看到的此 partition 的位置，这个涉及到多副本的概念，这里先提及一下，下文再详述。 言归正传，为了提高消息的可靠性，Kafka 每个 topic 的 partition 有 N 个副本（replicas），其中 N（大于等于 1）是 topic 的复制因子（replica fator）的个数。Kafka 通过多副本机制实现故障自动转移，当 Kafka 集群中出现 broker 失效时，副本机制可保证服务可用。对于任何一个 partition，它的 N 个 replicas 中，其中一个 replica 为 leader，其他都为 follower，leader 负责处理 partition 的所有读写请求，follower 则负责被动地去复制 leader 上的数据。如下图所示，Kafka 集群中有 4 个 broker，某 topic 有 3 个 partition，且复制因子即副本个数也为 3： 如果 leader 所在的 broker 发生故障或宕机，对应 partition 将因无 leader 而不能处理客户端请求，这时副本的作用就体现出来了：一个新 leader 将从 follower 中被选举出来并继续处理客户端的请求。 如何确保新选举出的 leader 是优选呢？ 一个 partition 有多个副本（replicas），为了提高可靠性，这些副本分散在不同的 broker 上，由于带宽、读写性能、网络延迟等因素，同一时刻，这些副本的状态通常是不一致的：即 followers 与 leader 的状态不一致。那么，如何保证新选举出的 leader 是优选呢？ Kafka 机制中，leader 将负责维护和跟踪一个 ISR（In-Sync Replicas）列表，即同步副本队列，这个列表里面的副本与 leader 保持同步，状态一致。如果新的 leader 从 ISR 列表中的副本中选出，那么就可以保证新 leader 为优选。当然，这不是唯一的策略，下文将继续解读。 2.3 同步副本 ISR上一节中讲到了同步副本队列 ISR（In-Sync Replicas）。虽然副本极大的增强了可用性，但是副本数量对 Kafka 的吞吐率有一定影响。默认情况下 Kafka 的 replica 数量为 1，即每个 partition 都只有唯一的 leader，无 follower，没有容灾能力。为了确保消息的可靠性，生产环境中，通常将其值（由 broker 的参数 offsets.topic.replication.factor 指定）大小设置为大于 1，比如 3。 所有的副本（replicas）统称为 Assigned Replicas，即 AR。ISR 是 AR 中的一个子集，由 leader 维护 ISR 列表，follower 从 leader 同步数据有一些延迟（由参数 replica.lag.time.max.ms 设置超时阈值），超过阈值的 follower 将被剔除出 ISR， 存入 OSR（Outof-Sync Replicas）列表，新加入的 follower 也会先存放在 OSR 中。AR=ISR+OSR。 1注：ISR中包括：leader + 与leader保持同步的followers。 上面一节还涉及到一个概念，即 HW。HW 俗称高水位，HighWatermark 的缩写，取一个 partition 对应的 ISR 中最小的 LEO 作为 HW，consumer 最多只能消费到 HW 所在的位置。另外每个 replica 都有 HW，leader 和 follower 各自负责更新自己的 HW 的状态。对于 leader 新写入的消息，consumer 不能立刻消费，leader 会等待该消息被所有 ISR 中的 replicas 同步后更新 HW，此时消息才能被 consumer 消费。这样就保证了如果 leader 所在的 broker 失效，该消息仍然可以从新选举的 leader 中获取。对于来自内部 broker 的读取请求，没有 HW 的限制。 下图详细的说明了当 producer 生产消息至 broker 后，ISR 以及 HW 和 LEO 的流转过程： 由此可见，Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 都复制完，这条消息才会被 commit，这种复制方式受限于复制最慢的 follower，会极大的影响吞吐率。而异步复制方式下，follower 异步的从 leader 复制数据，数据只要被 leader 写入 log 就被认为已经 commit，这种情况下如果 follower 都还没有复制完，落后于 leader 时，突然 leader 宕机，则会丢失数据，降低可靠性。而 Kafka 使用 ISR 的策略则在可靠性和吞吐率方面取得了较好的平衡。 Kafka 的 ISR 的管理最终都会反馈到 ZooKeeper 节点上，具体位置为： 1/brokers/topics/[topic]/partitions/[partition]/state 目前，有两个地方会对这个 ZooKeeper 的节点进行维护。 Controller 来维护：Kafka 集群中的其中一个 Broker 会被选举为 Controller，主要负责 Partition 管理和副本状态管理，也会执行类似于重分配 partition 之类的管理任务。在符合某些特定条件下，Controller 下的 LeaderSelector 会选举新的 leader，ISR 和新的 leader_epoch 及 controller_epoch 写入 ZooKeeper 的相关节点中。同时发起 LeaderAndIsrRequest 通知所有的 replicas。 leader 来维护：leader 有单独的线程定期检测 ISR 中 follower 是否脱离 ISR，如果发现 ISR 变化，则会将新的 ISR 的信息返回到 ZooKeeper 的相关节点中。 2.4 数据可靠性和持久性保证当 producer 向 leader 发送数据时，可以通过 request.required.acks 参数来设置数据可靠性的级别： 1. request.required.acks = 1 这是默认情况，即：producer 发送数据到 leader，leader 写本地日志成功，返回客户端成功；此时 ISR 中的其它副本还没有来得及拉取该消息，如果此时 leader 宕机了，那么此次发送的消息就会丢失。 2. request.required.acks = 0 producer 不停向leader发送数据，而不需要 leader 反馈成功消息，这种情况下数据传输效率最高，但是数据可靠性确是最低的。可能在发送过程中丢失数据，可能在 leader 宕机时丢失数据。 3. request.required.acks = -1（all） producer 发送数据给 leader，leader 收到数据后要等到 ISR 列表中的所有副本都同步数据完成后（强一致性），才向生产者返回成功消息，如果一直收不到成功消息，则认为发送数据失败会自动重发数据。这是可靠性最高的方案，当然，性能也会受到一定影响。 *注意：参数 min.insync.replicas * 如果要提高数据的可靠性，在设置 request.required.acks=-1 的同时，还需参数 min.insync.replicas 配合，如此才能发挥最大的功效。min.insync.replicas 这个参数用于设定 ISR 中的最小副本数，默认值为1，当且仅当 request.required.acks 参数设置为-1时，此参数才生效。当 ISR 中的副本数少于 min.insync.replicas 配置的数量时，客户端会返回异常：org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。不难理解，如果 min.insync.replicas 设置为 2，当 ISR 中实际副本数为 1 时（只有leader），将无法保证可靠性，此时拒绝客户端的写请求以防止消息丢失。 2.5 深入解读 HW 机制考虑这样一种场景：acks=-1，部分 ISR 副本完成同步，此时leader挂掉，如下图所示：follower1 同步了消息 4、5，follower2 同步了消息 4，与此同时 follower2 被选举为 leader，那么此时 follower1 中的多出的消息 5 该做如何处理呢？ 这里就需要 HW 的协同配合了。如前所述，一个 partition 中的 ISR 列表中，leader 的 HW 是所有 ISR 列表里副本中最小的那个的 LEO。类似于木桶原理，水位取决于最低那块短板。 如上图，某个 topic 的某 partition 有三个副本，分别为 A、B、C。A 作为 leader 肯定是 LEO 最高，B 紧随其后，C 机器由于配置比较低，网络比较差，故而同步最慢。这个时候 A 机器宕机，这时候如果 B 成为 leader，假如没有 HW，在 A 重新恢复之后会做同步（makeFollower) 操作，在宕机时 log 文件之后直接做追加操作，而假如 B 的 LEO 已经达到了 A 的 LEO，会产生数据不一致的情况，所以使用 HW 来避免这种情况。 A 在做同步操作的时候，先将 log 文件截断到之前自己的 HW 的位置，即 3，之后再从 B 中拉取消息进行同步。 如果失败的 follower 恢复过来，它首先将自己的 log 文件截断到上次 checkpointed 时刻的 HW 的位置，之后再从 leader 中同步消息。leader 挂掉会重新选举，新的 leader 会发送 “指令” 让其余的 follower 截断至自身的 HW 的位置然后再拉取新的消息。 当 ISR 中的个副本的 LEO 不一致时，如果此时 leader 挂掉，选举新的 leader 时并不是按照 LEO 的高低进行选举，而是按照 ISR 中的顺序选举。 2.6 Leader 选举为了保证可靠性，对于任意一条消息，只有它被 ISR 中的所有 follower 都从 leader 复制过去才会被认为已提交，并返回信息给 producer。如此，可以避免因部分数据被写进 leader，而尚未被任何 follower 复制就宕机的情况下而造成数据丢失。对于 producer 而言，它可以选择是否等待消息 commit，这可以通过参数 request.required.acks 来设置。这种机制可以确保：只要 ISR 中有一个或者以上的 follower，一条被 commit 的消息就不会丢失。 问题 1：如何在保证可靠性的前提下避免吞吐量下降？ 有一个很重要的问题是当 leader 宕机了，怎样在 follower 中选举出新的 leader，因为 follower 可能落后很多或者直接 crash 了，所以必须确保选择 “最新” 的 follower 作为新的 leader。一个基本的原则就是，如果 leader 挂掉，新的 leader 必须拥有原来的 leader 已经 commit 的所有消息，这不就是 ISR 中副本的特征吗？ 但是，存在一个问题，ISR 列表维持多大的规模合适呢？换言之，leader 在一个消息被 commit 前需要等待多少个 follower 确认呢？等待 follower 的数量越多，与 leader 保持同步的 follower 就越多，可靠性就越高，但这也会造成吞吐率的下降。 少数服从多数的选举原则 一种常用的选举 leader 的策略是 “少数服从多数” ，不过，Kafka 并不是采用这种方式。这种模式下，如果有 2f+1 个副本，那么在 commit 之前必须保证有 f+1 个 replica 复制完消息，同时为了保证能正确选举出新的 leader，失败的副本数不能超过 f 个。这种方式有个很大的优势，系统的延迟取决于最快的几台机器，也就是说比如副本数为 3，那么延迟就取决于最快的那个 follower 而不是最慢的那个。 “少数服从多数” 的策略也有一些劣势，为了保证 leader 选举的正常进行，它所能容忍的失败的 follower 数比较少，如果要容忍 1 个 follower 挂掉，那么至少要 3 个以上的副本，如果要容忍 2 个 follower 挂掉，必须要有 5 个以上的副本。也就是说，在生产环境下为了保证较高的容错率，必须要有大量的副本，而大量的副本又会在大数据量下导致性能的急剧下降。这种算法更多用在 ZooKeeper 这种共享集群配置的系统中，而很少在需要大量数据的系统中使用。 Kafka 选举 leader 的策略是怎样的？ 实际上，leader 选举的算法非常多，比如 ZooKeeper 的 Zab、Raft 以及 Viewstamped Replication。而 Kafka 所使用的 leader 选举算法更像是微软的 PacificA 算法。 Kafka 在 ZooKeeper 中为每一个 partition 动态的维护了一个 ISR，这个 ISR 里的所有 replica 都与 leader 保持同步，只有 ISR 里的成员才能有被选为 leader 的可能（通过参数配置：unclean.leader.election.enable=false）。在这种模式下，对于 f+1 个副本，一个 Kafka topic 能在保证不丢失已经 commit 消息的前提下容忍 f 个副本的失败，在大多数使用场景下，这种模式是十分有利的。事实上，对于任意一条消息，只有它被 ISR 中的所有 follower 都从 leader 复制过去才会被认为已提交，并返回信息给 producer，从而保证可靠性。但与 “少数服从多数” 策略不同的是，Kafka ISR 列表中副本的数量不需要超过副本总数的一半，即不需要满足 “多数派” 原则，通常，ISR 列表副本数大于等于 2 即可，如此，便在可靠性和吞吐量方面取得平衡。 极端情况下的 leader 选举策略 前已述及，当 ISR 中至少有一个 follower 时（ISR 包括 leader），Kafka 可以确保已经 commit 的消息不丢失，但如果某一个 partition 的所有 replica 都挂了，自然就无法保证数据不丢失了。这种情况下如何进行 leader 选举呢？通常有两种方案： 等待 ISR 中任意一个 replica 恢复过来，并且选它作为 leader； 选择第一个恢复过来的 replica（并不一定是在 ISR 中）作为leader。（默认） 如何选择呢？这就需要在可用性和一致性当中作出抉择。如果一定要等待 ISR 中的 replica 恢复过来，不可用的时间就可能会相对较长。而且如果 ISR 中所有的 replica 都无法恢复了，或者数据丢失了，这个 partition 将永远不可用。 选择第一个恢复过来的 replica 作为 leader，如果这个 replica 不是 ISR 中的 replica，那么，它可能并不具备所有已经 commit 的消息，从而造成消息丢失。默认情况下，Kafka 采用第二种策略，即 unclean.leader.election.enable=true，也可以将此参数设置为 false 来启用第一种策略。 unclean.leader.election.enable 这个参数对于 leader 的选举、系统的可用性以及数据的可靠性都有至关重要的影响。生产环境中应慎重权衡。 3. Kafka 架构中 ZooKeeper 以怎样的形式存在？ZooKeeper 是一个分布式的、开放源码的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现。分布式应用程序可以基于它实现统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等工作。在基于 Kafka 的分布式消息队列中，ZooKeeper 的作用有：broker 注册、topic 注册、producer 和 consumer 负载均衡、维护 partition 与 consumer 的关系、记录消息消费的进度以及 consumer 注册等。 3.1 broker 在 ZooKeeper 中的注册 为了记录 broker 的注册信息，在 ZooKeeper 上，专门创建了属于 Kafka 的一个节点，其路径为 /brokers； Kafka 的每个 broker 启动时，都会到 ZooKeeper 中进行注册，告诉 ZooKeeper 其 broker.id，在整个集群中，broker.id 应该全局唯一，并在 ZooKeeper 上创建其属于自己的节点，其节点路径为 /brokers/ids/{broker.id}； 创建完节点后，Kafka 会将该 broker 的 broker.name 及端口号记录到该节点； 另外，该 broker 节点属性为临时节点，当 broker 会话失效时，ZooKeeper 会删除该节点，这样，我们就可以很方便的监控到broker 节点的变化，及时调整负载均衡等。 3.2 Topic 在 ZooKeeper 中的注册在 Kafka 中，所有 topic 与 broker 的对应关系都由 ZooKeeper 进行维护，在 ZooKeeper 中，建立专门的节点来记录这些信息，其节点路径为 /brokers/topics/{topic_name}。 前面说过，为了保障数据的可靠性，每个 Topic 的 Partitions 实际上是存在备份的，并且备份的数量由 Kafka 机制中的 replicas 来控制。那么问题来了：如下图所示，假设某个 TopicA 被分为 2 个 Partitions，并且存在两个备份，由于这 2 个 Partitions（1-2）被分布在不同的 broker 上，同一个 partiton 与其备份不能（也不应该）存储于同一个 broker 上。以 Partition1 为例，假设它被存储于 broker2，其对应的备份分别存储于 broker1 和 broker4，有了备份，可靠性得到保障，但数据一致性却是个问题。 为了保障数据的一致性，ZooKeeper 机制得以引入。基于 ZooKeeper，Kafka 为每一个 partition 找一个节点作为 leader，其余备份作为 follower；接续上图的例子，就 TopicA 的 partition1 而言，如果位于 broker2（Kafka 节点）上的 partition1 为 leader，那么位于 broker1 和 broker4 上面的 partition1 就充当 follower，则有下图： 基于上图的架构，当 producer push 的消息写入 partition（分区) 时，作为 leader 的 broker（Kafka 节点） 会将消息写入自己的分区，同时还会将此消息复制到各个 follower，实现同步。如果，某个follower 挂掉，leader 会再找一个替代并同步消息；如果 leader 挂了，follower 们会选举出一个新的 leader 替代，继续业务，这些都是由 ZooKeeper 完成的。 3.3 consumer 在 ZooKeeper 中的注册注册新的消费者分组 当新的消费者组注册到 ZooKeeper 中时，ZooKeeper 会创建专用的节点来保存相关信息，其节点路径为 ls/consumers/{group_id}，其节点下有三个子节点，分别为 [ids, owners, offsets]。 ids 节点：记录该消费组中当前正在消费的消费者； owners 节点：记录该消费组消费的 topic 信息； offsets 节点：记录每个 topic 的每个分区的 offset。 注册新的消费者 当新的消费者注册到 Kafka 中时，会在 /consumers/{group_id}/ids 节点下创建临时子节点，并记录相关信息。 监听消费者分组中消费者的变化 每个消费者都要关注其所属消费者组中消费者数目的变化，即监听 /consumers/{group_id}/ids 下子节点的变化。一旦发现消费者新增或减少，就会触发消费者的负载均衡。 3.4 Producers 负载均衡对于同一个 topic 的不同 partition，Kafka会尽力将这些 partition 分布到不同的 broker 服务器上，这种均衡策略实际上是基于 ZooKeeper 实现的。在一个 broker 启动时，会首先完成 broker 的注册过程，并注册一些诸如 “有哪些可订阅的 topic” 之类的元数据信息。producers 启动后也要到 ZooKeeper 下注册，创建一个临时节点来监听 broker 服务器列表的变化。由于在 ZooKeeper 下 broker 创建的也是临时节点，当 brokers 发生变化时，producers 可以得到相关的通知，从改变自己的 broker list。其它的诸如 topic 的变化以及broker 和 topic 的关系变化，也是通过 ZooKeeper 的这种 Watcher 监听实现的。 在生产中，必须指定 topic；但是对于 partition，有两种指定方式： 明确指定 partition(0-N)，则数据被发送到指定 partition； 设置为 RD_KAFKA_PARTITION_UA，则 Kafka 会回调 partitioner 进行均衡选取，partitioner 方法需要自己实现。可以轮询或者传入 key 进行 hash。未实现则采用默认的随机方法 rd_kafka_msg_partitioner_random 随机选择。 3.5 Consumer 负载均衡Kafka 保证同一 consumer group 中只有一个 consumer 可消费某条消息，实际上，Kafka 保证的是稳定状态下每一个 consumer 实例只会消费某一个或多个特定的数据，而某个 partition 的数据只会被某一个特定的 consumer 实例所消费。这样设计的劣势是无法让同一个 consumer group 里的 consumer 均匀消费数据，优势是每个 consumer 不用都跟大量的 broker 通信，减少通信开销，同时也降低了分配难度，实现也更简单。另外，因为同一个 partition 里的数据是有序的，这种设计可以保证每个 partition 里的数据也是有序被消费。 consumer 数量不等于 partition 数量 如果某 consumer group 中 consumer 数量少于 partition 数量，则至少有一个 consumer 会消费多个 partition 的数据；如果 consumer 的数量与 partition 数量相同，则正好一个 consumer 消费一个 partition 的数据，而如果 consumer 的数量多于 partition 的数量时，会有部分 consumer 无法消费该 topic 下任何一条消息。 借助 ZooKeeper 实现负载均衡 关于负载均衡，对于某些低级别的 API，consumer 消费时必须指定 topic 和 partition，这显然不是一种友好的均衡策略。基于高级别的 API，consumer 消费时只需制定 topic，借助 ZooKeeper 可以根据 partition 的数量和 consumer 的数量做到均衡的动态配置。 consumers 在启动时会到 ZooKeeper 下以自己的 conusmer-id 创建临时节点 /consumer/[group-id]/ids/[conusmer-id]，并对 /consumer/[group-id]/ids 注册监听事件，当消费者发生变化时，同一 group 的其余消费者会得到通知。当然，消费者还要监听 broker 列表的变化。librdkafka 通常会将 partition 进行排序后，根据消费者列表，进行轮流的分配。 3.6 记录消费进度 Offset在 consumer 对指定消息 partition 的消息进行消费的过程中，需要定时地将 partition 消息的消费进度 Offset 记录到 ZooKeeper上，以便在该 consumer 进行重启或者其它 consumer 重新接管该消息分区的消息消费权后，能够从之前的进度开始继续进行消息消费。Offset 在 ZooKeeper 中由一个专门节点进行记录，其节点路径为： 12#节点内容就是Offset的值。/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id] PS：Kafka 已推荐将 consumer 的 Offset 信息保存在 Kafka 内部的 topic 中，即： 1__consumer_offsets(/brokers/topics/__consumer_offsets) 并且默认提供了 kafka_consumer_groups.sh 脚本供用户查看consumer 信息（命令：sh kafka-consumer-groups.sh –bootstrap-server * –describe –group *）。在当前版本中，offset 存储方式要么存储在本地文件中，要么存储在 broker 端，具体的存储方式取决 offset.store.method 的配置，默认是存储在 broker 端。 3.7 记录 Partition 与 Consumer 的关系consumer group 下有多个 consumer（消费者），对于每个消费者组（consumer group），Kafka都会为其分配一个全局唯一的 group ID，group 内部的所有消费者共享该 ID。订阅的 topic 下的每个分区只能分配给某个 group 下的一个consumer（当然该分区还可以被分配给其它 group）。同时，Kafka 为每个消费者分配一个 consumer ID，通常采用 hostname:UUID 形式表示。 在Kafka中，规定了每个 partition 只能被同组的一个消费者进行消费，因此，需要在 ZooKeeper 上记录下 partition 与 consumer 之间的关系，每个 consumer 一旦确定了对一个 partition 的消费权力，需要将其 consumer ID 写入到 ZooKeeper 对应消息分区的临时节点上，例如： 1/consumers/[group_id]/owners/[topic]/[broker_id-partition_id] 其中，[broker_id-partition_id] 就是一个消息分区的标识，节点内容就是该消息分区 消费者的 consumer ID。 4. 全程解析（Producer-kafka-consumer）4.1 producer 发布消息producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。 其路由机制为： 指定了 patition，则直接使用； 未指定 patition 但指定 key，通过对 key 进行 hash 选出一个 patition； patition 和 key 都未指定，使用轮询选出一个 patition。 写入流程： producer 先从 ZooKeeper 的 “/brokers/…/state” 节点找到该 partition 的leader； producer 将消息发送给该 leader； leader 将消息写入本地 log； followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK； leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK； 4.2 Broker 存储消息物理上把 topic 分成一个或多个 patition，每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件） 4.3 Consumer 消费消息high-level consumer API 提供了 consumer group 的语义，一个消息只能被 group 内的一个 consumer 所消费，且 consumer 消费消息时不关注 offset，最后一个 offset 由 ZooKeeper 保存（下次消费时，该group 中的consumer将从offset记录的位置开始消费）。 注意： 如果消费线程大于 patition 数量，则有些线程将收不到消息； 如果 patition 数量大于消费线程数，则有些线程多收到多个 patition 的消息； 如果一个线程消费多个 patition，则无法保证你收到的消息的顺序，而一个 patition 内的消息是有序的。 consumer 采用 pull 模式从 broker 中读取数据。 push 模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。 对于 Kafka 而言，pull 模式更合适，它可简化 broker 的设计，consumer 可自主控制消费消息的速率，同时 consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 上述文章参考自：深入浅出理解基于 Kafka 和 ZooKeeper 的分布式消息队列]]></content>
      <categories>
        <category>MQ</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中sleep和wait的区别]]></title>
    <url>%2F2019%2F06%2F17%2Fjava-sleep-wait%2F</url>
    <content type="text"><![CDATA[sleep() &amp; wait() sleep()方法 wait()方法 sleep()使当前线程进入停滞状态（阻塞当前线程，即会发生上下文的切换，会有很大的开销），让出CUP的使用，目的是不让当前线程独自霸占该进程所获的CPU资源，以留一定时间给其他线程执行的机会 wait()方法是Object类里的方法；当一个线程执行到wait()方法时，它就进入到一个和该对象相关的等待池中，同时失去（释放）了对象的机锁（暂时失去机锁，wait(long timeout)超时时间到后还需要返还对象锁）；其他线程可以访问 sleep()是Thread类的static(静态)的方法；因此它不能改变对象的锁，所以当在一个synchronized块中调用sleep()方法是，线程虽然休眠了，但是对象的机锁并没有被释放，其他线程无法获取对象锁（即使睡着也持有对象锁） wait()使用notify或者notifyAlll或者指定睡眠时间来唤醒当前等待池中的线程 在sleep()休眠时间期满后，该线程不一定会立即执行，这是因为其它线程可能正在运行而且没有被调度为放弃执行，除非此线程具有更高的优先级 wait()必须放在synchronized block中，否则会在program runtime时抛出”java.lang.IllegalMonitorStateException“异常 ## 区别 1sleep（）和wait()这两个函数被调用之后线程都应该放弃执行权，不同的是sleep（）不释放锁而wait（）的话是释放锁。直白的意思是一个线程调用Sleep（）之后进入了阻塞状态，它的意思就是当sleep()状态超时，线程重新转入可运行(runnable)状态。而Wait（）在释放执行权之后也把锁释放了,通过notify()或者notifyAll()或者指定睡眠时间来唤醒后，它要运行的话还是要和其他的线程去竞争锁，之后才可以获得执行权。 1234所以sleep()和wait()方法的最大区别是： sleep()睡眠时，保持对象锁，仍然占有该锁； 而wait()睡眠时，释放对象锁。 但是wait()和sleep()都可以通过interrupt()方法打断线程的暂停状态，从而使线程立刻抛出InterruptedException（但不建议使用该方法）。 Java中sleep方法的几个注意点 Thread.sleep()方法用来暂停线程的执行，将CPU放给线程调度器。 Thread.sleep()方法是一个静态方法，它暂停的是当前执行的线程。 Java有两种sleep方法，一个只有一个毫秒参数，另一个有毫秒和纳秒两个参数。 与wait方法不同，sleep方法不会释放锁。 如果其他的线程中断了一个休眠的线程，sleep方法会抛出Interrupted Exception。 休眠的线程在唤醒之后不保证能获取到CPU，它会先进入就绪态，与其他线程竞争CPU。 有一个易错的地方，当调用t.sleep()的时候，会暂停线程t。这是不对的，因为Thread.sleep是一个静态方法，它会使当前线程而不是线程t进入休眠状态。 wait方法必须正在同步环境下使用，比如synchronized方法或者同步代码块。如果你不在同步条件下使用，会抛出IllegalMonitorStateException异常。另外，sleep方法不需要再同步条件下调用，你可以任意正常的使用。 wait方法用于和定义于Object类的，而sleep方法操作于当前线程，定义在java.lang.Thread类里面。 参考自： Java sleep和wait的区别 在阻塞式io中，如果一个线程在等待io操作，那么cpu还会分配时间片给该线程吗？]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>sleep</tag>
        <tag>wait</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reactor线程模型]]></title>
    <url>%2F2019%2F06%2F17%2FReactor-thread-model%2F</url>
    <content type="text"><![CDATA[Reactor是什么? The reactor design_pattern is an event_handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to the associated request handlers. from wiki 通过wiki中的定义我们可以发现Reactor的重点 事件驱动 可以处理一个或多个输入源 通过多路复用将请求的事件分发给对应的处理器处理 根据大神Doug Lea 在 《Scalable IO in Java 》中的介绍，Reacotr模型主要分为三个角色 Reactor：把IO事件分配给对应的handler处理 Acceptor：处理客户端连接事件 Handler：处理非阻塞的任务 为什么使用Reactor？ 传统阻塞IO模型的不足 每个连接都需要独立线程处理，当并发数大时，创建线程数多，占用资源 采用阻塞IO模型，连接建立后，若当前线程没有数据可读，线程会阻塞（阻塞会有切换进程或线程上下文的开销）在读操作上，造成资源浪费 针对传统阻塞IO模型的两个问题，可以采用如下的方案 基于池化思想，避免为每个连接创建线程，连接完成后将业务处理交给线程池处理 基于IO复用模型，多个连接共用同一个阻塞对象，不用等待所有的连接。遍历到有新数据可以处理时，操作系统会通知程序，线程跳出阻塞状态，进行业务逻辑处理 Reactor线程模型的思想就是基于IO复用和线程池（线程复用）的结合 Reactor线程模型分类根据Reactor的数量和处理资源的线程数量的不同，分为三类： 单Reactor单线程模型 单Reactor多线程模型 多Reactor多线程模型 单Reactor单线程模型 这种模型在Reactor中处理事件，并分发事件，如果是连接事件交给acceptor处理，如果是读写事件和业务处理就交给handler处理，但始终只有一个线程执行所有的事情 该线程模型的不足 仅用一个线程处理请求，对于多核资源机器来说是有点浪费的 当处理读写任务的线程负载过高后，处理速度下降，事件会堆积，严重的会超时，可能导致客户端重新发送请求，性能越来越差 单线程也会有可靠性的问题 针对上面的种种不足，就有了下面的线程模型 单Reactor多线程模型 这种模型和第一种模型到的主要区别是把业务处理从之前的单一线程脱离出来，换成线程池处理，也就是Reactor线程只处理连接事件和读写事件，业务处理交给线程池处理，充分利用多核机器的资源、提高性能并且增加可靠性 该线程模型的不足 Reactor线程承担所有的事件，例如监听和响应，高并发场景下单线程存在性能问题 多Reactor多线程模型 这种模型下和第二种模型相比是把Reactor线程拆分了mainReactor和subReactor两个部分，mainReactor只处理连接事件，读写事件交给subReactor来处理。业务逻辑还是由线程池来处理 mainRactor只处理连接事件，用一个线程来处理就好。处理读写事件的subReactor个数一般和CPU数量相等，一个subReactor对应一个线程，业务逻辑由线程池处理 这种模型使各个模块职责单一，降低耦合度，性能和稳定性都有提高 这种模型在许多项目中广泛应用，比如Netty的主从线程模型等 Reactor三种模式形象比喻餐厅一般有接待员和服务员，接待员负责在门口接待顾客，服务员负责全程服务顾客 Reactor的三种线程模型可以用接待员和服务员类比 单Reactor单线程模型：接待员和服务员是同一个人，一直为顾客服务。客流量较少适合 单Reactor多线程模型：一个接待员，多个服务员。客流量大，一个人忙不过来，由专门的接待员在门口接待顾客，然后安排好桌子后，由一个服务员一直服务，一般每个服务员负责一片中的几张桌子 多Reactor多线程模型：多个接待员，多个服务员。这种就是客流量太大了，一个接待员忙不过来了 参考资料 《Scalable IO in Java》 -Doug Lea 【关注公众号，回复“Doug Lea” 获取该pdf】 1文章来源微信公众号：每天晒白牙]]></content>
      <categories>
        <category>IO模型</category>
      </categories>
      <tags>
        <tag>Reactor</tag>
        <tag>IO模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux grep命令详解]]></title>
    <url>%2F2019%2F06%2F16%2Flinux-grep%2F</url>
    <content type="text"><![CDATA[一篇讲解grep很好的文章，以便日后查阅。 文章来源：grep命令详解]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中永久代的垃圾回收]]></title>
    <url>%2F2019%2F06%2F16%2Fjava-perm-GC%2F</url>
    <content type="text"><![CDATA[今天面试被问到jvm中永久代会发生垃圾回收吗？ 首先，关于永久代的内容可以看这个：[jvm中方法区和永久代的关系] 垃圾回收不会出现在永久代，但是如果永久代满了会触发完全垃圾回收（Full GC）。 Hotspot的永久代是在方法区，主要存储的是类加载信息，静态变量以及常量，方法（字节码）等等，可以进行常量池回收和类型卸载。 如果这个常量在其它任何对象都没被引用，则可以被回收。 而类型卸载有点复杂，有以下三点要求： 该类型的所有实例都已经被回收 该类型的ClassLoader已经被回收 该类型的java.lang.Class没有在任何地方被引用，该类型不能在任何地方以反射的方式实例化一个对象。在java8中，已经取消了永久代，但是引入了一个元数据区的navite内存区。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>永久代</tag>
        <tag>metaspace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2019%2F06%2F16%2Fmarkdown-syntax%2F</url>
    <content type="text"><![CDATA[前言学习一下Markdown的语法，以便更好地组织文章结构。在此记录，以便日后的查阅。 主要内容 Markdown是什么？谁创造了它？为什么要使用它？怎么使用？谁在用？尝试一下 正文1. Markdown是什么？Markdown是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，并最终以HTML格式发布。Markdown也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。 2. 谁创造了它？它由Aaron Swartz和John Gruber共同设计，Aaron Swartz就是那位于去年（2013年1月11日）自杀,有着开挂一般人生经历的程序员。维基百科对他的介绍是：软件工程师、作家、政治组织者、互联网活动家、维基百科人。 他有着足以让你跪拜的人生经历： 14岁参与RSS 1.0规格标准的制订。 2004年入读斯坦福，之后退学。 2005年创建Infogami，之后与Reddit合并成为其合伙人。 2010年创立求进会（Demand Progress），积极参与禁止网络盗版法案（SOPA）活动，最终该提案被撤回。 2011年7月19日，因被控从MIT和JSTOR下载480万篇学术论文并以免费形式上传于网络被捕。 2013年1月自杀身亡。 天才都有早逝的归途。 3. 为什么要使用它？ 它是易读（看起来舒服）、易写（语法简单）、易更改纯文本。处处体现着极简主义的影子。 兼容HTML，可以转换为HTML格式发布。 跨平台使用。 越来越多的网站支持Markdown。 更方便清晰地组织你的电子邮件。（Markdown-here, Airmail） 摆脱Word（我不是认真的）。 4. 怎么使用？如果不算扩展，Markdown的语法绝对简单到让你爱不释手。 Markdown语法主要分为如下几大部分： 标题，段落，区块引用，代码区块，强调，列表，分割线，链接，图片，反斜杠 \，符号’`’。 4.1 标题两种形式：1）使用=和-标记一级和二级标题。 一级标题=========二级标题--------- 效果： 一级标题二级标题 2）使用#，可表示1-6级标题。 一级标题二级标题三级标题四级标题五级标题六级标题 效果： 一级标题二级标题三级标题四级标题五级标题六级标题 4.2 段落段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用两个以上空格加上回车（引用中换行省略回车）。 4.3 区块引用在段落的每行或者只在第一行使用符号&gt;,还可使用多个嵌套引用，如： &gt; 区块引用&gt;&gt; 嵌套引用 效果： 区块引用 嵌套引用 4.4 代码区块代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如普通段落： void main(){ printf(“Hello, Markdown.”);} 代码区块： 1234void main()&#123; printf("Hello, Markdown.");&#125; 注意:需要和普通段落之间存在空行。 4.5 强调在强调内容两侧分别加上*或者_，如： 斜体，斜体粗体，粗体 效果： 斜体，斜体粗体，粗体 4.6 列表使用·、+、或-标记无序列表，如： -（+） 第一项 -（+） 第二项 - （+*）第三项 注意：标记有一个_空格或制表符_。若不在引用区块中，必须和前方段落之间存在空行。 效果： 第一项 第二项 第三项 有序列表的标记方式是将上述的符号换成数字,并辅以.，如： 1 . 第一项2 . 第二项3 . 第三项 效果： 第一项 第二项 第三项 4.7 分割线分割线最常使用就是三个或以上*，还可以使用-和_。 4.8 链接链接可以由两种形式生成：行内式和参考式。 行内式： younghz的Markdown库。 效果： younghz的Markdown库。 参考式： younghz的Markdown库1younghz的Markdown库2 效果： younghz的Markdown库1younghz的Markdown库2 注意：上述的[1]:https:://github.com/younghz/Markdown &quot;Markdown&quot;不出现在区块中。 4.9 图片添加图片的形式和链接相似，只需在链接的基础上前方加一个！。 4.10 反斜杠\相当于反转义作用。使符号成为普通符号。 4.11 符号’`’起到标记作用。如： 12&gt; ctrl+a&gt; 效果： 12&gt; ctrl+a&gt; 5. 谁在用？Markdown的使用者： GitHub 简书 Stack Overflow Apollo Moodle Reddit 等等 6. 尝试一下 Chrome下的插件诸如stackedit与markdown-here等非常方便，也不用担心平台受限。 在线的dillinger.io评价也不错 Windowns下的MarkdownPad也用过，不过免费版的体验不是很好。 Mac下的Mou是国人贡献的，口碑很好。 Linux下的ReText不错。 当然，最终境界永远都是笔下是语法，心中格式化 :)。 注意：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。 虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber] (http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/ )。 以上基本是所有traditonal markdown的语法。 其它：列表的使用(非traditonal markdown)： 用|表示表格纵向边界，表头和表内容用-隔开，并可用:进行对齐设置，两边都有:则表示居中，若不加:则默认左对齐。 代码库 链接 MarkDown https://github.com/younghz/Markdown MarkDownCopy https://github.com/younghz/Markdown 关于其它扩展语法可参见具体工具的使用说明。 1以上内容来源自：https://github.com/younghz/Markdown]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm中方法区和永久代的关系]]></title>
    <url>%2F2019%2F06%2F16%2Fjvm-MethodArea-PERM%2F</url>
    <content type="text"><![CDATA[前言知道有方法区，知道里面存放的是什么东西。知道有永久代，也知道它在jdk1.7和jdk1.8的区别。但是对他们的关系有点稀里糊涂。。。 什么是方法区？方法区（Method Area）是jvm规范里面的运行时数据区的一个组成部分，jvm规范中的运行时数据区还包含了：pc寄存器、虚拟机栈、堆、方法区、运行时常量池、本地方法栈，还应该有堆外内存。 方法区存储什么？主要用来存储class、运行时常量池、字段、方法、代码、JIT代码等。 注意： 运行时数据区跟内存不是一个概念。 方法区是运行时数据区的一部分。 方法区是jvm规范中的一部分，并不是实际的实现，切忌将规范跟实现混为一谈。 永久代（Perm区）永久代又叫Perm区，只存在于hotspot jvm中，并且只存在于jdk7和之前的版本中，jdk8中已经彻底移除了永久代，jdk8中引入了一个新的内存区域叫metaspace。 并不是所有的jvm中都有永久代，ibm的j9，oracle的JRocket都没有永久代。 永久代是实现层面的东西。 永久代里面存的东西基本上就是方法区规定的那些东西。 因此，我们可以说，永久代是方法区的一种实现，当然，在hotspot jdk8中metaspace可以看成是方法区的一种实现。 下面我们来看下hotspot jdk8中移除了永久代以后的内存结构： 结论 方法区是规范层面的东西，规定了这一个区域要存放哪些东西 永久代（Hotspot虚拟机特有的概念）或者是metaspace是对方法区的不同实现，是实现层面的东西。 1234作者：若鱼1919链接：https://www.imooc.com/article/47149来源：慕课网本文原创发布于慕课网 ，转载请注明出处，谢谢合作]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>永久代</tag>
        <tag>metaspace</tag>
        <tag>jvm</tag>
        <tag>方法区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产者-消费者问题详解]]></title>
    <url>%2F2019%2F06%2F15%2Fproducer-consumer%2F</url>
    <content type="text"><![CDATA[文章参考自：生产者-消费者问题详解 一、明确定义要理解生产消费者问题，首先应弄清PV操作的含义：PV操作是由P操作原语和V操作原语组成（原语是不可中断的过程），对信号量进行操作，具体定义如下： P（S）：①将信号量S的值减1，即S=S-1； ​ ②如果S&gt;=0，则该进程继续执行；否则该进程置为等待状态，排入等待队列。 1解读：P操作可以理解为申请资源。P操作每次申请一个资源（S = S - 1，可用资源个数减少1），如果可用资源的个数大于等于0（S&gt;=0），那么说明本次申请资源操作成功，继续执行后续程序。否则说明没有足够的资源供该进程使用，该进程置为等待状态，加入等待该资源的等待队列。 V（S）：①将信号量S的值加1，即S=S+1； ​ ②如果S&gt;0，则该进程继续执行；否则释放队列中第一个等待信号量的进程。 1解读：V操作可以理解为释放资源。V操作每次释放一个资源（S = S + 1，可用资源个数增加1），如果可用资源的个数大于0（S &gt; 0），说明本次释放资源操作成功，继续执行后续程序。否则释放该资源的等待队列中第一个等待信号量的进程。 这只是书本的定义，对于这部分内容，老师先不要急于解释上面的程序流程，而是应该让学生首先知道P操作与V操作到底有什么作用。 P操作相当于申请资源，而V操作相当于释放资源。所以要学生记住以下几个关键字： 123P操作-----&gt;申请资源V操作-----&gt;释放资源 二、形象启发为此举两个生活中的例子： 例一：在公共电话厅打电话 公共电话厅里有多个电话，如某人要打电话，首先要进行申请，看是否有电话空闲，若有，则可以使用电话，如果电话亭里所有电话都有人正在使用，那后来的人只有排队等候。当某人用完电话后，则有空电话腾出，正在排队的第一个人就可以使用电话。这就相当于PV操作： 123某人要打电话，首先要进行申请，相当于执行一次P操作，申请一个可用资源（电话）；某人用完电话，则有空电话腾出，相当于执行一次V操作，释放一个可用资源（电话）。 三、分层解剖在理解了PV操作的的含义后，就必须讲解利用PV操作可以实现进程的两种情况：互斥和同步。根据互斥和同步不同的特点，就有利用PV操作实现互斥与同步相对固定的结构模式。这里就不详细讲解了。但生产者-消费者问题是一个有代表性的进程同步问题。但是如果我们将问题细分成三种情况进行讲解，理解难度将大大降低。 1）一个生产者，一个消费者，公用一个缓冲区。可以作以下比喻：将一个生产者比喻为一个生产厂家，如伊利牛奶厂家，而一个消费者，比喻是学生小明，而一个缓冲区则比喻成一间好又多(类似于商店)。第一种情况，可以理解成伊利牛奶生产厂家生产一盒牛奶，把它放在好又多一分店进行销售，而小明则可以从那里买到这盒牛奶。只有当厂家把牛奶放在商店里面后，小明才可以从商店里买到牛奶。所以很明显这是最简单的同步问题。 解题如下： 定义两个同步信号量： 123empty——表示缓冲区是否为空，初值为1。full——表示缓冲区中是否为满，初值为0。 生产者进程 123456while(TRUE)&#123; 生产一个产品; P(empty); 产品送往Buffer; V(full);&#125; 生产者行为： 生成一个商品 拿到 empty 同步信号量，执行P操作（empty -= 1，empty现在等于0，表示不为空），表示缓冲区有数据 产品运送Buffer 拿到 full 同步信号量，执行V操作（full += 1，full现在等于1，表示满），表示缓冲区有数据 消费者进程 123456while(TRUE)&#123; P(full); 从Buffer取出一个产品; V(empty); 消费该产品;&#125; 消费者行为： 拿到 full 同步信号量，执行P操作（full -= 1，full现在等于0，表示不满），表示缓冲区无数据 从Buffer取出一个产品 拿到 empty 同步信号量，执行V操作（empty += 1，empty现在等于1，表示空），表示缓冲区无数据 消费该产品 2）一个生产者，一个消费者，公用n个环形缓冲区。第二种情况可以理解为伊利牛奶生产厂家可以生产好多牛奶，并将它们放在多个好又多分店进行销售，而小明可以从任一间好又多分店中购买到牛奶。同样，只有当厂家把牛奶放在某一分店里，小明才可以从这间分店中买到牛奶。不同于第一种情况的是，第二种情况有N个分店（即N个缓冲区形成一个环形缓冲区），所以要利用指针，要求厂家必须按一定的顺序将商品依次放到每一个分店中。缓冲区的指向则通过模运算得到。 解题如下： 定义两个同步信号量： 123empty——表示缓冲区是否为空，初值为n。full——表示缓冲区中是否为满，初值为0。 设缓冲区的编号为1～n-1，定义两个指针in和out，分别是生产者进程和消费者进程使用的指针，指向下一个可用的缓冲区。 生产者进程 1234567while(TRUE)&#123; 生产一个产品; P(empty); 产品送往buffer（in）； in=(in+1)mod n； V(full);&#125; 消费者进程 1234567while(TRUE)&#123; P(full); 从buffer（out）中取出产品； out=(out+1)mod n； V(empty); 消费该产品;&#125; 3）一组生产者，一组消费者，公用n个环形缓冲区第三种情况，可以理解成有多间牛奶生产厂家，如蒙牛，达能，光明等，消费者也不只小明一人，有许许多多消费者。不同的牛奶生产厂家生产的商品可以放在不同的好又多分店中销售，而不同的消费者可以去不同的分店中购买。当某一分店已放满某个厂家的商品时，下一个厂家只能把商品放在下一间分店。所以在这种情况中，生产者与消费者存在同步关系，而且各个生产者之间、各个消费者之间存在互斥关系,他们必须互斥地访问缓冲区。（得好好揣摩这句话） 解题如下： 定义四个信号量： 1234567empty——表示缓冲区是否为空，初值为n。full——表示缓冲区中是否为满，初值为0。mutex1——生产者之间的互斥信号量，初值为1。mutex2——消费者之间的互斥信号量，初值为1。 设缓冲区的编号为1～n-1，定义两个指针in和out，分别是生产者进程和消费者进程使用的指针，指向下一个可用的缓冲区。 生产者进程 123456789while(TRUE)&#123; 生产一个产品; P(empty); //生成产品，那么缓冲区一定不为空 P(mutex1)；//同一时间只能有一个生产者生成商品 产品送往buffer（in）； in=(in+1)mod n； V(mutex1); V(full);&#125; 消费者进程 12345678while(TRUE)&#123; P(full); P(mutex2)； 从buffer（out）中取出产品； out=(out+1)mod n； V（mutex2）； V(empty);&#125;]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>生产者&amp;消费者</tag>
        <tag>PV操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitignore]]></title>
    <url>%2F2019%2F06%2F15%2Fgitignore%2F</url>
    <content type="text"><![CDATA[下面文章参考自：Git 忽略提交 .gitignore 前言在使用Git的过程中，有的文件比如日志，临时文件，编译的中间文件等不需要提交到代码仓库，这时就要设置相应的忽略规则，来忽略这些文件的提交。 规则 作用 /mtk 过滤整个文件夹 *.zip 过滤所有.zip文件 /mtk/do.c 过滤某个具体文件 !/mtk/one.txt 追踪（不过滤）某个具体文件 注意：如果你创建.gitignore文件之前就push了某一文件，那么即使你在.gitignore文件中写入过滤该文件的规则，该规则也不会起作用，git仍然会对该文件进行版本管理。 配置语法以斜杠“/”开头表示目录； 以星号“*”通配多个字符； 以问号“?”通配单个字符 以方括号“[]”包含单个字符的匹配列表； 以叹号“!”表示不忽略(跟踪)匹配到的文件或目录。 注意： git 对于 .gitignore配置文件是按行从上到下进行规则匹配的 Git 忽略文件提交的方法有三种方法可以实现忽略Git中不想提交的文件。 在Git项目中定义 .gitignore 文件这种方式通过在项目的某个文件夹下定义 .gitignore 文件，在该文件中定义相应的忽略规则，来管理当前文件夹下的文件的Git提交行为。 .gitignore 文件是可以提交到共有仓库中，这就为该项目下的所有开发者都共享一套定义好的忽略规则。 在 .gitingore 文件中，遵循相应的语法，在每一行指定一个忽略规则。如： 123*.log*.temp/vendor 在Git项目的设置中指定排除文件这种方式只是临时指定该项目的行为，需要编辑当前项目下的 .git/info/exclude 文件，然后将需要忽略提交的文件写入其中。 需要注意的是，这种方式指定的忽略文件的根目录是项目根目录。 定义Git全局的 .gitignore 文件除了可以在项目中定义 .gitignore 文件外，还可以设置全局的 git .gitignore 文件来管理所有Git项目的行为。这种方式在不同的项目开发者之间是不共享的，是属于项目之上Git应用级别的行为。 这种方式也需要创建相应的 .gitignore 文件，可以放在任意位置。然后在使用以下命令配置Git： 1git config --global core.excludesfile ~/.gitignore Git 忽略规则详细的忽略规则可以参考官方英文文档 Git 忽略规则优先级在 .gitingore 文件中，每一行指定一个忽略规则，Git 检查忽略规则的时候有多个来源，它的优先级如下（由高到低）： 从命令行中读取可用的忽略规则 当前目录定义的规则 父级目录定义的规则，依次地推 $GIT_DIR/info/exclude 文件中定义的规则 core.excludesfile中定义的全局规则 Git 忽略规则匹配语法在 .gitignore 文件中，每一行的忽略规则的语法如下： 空格不匹配任意文件，可作为分隔符，可用反斜杠转义 # 开头的模式标识注释，可以使用反斜杠进行转义 ! 开头的模式标识否定，该文件将会再次被包含，如果排除了该文件的父级目录，则使用 ! 也不会再次被包含。可以使用反斜杠进行转义 / 结束的模式只匹配文件夹以及在该文件夹路径下的内容，但是不匹配该文件 / 开始的模式匹配项目跟目录 如果一个模式不包含斜杠，则它匹配相对于当前 .gitignore 文件路径的内容，如果该模式不在 .gitignore 文件中，则相对于项目根目录 **匹配多级目录，可在开始，中间，结束 ?通用匹配单个字符 []通用匹配单个字符列表 常用匹配示例： bin/: 忽略当前路径下的bin文件夹，该文件夹下的所有内容都会被忽略，不忽略 bin 文件 /bin: 忽略根目录下的bin文件 /*.c: 忽略 cat.c，不忽略 build/cat.c debug/*.obj: 忽略 debug/io.obj，不忽略 debug/common/io.obj 和 tools/debug/io.obj **/foo: 忽略/foo, a/foo, a/b/foo等 a/**/b: 忽略a/b, a/x/b, a/x/y/b等 !/bin/run.sh: 不忽略 bin 目录下的 run.sh 文件 *.log: 忽略所有 .log 文件 config.php: 忽略当前路径的 config.php 文件 .gitignore规则不生效.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。 解决方法就是先把本地缓存删除（改变成未track状态），然后再提交: 123git rm -r --cached .git add .git commit -m 'update .gitignore' 作者：王伟desire链接：https://www.jianshu.com/p/74bd0ceb6182来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js css 404]]></title>
    <url>%2F2019%2F06%2F15%2Fjs-404%2F</url>
    <content type="text"><![CDATA[问题描述本地调试Hexo的过程中，无意间打开google浏览器的开发者模式，然后发现很多有关 css、js404的错误： 问题分析首先打开Hexo所使用的主题所在目录（我用的是next6，点击这里下载），依次进入source/lib目录下，发现并没有 canvas-nest等文件夹，所以也并没有canvas-nest.min.js等文件，问题锁定。 问题解决打开Hexo所用主题所在目录，找到主题配置文件_config.yml，将其打开，然后搜索canvas_nest，结果如下图： canvas_nest下面 enable 属性为 true，说明开启了canvas_nest，但是并没有对应的lib支持，所以需要安装对应的lib。 安装方法切换到Hexo主题根目录下，我的是：D:\work\hexo\changsk\themes\next6。 看上图，写明了lib的下载地址：Dependencies: https://github.com/theme-next/theme-next-canvas-nest 只需下载即可，在这里利用 git clone，执行以下命令： 1git clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest note：source/lib/canvas-nest 表示下载的lib存放的位置，next6的lib资源文件都放在source/lib下，canvas-nest文件夹的名称要和报错信息里面文件夹名称一致，不然还是会报404错误，找不到该资源文件。 当然也可以手动进行下载，然后放到正确的位置。执行完成后，可以发现lib下面多了个文件夹，里面有前端所需要的canvas-nest.min.js然后执行 123hexo cleanhexo ghexo s 进行本地调试，发现canvas-nest.min.js 404错误已经消失，其他类似的错误都可以通过这种方式解决。 后记执行git clone *之后，下载目录下面会有 *.git文件夹，最好把它删掉。 因为如果进行hexo源文件备份的话，会把整个hexo源文件push到github仓库，包括theme文件夹，因为theme文件夹/source/lib/的一些lib是通过*git clone的方式获取的。这种方式下载的lib会在文件夹下面生成.git。那么进行hexo备份的时候，会发现有多个.git**文件夹存在，就会报错，详情看这里]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>404</tag>
        <tag>css</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 创建分支]]></title>
    <url>%2F2019%2F06%2F12%2Fgit-command%2F</url>
    <content type="text"><![CDATA[搭建好本地 Hexo ，然后链接到了 github pages，也绑定了域名changsk.top 。但是 github 博客仓库里面中的文件和本地文件不一样，有些差异，这是因为本地文件经 hexo g 命令生成静态页面后，然后经hexo d deploy（部署）到github上，所以github仓库是没有本地hexo源文件。如果某一天不小心把本地仓库文件删了，或者换了电脑等原因，致使hexo源文件丢失，那么会造成一定的损失。所以可以把本地 hexo 源文件也同步到 github 上面。方法是在原仓库另创建一个分支，专门用于同步本地 Hexo 源文件。 执行以下命令的前提：当前主机已经可以通过SSH连接到 github 博客仓库（即本机生成的SSH KEY放入到 github 博客仓库当中去） 新建 git 仓库首先新建一个文件夹，比如起名为 changsk_backup ，在此文件夹内打开Git Bash，输入命令： 1git init # 在当前目录创建新的 Git 仓库 可以看到在当前文件夹里面会生成隐藏文件夹 .git，表示当前文件夹是一个git仓库 添加远程仓库1git remote add origin https://github.com/Tkzccsk/changsk.git https://github.com/Tkzccsk/changsk.git 是博客仓库的地址，获取方式是登录GitHub，找到自己的博客的仓库。远程库的名字就是origin，这是Git默认的叫法，可以起其他名。 查看远程仓库的名称 1git remote 下载远程仓库将GitHub上的博客仓库完全下载下来 1git pull origin master # 将远程仓库 origin 的 master 分支 pull 到本地仓库 创建新分支创建并切换到一个新分支（原来的分支名为master），输入命令： 1git checkout -b changsk changsk 为新分支名上述命令相当于两条命令 12git branch changsk # 创建分支git checkout changsk # 切换分支 上传本地hexo源文件到github博客仓库的changsk分支这样就在我们的博客仓库中新建了一个名为changsk的分支，接下来把生成的.git文件复制到本地 hexo 仓库中去，现在changsk_backup就没有用了，可以删了。接下来我们把 Hexo 本地博客仓库源文件中上传到GitHub的博客仓库的changsk分支中。 123git add .git commit -m "backup"git push origin changsk git add . ： 使用它会把工作时的所有变化提交到暂存区，包括文件内容修改(modified)以及新文件(new)，但不包括被删除的文件git commit -m “backup” ： 主要是将暂存区里的改动给提交到本地的版本库。-m 表示使用消息参数， “backup” 为 -m 的内容，用来表示这次提交的简要说明。git push origin changsk ： 将本地仓库的代码推送到changsk分支。 进入到GitHub的博客仓库中，可以发现出现了一个新的分支changsk，并且里面是我们的博客原件。最好在上传备份之前写一份README.md 文件，作为一项说明（因为这是GitHub建议的）。通过以上方式，我们就完成了备份啦，下一次更新了博客，首先执行 1hexo clean &amp; hexo d -g 注意：部署本地 Hexo 到 github 用的是 master 分支(__config.yml中的声明)生成及部署hexo，然后执行 123git add .git commit -m "backup"git push origin changsk 进行本地 Hexo 源文件的备份。参考 ：https://blog.csdn.net/qq_34229391/article/details/82251852]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[changes not staged for commit]]></title>
    <url>%2F2019%2F06%2F12%2Fchanges-not-staged-for-commit%2F</url>
    <content type="text"><![CDATA[在使用 git 的过程中，要把当前文件夹下的内容 push 到 changsk 分支，先执行 1git add . 然后执行 1git commit -m "backup" 最后执行 push 操作，将当前本地 git 仓库提交到 origin 远程仓库的 changsk 分支 1git push origin changsk # changsk 是我创建的一个分支 然后就收到报错，报错信息如下：大概意思是： 1更新（push操作）被拒绝，因为github远程仓库changsk分支的有些内容在本地仓库没有。 然后我想起了我昨天在 github 仓库 changsk 分支里面创建了一个 README.md，所以本地仓库是没有的，造成了远方仓库和本地仓库的不一致（精确来说是远方仓库有，但是本地仓库没有），所以 push 之前应该先把远程仓库的内容pull（拉取）到本地，然后才可以进行push。 所以先应该执行 1git pull origin changsk 将远程仓库 origin 的 chagnsk 分支和当前本地 git 仓库进行合并，使它们保持一致然后执行 1git add . 然后执行 1git commit -m "backup" 又收到一个不同的错误经过网上查阅，大部分人都说是因为没有执行 git add .，但我显然不是这个问题。原因是我要 push 的本地仓库里面还有另外的clone过来的git仓库，我查看文件夹，就像报错信息里面说的， themes/next（Hexo 的一个主题，也是本网站使用的主题） 里面是我 git clone 下来的一个仓库。解决办法是删除 themes/next 文件夹里面的隐藏文件夹 .git然后再执行就没有问题了。 1git commit -m "backup" 最后执行 push 操作 1git push origin changsk 问题解决。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CRLF CR LF]]></title>
    <url>%2F2019%2F06%2F12%2FCRLF-CR-LF%2F</url>
    <content type="text"><![CDATA[deploy 本地 Hexo 到 github pages 的时候，遇到了两陌生的单词：LF和CRLF ，本着不求甚解的态度，去网上找了相关的资料，特此记录，以便日后查看。其实前几天在安装 Hexo 的过程中有过一个设置LF和CRLF 的转换的选项，当时没在意。 名词解释CR：Carriage Return，对应ASCII中转义字符\r，表示回车LF：Linefeed，对应ASCII中转义字符\n，表示换行CRLF：Carriage Return &amp; Linefeed，\r\n，表示回车并换行 Windows操作系统采用两个字符来进行换行，即CRLF Unix/Linux/Mac OS X操作系统采用单个字符LF来进行换行 野史据野史记载，在很久以前的机械打字机时代，CR和LF分别具有不同的作用：LF会将打印纸张上移一行位置，但是保持当前打字的水平位置不变；CR则会将“Carriage”（打字机上的滚动托架）滚回到打印纸张的最左侧，但是保持当前打字的垂直位置不变，即还是在同一行。当CR和LF组合使用时，则会将打印纸张上移一行，且下一个打字位置将回到该行的最左侧，也就是我们今天所理解的换行操作。 随着时间的推移，机械打字机渐渐地退出了历史舞台，当初的纸张变成了今天的显示器，打字机的按键也演变为了如今的键盘。在操作系统出现的年代，受限于内存和软盘空间的不足，一些操作系统的设计者决定采用单个字符来表示换行符，如Unix的LF、MacIntosh的CR。他们的意图都是为了进行换行操作，只是当初并没有一个国际标准，所以才有这样字符上的不同。参考：https://www.jianshu.com/p/b03ad01acd69]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 的常用命令]]></title>
    <url>%2F2019%2F06%2F11%2Fhexo-command%2F</url>
    <content type="text"><![CDATA[常用 hexo 命令hexo new “postName” #新建文章hexo new page “pageName” #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help #查看帮助hexo version #查看Hexo的版本 缩写hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 组合命令：hexo s -g #生成并本地预览hexo d -g #生成并上传hexo clear # 删除无用 tags 和 categorieshexo clean &amp; hexo d -g # 清除缓存 生成静态页面并发布 给文章添加标签和分类1234567title: hexo 的常用命令date: 2019-06-11 11:43:56tags: - hexo # 文章标签- aaa- bbbcategories: hexo # 该文章类别为 categories\hexo 效果图：]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error JAVA_HOME is incorrectly set]]></title>
    <url>%2F2019%2F06%2F11%2FError_JAVA_HOME_is%20_incorrectly_set%2F</url>
    <content type="text"><![CDATA[在安装、调试、运行Hexo的过程中，出现了以下错误： Error: JAVA_HOME is incorrectly set. Please update D:\software\software\hadoop3\hadoop-3.0.2\etc\hadoop\hadoop-env.cmd 然后找到对应的目录，打开hadoop-env.cmd，发现其中的 JAVA_HOME 是这样的： 然后打开 terminal，查询 java 版本 以及 JAVA_HOME 环境变量： 发现 JAVA_HOME 已正确配置。那么问题究竟出在哪里？经网上查阅，因为Program Files中存在空格，所以出现错误，只需要用PROGRA~1代替Program Files即可。如图： 或者也可以将jdk装到其他不存在空格的目录下。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
