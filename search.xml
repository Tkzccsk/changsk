<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[剑指offer-链表中环的入口结点]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-entrance-node%2F</url>
    <content type="text"><![CDATA[题目描述给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 解题思路 假设x为环前面的路程（黑色路程），a为环入口到相遇点的路程（蓝色路程，假设顺时针走）， c为环的长度（蓝色+橙色路程）当快慢指针相遇的时候： 此时慢指针走的路程为Sslow = x + m * c + a快指针走的路程为Sfast = x + n * c + a2 Sslow = Sfast2 * ( x + mc + a ) = (x + n *c + a)从而可以推导出：x = (n - 2 * m )c - a= (n - 2 m -1 )c + c - a即环前面的路程 = 数个环的长度（为可能为0） + c - a什么是c - a？这是相遇点后，环后面部分的路程。（橙色路程）所以，我们可以让一个指针从起点A开始走，让一个指针从相遇点B开始继续往后走，2个指针速度一样，那么，当从原点的指针走到环入口点的时候（此时刚好走了x）从相遇点开始走的那个指针也一定刚好到达环入口点。所以2者会相遇，且恰好相遇在环的入口点。 最后，判断是否有环，且找环的算法复杂度为： 时间复杂度：O(n)空间复杂度：O(1) AC代码12345678910111213141516171819202122232425262728293031323334/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode EntryNodeOfLoop(ListNode pHead) &#123; //头结点为空，或者只有头结点的情况 if(pHead == null || pHead.next == null) return null; ListNode p1 = pHead;//慢指针 ListNode p2 = pHead;//快指针 while(p2 != null &amp;&amp; p2.next != null)&#123; p1 = p1.next; p2 = p2.next.next; //快慢指针相遇 if(p1 == p2)&#123; p1 = pHead; while(p1 != p2)&#123; p1 = p1.next; p2 = p2.next; &#125; return p1; &#125; &#125; return null; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-字符流中第一个不重复的字符]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-first-char%2F</url>
    <content type="text"><![CDATA[题目描述请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 输出描述如果当前字符流没有存在出现一次的字符，返回#字符。 AC代码 创建一个长度为256的char数组(ascii字符用一个字节表示)。 数组的下标表示字符（数字可以表示字符），数组中的值由3种数字组成 0，代表该字符没有出现过 -1，表示该字符出现过不止一次 其他值，表示该字符是第一次出现的字符当中的第几个 每次遍历数组，寻找只出现过一次且出现次序最小的字符。 12345678910111213141516171819202122232425public class Solution &#123; //记录一个字符第一次出现的次序 int index = 0; int[] chars = new int[256]; //Insert one char from stringstream public void Insert(char ch) &#123; if(chars[ch] == 0) chars[ch] = ++index; else chars[ch] = -1; //-1代表重复出现 &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; int min = Integer.MAX_VALUE; char c = '#'; for(int i = 0;i &lt; 255;i++)&#123; //一个字符出现过，且只出现过一次，且出现的次序比之前的小 if(chars[i] != 0 &amp;&amp; chars[i] != -1 &amp;&amp; chars[i] &lt; min)&#123; min = chars[i]; c = (char)i; &#125; &#125; return c; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-表示数值的字符串]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-num-str%2F</url>
    <content type="text"><![CDATA[题目描述请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,“5e2”,”-123”,“3.1416”和”-1E-16”都表示数值。 但是”12e”,“1a3.14”,“1.2.3”,”±5”和”12e+4.3”都不是。 AC代码 利用 java中的正则表达式匹配可以很容易的写出来 123456789101112public class Solution &#123; public boolean isNumeric(char[] str) &#123; String s=String.valueOf(str); /* 1. X? X，一次或一次也没有 2. X* X，零次或多次 3. java 正则表达式中 . 表示任何字符。若要匹配 . 要用转义 \\. 4. X+ X，一次或多次 */ return s.matches("[+-]?[0-9]*(\\.[0-9]+)?([eE][+-]?[0-9]+)?"); &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-构建乘积数组]]></title>
    <url>%2F2019%2F07%2F03%2Fjianzhioffer-product-array%2F</url>
    <content type="text"><![CDATA[题目描述给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]。不能使用除法。 解题思路 B[i]的值可以看作上图的矩阵中每行的乘积。每个B[i]都可以分为两部分（1左边的位一部分，右边的又是一部分）。下三角用连乘可以很容求得，同理可得上三角。先算下三角中的连乘，即我们先算出B[i]中的一部分，然后倒过来按上三角中的分布规律，把另一部分也乘进去。 AC代码123456789101112131415161718import java.util.ArrayList;public class Solution &#123; public int[] multiply(int[] A) &#123; if(A == null) return null; int len = A.length; int[] B = new int[len]; //下三角 B[0] = 1; for(int i = 1;i &lt; len;i++) B[i] = B[i - 1] * A[i - 1]; //上三角 int tmp = 1; for(int i = len - 2;i &gt;= 0;i--)&#123; tmp *= A[i + 1]; B[i] *= tmp; &#125; return B; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis概述]]></title>
    <url>%2F2019%2F07%2F03%2Fredis-summary%2F</url>
    <content type="text"><![CDATA[参考：https://github.com/CyC2018/CS-Notes 概述Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。 键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。 Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。 数据类型(值的类型) 数据类型 可以存储的值 操作 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 对单个或者多个元素 进行修剪，只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 What Redis data structures look like STRING set hello worldOKget hello“world”del hello(integer) 1get hello(nil) LIST 123456789101112131415161718192021&gt; rpush list-key item(integer) 1&gt; rpush list-key item2(integer) 2&gt; rpush list-key item(integer) 3&gt; lrange list-key 0 -11) "item"2) "item2"3) "item"&gt; lindex list-key 1"item2"&gt; lpop list-key"item"&gt; lrange list-key 0 -11) "item2"2) "item" SET 123456789101112131415161718192021222324252627&gt; sadd set-key item(integer) 1&gt; sadd set-key item2(integer) 1&gt; sadd set-key item3(integer) 1&gt; sadd set-key item(integer) 0&gt; smembers set-key1) "item"2) "item2"3) "item3"&gt; sismember set-key item4(integer) 0&gt; sismember set-key item(integer) 1&gt; srem set-key item2(integer) 1&gt; srem set-key item2(integer) 0&gt; smembers set-key1) "item"2) "item3" HASH 123456789101112131415161718192021222324&gt; hset hash-key sub-key1 value1(integer) 1&gt; hset hash-key sub-key2 value2(integer) 1&gt; hset hash-key sub-key1 value1(integer) 0&gt; hgetall hash-key1) "sub-key1"2) "value1"3) "sub-key2"4) "value2"&gt; hdel hash-key sub-key2(integer) 1&gt; hdel hash-key sub-key2(integer) 0&gt; hget hash-key sub-key1"value1"&gt; hgetall hash-key1) "sub-key1"2) "value1" ZSET 12345678910111213141516171819202122232425&gt; zadd zset-key 728 member1(integer) 1&gt; zadd zset-key 982 member0(integer) 1&gt; zadd zset-key 982 member0(integer) 0&gt; zrange zset-key 0 -1 withscores1) "member1"2) "728"3) "member0"4) "982"&gt; zrangebyscore zset-key 0 800 withscores1) "member1"2) "728"&gt; zrem zset-key member1(integer) 1&gt; zrem zset-key member1(integer) 0&gt; zrange zset-key 0 -1 withscores1) "member0"2) "982" 数据结构字典dictht 是一个散列表结构，使用拉链法保存哈希冲突。 123456789/* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */typedef struct dictht &#123; dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used;&#125; dictht; 12345678910typedef struct dictEntry &#123; void *key; union &#123; void *val; uint64_t u64; int64_t s64; double d; &#125; v; struct dictEntry *next;&#125; dictEntry; Redis 的字典 dict 中包含两个哈希表 dictht，这是为了方便进行 rehash 操作。在扩容时，将其中一个 dictht 上的键值对 rehash 到另一个 dictht 上面，完成之后释放空间并交换两个 dictht 的角色。 1234567typedef struct dict &#123; dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */&#125; dict; rehash 操作不是一次性完成，而是采用渐进方式，这是为了避免一次性执行过多的 rehash 操作给服务器带来过大的负担。 渐进式 rehash 通过记录 dict 的 rehashidx 完成，它从 0 开始，然后每执行一次 rehash 都会递增。例如在一次 rehash 中，要把 dict[0] rehash 到 dict[1]，这一次会把 dict[0] 上 table[rehashidx] 的键值对 rehash 到 dict[1] 上，dict[0] 的 table[rehashidx] 指向 null，并令 rehashidx++。 在 rehash 期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式 rehash。 采用渐进式 rehash 会导致字典中的数据分散在两个 dictht 上，因此对字典的查找操作也需要到对应的 dictht 去执行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* Performs N steps of incremental rehashing. Returns 1 if there are still * keys to move from the old to the new hash table, otherwise 0 is returned. * * Note that a rehashing step consists in moving a bucket (that may have more * than one key as we use chaining) from the old to the new hash table, however * since part of the hash table may be composed of empty spaces, it is not * guaranteed that this function will rehash even a single bucket, since it * will visit at max N*10 empty buckets in total, otherwise the amount of * work it does would be unbound and the function may block for a long time. */int dictRehash(dict *d, int n) &#123; int empty_visits = n * 10; /* Max number of empty buckets to visit. */ if (!dictIsRehashing(d)) return 0; while (n-- &amp;&amp; d-&gt;ht[0].used != 0) &#123; dictEntry *de, *nextde; /* Note that rehashidx can't overflow as we are sure there are more * elements because ht[0].used != 0 */ assert(d-&gt;ht[0].size &gt; (unsigned long) d-&gt;rehashidx); while (d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) &#123; d-&gt;rehashidx++; if (--empty_visits == 0) return 1; &#125; de = d-&gt;ht[0].table[d-&gt;rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while (de) &#123; uint64_t h; nextde = de-&gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask; de-&gt;next = d-&gt;ht[1].table[h]; d-&gt;ht[1].table[h] = de; d-&gt;ht[0].used--; d-&gt;ht[1].used++; de = nextde; &#125; d-&gt;ht[0].table[d-&gt;rehashidx] = NULL; d-&gt;rehashidx++; &#125; /* Check if we already rehashed the whole table... */ if (d-&gt;ht[0].used == 0) &#123; zfree(d-&gt;ht[0].table); d-&gt;ht[0] = d-&gt;ht[1]; _dictReset(&amp;d-&gt;ht[1]); d-&gt;rehashidx = -1; return 0; &#125; /* More to rehash... */ return 1;&#125; 跳跃表是有序集合的底层实现之一。 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。 在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程。 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快速，因为不需要进行旋转等操作来维护平衡性； 更容易实现； 支持无锁操作。 使用场景计数器可以对 String 进行自增自减运算，从而实现计数器功能。 Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。 缓存将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。 查找表例如 DNS 记录就很适合使用 Redis 进行存储。 查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效,因为缓存不作为可靠的数据来源。 消息队列List 是一个双向链表，可以通过 lpop 和 lpush 写入和读取消息。 不过最好使用 Kafka、RabbitMQ 等消息中间件。 会话缓存可以使用 Redis 来统一存储多台应用服务器的会话信息。 当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。 分布式锁实现在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。 可以使用 Reids 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。 其它Set 可以实现交集、并集等操作，从而实现共同好友等功能。 ZSet 可以实现有序性操作，从而实现排行榜等功能。 Redis 与 Memcached两者都是非关系型内存键值数据库，主要有以下不同： 数据类型Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型，可以更灵活地解决问题。 数据持久化Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。 分布式Memcached 不支持分布式，只能通过在客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。 Redis Cluster 实现了分布式的支持。 内存管理机制 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。 Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 键的过期时间Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。 对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。 数据淘汰策略可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。 Reids 具体有 6 种淘汰策略： 策略 描述 volatile-lru 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集中任意选择数据淘汰 allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰 allkeys-random 从所有数据集中任意选择数据进行淘汰 noeviction 禁止驱逐数据 作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法实际实现上并非针对所有 key，而是抽样一小部分并且从中选出被淘汰的 key。 使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。 Redis 4.0 引入了 volatile-lfu 和 allkeys-lfu 淘汰策略，LFU 策略通过统计访问频率，将访问频率最少的键值对淘汰。 持久化Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 RDB 持久化将某个时间点的所有数据都存放到硬盘上。 可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。 如果系统发生故障，将会丢失最后一次创建快照之后的数据。 如果数据量很大，保存快照的时间会很长。 AOF 持久化将写命令添加到 AOF 文件（Append Only File）的末尾。 使用 AOF 持久化需要设置同步选项，从而确保写命令什么时候会同步到磁盘文件上。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项： 选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统来决定何时同步 always 选项会严重减低服务器的性能； everysec 选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响； no 选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量。 随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。 事务一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其它客户端的命令请求。 事务中的多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。 Redis 最简单的事务实现方式是使用 MULTI 和 EXEC 命令将事务操作包围起来。 事件Redis 服务器是一个事件驱动程序。 文件事件服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。 Redis 基于 Reactor 模式开发了自己的网络事件处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。 时间事件服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。 事件的调度与执行服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下： 12345678910111213141516def aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： 12345678def main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 复制通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。 一个从服务器只能有一个主服务器，并且不支持主主复制。 连接过程 主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令； 从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令； 主服务器每执行一次写命令，就向从服务器发送相同的写命令。 主从链随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。 SentinelSentinel（哨兵）可以监听集群中的服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。 分片分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，这种方法在解决某些问题时可以获得线性级别的性能提升。 假设有 4 个 Reids 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，… ，有不同的方式来选择一个指定的键存储在哪个实例中。 最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。 还有一种方式是哈希分片，使用CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。 根据执行分片的位置，可以分为三种分片方式： 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。 服务器分片：Redis Cluster。 一个简单的论坛系统分析该论坛系统功能如下： 可以发布文章； 可以对文章进行点赞； 在首页可以按文章的发布时间或者文章的点赞数进行排序显示。 文章信息文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。 Redis 没有关系型数据库中的表这一概念来将同种类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。 点赞功能当有用户为一篇文章点赞时，除了要对该文章的 votes 字段进行加 1 操作，还必须记录该用户已经对该文章进行了点赞，防止用户点赞次数超过 1。可以建立文章的已投票用户集合来进行记录。 为了节约内存，规定一篇文章发布满一周之后，就不能再对它进行投票，而文章的已投票集合也会被删除，可以为文章的已投票集合设置一个一周的过期时间就能实现这个规定。 对文章进行排序为了按发布时间和点赞数进行排序，可以建立一个文章发布时间的有序集合和一个文章点赞数的有序集合。（下图中的 score 就是这里所说的点赞数；下面所示的有序集合分值并不直接是时间和点赞数，而是根据时间和点赞数间接计算出来的） 参考资料 Carlson J L. Redis in Action[J]. Media.johnwiley.com.au, 2013. 黄健宏. Redis 设计与实现 [M]. 机械工业出版社, 2014. REDIS IN ACTION Skip Lists: Done Right 论述 Redis 和 Memcached 的差异 Redis 3.0 中文版- 分片 Redis 应用场景 Using Redis as an LRU cache]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ping原理和ICMP协议]]></title>
    <url>%2F2019%2F07%2F03%2Fping-icmp%2F</url>
    <content type="text"><![CDATA[本文转载自:ping 原理与ICMP协议 ping 的原理​ ping 程序是用来探测主机到主机之间是否可通信，如果不能ping到某台主机，表明不能和这台主机建立连接。ping 使用的是ICMP协议，它发送icmp回送请求消息给目的主机。ICMP协议规定：目的主机必须返回ICMP回送应答消息给源主机。如果源主机在一定时间内收到应答，则认为主机可达。​ ICMP协议通过IP协议发送的，IP协议是一种无连接的，不可靠的数据包协议。在Unix/Linux，序列号从0开始计数，依次递增。而Windows ping程序的ICMP序列号是没有规律。​ ICMP协议在实际传输中数据包：20字节IP首部 + 8字节ICMP首部+ 数据​ ICMP报文格式:IP首部(20字节)+8位类型+8位代码+16位校验和+(不同的类型和代码，格式也有所不同) Ping工作过程​ 假定主机A的IP地址是192.168.1.1，主机B的IP地址是192.168.1.2，都在同一子网内，则当你在主机A上运行“Ping 192.168.1.2”后，都发生了些什么呢? 首先，Ping命令会构建一个固定格式的ICMP请求数据包，然后由ICMP协议将这个数据包连同地址“192.168.1.2”一起交给IP层协议（和ICMP一样，实际上是一组后台运行的进程），IP层协议将以地址“192.168.1.2”作为目的地址，本机IP地址作为源地址，加上一些其他的控制信息，构建一个IP数据包，并在一个映射表中查找出IP地址192.168.1.2所对应的物理地址（也叫MAC地址，熟悉网卡配置的朋友不会陌生，这是数据链路层协议构建数据链路层的传输单元——帧所必需的），一并交给数据链路层。后者构建一个数据帧，目的地址是IP层传过来的物理地址，源地址则是本机的物理地址，还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。其中映射表由ARP实现。ARP(Address Resolution Protocol)是地址解析协议,是一种将IP地址转化成物理地址的协议。ARP具体说来就是将网络层（IP层，也就是相当于OSI的第三层）地址解析为数据连接层（MAC层，也就是相当于OSI的第二层）的MAC地址。 主机B收到这个数据帧后，先检查它的目的地址，并和本机的物理地址对比，如符合，则接收；否则丢弃。接收后检查该数据帧，将IP数据包从帧中提取出来，交给本机的IP层协议。同样，IP层检查后，将有用的信息提取后交给ICMP协议，后者处理后，马上构建一个ICMP应答包，发送给主机A，其过程和主机A发送ICMP请求包到主机B一模一样。即先由IP地址，在网络层传输，然后再根据mac地址由数据链路层传送到目的主机。 ICMPICMP协议介绍前面讲到了，IP协议并不是一个可靠的协议，它不保证数据被送达，那么，自然的，保证数据送达的工作应该由其他的模块来完成。其中一个重要的模块就是ICMP(网络控制报文)协议。 当传送IP数据包发生错误－－比如主机不可达，路由不可达等等，ICMP协议将会把错误信息封包，然后传送回给主机。给主机一个处理错误的机会，这 也就是为什么说建立在IP层以上的协议是可能做到安全的原因。ICMP数据包由8bit的错误类型和8bit的代码和16bit的校验和组成。而前 16bit就组成了ICMP所要传递的信息。 尽管在大多数情况下，错误的包传送应该给出ICMP报文，但是在特殊情况下，是不产生ICMP错误报文的。如下 ICMP差错报文不会产生ICMP差错报文（出IMCP查询报文）（防止IMCP的无限产生和传送） 目的地址是广播地址或多播地址的IP数据报。 作为链路层广播的数据报。 不是IP分片的第一片。 源地址不是单个主机的数据报。这就是说，源地址不能为零地址、环回地址、广播地 址或多播地址。 虽然里面的一些规定现在还不是很明白，但是所有的这一切规定，都是为了防止产生ICMP报文的无限传播而定义的。 ICMP协议大致分为两类，一种是查询报文，一种是差错报文。其中查询报文有以下几种用途: ping查询 子网掩码查询（用于无盘工作站在初始化自身的时候初始化子网掩码） 时间戳查询（可以用来同步时间） 而差错报文则产生在数据传送发生错误的时候。就不赘述了。 ICMP的应用-ping（IP）ping可以说是ICMP的最著名的应用，当我们某一个网站上不去的时候。通常会ping一下这个网站。ping会回显出一些有用的信息。一般的信息如下: Reply from 10.4.24.1: bytes=32 time&lt;1ms TTL=255Reply from 10.4.24.1: bytes=32 time&lt;1ms TTL=255Reply from 10.4.24.1: bytes=32 time&lt;1ms TTL=255Reply from 10.4.24.1: bytes=32 time&lt;1ms TTL=255 Ping statistics for 10.4.24.1: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),Approximate round trip times in milli-seconds: Minimum = 0ms, Maximum = 0ms, Average = 0ms ping这个单词源自声纳定位，而这个程序的作用也确实如此，它利用ICMP协议包来侦测另一个主机是否可达。原理是用类型码为0的ICMP发请 求，受到请求的主机则用类型码为8的ICMP回应。ping程序来计算间隔时间，并计算有多少个包被送达。用户就可以判断网络大致的情况。我们可以看到， ping给出来了传送的时间和TTL的数据。我给的例子不太好，因为走的路由少，有兴趣地可以ping一下国外的网站比如sf.net，就可以观察到一些 丢包的现象，而程序运行的时间也会更加的长。ping还给我们一个看主机到目的主机的路由的机会。这是因为，ICMP的ping请求数据报在每经过一个路由器的时候，路由器都会把自己的ip放到该数据报中。而目的主机则会把这个ip列表复制到回应icmp数据包中发回给主机。但是，无论如何，ip头所能纪录的路由列表是非常的有限。如果要观察路由， 我们还是需要使用更好的工具，就是要讲到的Traceroute(windows下面的名字叫做tracert)。 ICMP的应用-Traceroute（UDP）Traceroute是用来侦测主机到目的主机之间所经路由情况的重要工具，也是最便利的工具。前面说到，尽管ping工具也可以进行侦测，但是，因为ip头的限制，ping不能完全的记录下所经过的路由器。所以Traceroute正好就填补了这个缺憾。 Traceroute的原理是非常非常的有意思，它收到目的主机的IP后，首先给目的主机发送一个TTL=1（还记得TTL是什么吗？）的UDP(后面就 知道UDP是什么了)数据包，而经过的第一个路由器收到这个数据包以后，就自动把TTL减1，而TTL变为0以后，路由器就把这个包给抛弃了，并同时产生 一个主机不可达的ICMP数据报给主机。主机收到这个数据报以后再发一个TTL=2的UDP数据报给目的主机，然后刺激第二个路由器给主机发ICMP数据 报。如此往复直到到达目的主机。这样，traceroute就拿到了所有的路由器ip。从而避开了ip头只能记录有限路由IP的问题。 有人要问，我怎么知道UDP到没到达目的主机呢？这就涉及一个技巧的问题，TCP和UDP协议有一个端口号定义，而普通的网络程序只监控少数的几个号码较 小的端口，比如说80,比如说23,等等。而traceroute发送的是端口号&gt;30000(真变态)的UDP报，所以到达目的主机的时候，目的主机只能发送一个端口不可达的ICMP数据报给主机。主机接到这个报告以后就知道，主机到了。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>ping</tag>
        <tag>ICMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-82:Remove Duplicates from Sorted List II]]></title>
    <url>%2F2019%2F07%2F03%2Fleetcode-82%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/remove-duplicates-from-sorted-list-ii/ 这道题目和leetcode-83:Remove Duplicates from Sorted List(排序链表中删除重复结点)的区别是，这道题要求把重复的数字一个都不保留，全都删除。 题目描述 题目难度：Medium Given a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Example 1 Input: 1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5Output: 1-&gt;2-&gt;5 Example 2 Input: 1-&gt;1-&gt;1-&gt;2-&gt;3Output: 2-&gt;3 AC代码1234567891011121314151617181920212223242526272829303132/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode deleteDuplicates(ListNode head) &#123; //head 为空，或者只有一个结点，那么直接返回head if(head == null || head.next == null) return head; //创建新的结点，并和原来的链表相连 ListNode newHead = new ListNode(0); newHead.next =head; ListNode pre = newHead, cur = head; while(cur != null)&#123; while(cur.next != null &amp;&amp; cur.val == cur.next.val) cur = cur.next; //cur结点没有重复的情况 if(pre.next == cur)&#123; pre = cur; cur = cur.next; &#125; //cur结点有重复的情况下，pre结点不动 else&#123; cur = cur.next; pre.next = cur; &#125; &#125; return newHead.next; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-83:Remove Duplicates from Sorted List(排序链表中删除重复结点)]]></title>
    <url>%2F2019%2F07%2F02%2Fleetcode-83%2F</url>
    <content type="text"><![CDATA[题目描述 题目难度：Easy Given a sorted linked list, delete all duplicates such that each element appear only once. Example 1 Input: 1-&gt;1-&gt;2Output: 1-&gt;2 Example 2 Input: 1-&gt;1-&gt;2-&gt;3-&gt;3Output: 1-&gt;2-&gt;3 AC代码1234567891011121314151617181920/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode deleteDuplicates(ListNode head) &#123; if(head == null) return head; ListNode curNode = head; while(curNode.next != null)&#123; if(curNode.val == curNode.next.val) curNode.next = curNode.next.next; else curNode = curNode.next; &#125; return head; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-数组中重复的数字]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-repeated-number%2F</url>
    <content type="text"><![CDATA[题目描述在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 AC代码112345678910111213141516171819202122232425public class Solution &#123; // Parameters: // numbers: an array of integers // length: the length of array numbers // duplication: (Output) the duplicated number in the array number,length of duplication array is 1,so using duplication[0] = ? in implementation; // Here duplication like pointor in C/C++, duplication[0] equal *duplication in C/C++ // 这里要特别注意~返回任意重复的一个，赋值duplication[0] // Return value: true if the input is valid, and there are some duplications in the array number // otherwise false public boolean duplicate(int numbers[],int length,int [] duplication) &#123; if(numbers == null || numbers.length == 0 || numbers.length == 1) return false; for(int i = 0;i &lt; numbers.length;i++)&#123; while(numbers[i] != i)&#123; if(numbers[numbers[i]] == numbers[i])&#123; //判断数组下标为 numbers[i] 的元素是否为 numbers[i]，是的话表示重复 duplication[0] = numbers[i]; return true; &#125; int temp = numbers[i]; //要注意这一块的交换 numbers[i] = numbers[temp]; numbers[temp] = temp; &#125; &#125; return false; &#125;&#125; AC代码2123456789101112131415161718192021222324252627public class Solution &#123; // Parameters: // numbers: an array of integers // length: the length of array numbers // duplication: (Output) the duplicated number in the array number,length of duplication array is 1,so using duplication[0] = ? in implementation; // Here duplication like pointor in C/C++, duplication[0] equal *duplication in C/C++ // 这里要特别注意~返回任意重复的一个，赋值duplication[0] // Return value: true if the input is valid, and there are some duplications in the array number // otherwise false public boolean duplicate(int numbers[],int length,int [] duplication) &#123; if(numbers == null || numbers.length == 0) return false; for(int i = 0;i &lt; length;i++)&#123; if(i == numbers[i]) continue; int j = i, temp; while(j != numbers[j])&#123; if(numbers[numbers[j]] == numbers[j])&#123; duplication[0] = numbers[j]; return true; &#125; temp = numbers[j]; numbers[numbers[j]] = numbers[j]; j = temp; &#125; &#125; return false; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中索引无效的情况]]></title>
    <url>%2F2019%2F07%2F02%2Fmysql-index-Invalid%2F</url>
    <content type="text"><![CDATA[最佳左前缀原则——如果索引了多列，要遵守最左前缀原则。指的是查询要从索引的最左前列开始并且不跳过索引中的列。违反了最佳左前缀原则，索引会失效，进行全表扫描 不在索引列上做任何操作（计算，函数【avg，max，min等】，（自动或者手动）类型装换），会导致索引失效而导致全表扫描 mysql使用不等于(!= 或者&lt;&gt;)的时候，无法使用索引，会导致索引失效 mysql中使用is not null 或者 is null会导致无法使用索引 mysql中like查询是以%开头，索引会失效变成全表扫描，覆盖索引。 mysql中，如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)。要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql使用全表扫描要比使用索引快,则不会使用到索引 参考：mysql索引失效情况]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cookie和session区别]]></title>
    <url>%2F2019%2F07%2F02%2Fcookie-session%2F</url>
    <content type="text"><![CDATA[本文转载自：你真的了解 Cookie 和 Session 吗 我在做面试官的时候，曾经问过很多朋友这个问题： Cookie 和 Session 有什么区别呢？大部分的面试者应该都可以说上一两句，比如：什么是 Cookie？什么是 Session？两者的区别等。 但如果再往深入探讨的话，就慢慢有一些朋友不太了解了，谈起原理时就很少有朋友全部回答准确。今天和大家一起深入聊聊有关 Cookie 和 Session 的话题 。 第一层楼什么是 Cookie 和 Session ?初级程序员高频面试题。 什么是 Cookie HTTP Cookie（也叫 Web Cookie或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。 Cookie 主要用于以下三个方面： 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） 什么是 Session Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。 第二层楼Cookie 和 Session 有什么不同？ 作用范围不同，Cookie 保存在客户端（浏览器），Session 保存在服务器端。 session会服务器上保存一段时间，这个会产生一个问题——当用户访问增多，会占用你服务器的资源，从而影响性能。 存取方式的不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等。 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。 存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。 前两层楼内容，绝大部分同学都可以准确回答 第三层楼为什么需要 Cookie 和 Session，他们有什么关联？ 说起来为什么需要 Cookie ，这就需要从浏览器开始说起，我们都知道浏览器是没有状态的(HTTP 协议无状态)，这意味着浏览器并不知道是张三还是李四在和服务端打交道。这个时候就需要有一个机制来告诉服务端，本次操作用户是否登录，是哪个用户在执行的操作，那这套机制的实现就需要 Cookie 和 Session 的配合。 那么 Cookie 和 Session 是如何配合的呢？我画了一张图大家可以先了解下。 用户第一次请求服务器的时候，服务器根据用户提交的相关信息，创建对应的 Session ，请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器，浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入到 Cookie 中，同时 Cookie 记录此 SessionID 属于哪个域名。 当用户第二次访问服务器的时候，请求会自动判断此域名下是否存在 Cookie 信息，如果存在自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。 根据以上流程可知，SessionID 是连接 Cookie 和 Session 的一道桥梁，大部分系统也是根据此原理来验证用户登录状态。 三层楼的内容，大部分同学可以讲清楚。 第四层楼既然服务端是根据 Cookie 中的信息判断用户是否登录，那么如果浏览器中禁止了 Cookie，如何保障整个机制的正常运转。 第一种方案，每次请求中都携带一个 SessionID 的参数，也可以 Post 的方式提交，也可以在请求的地址后面拼接 xxx?SessionID=123456...。 第二种方案，Token 机制。Token 机制多用于 App 客户端和服务器交互的模式，也可以用于 Web 端做用户状态管理。 Token 的意思是“令牌”，是服务端生成的一串字符串，作为客户端进行请求的一个标识。Token 机制和 Cookie 和 Session 的使用机制比较类似。 当用户第一次登录后，服务器根据提交的用户信息生成一个 Token，响应时将 Token 返回给客户端，以后客户端只需带上这个 Token 前来请求数据即可，无需再次登录验证。 四层楼的内容，一部分同学可以讲清楚。 第五层楼如何考虑分布式 Session 问题？ 在互联网公司为了可以支撑更大的流量，后端往往需要多台服务器共同来支撑前端用户请求，那如果用户在 A 服务器登录了，第二次请求跑到服务 B 就会出现登录失效问题。 分布式 Session 一般会有以下几种解决方案： Nginx ip_hash 策略，服务端使用 Nginx 代理，每个请求按访问 IP 的 hash 分配，这样来自同一 IP 固定访问一个后台服务器，避免了在服务器 A 创建 Session，第二次分发到服务器 B 的现象。 Session 复制，任何一个服务器上的 Session 发生改变（增删改），该节点会把这个 Session 的所有内容序列化，然后广播给所有其它节点。 共享 Session，服务端无状态话，将用户的 Session 等信息使用缓存中间件来统一管理，保障分发到每一个服务器的响应结果都一致。 建议采用第三种方案。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Cookie</tag>
        <tag>Session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http状态码]]></title>
    <url>%2F2019%2F07%2F02%2Fhttp-status-code%2F</url>
    <content type="text"><![CDATA[HTTP状态码含义： 状态码 含义 100 客户端应当继续发送请求。这个临时响应是用来通知客户端它的部分请求已经被服务器接收，且仍未被拒绝。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。 101 服务器已经理解了客户端的请求，并将通过Upgrade 消息头通知客户端采用不同的协议来完成这个请求。在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。 只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的HTTP 版本比旧版本更有优势，或者切换到一个实时且同步的协议以传送利用此类特性的资源。 102 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。 200 请求已成功，请求所希望的响应头或数据体将随此响应返回。 201 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其 URI 已经随Location 头信息返回。假如需要的资源无法及时建立的话，应当返回 ‘202 Accepted’。 202 服务器已接受请求，但尚未处理。正如它可能被拒绝一样，最终该请求可能会也可能不会被执行。在异步操作的场合下，没有比发送这个状态码更方便的做法了。 返回202状态码的响应的目的是允许服务器接受其他过程的请求（例如某个每天只执行一次的基于批处理的操作），而不必让客户端一直保持与服务器的连接直到批处理操作全部完成。在接受请求处理并返回202状态码的响应应当在返回的实体中包含一些指示处理当前状态的信息，以及指向处理状态监视器或状态预测的指针，以便用户能够估计操作是否已经完成。 203 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超级。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。响应可能通过实体头部的形式，返回新的或更新后的元信息。如果存在这些头部信息，则应当与所请求的变量相呼应。 如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。 由于204响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。 205 服务器成功处理了请求，且没有返回任何内容。但是与204响应不同，返回此状态码的响应要求请求者重置文档视图。该响应主要是被用于接受用户输入后，立即重置表单，以便用户能够轻松地开始另一次输入。 与204响应一样，该响应也被禁止包含任何消息体，且以消息头后的第一个空行结束。 206 服务器已经成功处理了部分 GET 请求。类似于 FlashGet 或者迅雷这类的 HTTP 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。 该请求必须包含 Range 头信息来指示客户端希望得到的内容范围，并且可能包含 If-Range 来作为请求条件。 响应必须包含如下的头部域： Content-Range 用以指示本次响应中返回的内容的范围；如果是 Content-Type 为 multipart/byteranges 的多段下载，则每一 multipart 段中都应包含 Content-Range 域用以指示本段的内容范围。假如响应中包含 Content-Length，那么它的数值必须匹配它返回的内容范围的真实字节数。 Date ETag 和/或 Content-Location，假如同样的请求本应该返回200响应。 Expires, Cache-Control，和/或 Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了 If-Range 强缓存验证，那么本次响应不应该包含其他实体头；假如本响应的请求使用了 If-Range 弱缓存验证，那么本次响应禁止包含其他实体头；这避免了缓存的实体内容和更新了的实体头信息之间的不一致。否则，本响应就应当包含所有本应该返回200响应中应当返回的所有实体头部域。 假如 ETag 或 Last-Modified 头部不能精确匹配的话，则客户端缓存应禁止将206响应返回的内容与之前任何缓存过的内容组合在一起。 任何不支持 Range 以及 Content-Range 头的缓存都禁止缓存206响应返回的内容。 207 由WebDAV(RFC 2518)扩展的状态码，代表之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。 300 被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。 除非这是一个 HEAD 请求，否则该响应应当包括一个资源特性及地址的列表的实体，以便用户或浏览器从中选择最合适的重定向地址。这个实体的格式由 Content-Type 定义的格式所决定。浏览器可能根据响应的格式以及浏览器自身能力，自动作出最合适的选择。当然，RFC 2616规范并没有规定这样的自动选择该如何进行。 如果服务器本身已经有了首选的回馈选择，那么在 Location 中应当指明这个回馈的 URI；浏览器可能会将这个 Location 值作为自动重定向的地址。此外，除非额外指定，否则这个响应也是可缓存的。 301 被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。除非额外指定，否则这个响应也是可缓存的。 新的永久性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，因此浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：对于某些使用 HTTP/1.0 协议的浏览器，当它们发送的 POST 请求得到了一个301响应的话，接下来的重定向请求将会变成 GET 方式。 302 请求的资源现在临时从不同的 URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：虽然RFC 1945和RFC 2068规范不允许客户端在重定向时改变请求的方法，但是很多现存的浏览器将302响应视作为303响应，并且使用 GET 方式访问在 Location 中规定的 URI，而无视原先请求的方法。状态码303和307被添加了进来，用以明确服务器期待客户端进行何种反应。 303 对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的POST请求输出重定向到一个新的资源。这个新的 URI 不是原始资源的替代引用。同时，303响应禁止被缓存。当然，第二个请求（重定向）可能被缓存。 新的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 注意：许多 HTTP/1.1 版以前的 浏览器不能正确理解303状态。如果需要考虑与这些浏览器之间的互动，302状态码应该可以胜任，因为大多数的浏览器处理302响应时的方式恰恰就是上述规范要求客户端处理303响应时应当做的。 304 如果客户端发送了一个带条件的 GET 请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。304响应禁止包含消息体，因此始终以消息头后的第一个空行结尾。 该响应必须包含以下的头信息： Date，除非这个服务器没有时钟。假如没有时钟的服务器也遵守这些规则，那么代理服务器以及客户端可以自行将 Date 字段添加到接收到的响应头中去（正如RFC 2068中规定的一样），缓存机制将会正常工作。 ETag 和/或 Content-Location，假如同样的请求本应返回200响应。 Expires, Cache-Control，和/或Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了强缓存验证，那么本次响应不应该包含其他实体头；否则（例如，某个带条件的 GET 请求使用了弱缓存验证），本次响应禁止包含其他实体头；这避免了缓存了的实体内容和更新了的实体头信息之间的不一致。 假如某个304响应指明了当前某个实体没有缓存，那么缓存系统必须忽视这个响应，并且重复发送不包含限制条件的请求。 假如接收到一个要求更新某个缓存条目的304响应，那么缓存系统必须更新整个条目以反映所有在响应中被更新的字段的值。 305 被请求的资源必须通过指定的代理才能被访问。Location 域中将给出指定的代理所在的 URI 信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能建立305响应。 注意：RFC 2068中没有明确305响应是为了重定向一个单独的请求，而且只能被原始服务器建立。忽视这些限制可能导致严重的安全后果。 306 在最新版的规范中，306状态码已经不再被使用。 307 请求的资源现在临时从不同的URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的URI 应当在响应的 Location 域中返回。除非这是一个HEAD 请求，否则响应的实体中应当包含指向新的URI 的超链接及简短说明。因为部分浏览器不能识别307响应，因此需要添加上述必要信息以便用户能够理解并向新的 URI 发出访问请求。 如果这不是一个GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 400 1、语义有误，当前请求无法被服务器理解。除非进行修改，否则客户端不应该重复提交这个请求。 2、请求参数有误。 401 当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的 Authorization 头信息的请求。如果当前请求已经包含了 Authorization 证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。参见RFC 2617。 402 该状态码是为了将来可能的需求而预留的。 403 服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个 HEAD 请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。 404 请求失败，请求所希望得到的资源未被在服务器上发现。没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。 405 请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个Allow 头信息用以表示出当前资源能够接受的请求方法的列表。 鉴于 PUT，DELETE 方法会对服务器上的资源进行写操作，因而绝大部分的网页服务器都不支持或者在默认配置下不允许上述请求方法，对于此类请求均会返回405错误。 406 请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。 除非这是一个 HEAD 请求，否则该响应就应当返回一个包含可以让用户或者浏览器从中选择最合适的实体特性以及地址列表的实体。实体的格式由 Content-Type 头中定义的媒体类型决定。浏览器可以根据格式及自身能力自行作出最佳选择。但是，规范中并没有定义任何作出此类自动选择的标准。 407 与401响应类似，只不过客户端必须在代理服务器上进行身份验证。代理服务器必须返回一个 Proxy-Authenticate 用以进行身份询问。客户端可以返回一个 Proxy-Authorization 信息头用以验证。参见RFC 2617。 408 请求超时。客户端没有在服务器预备等待的时间内完成一个请求的发送。客户端可以随时再次提交这一请求而无需进行任何更改。 409 由于和被请求的资源的当前状态之间存在冲突，请求无法完成。这个代码只允许用在这样的情况下才能被使用：用户被认为能够解决冲突，并且会重新提交新的请求。该响应应当包含足够的信息以便用户发现冲突的源头。 冲突通常发生于对 PUT 请求的处理中。例如，在采用版本检查的环境下，某次 PUT 提交的对特定资源的修改请求所附带的版本信息与之前的某个（第三方）请求向冲突，那么此时服务器就应该返回一个409错误，告知用户请求无法完成。此时，响应实体中很可能会包含两个冲突版本之间的差异比较，以便用户重新提交归并以后的新版本。 410 被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。这样的状况应当被认为是永久性的。如果可能，拥有链接编辑功能的客户端应当在获得用户许可后删除所有指向这个地址的引用。如果服务器不知道或者无法确定这个状况是否是永久的，那么就应该使用404状态码。除非额外说明，否则这个响应是可缓存的。 410响应的目的主要是帮助网站管理员维护网站，通知用户该资源已经不再可用，并且服务器拥有者希望所有指向这个资源的远端连接也被删除。这类事件在限时、增值服务中很普遍。同样，410响应也被用于通知客户端在当前服务器站点上，原本属于某个个人的资源已经不再可用。当然，是否需要把所有永久不可用的资源标记为’410 Gone’，以及是否需要保持此标记多长时间，完全取决于服务器拥有者。 411 服务器拒绝在没有定义 Content-Length 头的情况下接受请求。在添加了表明请求消息体长度的有效 Content-Length 头之后，客户端可以再次提交该请求。 412 服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。这个状态码允许客户端在获取资源时在请求的元信息（请求头字段数据）中设置先决条件，以此避免该请求方法被应用到其希望的内容以外的资源上。 413 服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。此种情况下，服务器可以关闭连接以免客户端继续发送此请求。 如果这个状况是临时的，服务器应当返回一个 Retry-After 的响应头，以告知客户端可以在多少时间以后重新尝试。 414 请求的URI 长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。这比较少见，通常的情况包括： 本应使用POST方法的表单提交变成了GET方法，导致查询字符串（Query String）过长。 重定向URI “黑洞”，例如每次重定向把旧的 URI 作为新的 URI 的一部分，导致在若干次重定向后 URI 超长。 客户端正在尝试利用某些服务器中存在的安全漏洞攻击服务器。这类服务器使用固定长度的缓冲读取或操作请求的 URI，当 GET 后的参数超过某个数值后，可能会产生缓冲区溢出，导致任意代码被执行[1]。没有此类漏洞的服务器，应当返回414状态码。 415 对于当前请求的方法和所请求的资源，请求中提交的实体并不是服务器中所支持的格式，因此请求被拒绝。 416 如果请求中包含了 Range 请求头，并且 Range 中指定的任何数据范围都与当前资源的可用范围不重合，同时请求中又没有定义 If-Range 请求头，那么服务器就应当返回416状态码。 假如 Range 使用的是字节范围，那么这种情况就是指请求指定的所有数据范围的首字节位置都超过了当前资源的长度。服务器也应当在返回416状态码的同时，包含一个 Content-Range 实体头，用以指明当前资源的长度。这个响应也被禁止使用 multipart/byteranges 作为其 Content-Type。 417 在请求头 Expect 中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服务器，它有明显的证据证明在当前路由的下一个节点上，Expect 的内容无法被满足。 421 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 请求格式正确，但是由于含有语义错误，无法响应。（RFC 4918 WebDAV）423 Locked 当前资源被锁定。（RFC 4918 WebDAV） 424 由于之前的某个请求发生的错误，导致当前请求失败，例如 PROPPATCH。（RFC 4918 WebDAV） 425 在WebDav Advanced Collections 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中。 426 客户端应当切换到TLS/1.0。（RFC 2817） 449 由微软扩展，代表请求应当在执行完适当的操作后进行重试。 500 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。 501 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。 502 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。如果能够预计延迟时间，那么响应中可以包含一个 Retry-After 头用以标明这个延迟时间。如果没有给出这个 Retry-After 信息，那么客户端应当以处理500响应的方式处理它。 注意：503状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。 504 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。 注意：某些代理服务器在DNS查询超时时会返回400或者500错误 505 服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本。这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。 506 由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误：被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。 507 服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。WebDAV (RFC 4918) 509 服务器达到带宽限制。这不是一个官方的状态码，但是仍被广泛使用。 510 获取资源所需要的策略并没有没满足。（RFC 2774） 参考：http://tool.oschina.net/commons?type=5]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-236:Lowest Common Ancestor of a Binary Tree（LCA，最近公共祖先节点）]]></title>
    <url>%2F2019%2F07%2F02%2Fleetcode-236%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/ 题目描述 题目难度：Medium Given a binary tree, find the lowest common ancestor (LCA) of two given nodes in the tree. According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).” Given the following binary tree: root = [3,5,1,6,2,0,8,null,null,7,4] Example 1Input: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1Output: 3Explanation: The LCA of nodes 5 and 1 is 3. Example 2Input: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4Output: 5Explanation: The LCA of nodes 5 and 4 is 5, since a node can be a descendant of itself according to the LCA definition. NoteAll of the nodes’ values will be unique.p and q are different and both values will exist in the binary tree. AC代码1最简单粗暴的解法：从下往上依次遍历树中每个节点，第一次出现子树(包括节点本身)既有p又有q的结点为LCA所求结点。时间复杂度：O(n * n) 123456789101112131415161718192021222324class Solution &#123; private TreeNode resNode = null;public TreeNode findCommonNode(TreeNode root, TreeNode p, TreeNode q)&#123; if(root == null) return null; if(p == null &amp;&amp; q == null) return null; if(root.left != null) findCommonNode(root.left, p, q); if(root.right != null) findCommonNode(root.right, p, q); if(hasNode(root, p) &amp;&amp; hasNode(root, q)) if(resNode == null) resNode = root; return resNode;&#125;private boolean hasNode(TreeNode root, TreeNode node)&#123; if(root == null) return false; if(root == node) return true; if(root.left == node || root.right == node) return true; return hasNode(root.left, node) || hasNode(root.right, node);&#125; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; return findCommonNode(root, p, q); &#125;&#125; AC代码2leetcode大神的解法，以递归的方式解决LCA问题。时间复杂度：O(n)空间复杂度：O(n)。主要是递归栈的深度。 12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; /** 以下三种情况会返回： 1. root == null，return null 2. root 只等于 p 或者 q 其中一个结点 3. q 和 p 相等的情况下，root == q == p */ if(root == null || root == p || root == q) &#123; return root; &#125; TreeNode left = lowestCommonAncestor(root.left, p , q); TreeNode right = lowestCommonAncestor(root.right, p , q); /** 1. 如果 left == null，返回 right(不管right是否为null) 2. left != null，right == null，返回 left 3. left 和 right 都不等于null，说明 left 和 right 只和 q 或者 p 其中一个结点相等，root为p和q的最近公共子节点，返回root */ return left == null ? right: right == null ? left: root;&#125;&#125; AC代码3建立两队列，一个队列保存从根节点到p的路径，另一个队列保存从根节点到q的路径。两个路径中第一个不相同的节点的前一个节点即为LCA所求。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root == null || root == p || root == q) &#123; return root; &#125; LinkedList&lt;TreeNode&gt; q1 = new LinkedList&lt;&gt;(); LinkedList&lt;TreeNode&gt; q2 = new LinkedList&lt;&gt;(); getTrace(root, q1, p); getTrace(root, q2, q); return LCA(q1, q2);&#125; private boolean getTrace(TreeNode root, LinkedList&lt;TreeNode&gt; queue, TreeNode node)&#123; if(root == null) return false; if(root == node)&#123; queue.offer(root); return true; &#125; queue.offer(root); if(getTrace(root.left, queue, node) || getTrace(root.right, queue, node)) return true; queue.pollLast(); return false; &#125; private TreeNode LCA(Queue&lt;TreeNode&gt; q1, Queue&lt;TreeNode&gt; q2)&#123; TreeNode node = null; while(true)&#123; TreeNode n1 = q1.poll(); TreeNode n2 = q2.poll(); if(n1 == n2) node = n1; else break; &#125; return node; &#125;&#125; AC代码4离线Tarjan算法 后续有时间再研究。]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-23:Merge k Sorted Lists(合并k个有序链表)]]></title>
    <url>%2F2019%2F07%2F02%2Fleetcode-23%2F</url>
    <content type="text"><![CDATA[题目链接：https://leetcode.com/problems/merge-k-sorted-lists/ 题目描述 题目难度：Hard Merge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity. ExampleInput: [1-&gt;4-&gt;5,1-&gt;3-&gt;4,2-&gt;6] Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 AC代码leetcode 优秀的解法：灵活运用二路归并排序的思想。 时间复杂度：O(log(k) * n) ,k为链表长度，n为单个链表长度 12345678910111213141516171819202122232425262728293031323334353637383940/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode mergeKLists(ListNode[] lists) &#123; return partition(lists, 0, lists.length - 1); &#125; private ListNode partition(ListNode[] lists, int start, int end)&#123; if(start == end) return lists[start]; if(start &lt; end)&#123; int mid = (start + end) &gt;&gt; 1; ListNode l1 = partition(lists, start, mid); ListNode l2 = partition(lists, mid + 1, end); return mergeCore(l1, l2); &#125; else return null; &#125; //合并两个链表，返回合并后的头结点 private ListNode mergeCore(ListNode l1, ListNode l2)&#123; if(l1 != null &amp;&amp; l2 != null)&#123; if(l1.val &lt; l2.val)&#123; l1.next = mergeCore(l1.next, l2); return l1; &#125; else&#123; l2.next = mergeCore(l1, l2.next); return l2; &#125; &#125; if(l1 != null) return l1; else return l2; &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-不用加减乘除做加法]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-addition2%2F</url>
    <content type="text"><![CDATA[题目描述写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 AC代码用位运算 1234567891011121314public class Solution &#123; public int Add(int num1,int num2) &#123; int row_sum = 0; // 不含进位的和 int carry = 0; // 进位 while(num2 != 0)&#123; // 当进位为 0 时停止循环，条件row_sum + carry == num1 + num2 始终成立 row_sum = num1 ^ num2; // 不含进位的和可以用异或 ^ 实现 carry = (num1 &amp; num2) &lt;&lt; 1; // 通过 &amp; ，可以确定哪里有进位，然后左移一位表示进位 //计算 row_sum 和进位的和 num1 = row_sum; num2 = carry; &#125; return num1; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-求1+2+3+...+n]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-addition%2F</url>
    <content type="text"><![CDATA[题目描述求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 AC代码利用递归的思想 短路与&amp;&amp;可以充当if else的功能 1234567public class Solution &#123; public int Sum_Solution(int n) &#123; int sum = n; boolean ans = (n &gt; 0) &amp;&amp; ((sum += Sum_Solution(n - 1)) &gt; 0); return sum; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-孩子们的游戏(圆圈中最后剩下的数)]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-Joseph-ring%2F</url>
    <content type="text"><![CDATA[题目描述每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) AC代码1这道题其实就是一个约瑟夫环。 1234567891011121314151617181920public class Solution &#123; public int LastRemaining_Solution(int n, int m) &#123; if(n == 0 || m == 0) return -1; boolean[] selected = new boolean[n]; int count = 0; int cur = -1; //循环次数为 n - 1次，会把selected中n-1个数置为true，剩下的最后一个false即为所求 while(count != n - 1)&#123; for(int i = 0;i &lt; m;i++)&#123; cur = (cur + 1) % n; while(selected[cur] == true) cur = (cur + 1) % n; &#125; selected[cur] = true; count++; &#125; for(int i = 0;i &lt; n;i++) if(selected[i] == false) return i; return -1; &#125;&#125; AC代码2原理请看：https://blog.csdn.net/crazy__chen/article/details/45115911 12345678910111213public class Solution&#123; public int LastRemaining_Solution(int n, int m) &#123; if(n==0||m==0)return -1; int s=0; for(int i=2;i&lt;=n;i++) &#123; s=(s+m)%i; &#125; return s ; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-扑克牌顺子]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-Playing-cards%2F</url>
    <content type="text"><![CDATA[题目描述LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 解题思路 将原数组升序排序 计算0的个数zeros 计算两两非0元素之间的间隔之和gaps（1和3之间的间隔为3 - 1 - 1 = 1，代表数字1和3之间插入一个数字2，可形成顺子），如果出现两个非0元素相等，直接返回false（不可能形成顺子） 计算zeros和gaps的差值。如果zeros大于等于gaps，表示zeros可以填补形成顺子所需的数字，返回true；否则返回false。 AC代码12345678910111213141516171819import java.util.Arrays; public class Solution &#123; public boolean isContinuous(int [] numbers) &#123; if(numbers == null || numbers.length == 0) return false; Arrays.sort(numbers); //升序排序 int numsOfZero = 0;//0的个数 int gaps = 0;//间隔的个数 for(int i = 0;i &lt; numbers.length;i++)&#123; //两相邻数字相等，不可能形成顺子 if(i != 0 &amp;&amp; numbers[i] != 0 &amp;&amp; numbers[i - 1] == numbers[i]) return false; //间隔的计算，两相邻数字都不为0，且相差要大于1 if(i != 0 &amp;&amp; numbers[i - 1] != 0 &amp;&amp; numbers[i] - numbers[i - 1] != 1) gaps += numbers[i] - numbers[i - 1] - 1; if(numbers[i] == 0) numsOfZero++; &#125; if(gaps &lt;= numsOfZero) return true; return false; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库中聚集索引和非聚集索引的区别]]></title>
    <url>%2F2019%2F07%2F02%2Fdatabase-Clustered-index-Nonclustered-index%2F</url>
    <content type="text"><![CDATA[本文转载自：聚集索引和非聚集索引 索引简介众所周知，索引是关系型数据库中给数据库表中一列或多列的值排序后的存储结构，SQL的主流索引结构有B+树以及Hash结构，聚集索引以及非聚集索引用的是B+树索引。 SQL Sever索引类型有：唯一索引，主键索引，聚集索引，非聚集索引。 MySQL 索引类型有：唯一索引，主键（聚集）索引，非聚集索引，全文索引。 聚集索引聚集（clustered）索引，也叫聚簇索引。 定义：数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引。 由于聚集索引规定数据在表中的物理存储顺序，因此一个表只能包含一个聚集索引 单单从定义来看是不是显得有点抽象，打个比方，一个表就像是我们以前用的新华字典，聚集索引就像是拼音目录，而每个字存放的页码就是我们的数据物理地址，我们如果要查询一个“哇”字，我们只需要查询“哇”字对应在新华字典拼音目录对应的页码，就可以查询到对应的“哇”字所在的位置，而拼音目录对应的A-Z的字顺序，和新华字典实际存储的字的顺序A-Z也是一样的，如果我们中文新出了一个字，拼音开头第一个是B，那么他插入的时候也要按照拼音目录顺序插入到A字的后面。 非聚集索引非聚集（unclustered）索引。 定义：该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同，一个表中可以拥有多个非聚集索引。 其实按照定义，除了聚集索引以外的索引都是非聚集索引，只是人们想细分一下非聚集索引，分成普通索引，唯一索引，全文索引。如果非要把非聚集索引类比成现实生活中的东西，那么非聚集索引就像新华字典的偏旁字典，他结构顺序与实际存放顺序不一定一致。 聚集索引和非聚集索引的区别 聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个 聚集索引存储记录是物理上连续存在，而非聚集索引是逻辑上的连续，物理存储并不连续 聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。 聚集索引表记录的排列顺序与索引的排列顺序一致 优点是查询速度快，可以进行范围查找，因为一旦具有第一个索引值的纪录被找到，具有连续索引值的记录也一定物理的紧跟其后。 缺点是对表进行修改速度较慢，这是为了保持表中的记录的物理顺序与索引的顺序一致，而把记录插入到数据页的相应位置，必须在数据页中进行数据重排， 降低了执行速度。 建议使用聚集索引的场合为：a. 此列包含有限数目的不同值；b. 查询的结果返回一个区间的值；c. 查询的结果返回某值相同的大量结果集。 非聚集索引指定了表中记录的逻辑顺序，但记录的物理顺序和索引的顺序不一致 聚集索引和非聚集索引都采用了B+（平衡树）树的结构，但非聚集索引的叶子层并不与实际的数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针的方式。非聚集索引比聚集索引层次多，添加记录不会引起数据顺序的重组。建议使用非聚集索引的场合为：a. 此列包含了大量数目不同的值；b. 查询的结束返回的是少量的结果集；c. order by 子句中使用了该列。 表中经常有一个列或列的组合，其值能唯一地标识表中的每一行。这样的一列或多列称为表的主键.(默认为聚集索引) 聚集索引确定表中数据的物理顺序。聚集索引类似于电话簿，后者按姓氏排列数据。但该索引可以包含多个列（组合索引），就像电话簿按姓氏和名字进行组织一样。 聚集索引相当于我们书本上前面的目录的一样，它可以方便快速的找到你想找的内容，而非聚集索引就相当于书最后几页的解释，它是对书中某个语句或者是生词的解释，就像我们上学时候的地理说一样，书后面都有各种地理名称的英文解释；《数据库原理》里面的解释：聚集索引的顺序就是数据的物理存储顺序，而非聚集索引的顺序和数据物理排列无关。因为数据在物理存放时只能有一种排列方式，所以一个表只能有一个聚集索引。在SQL SERVER中，索引是通过二叉树的数据结构来描述的；我们可以如此理解这个两种索引：聚集索引的叶节点就是数据节点，而非聚集索引的叶节点仍然是索引节点，只不过其包含一个指向对应数据块的指针。聚集索引会降低 insert，和update操作的性能，所以，是否使用聚集索引要全面衡量。 参考文章：https://www.jianshu.com/p/5681ebd5b0efhttps://www.cnblogs.com/weixing/p/4317774.htmlhttps://www.cnblogs.com/s-b-b/p/8334593.html]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>聚集索引</tag>
        <tag>非聚集索引</tag>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-翻转单词顺序列]]></title>
    <url>%2F2019%2F07%2F02%2Fjianzhioffer-flip-word-order%2F</url>
    <content type="text"><![CDATA[题目描述牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ AC代码123456789101112131415161718192021222324252627282930public class Solution &#123; public String ReverseSentence(String str) &#123; if(str == null || str.length() == 0) return str; char[] chars = str.toCharArray(); int start = -1, end = -1; if(chars[0] != ' ') start = 0; for(int i = 0;i &lt; chars.length;i++)&#123; if(i &gt; 0 &amp;&amp; chars[i] != ' ' &amp;&amp; chars[i - 1] == ' ') start = i; if(chars[i] != ' ')&#123; if((i + 1 &lt; chars.length &amp;&amp; chars[i + 1] == ' ') || i + 1 == chars.length)&#123; end = i; reverse(chars, start, end); &#125; &#125; &#125; reverse(chars, 0, chars.length - 1); return new String(chars); &#125; //翻转chars数组中start至end(包括start和end)之间的字符 private void reverse(char[] chars, int start, int end)&#123; while(start &lt; end)&#123; char c = chars[start]; chars[start] = chars[end]; chars[end] = c; start++; end--; &#125; &#125;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引原理]]></title>
    <url>%2F2019%2F07%2F02%2Fdatabase-index-principle%2F</url>
    <content type="text"><![CDATA[本文转载自：深入浅出数据库索引原理 使用索引很简单，只要能写创建表的语句，就肯定能写创建索引的语句，要知道这个世界上是不存在不会创建表的服务器端程序员的。然而， 会使用索引是一回事， 而深入理解索引原理又能恰到好处使用索引又是另一回事，这完全是两个天差地别的境界（我自己也还没有达到这层境界）。很大一部份程序员对索引的了解仅限于到“加索引能使查询变快”这个概念为止。 为什么要给表加上主键？ 为什么加索引后会使查询变快？ 为什么加索引后会使写入、修改、删除变慢？ 什么情况下要同时在两个字段上建索引？ 这些问题他们可能不一定能说出答案。知道这些问题的答案有什么好处呢？如果开发的应用使用的数据库表中只有1万条数据，那么了解与不了解真的没有差别， 然而， 如果开发的应用有几百上千万甚至亿级别的数据，那么不深入了解索引的原理， 写出来程序就根本跑不动，就好比如果给货车装个轿车的引擎，这货车还能拉的动货吗？ 接下来就讲解一下上面提出的几个问题，希望对阅读者有帮助。 网上很多讲解索引的文章对索引的描述是这样的「索引就像书的目录， 通过书的目录就准确的定位到了书籍具体的内容」，这句话描述的非常正确， 但就像脱了裤子放屁，说了跟没说一样，通过目录查找书的内容自然是要比一页一页的翻书找来的快，同样使用的索引的人难到会不知道，通过索引定位到数据比直接一条一条的查询来的快，不然他们为什么要建索引。 想要理解索引原理必须清楚一种数据结构「平衡树」(非二叉)，也就是b tree或者 b+ tree，重要的事情说三遍：“平衡树，平衡树，平衡树”。当然， 有的数据库也使用哈希桶作用索引的数据结构 ， 然而， 主流的RDBMS都是把平衡树当做数据表默认的索引数据结构的。 我们平时建表的时候都会为表加上主键， 在某些关系数据库中， 如果建表时不指定主键，数据库会拒绝建表的语句执行。 事实上， 一个没加主键的表，并不能被称之为「表」。一个没加主键的表，它的数据无序的放置在磁盘存储器上，一行一行的排列的很整齐， 跟我认知中的「表」很接近。如果给表加上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是上面说的「平衡树」结构，换句话说，就是整个表就变成了一个索引。没错， 再说一遍， 整个表变成了一个索引，也就是所谓的「聚集索引」。 这就是为什么一个表只能有一个主键， 一个表只能有一个「聚集索引」，因为主键的作用就是把「表」的数据格式转换成「索引（平衡树）」的格式放置。 上图就是带有主键的表（聚集索引）的结构图。图画的不是很好， 将就着看。其中树的所有结点（底部除外）的数据都是由主键字段中的数据构成，也就是通常我们指定主键的id字段。最下面部分是真正表中的数据。 假如我们执行一个SQL语句： select * from table where id = 1256; 首先根据索引定位到1256这个值所在的叶结点，然后再通过叶结点取到id等于1256的数据行。 这里不讲解平衡树的运行细节， 但是从上图能看出，树一共有三层， 从根节点至叶节点只需要经过三次查找就能得到结果。如下图 假如一张表有一亿条数据 ，需要查找其中某一条数据，按照常规逻辑， 一条一条的去匹配的话， 最坏的情况下需要匹配一亿次才能得到结果，用大O标记法就是O(n)最坏时间复杂度，这是无法接受的，而且这一亿条数据显然不能一次性读入内存供程序使用， 因此， 这一亿次匹配在不经缓存优化的情况下就是一亿次IO开销，以现在磁盘的IO能力和CPU的运算能力， 有可能需要几个月才能得出结果 。如果把这张表转换成平衡树结构（一棵非常茂盛和节点非常多的树），假设这棵树有10层，那么只需要10次IO开销就能查找到所需要的数据， 速度以指数级别提升，用大O标记法就是O(log n)，n是记录总树，底数是树的分叉数，结果就是树的层次数。换言之，查找次数是以树的分叉数为底，记录总数的对数，用公式来表示就是 用程序来表示就是Math.Log(100000000,10)，100000000是记录数，10是树的分叉数（真实环境下分叉数远不止10）， 结果就是查找次数，这里的结果从亿降到了个位数。因此，利用索引会使数据库查询有惊人的性能提升。 然而， 事物都是有两面的， 索引能让数据库查询数据的速度上升， 而使写入数据的速度下降，原因很简单的， 因为平衡树这个结构必须一直维持在一个正确的状态， 增删改数据都会改变平衡树各节点中的索引数据内容，破坏树结构， 因此，在每次数据改变时， DBMS必须去重新梳理树（索引）的结构以确保它的正确，这会带来不小的性能开销，也就是为什么索引会给查询以外的操作带来副作用的原因。 讲完聚集索引 ， 接下来聊一下非聚集索引， 也就是我们平时经常提起和使用的常规索引。 非聚集索引和聚集索引一样， 同样是采用平衡树作为索引的数据结构。索引树结构中各节点的值来自于表中的索引字段， 假如给user表的name字段加上索引 ， 那么索引就是由name字段中的值构成，在数据改变时， DBMS需要一直维护索引结构的正确性。如果给表中多个字段加上索引 ， 那么就会出现多个独立的索引结构，每个索引（非聚集索引）互相之间不存在关联。 如下图 每次给字段建一个新索引， 字段中的数据就会被复制一份出来， 用于生成索引。 因此， 给表添加索引，会增加表的体积， 占用磁盘存储空间。 非聚集索引和聚集索引的区别在于， 通过聚集索引可以查到需要查找的数据， 而通过非聚集索引可以查到记录对应的主键值 ， 再使用主键的值通过聚集索引查找到需要的数据，如下图 不管以任何方式查询表， 最终都会利用主键通过聚集索引来定位到数据， 聚集索引（主键）是通往真实数据所在的唯一路径。 然而， 有一种例外可以不使用聚集索引就能查询出所需要的数据， 这种非主流的方法 称之为「覆盖索引」查询， 也就是平时所说的复合索引或者多字段索引查询。 文章上面的内容已经指出， 当为字段建立索引以后， 字段中的内容会被同步到索引之中， 如果为一个索引指定两个字段， 那么这个两个字段的内容都会被同步至索引之中。 先看下面这个SQL语句 //建立索引create index index_birthday on user_info(birthday);//查询生日在1991年11月1日出生用户的用户名select user_name from user_info where birthday = ‘1991-11-1’ 这句SQL语句的执行过程如下 首先，通过非聚集索引index_birthday查找birthday等于1991-11-1的所有记录的主键ID值 然后，通过得到的主键ID值执行聚集索引查找，找到主键ID值对就的真实数据（数据行）存储的位置 最后， 从得到的真实数据中取得user_name字段的值返回， 也就是取得最终的结果 我们把birthday字段上的索引改成双字段的覆盖索引 create index index_birthday_and_user_name on user_info(birthday, user_name); 这句SQL语句的执行过程就会变为 通过非聚集索引index_birthday_and_user_name查找birthday等于1991-11-1的叶节点的内容，然而， 叶节点中除了有user_name表主键ID的值以外， user_name字段的值也在里面， 因此不需要通过主键ID值的查找数据行的真实所在， 直接取得叶节点中user_name的值返回即可。 通过这种覆盖索引直接查找的方式， 可以省略不使用覆盖索引查找的后面两个步骤， 大大的提高了查询性能，如下图 数据库索引的大致工作原理就是像文中所述， 然而细节方面可能会略有偏差，这但并不会对概念阐述的结果产生影响 。 最后， 推荐三本关系数据库方面的书籍， 文中所讲解的概念内容都是来自于此。 《SQL Server2005技术内幕之T-SQL查询》 这本书虽然是针对SQL Server写的， 但是里面的大部份内容同样适用于其它关系数据库，此书对查询编写的技巧和优化讲解的非常透彻。 《关系数据库系统概论》第四版 王珊和萨师煊写的那本， 是大学计算机教材， 讲的通俗易懂， 在国内计算机书图书出版领域质量是排的上号的。 《数据库系统概念》 这本书在数据库领域非常出名， 被称之为帆船书， 书中内容博大精深，非一朝一夕可参透的。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode_416:Partition Equal Subset Sum]]></title>
    <url>%2F2019%2F07%2F01%2Fleetcode-416%2F</url>
    <content type="text"><![CDATA[Partition Equal Subset Sum 题目链接：https://leetcode.com/problems/partition-equal-subset-sum/ 题目描述Given a non-empty array containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal. Note: Each of the array element will not exceed 100. The array size will not exceed 200. 题目难度:Medium 测试用例Example 1 Input: [1, 5, 11, 5]Output: trueExplanation: The array can be partitioned as [1, 5, 5] and [11]. Example 2 Input: [1, 2, 3, 5]Output: falseExplanation: The array cannot be partitioned into equal sum subsets. AC代码1123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123; public boolean canPartition(int[] nums) &#123; int sum = 0; for (int num : nums) &#123; sum += num; &#125; //和为奇数，直接返回false if ((sum &amp; 1) == 1) &#123; return false; &#125; Arrays.sort(nums); return dfs(nums, sum/2, 0); &#125; /** function: 判断在nums数组中，下标从 start开始到数组结束，是否存在和为 target 的子集（可以不连续） nums: 原始数组 target: 目标值，即nums数组中选择子集，使其和为target, start: 数目遍历的开始索引 return: nums数组中存在子集，使其和为target,返回true;否则返回false */ private boolean dfs(int[] nums, int target, int start) &#123; if(target == 0) return true; for(int i = start; i &lt; nums.length; i++) &#123; if(target - nums[i] &lt; 0) break; //提前结束 if(i != start &amp;&amp; nums[i] == nums[i-1]) //很关键的一步判断，防止做多余的操作 continue; if(dfs(nums, target - nums[i], i+1)) return true; &#125; return false; &#125; &#125; AC代码2该问题我们可以利用0-1背包问题的思想进行求解。 0-1背包问题请看这里：0-1背包问题 假设给定元素个数为n的数组nums，数组元素的和为sum，对应于背包问题，等价于有n个物品，每个物品的重量和价值均为nums[i]，背包的限重为sum/2，求解背包中的物品价值是否可以为sum/2？ 12345678910111213141516171819202122232425262728293031class Solution &#123; private boolean knapSack(int[] nums,int sum)&#123; int size = nums.length; //dp[i] 表示下标从0到i(包括i)的所有数中是否存在和为i的子集。 boolean[] dp = new boolean[sum + 1]; for (int i = 0;i &lt;= sum;i ++)&#123; dp[i] = i == nums[0]; &#125; for (int i = 1;i &lt; size;i++)&#123; for (int j = sum;j &gt;= nums[i];j--)&#123; dp[j] = dp[j] || dp[j-nums[i]]; &#125; &#125; return dp[sum]; &#125; public boolean canPartition(int[] nums) &#123; int sum = 0; for (int item : nums)&#123; sum += item; &#125; //如果数组元素和不是2的倍数，直接返回false if ((sum &amp; 1) == 1) &#123; return false; &#125; return knapSack(nums,sum/2); &#125;&#125;]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0-1背包问题]]></title>
    <url>%2F2019%2F07%2F01%2F0-1-Backpack-problem%2F</url>
    <content type="text"><![CDATA[本文转载自：彻底理解0-1背包问题 0-1背包问题给定n个重量为w1,w2,w3,…,wn,价值为v1,v2,v3,…,vn,的物品和容量为C的背包，求这个物品中一个最有价值的子集，使得在满足背包的容量的前提下，包内的总价值最大。 0-1背包问题指的是每个物品只能使用一次 递归方法首先我们用递归的方式来尝试解决这个问题。我们用F(n,C) 表示将前n个物品放进容量为C的背包里，得到的最大的价值。 我们用自顶向下的角度来看，假如我们已经进行到了最后一步（即求解将n个物品放到背包里获得的最大价值），此时我们便有两种选择: 不放第n个物品，此时总价值为F(n−1,C) 放置第n个物品，此时总价值为vn+F(n−1,C−wn) 两种选择中总价值最大的方案就是我们的最终方案，递推式（有时也称之为状态转移方程）如下: F(i,C)=max(F(i−1,C),v(i)+F(i−1,C−w(i))) (前提是 w[i] &lt;= C) 代码如下： 1234567891011121314151617181920212223242526272829303132333435public class KnapSack01 &#123; /** * 解决背包问题的递归函数 * * @param w 物品的重量数组 * @param v 物品的价值数组 * @param index 当前待选择的物品索引 * @param capacity 当前背包有效容量 * @return 最大价值 */ private static int solveKS(int[] w, int[] v, int index, int capacity) &#123; //基准条件：如果索引无效或者容量不足，直接返回当前价值0 if (index &lt; 0 || capacity &lt;= 0) return 0; //不放第index个物品所得价值 int res = solveKS(w, v, index - 1, capacity); //放第index个物品所得价值（前提是：第index个物品可以放得下） if (w[index] &lt;= capacity) &#123; res = Math.max(res, v[index] + solveKS(w, v, index - 1, capacity - w[index])); &#125; return res; &#125; public static int knapSack(int[] w, int[] v, int C) &#123; int size = w.length; return solveKS(w, v, size - 1, C); &#125; public static void main(String[] args)&#123; int[] w = &#123;2,1,3,2&#125;; int[] v = &#123;12,10,20,15&#125;; System.out.println(knapSack(w,v,5)); &#125;&#125; 记忆化搜索我们用递归方法可以很简单的实现以上代码，但是有个严重的问题就是，直接采用自顶向下的递归算法会导致要不止一次的解决公共子问题，因此效率是相当低下的。我们可以将已经求得的子问题的结果保存下来，这样对子问题只会求解一次，这便是记忆化搜索。下面在上述代码的基础上加上记忆化搜索 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class KnapSack01 &#123; private static int[][] memo; /** * 解决背包问题的递归函数 * * @param w 物品的重量数组 * @param v 物品的价值数组 * @param index 当前待选择的物品索引 * @param capacity 当前背包有效容量 * @return 最大价值 */ private static int solveKS(int[] w, int[] v, int index, int capacity) &#123; //基准条件：如果索引无效或者容量不足，直接返回当前价值0 if (index &lt; 0 || capacity &lt;= 0) return 0; //如果此子问题已经求解过，则直接返回上次求解的结果 if (memo[index][capacity] != 0) &#123; return memo[index][capacity]; &#125; //不放第index个物品所得价值 int res = solveKS(w, v, index - 1, capacity); //放第index个物品所得价值（前提是：第index个物品可以放得下） if (w[index] &lt;= capacity) &#123; res = Math.max(res, v[index] + solveKS(w, v, index - 1, capacity - w[index])); &#125; //添加子问题的解，便于下次直接使用 memo[index][capacity] = res; return res; &#125; public static int knapSack(int[] w, int[] v, int C) &#123; int size = w.length; memo = new int[size][C + 1]; return solveKS(w, v, size - 1, C); &#125; public static void main(String[] args) &#123; int[] w = &#123;2, 1, 3, 2&#125;; int[] v = &#123;12, 10, 20, 15&#125;; System.out.println(knapSack(w, v, 5)); &#125;&#125; 动态规划算法 12345678910111213141516171819202122232425262728293031public class KnapSack01 &#123; public static int knapSack(int[] w, int[] v, int C) &#123; int size = w.length; if (size == 0) &#123; return 0; &#125; int[][] dp = new int[size][C + 1]; //初始化第一行 //仅考虑容量为i的背包放第0个物品的情况 for (int i = 0; i &lt;= C; i++) &#123; dp[0][i] = w[0] &lt;= i ? v[0] : 0; &#125; //填充其他行和列 for (int i = 1; i &lt; size; i++) &#123; for (int j = 0; j &lt;= C; j++) &#123; dp[i][j] = dp[i - 1][j]; if (w[i] &lt;= j) &#123; dp[i][j] = Math.max(dp[i][j], v[i] + dp[i - 1][j - w[i]]); &#125; &#125; &#125; return dp[size - 1][C]; &#125; public static void main(String[] args) &#123; int[] w = &#123;2, 1, 3, 2&#125;; int[] v = &#123;12, 10, 20, 15&#125;; System.out.println(knapSack(w, v, 5)); &#125;&#125; 空间复杂度的极致优化上面的动态规划算法使用了O(n*C)的空间复杂度（因为我们使用了二维数组来记录子问题的解），其实我们完全可以只使用一维数组来存放结果，但同时我们需要注意的是，为了防止计算结果被覆盖，我们必须从后向前分别进行计算。 我们仍然假设背包空间为5，根据 F(i,C)=max(F(i−1,C),v(i)+F(i−1,C−w(i))) 我们可以知道，当我们利用一维数组进行记忆化的时候，我们只需要使用到当前位置的值和该位置之前的值，举个例子假设我们要计算F(i,4),我们需要用到的值为F(i−1,4) ,和F(i−1,4−w(i)),因此为了防止结果被覆盖，我们需要从后向前依次计算结果最终的动态规划代码如下: 12345678910111213141516171819202122232425262728public class KnapSack01 &#123; public static int knapSack(int[] w, int[] v, int C) &#123; int size = w.length; if (size == 0) &#123; return 0; &#125; int[] dp = new int[C + 1]; //初始化第一行 //仅考虑容量为C的背包放第0个物品的情况 for (int i = 0; i &lt;= C; i++) &#123; dp[i] = w[0] &lt;= i ? v[0] : 0; &#125; for (int i = 1; i &lt; size; i++) &#123; for (int j = C; j &gt;= w[i]; j--) &#123; dp[j] = Math.max(dp[j], v[i] + dp[j - w[i]]); &#125; &#125; return dp[C]; &#125; public static void main(String[] args) &#123; int[] w = &#123;2, 1, 3, 2&#125;; int[] v = &#123;12, 10, 20, 15&#125;; System.out.println(knapSack(w, v, 5)); &#125;&#125; 利用背包问题的思想解决问题leetcode 416 Partition Equal Subset Sum 给定一个仅包含正整数的非空数组，确定该数组是否可以分成两部分，要求两部分的和相等 问题分析该问题我们可以利用背包问题的思想进行求解。 假设给定元素个数为n的数组arr，数组元素的和为sum，对应于背包问题，等价于有n个物品，每个物品的重量和价值均为为arr[i]，背包的限重为sum/2，求解背包中的物品最大价值为多少？ 123456789101112131415161718192021222324252627282930class Solution &#123; private boolean knapSack(int[] nums,int sum)&#123; int size = nums.length; boolean[] dp = new boolean[sum + 1]; for (int i = 0;i &lt;= sum;i ++)&#123; dp[i] = i == nums[0]; &#125; for (int i = 1;i &lt; size;i++)&#123; for (int j = sum;j &gt;= nums[i];j--)&#123; dp[j] = dp[j] || dp[j-nums[i]]; &#125; &#125; return dp[sum]; &#125; public boolean canPartition(int[] nums) &#123; int sum = 0; for (int item : nums)&#123; sum += item; &#125; //如果数组元素和不是2的倍数，直接返回false if (sum % 2 != 0) return false; return knapSack(nums,sum/2); &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2019%2F07%2F01%2FLinux-common-command%2F</url>
    <content type="text"><![CDATA[本文转载自：Linux最常用150个命令汇总 线上查询及帮助命令(2个) man 查看命令帮助，命令的词典，更复杂的还有info，但不常用。 help 查看Linux内置命令的帮助，比如cd命令。 文件和目录操作命令(18个) ls 全拼list，功能是列出目录的内容及其内容属性信息。 cd 全拼change directory，功能是从当前工作目录切换到指定的工作目录。 cp 全拼copy，其功能为复制文件或目录。 find 查找的意思，用于查找目录及目录下的文件。 mkdir 全拼make directories，其功能是创建目录。 mv 全拼move，其功能是移动或重命名文件。 pwd 全拼print working directory，其功能是显示当前工作目录的绝对路径。 rename 用于重命名文件。 rm 全拼remove，其功能是删除一个或多个文件或目录。 rmdir 全拼remove empty directories，功能是删除空目录。 touch 创建新的空文件，改变已有文件的时间戳属性。 tree 功能是以树形结构显示目录下的内容。 basename 显示文件名或目录名。 dirname 显示文件或目录路径。 chattr 改变文件的扩展属性。 lsattr 查看文件扩展属性。 file 显示文件的类型。 md5sum 计算和校验文件的MD5值。 查看文件及内容处理命令（21个） cat 全拼concatenate，功能是用于连接多个文件并且打印到屏幕输出或重定向到指定文件中。 tac tac是cat的反向拼写，因此命令的功能为反向显示文件内容。 more 分页显示文件内容。 less 分页显示文件内容，more命令的相反用法。 head 显示文件内容的头部。 tail 显示文件内容的尾部。 cut 将文件的每一行按指定分隔符分割并输出。 split 分割文件为不同的小片段。 paste 按行合并文件内容。 sort 对文件的文本内容排序。 uniq 去除重复行。oldboy wc 统计文件的行数、单词数或字节数。 iconv 转换文件的编码格式。 dos2unix 将DOS格式文件转换成UNIX格式。 diff 全拼difference，比较文件的差异，常用于文本文件。 vimdiff 命令行可视化文件比较工具，常用于文本文件。 rev 反向输出文件内容。 grep/egrep 过滤字符串，三剑客老三。 join 按两个文件的相同字段合并。 tr 替换或删除字符。 vi/vim 命令行文本编辑器。 文件压缩及解压缩命令（4个） tar 打包压缩。oldboy unzip 解压文件。 gzip gzip压缩工具。 zip 压缩工具。 信息显示命令（11个） uname 显示操作系统相关信息的命令。 hostname 显示或者设置当前系统的主机名。 dmesg 显示开机信息，用于诊断系统故障。 uptime 显示系统运行时间及负载。 stat 显示文件或文件系统的状态。 du 计算磁盘空间使用情况。 df 报告文件系统磁盘空间的使用情况。 top 实时显示系统资源使用情况。 free 查看系统内存。 date 显示与设置系统时间。 cal 查看日历等时间信息。 搜索文件命令（4个） which 查找二进制命令，按环境变量PATH路径查找。 find 从磁盘遍历查找文件或目录。 whereis 查找二进制命令，按环境变量PATH路径查找。 locate 从数据库 (/var/lib/mlocate/mlocate.db) 查找命令，使用updatedb更新库。 用户管理命令（10个） useradd 添加用户。 usermod 修改系统已经存在的用户属性。 userdel 删除用户。 groupadd 添加用户组。 passwd 修改用户密码。 chage 修改用户密码有效期限。 id 查看用户的uid,gid及归属的用户组。 su 切换用户身份。 visudo 编辑/etc/sudoers文件的专属命令。 sudo 以另外一个用户身份（默认root用户）执行事先在sudoers文件允许的命令。 基础网络操作命令（11个） telnet 使用TELNET协议远程登录。 ssh 使用SSH加密协议远程登录。 scp 全拼secure copy，用于不同主机之间复制文件。 wget 命令行下载文件。 ping 测试主机之间网络的连通性。 route 显示和设置linux系统的路由表。 ifconfig 查看、配置、启用或禁用网络接口的命令。 ifup 启动网卡。 ifdown 关闭网卡。 netstat 查看网络状态。 ss 查看网络状态。 深入网络操作命令（9个） nmap 网络扫描命令。 lsof 全名list open files，也就是列举系统中已经被打开的文件。 mail 发送和接收邮件。 mutt 邮件管理命令。 nslookup 交互式查询互联网DNS服务器的命令。 dig 查找DNS解析过程。 host 查询DNS的命令。 traceroute 追踪数据传输路由状况。 tcpdump 命令行的抓包工具。 有关磁盘与文件系统的命令（16个） mount 挂载文件系统。 umount 卸载文件系统。 fsck 检查并修复Linux文件系统。 dd 转换或复制文件。 dumpe2fs 导出ext2/ext3/ext4文件系统信息。 dump ext2/3/4文件系统备份工具。 fdisk 磁盘分区命令，适用于2TB以下磁盘分区。 parted 磁盘分区命令，没有磁盘大小限制，常用于2TB以下磁盘分区。 mkfs 格式化创建Linux文件系统。 partprobe 更新内核的硬盘分区表信息。 e2fsck 检查ext2/ext3/ext4类型文件系统。 mkswap 创建Linux交换分区。 swapon 启用交换分区。 swapoff 关闭交换分区。 sync 将内存缓冲区内的数据写入磁盘。 resize2fs 调整ext2/ext3/ext4文件系统大小。 系统权限及用户授权相关命令（4个） chmod 改变文件或目录权限。 chown 改变文件或目录的属主和属组。 chgrp 更改文件用户组。 umask 显示或设置权限掩码。 查看系统用户登陆信息的命令（7个） whoami 显示当前有效的用户名称，相当于执行id -un命令。 who 显示目前登录系统的用户信息。 w 显示已经登陆系统的用户列表，并显示用户正在执行的指令。 last 显示登入系统的用户。 lastlog 显示系统中所有用户最近一次登录信息。 users 显示当前登录系统的所有用户的用户列表。 finger 查找并显示用户信息。 内置命令及其它（19个） echo 打印变量，或直接输出指定的字符串 printf 将结果格式化输出到标准输出。 rpm 管理rpm包的命令。 yum 自动化简单化地管理rpm包的命令。 watch 周期性的执行给定的命令，并将命令的输出以全屏方式显示。 alias 设置系统别名。 unalias 取消系统别名。 date 查看或设置系统时间。 lear 清除屏幕，简称清屏。 history 查看命令执行的历史纪录。 eject 弹出光驱。 time 计算命令执行时间。 nc 功能强大的网络工具。 xargs 将标准输入转换成命令行参数。 exec 调用并执行指令的命令。 export 设置或者显示环境变量。 unset 删除变量或函数。 type 用于判断另外一个命令是否是内置命令。 bc 命令行科学计算器 系统管理与性能监视命令(9个) chkconfig 管理Linux系统开机启动项。 vmstat 虚拟内存统计。 mpstat 显示各个可用CPU的状态统计。 iostat 统计系统IO。 sar 全面地获取系统的CPU、运行队列、磁盘 I/O、分页（交换区）、内存、 CPU中断和网络等性能数据。 ipcs 用于报告Linux中进程间通信设施的状态，显示的信息包括消息列表、共享内存和信号量的信息。 ipcrm 用来删除一个或更多的消息队列、信号量集或者共享内存标识。 strace 用于诊断、调试Linux用户空间跟踪器。我们用它来监控用户空间进程和内核的交互，比如系统调用、信号传递、进程状态变更等。 ltrace 命令会跟踪进程的库函数调用,它会显现出哪个库函数被调用。 关机/重启/注销和查看系统信息的命令（6个） shutdown 关机。 halt 关机。 poweroff 关闭电源。 logout 退出当前登录的Shell。 exit 退出当前登录的Shell。 Ctrl+d 退出当前登录的Shell的快捷键。 进程管理相关命令（15个） bg 将一个在后台暂停的命令，变成继续执行 （在后台执行）。 fg 将后台中的命令调至前台继续运行。 jobs 查看当前有多少在后台运行的命令。 kill 终止进程。 killall 通过进程名终止进程。 pkill 通过进程名终止进程。 crontab 定时任务命令。 ps 显示进程的快照。 pstree 树形显示进程。 nice/renice 调整程序运行的优先级。 nohup 忽略挂起信号运行指定的命令。 pgrep 查找匹配条件的进程。 runlevel 查看系统当前运行级别。 init 切换运行级别。 service 启动、停止、重新启动和关闭系统服务，还可以显示所有系统服务的当前状态。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中兴2020届秋招提前批在线笔试题:排列员工工资顺序]]></title>
    <url>%2F2019%2F07%2F01%2Fzhongxin-algorithm%2F</url>
    <content type="text"><![CDATA[本文转载自：题目：排列员工工资顺序（Java和C++）（中兴在线笔试题） 题目描述某公司中有N名员工。给定所有员工工资的清单，财务人员要按照特定的顺序排列员工的工资。他按照工资的频次降序排列。即给定清单中所有频次较高的工资将在频次较低的工资之前出现。如果相同数量的员工都有相同的工资，则将按照给定清单中该工资第一次出现的顺序排列。 写一个算法来帮助财务人员排列员工工资的顺序。 输入该函数/方法的输入包括两个参数—— num，一个整数，表示员工的人数 salaries，一个正整数列表，表示N名员工的工资 输出返回一个正整数列表，该列表按照员工工资的频次排序。 约束条件： 1&lt;num&lt;100000 1&lt;salaries&lt;1000000000 0&lt;i&lt;num 示例 输入： num=19 salaries=[10000,20000,40000,30000,30000,30000,40000,20000,50000, 50000,50000,50000,60000,60000,60000,70000,80000,90000,100000] 输出： [50000,50000,50000,50000,30000,30000,30000,60000,60000, 60000,20000,20000,40000,40000,10000,70000,80000,90000,100000] 分析本题要求把根据数据出现的频数从高到低排序，所以基本的思路是算出每个数的频次，然后直接从高到低排序。 可是这样会出现一个问题，频次相同的数据没法排序。比如 20 40 20 40 50 这组数据分别得到频次 2 2 2 2 1，本身就是一个降序排序，所以最后结果还是20 40 20 40 50。 那怎么办呢？我们可以上面那种数据中间那个20为例，即20 40 20 40 50。首先我们来判断20的前半部分是否存在20，如果存在20的话则插入到该20的后面即可，即变为20 20 40 40 50。 所有排序过程均使用插入排序。插入排序的详细原理读者可以自行百度，插入排序从后往前移动，移动过程中不会改变原有已经排列好的数字的顺序。 思路 计算每个数的频数。 根据频数的大小，对数据从高到低排序。排序的方法使用插入排序。 对第2部排好序的结果再排序。这次从前向后遍历，判断该数的前半部分是否存在该数，存在的话就插入到最近和它相同数的后面。 代码实现Java实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class salaryArrangement &#123; public static void main(String[] args) &#123; int num = 19; int slalay[] = &#123; 10000, 20000, 40000, 30000, 30000, 30000, 40000, 20000, 50000, 50000, 50000, 50000, 60000, 60000, 60000, 70000, 80000, 90000, 100000 &#125;;// 测试数据 slalay = b(num, slalay); // 保存结果 int i; System.out.println("输出结果为："); for (i = 0; i &lt; num; i++)// 输出结果 &#123; System.out.print(" "); System.out.print(slalay[i]); &#125; &#125; public static int[] b(int num, int slalay[]) &#123; int i, j, k; int temp_frequency; int temp_slalay; int frequency[] = new int[num]; // 频次 for (i = 0; i &lt; num; i++) // 给频次赋初值 &#123; frequency[i] = 0; &#125; for (i = 0; i &lt; num; i++) // 对已排序的数据统计频次 &#123; for (j = 0; j &lt; num; j++) &#123; if (slalay[i] == slalay[j]) &#123; frequency[i]++; &#125; &#125; &#125; for (i = 1; i &lt; num; i++) // 对频次进行插入排序，同时根据频数交换的顺序排列原数据 &#123; temp_frequency = frequency[i]; temp_slalay = slalay[i]; j = i - 1; while (j &gt;= 0 &amp;&amp; temp_frequency &gt; frequency[j]) //频次递减排序 &#123; frequency[j + 1] = frequency[j]; slalay[j + 1] = slalay[j]; // 变换原数据 j--; &#125; frequency[j + 1] = temp_frequency; slalay[j + 1] = temp_slalay; &#125; for (i = 1; i &lt; num; i++) // 再进行一次插入排序 &#123; temp_slalay = slalay[i]; j = i - 1; k = i - 1; while (k &gt;= 0) &#123; if (temp_slalay == slalay[k]) // 判断前半部分子序列是否存在当前当前数据 &#123; while (j &gt;= 0 &amp;&amp; temp_slalay != slalay[j]) // 插入到相同的数的后面 &#123; slalay[j + 1] = slalay[j]; j--; &#125; slalay[j + 1] = temp_slalay; break; // 退出循环判断下个数 &#125; k--; &#125; &#125; return slalay; &#125;&#125; C++实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include &lt;stdio.h&gt; int* b(int num, int* slalay) &#123; int i, j, k; int temp_frequency; int temp_slalay; int frequency[num]; // 频次 for (i = 0; i &lt; num; i++) // 给频次赋初值 &#123; frequency[i] = 0; &#125; for (i = 0; i &lt; num; i++) // 对已排序的数据统计频次 &#123; for (j = 0; j &lt; num; j++) &#123; if (slalay[i] == slalay[j]) &#123; frequency[i]++; &#125; &#125; &#125; for (i = 1; i &lt; num; i++) // 对频次进行插入排序，同时根据频数交换的顺序排列原数据 &#123; temp_frequency = frequency[i]; temp_slalay = slalay[i]; j = i - 1; while (j &gt;= 0 &amp;&amp; temp_frequency &gt; frequency[j]) //频次递减排序 &#123; frequency[j + 1] = frequency[j]; slalay[j + 1] = slalay[j]; // 变换原数据 j--; &#125; frequency[j + 1] = temp_frequency; slalay[j + 1] = temp_slalay; &#125; for (i = 1; i &lt; num; i++) // 再进行一次插入排序 &#123; temp_slalay = slalay[i]; j = i - 1; k = i - 1; while (k &gt;= 0) &#123; if (temp_slalay == slalay[k]) // 判断前半部分子序列是否存在当前当前数据 &#123; while (j &gt;= 0 &amp;&amp; temp_slalay != slalay[j]) // 插入到相同的数的后面 &#123; slalay[j + 1] = slalay[j]; j--; &#125; slalay[j + 1] = temp_slalay; break; // 退出循环判断下个数 &#125; k--; &#125; &#125; return slalay;&#125; int main()&#123; int num = 19; int slalay[19] = &#123; 10000, 20000, 40000, 30000, 30000, 30000, 40000, 20000, 50000, 50000, 50000, 50000, 60000,60000, 60000, 70000, 80000, 90000, 100000 &#125;;// 测试数据 int *slalayResult; slalayResult = b(num, slalay); // 保存结果 int i; printf("输出结果为："); for (i = 0; i &lt; num; i++)// 输出结果 &#123; printf("%d ",slalayResult[i]); &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识azkaban]]></title>
    <url>%2F2019%2F07%2F01%2Fazkaban-introduce%2F</url>
    <content type="text"><![CDATA[本文转载自：Azkaban 简介 本文简单介绍一下Azkaban及其特点。azkaban是一个开源的任务调度系统，用于负责任务的调度运行（如数据仓库调度），用以替代linux中的crontab。 Azkaban是什么？Azkaban是什么? Azkaban是一套简单的任务调度服务，整体包括三部分webserver、dbserver、executorserver。 Azkaban是linkin的开源项目，开发语言为Java。 Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。 Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。 Azkaban典型使用场景 实际当中经常有这些场景：每天有一个大任务，这个大任务可以分成A,B,C,D四个小任务，A,B任务之间没有依赖关系，C任务依赖A,B任务的结果，D任务依赖C任务的结果。一般的做法是，开两个终端同时执行A,B，两个都执行完了再执行C，最后再执行D。这样的话，整个的执行过程都需要人工参加，并且得盯着各任务的进度。但是我们的很多任务都是在深更半夜执行的，通过写脚本设置crontab执行。其实，整个过程类似于一个有向无环图（DAG）。每个子任务相当于大任务中的一个流，任务的起点可以从没有度的节点开始执行，任何没有通路的节点之间可以同时执行，比如上述的A,B。总结起来的话，我们需要的就是一个工作流的调度器，而azkaban就是能解决上述问题的一个调度器。 Azkaban官网 https://azkaban.github.io/ Azkaban的功能特点它具有如下功能特点： 1、Web用户界面 2、方便上传工作流 3、方便设置任务之间的关系 4、工作流调度 5、认证/授权 6、能够杀死并重启工作流 7、模块化和可插拔的插件机制 8、项目工作区 9、工作流和任务的日志记录和审计 Azkaban的架构Azkaban是一种类似于Oozie的工作流控制引擎，可以用来解决多个Hadoop（或Spark等）离线计算任务之间的依赖关系问题。也可以用其代替crontab来对周期性任务进行调度，并且更为直观，可靠，同时提供了美观的可视化管理界面。Azkaban由三部分构成： Relational Database(Mysql)azkaban将大多数状态信息都存于MySQL中,Azkaban Web Server 和 Azkaban Executor Server也需要访问DB。 Azkaban Web Server提供了Web UI，是azkaban的主要管理者，包括 project 的管理，认证，调度，对工作流执行过程的监控等。 Azkaban Executor Server调度工作流和任务，记录工作流活任务的日志，之所以将AzkabanWebServer和AzkabanExecutorServer分开，主要是因为在某个任务流失败后，可以更方便的将重新执行。而且也更有利于Azkaban系统的升级 MySQL实例：Azkaban使用MySQL来存储项目和执行。 Azkaban Web服务器：Azkaban使用Jetty作为Web服务器，用作控制器以及提供Web界面 Azkaban执行服务器：Azkaban执行服务器执行提交工作流。 参考资料： http://www.cnblogs.com/zlslch/p/6294321.html http://www.cnblogs.com/zlslch/p/6296079.html http://www.cnblogs.com/zlslch/p/6296214.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java之线程池]]></title>
    <url>%2F2019%2F06%2F29%2Fjava-ThreadPool%2F</url>
    <content type="text"><![CDATA[本文转载自：Java并发编程：线程池的使用 在前面的文章中，我们使用线程的时候就去创建一个线程，这样实现起来非常简便，但是就会有一个问题： 如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，这样频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间。 那么有没有一种办法使得线程可以复用，就是执行完一个任务，并不被销毁，而是可以继续执行其他的任务？ 在Java中可以通过线程池来达到这样的效果。今天我们就来详细讲解一下Java的线程池，首先我们从最核心的ThreadPoolExecutor类中的方法讲起，然后再讲述它的实现原理，接着给出了它的使用示例，最后讨论了一下如何合理配置线程池的大小。 Java中的ThreadPoolExecutor类java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类，因此如果要透彻地了解Java中的线程池，必须先了解这个类。下面我们来看一下ThreadPoolExecutor类的具体实现源码。 在ThreadPoolExecutor类中提供了四个构造方法： 123456789101112131415public class ThreadPoolExecutor extends AbstractExecutorService &#123; ..... public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,RejectedExecutionHandler handler); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory,RejectedExecutionHandler handler); ...&#125; 从上面的代码可以得知，ThreadPoolExecutor继承了AbstractExecutorService类，并提供了四个构造器，事实上，通过观察每个构造器的源码具体实现，发现前面三个构造器都是调用的第四个构造器进行的初始化工作。 下面解释下一下构造器中各个参数的含义： corePoolSize：核心池的大小，这个参数跟后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数，这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性： 1234567TimeUnit.DAYS; //天TimeUnit.HOURS; //小时TimeUnit.MINUTES; //分钟TimeUnit.SECONDS; //秒TimeUnit.MILLISECONDS; //毫秒TimeUnit.MICROSECONDS; //微妙TimeUnit.NANOSECONDS; //纳秒 workQueue：一个阻塞队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列有以下几种选择： 123ArrayBlockingQueue;LinkedBlockingQueue;SynchronousQueue; ArrayBlockingQueue和PriorityBlockingQueue使用较少，一般使用LinkedBlockingQueue和Synchronous。线程池的排队策略与BlockingQueue有关。 threadFactory：线程工厂，主要用来创建线程； handler：表示当拒绝处理任务时的策略，有以下四种取值： 1234ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 具体参数的配置与线程池的关系将在下一节讲述。 从上面给出的hreadPoolExecutor类的代码可以知道，ThreadPoolExecutor继承了AbstractExecutorService，我们来看一下AbstractExecutorService的实现： 123456789101112131415161718192021222324252627public abstract class AbstractExecutorService implements ExecutorService &#123; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; &#125;; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; &#125;; public Future&lt;?&gt; submit(Runnable task) &#123;&#125;; public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; &#125;; public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; &#125;; private &lt;T&gt; T doInvokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException &#123; &#125;; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException &#123; &#125;; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; &#125;; public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException &#123; &#125;; public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException &#123; &#125;;&#125; AbstractExecutorService是一个抽象类，它实现了ExecutorService接口。 我们接着看ExecutorService接口的实现： 12345678910111213141516171819202122public interface ExecutorService extends Executor &#123; void shutdown(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future&lt;?&gt; submit(Runnable task); &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 而ExecutorService又是继承了Executor接口，我们看一下Executor接口的实现： 123public interface Executor &#123; void execute(Runnable command);&#125; 到这里，大家应该明白了ThreadPoolExecutor、AbstractExecutorService、ExecutorService和Executor几个之间的关系了。 Executor是一个顶层接口，在它里面只声明了一个方法execute(Runnable)，返回值为void，参数为Runnable类型，从字面意思可以理解，就是用来执行传进去的任务的； 然后ExecutorService接口继承了Executor接口，并声明了一些方法：submit、invokeAll、invokeAny以及shutDown等； 抽象类AbstractExecutorService实现了ExecutorService接口，基本实现了ExecutorService中声明的所有方法； 然后ThreadPoolExecutor继承了类AbstractExecutorService。 在ThreadPoolExecutor类中有几个非常重要的方法： 1234execute()submit()shutdown()shutdownNow() execute()方法实际上是Executor中声明的方法，在ThreadPoolExecutor进行了具体的实现，这个方法是ThreadPoolExecutor的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 submit()方法是在ExecutorService中声明的方法，在AbstractExecutorService就已经有了具体的实现，在ThreadPoolExecutor中并没有对其进行重写，这个方法也是用来向线程池提交任务的，但是它和execute()方法不同，它能够返回任务执行的结果，去看submit()方法的实现，会发现它实际上还是调用的execute()方法，只不过它利用了Future来获取任务执行结果（Future相关内容将在下一篇讲述）。 shutdown()和shutdownNow()是用来关闭线程池的。 还有很多其他的方法： 比如：getQueue() 、getPoolSize() 、getActiveCount()、getCompletedTaskCount()等获取与线程池相关属性的方法，有兴趣的朋友可以自行查阅API。 深入剖析线程池实现原理 在上一节我们从宏观上介绍了ThreadPoolExecutor，下面我们来深入解析一下线程池的具体实现原理，将从下面几个方面讲解： 1.线程池状态 2.任务的执行 3.线程池中的线程初始化 4.任务缓存队列及排队策略 5.任务拒绝策略 6.线程池的关闭 7.线程池容量的动态调整 线程池状态 在ThreadPoolExecutor中定义了一个volatile变量，另外定义了几个static final变量表示线程池的各个状态： 12345volatile int runState;static final int RUNNING = 0;static final int SHUTDOWN = 1;static final int STOP = 2;static final int TERMINATED = 3; runState表示当前线程池的状态，它是一个volatile变量用来保证线程之间的可见性； 下面的几个static final变量表示runState可能的几个取值。 当创建线程池后，初始时，线程池处于RUNNING状态； 如果调用了shutdown()方法，则线程池处于SHUTDOWN状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕； 如果调用了shutdownNow()方法，则线程池处于STOP状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务； 当线程池处于SHUTDOWN或STOP状态，并且所有工作线程已经销毁，任务缓存队列已经清空或执行结束后，线程池被设置为TERMINATED状态。 任务的执行 在了解将任务提交给线程池到任务执行完毕整个过程之前，我们先来看一下ThreadPoolExecutor类中其他的一些比较重要成员变量： 12345678910111213141516171819private final BlockingQueue&lt;Runnable&gt; workQueue; //任务缓存队列，用来存放等待执行的任务private final ReentrantLock mainLock = new ReentrantLock(); //线程池的主要状态锁，对线程池状态（比如线程池大小 //、runState等）的改变都要使用这个锁private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); //用来存放工作集 private volatile long keepAliveTime; //线程存货时间 private volatile boolean allowCoreThreadTimeOut; //是否允许为核心线程设置存活时间private volatile int corePoolSize; //核心池的大小（即线程池中的线程数目大于这个参数时，提交的任务会被放进任务缓存队列）private volatile int maximumPoolSize; //线程池最大能容忍的线程数 private volatile int poolSize; //线程池中当前的线程数 private volatile RejectedExecutionHandler handler; //任务拒绝策略 private volatile ThreadFactory threadFactory; //线程工厂，用来创建线程 private int largestPoolSize; //用来记录线程池中曾经出现过的最大线程数 private long completedTaskCount; //用来记录已经执行完毕的任务个数 每个变量的作用都已经标明出来了，这里要重点解释一下corePoolSize、maximumPoolSize、largestPoolSize三个变量。 ​ corePoolSize在很多地方被翻译成核心池大小，其实我的理解这个就是线程池的大小。举个简单的例子： 假如有一个工厂，工厂里面有10个工人，每个工人同时只能做一件任务。 因此只要当10个工人中有工人是空闲的，来了任务就分配给空闲的工人做； 当10个工人都有任务在做时，如果还来了任务，就把任务进行排队等待； 如果说新任务数目增长的速度远远大于工人做任务的速度，那么此时工厂主管可能会想补救措施，比如重新招 4个临时工人进来； 然后就将任务也分配给这4个临时工人做； 如果说着14个工人做任务的速度还是不够，此时工厂主管可能就要考虑不再接收新的任务或者抛弃前面的一些任务了。 当这14个工人当中有人空闲时，而新任务增长的速度又比较缓慢，工厂主管可能就考虑辞掉4个临时工了，只保持原来的10个工人，毕竟请额外的工人是要花钱的。 这个例子中的corePoolSize就是10，而maximumPoolSize就是14（10+4）。 也就是说corePoolSize就是线程池大小，maximumPoolSize在我看来是线程池的一种补救措施，即任务量突然过大时的一种补救措施。 不过为了方便理解，在本文后面还是将corePoolSize翻译成核心池大小。 ​ largestPoolSize只是一个用来起记录作用的变量，用来记录线程池中曾经有过的最大线程数目，跟线程池的容量没有任何关系。 下面我们进入正题，看一下任务从提交到最终执行完毕经历了哪些过程。 在ThreadPoolExecutor类中，最核心的任务提交方法是execute()方法，虽然通过submit也可以提交任务，但是实际上submit方法里面最终调用的还是execute()方法，所以我们只需要研究execute()方法的实现原理即可： 123456789101112public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) &#123; if (runState == RUNNING &amp;&amp; workQueue.offer(command)) &#123; if (runState != RUNNING || poolSize == 0) ensureQueuedTaskHandled(command); &#125; else if (!addIfUnderMaximumPoolSize(command)) reject(command); // is shutdown or saturated &#125;&#125; 上面的代码可能看起来不是那么容易理解，下面我们一句一句解释： 首先，判断提交的任务command是否为null，若是null，则抛出空指针异常； 接着是这句，这句要好好理解一下： 1if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) 由于是或条件运算符，所以先计算前半部分的值，如果线程池中当前线程数不小于核心池大小，那么就会直接进入下面的if语句块了。 如果线程池中当前线程数小于核心池大小，则接着执行后半部分，也就是执行 1addIfUnderCorePoolSize(command) 如果执行完addIfUnderCorePoolSize这个方法返回false，则继续执行下面的if语句块，否则整个方法就直接执行完毕了。 如果执行完addIfUnderCorePoolSize这个方法返回false，然后接着判断： 1if (runState == RUNNING &amp;&amp; workQueue.offer(command)) 如果当前线程池处于RUNNING状态，则将任务放入任务缓存队列；如果当前线程池不处于RUNNING状态或者任务放入缓存队列失败，则执行： 1addIfUnderMaximumPoolSize(command) 如果执行addIfUnderMaximumPoolSize方法失败，则执行reject()方法进行任务拒绝处理。 回到前面： 1if (runState == RUNNING &amp;&amp; workQueue.offer(command)) 这句的执行，如果说当前线程池处于RUNNING状态且将任务放入任务缓存队列成功，则继续进行判断： 1if (runState != RUNNING || poolSize == 0) 这句判断是为了防止在将此任务添加进任务缓存队列的同时其他线程突然调用shutdown或者shutdownNow方法关闭了线程池的一种应急措施。如果是这样就执行： 1ensureQueuedTaskHandled(command) 进行应急处理，从名字可以看出是保证添加到任务缓存队列中的任务得到处理。 我们接着看2个关键方法的实现：addIfUnderCorePoolSize和addIfUnderMaximumPoolSize： 123456789101112131415private boolean addIfUnderCorePoolSize(Runnable firstTask) &#123; Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (poolSize &lt; corePoolSize &amp;&amp; runState == RUNNING) t = addThread(firstTask); //创建线程去执行firstTask任务 &#125; finally &#123; mainLock.unlock(); &#125; if (t == null) return false; t.start(); return true;&#125; 这个是addIfUnderCorePoolSize方法的具体实现，从名字可以看出它的意图就是当低于核心池大小时执行的方法。下面看其具体实现，首先获取到锁，因为这地方涉及到线程池状态的变化，先通过if语句判断当前线程池中的线程数目是否小于核心池大小，有朋友也许会有疑问：前面在execute()方法中不是已经判断过了吗，只有线程池当前线程数目小于核心池大小才会执行addIfUnderCorePoolSize方法的，为何这地方还要继续判断？原因很简单，前面的判断过程中并没有加锁，因此可能在execute方法判断的时候poolSize小于corePoolSize，而判断完之后，在其他线程中又向线程池提交了任务，就可能导致poolSize不小于corePoolSize了，所以需要在这个地方继续判断。然后接着判断线程池的状态是否为RUNNING，原因也很简单，因为有可能在其他线程中调用了shutdown或者shutdownNow方法。然后就是执行 1t = addThread(firstTask); 这个方法也非常关键，传进去的参数为提交的任务，返回值为Thread类型。然后接着在下面判断t是否为空，为空则表明创建线程失败（即poolSize&gt;=corePoolSize或者runState不等于RUNNING），否则调用t.start()方法启动线程。 我们来看一下addThread方法的实现： 123456789101112private Thread addThread(Runnable firstTask) &#123; Worker w = new Worker(firstTask); Thread t = threadFactory.newThread(w); //创建一个线程，执行任务 if (t != null) &#123; w.thread = t; //将创建的线程的引用赋值为w的成员变量 workers.add(w); int nt = ++poolSize; //当前线程数加1 if (nt &gt; largestPoolSize) largestPoolSize = nt; &#125; return t;&#125; 在addThread方法中，首先用提交的任务创建了一个Worker对象，然后调用线程工厂threadFactory创建了一个新的线程t，然后将线程t的引用赋值给了Worker对象的成员变量thread，接着通过workers.add(w)将Worker对象添加到工作集当中。 下面我们看一下Worker类的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364private final class Worker implements Runnable &#123; private final ReentrantLock runLock = new ReentrantLock(); private Runnable firstTask; volatile long completedTasks; Thread thread; Worker(Runnable firstTask) &#123; this.firstTask = firstTask; &#125; boolean isActive() &#123; return runLock.isLocked(); &#125; void interruptIfIdle() &#123; final ReentrantLock runLock = this.runLock; if (runLock.tryLock()) &#123; try &#123; if (thread != Thread.currentThread()) thread.interrupt(); &#125; finally &#123; runLock.unlock(); &#125; &#125; &#125; void interruptNow() &#123; thread.interrupt(); &#125; private void runTask(Runnable task) &#123; final ReentrantLock runLock = this.runLock; runLock.lock(); try &#123; if (runState &lt; STOP &amp;&amp; Thread.interrupted() &amp;&amp; runState &gt;= STOP) boolean ran = false; beforeExecute(thread, task); //beforeExecute方法是ThreadPoolExecutor类的一个方法，没有具体实现，用户可以根据 //自己需要重载这个方法和后面的afterExecute方法来进行一些统计信息，比如某个任务的执行时间等 try &#123; task.run(); ran = true; afterExecute(task, null); ++completedTasks; &#125; catch (RuntimeException ex) &#123; if (!ran) afterExecute(task, ex); throw ex; &#125; &#125; finally &#123; runLock.unlock(); &#125; &#125; public void run() &#123; try &#123; Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) &#123; runTask(task); task = null; &#125; &#125; finally &#123; workerDone(this); //当任务队列中没有任务时，进行清理工作 &#125; &#125;&#125; 它实际上实现了Runnable接口，因此上面的Thread t = threadFactory.newThread(w);效果跟下面这句的效果基本一样： 1Thread t = new Thread(w); 相当于传进去了一个Runnable任务，在线程t中执行这个Runnable。 既然Worker实现了Runnable接口，那么自然最核心的方法便是run()方法了： 123456789101112public void run() &#123; try &#123; Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) &#123; runTask(task); task = null; &#125; &#125; finally &#123; workerDone(this); &#125;&#125; 从run方法的实现可以看出，它首先执行的是通过构造器传进来的任务firstTask，在调用runTask()执行完firstTask之后，在while循环里面不断通过getTask()去取新的任务来执行，那么去哪里取呢？自然是从任务缓存队列里面去取，getTask是ThreadPoolExecutor类中的方法，并不是Worker类中的方法，下面是getTask方法的实现： 123456789101112131415161718192021222324252627Runnable getTask() &#123; for (;;) &#123; try &#123; int state = runState; if (state &gt; SHUTDOWN) return null; Runnable r; if (state == SHUTDOWN) // Help drain queue r = workQueue.poll(); else if (poolSize &gt; corePoolSize || allowCoreThreadTimeOut) //如果线程数大于核心池大小或者允许为核心池线程设置空闲时间， //则通过poll取任务，若等待一定的时间取不到任务，则返回null r = workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS); else r = workQueue.take(); if (r != null) return r; if (workerCanExit()) &#123; //如果没取到任务，即r为null，则判断当前的worker是否可以退出 if (runState &gt;= SHUTDOWN) // Wake up others interruptIdleWorkers(); //中断处于空闲状态的worker return null; &#125; // Else retry &#125; catch (InterruptedException ie) &#123; // On interruption, re-check runState &#125; &#125;&#125; 在getTask中，先判断当前线程池状态，如果runState大于SHUTDOWN（即为STOP或者TERMINATED），则直接返回null。 如果runState为SHUTDOWN或者RUNNING，则从任务缓存队列取任务。 如果当前线程池的线程数大于核心池大小corePoolSize或者允许为核心池中的线程设置空闲存活时间，则调用poll(time,timeUnit)来取任务，这个方法会等待一定的时间，如果取不到任务就返回null。 然后判断取到的任务r是否为null，为null则通过调用workerCanExit()方法来判断当前worker是否可以退出，我们看一下workerCanExit()的实现： 12345678910111213141516private boolean workerCanExit() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); boolean canExit; //如果runState大于等于STOP，或者任务缓存队列为空了 //或者 允许为核心池线程设置空闲存活时间并且线程池中的线程数目大于1 try &#123; canExit = runState &gt;= STOP || workQueue.isEmpty() || (allowCoreThreadTimeOut &amp;&amp; poolSize &gt; Math.max(1, corePoolSize)); &#125; finally &#123; mainLock.unlock(); &#125; return canExit;&#125; 也就是说如果线程池处于STOP状态、或者任务队列已为空或者允许为核心池线程设置空闲存活时间并且线程数大于1时，允许worker退出。如果允许worker退出，则调用interruptIdleWorkers()中断处于空闲状态的worker，我们看一下interruptIdleWorkers()的实现： 12345678910oid interruptIdleWorkers() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) //实际上调用的是worker的interruptIfIdle()方法 w.interruptIfIdle(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; 从实现可以看出，它实际上调用的是worker的interruptIfIdle()方法，在worker的interruptIfIdle()方法中： 123456789101112void interruptIfIdle() &#123; final ReentrantLock runLock = this.runLock; if (runLock.tryLock()) &#123; //注意这里，是调用tryLock()来获取锁的，因为如果当前worker正在执行任务，锁已经被获取了，是无法获取到锁的 //如果成功获取了锁，说明当前worker处于空闲状态 try &#123; if (thread != Thread.currentThread()) thread.interrupt(); &#125; finally &#123; runLock.unlock(); &#125; &#125;&#125; 这里有一个非常巧妙的设计方式，假如我们来设计线程池，可能会有一个任务分派线程，当发现有线程空闲时，就从任务缓存队列中取一个任务交给空闲线程执行。但是在这里，并没有采用这样的方式，因为这样会要额外地对任务分派线程进行管理，无形地会增加难度和复杂度，这里直接让执行完任务的线程去任务缓存队列里面取任务来执行。 我们再看addIfUnderMaximumPoolSize方法的实现，这个方法的实现思想和addIfUnderCorePoolSize方法的实现思想非常相似，唯一的区别在于addIfUnderMaximumPoolSize方法是在线程池中的线程数达到了核心池大小并且往任务队列中添加任务失败的情况下执行的： 123456789101112131415private boolean addIfUnderMaximumPoolSize(Runnable firstTask) &#123; Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (poolSize &lt; maximumPoolSize &amp;&amp; runState == RUNNING) t = addThread(firstTask); &#125; finally &#123; mainLock.unlock(); &#125; if (t == null) return false; t.start(); return true;&#125; 看到没有，其实它和addIfUnderCorePoolSize方法的实现基本一模一样，只是if语句判断条件中的poolSize &lt; maximumPoolSize不同而已。 到这里，大部分朋友应该对任务提交给线程池之后到被执行的整个过程有了一个基本的了解，下面总结一下： 首先，要清楚corePoolSize和maximumPoolSize的含义； 其次，要知道Worker是用来起到什么作用的； 要知道任务提交给线程池之后的处理策略，这里总结一下主要有4点： 如果当前线程池中的线程数目小于corePoolSize，则每来一个任务，就会创建一个线程去执行这个任务； 如果当前线程池中的线程数目&gt;=corePoolSize，则每来一个任务，会尝试将其添加到任务缓存队列当中，若添加成功，则该任务会等待空闲线程将其取出去执行；若添加失败（一般来说是任务缓存队列已满），则会尝试创建新的线程去执行这个任务； 如果当前线程池中的线程数目达到maximumPoolSize，则会采取任务拒绝策略进行处理； 如果线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止，直至线程池中的线程数目不大于corePoolSize；如果允许为核心池中的线程设置存活时间，那么核心池中的线程空闲时间超过keepAliveTime，线程也会被终止。 线程池中的线程初始化 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。 在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程； prestartAllCoreThreads()：初始化所有核心线程 下面是这2个方法的实现： 12345678910public boolean prestartCoreThread() &#123; return addIfUnderCorePoolSize(null); //注意传进去的参数是null&#125; public int prestartAllCoreThreads() &#123; int n = 0; while (addIfUnderCorePoolSize(null))//注意传进去的参数是null ++n; return n;&#125; 注意上面传进去的参数是null，根据第2小节的分析可知如果传进去的参数为null，则最后执行线程会阻塞在getTask方法中的 1r = workQueue.take(); 即等待任务队列中有任务。 ### 任务缓存队列及排队策略 在前面我们多次提到了任务缓存队列，即workQueue，它用来存放等待执行的任务。 workQueue的类型为BlockingQueue&lt;Runnable&gt;，通常可以取下面三种类型： 1）ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小； 2）LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE； 3）synchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。 任务拒绝策略 当线程池的任务缓存队列已满并且线程池中的线程数目达到maximumPoolSize，如果还有任务到来就会采取任务拒绝策略，通常有以下四种策略： 1234ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 线程池的关闭 ThreadPoolExecutor提供了两个方法，用于线程池的关闭，分别是shutdown()和shutdownNow()，其中： shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务 线程池容量的动态调整 ThreadPoolExecutor提供了动态调整线程池容量大小的方法：setCorePoolSize()和setMaximumPoolSize()， setCorePoolSize：设置核心池大小 setMaximumPoolSize：设置线程池最大能创建的线程数目大小 当上述参数从小变大时，ThreadPoolExecutor进行线程赋值，还可能立即创建新的线程来执行任务。 使用示例 前面我们讨论了关于线程池的实现原理，这一节我们来看一下它的具体使用： 12345678910111213141516171819202122232425262728293031323334public class Test &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); for(int i=0;i&lt;15;i++)&#123; MyTask myTask = new MyTask(i); executor.execute(myTask); System.out.println("线程池中线程数目："+executor.getPoolSize()+"，队列中等待执行的任务数目："+ executor.getQueue().size()+"，已执行玩别的任务数目："+executor.getCompletedTaskCount()); &#125; executor.shutdown(); &#125;&#125; class MyTask implements Runnable &#123; private int taskNum; public MyTask(int num) &#123; this.taskNum = num; &#125; @Override public void run() &#123; System.out.println("正在执行task "+taskNum); try &#123; Thread.currentThread().sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("task "+taskNum+"执行完毕"); &#125;&#125; 执行结果： 正在执行task 0线程池中线程数目：1，队列中等待执行的任务数目：0，已执行玩别的任务数目：0线程池中线程数目：2，队列中等待执行的任务数目：0，已执行玩别的任务数目：0正在执行task 1线程池中线程数目：3，队列中等待执行的任务数目：0，已执行玩别的任务数目：0正在执行task 2线程池中线程数目：4，队列中等待执行的任务数目：0，已执行玩别的任务数目：0正在执行task 3线程池中线程数目：5，队列中等待执行的任务数目：0，已执行玩别的任务数目：0正在执行task 4线程池中线程数目：5，队列中等待执行的任务数目：1，已执行玩别的任务数目：0线程池中线程数目：5，队列中等待执行的任务数目：2，已执行玩别的任务数目：0线程池中线程数目：5，队列中等待执行的任务数目：3，已执行玩别的任务数目：0线程池中线程数目：5，队列中等待执行的任务数目：4，已执行玩别的任务数目：0线程池中线程数目：5，队列中等待执行的任务数目：5，已执行玩别的任务数目：0线程池中线程数目：6，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 10线程池中线程数目：7，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 11线程池中线程数目：8，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 12线程池中线程数目：9，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 13线程池中线程数目：10，队列中等待执行的任务数目：5，已执行玩别的任务数目：0正在执行task 14task 3执行完毕task 0执行完毕task 2执行完毕task 1执行完毕正在执行task 8正在执行task 7正在执行task 6正在执行task 5task 4执行完毕task 10执行完毕task 11执行完毕task 13执行完毕task 12执行完毕正在执行task 9task 14执行完毕task 8执行完毕task 5执行完毕task 7执行完毕task 6执行完毕task 9执行完毕 从执行结果可以看出，当线程池中线程的数目大于5时，便将任务放入任务缓存队列里面，当任务缓存队列满了之后，便创建新的线程。如果上面程序中，将for循环中改成执行20个任务，就会抛出任务拒绝异常了。 不过在java doc中，并不提倡我们直接使用ThreadPoolExecutor，而是使用Executors类中提供的几个静态方法来创建线程池： 123Executors.newCachedThreadPool(); //创建一个缓冲池，缓冲池容量大小为Integer.MAX_VALUEExecutors.newSingleThreadExecutor(); //创建容量为1的缓冲池Executors.newFixedThreadPool(int); //创建固定容量大小的缓冲池 下面是这三个静态方法的具体实现; 12345678910111213141516public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 从它们的具体实现来看，它们实际上也是调用了ThreadPoolExecutor，只不过参数都已配置好了。 newFixedThreadPool创建的线程池corePoolSize和maximumPoolSize值是相等的，它使用的LinkedBlockingQueue； 创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但是，在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 newSingleThreadExecutor将corePoolSize和maximumPoolSize都设置为1，也使用的LinkedBlockingQueue； 创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO,LIFO, 优先级)执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。 newCachedThreadPool将corePoolSize设置为0，将maximumPoolSize设置为Integer.MAX_VALUE，使用的SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过60秒，就销毁线程。 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。这种类型的线程池特点是： 工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger.MAX_VALUE), 这样可灵活的往线程池中添加线程。 如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪。 实际中，如果Executors提供的三个静态方法能满足要求，就尽量使用它提供的三个方法，因为自己去手动配置ThreadPoolExecutor的参数有点麻烦，要根据实际任务的类型和数量来进行配置。 另外，如果ThreadPoolExecutor达不到要求，可以自己继承ThreadPoolExecutor类进行重写。 如何合理配置线程池的大小 本节来讨论一个比较重要的话题：如何合理配置线程池大小，仅供参考。 一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1 如果是IO密集型任务，参考值可以设置为2*NCPU 当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。 参考资料： http://ifeve.com/java-threadpool/ http://blog.163.com/among_1985/blog/static/275005232012618849266/ http://developer.51cto.com/art/201203/321885.htm http://blog.csdn.net/java2000_wl/article/details/22097059 http://blog.csdn.net/cutesource/article/details/6061229 http://blog.csdn.net/xieyuooo/article/details/8718741 《JDK API 1.6》]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识ALS]]></title>
    <url>%2F2019%2F06%2F28%2FALS%2F</url>
    <content type="text"><![CDATA[前言Spark平台推出至今已经地带到2.4.x版本，很多地方都有了重要的更新，加入了很多新的东西。但是在协同过滤这一块却一直以来都只有ALS一种算法。同样是大规模计算平台，Hadoop中的机器学习算法库Mahout就集成了多种推荐算法，不但有user-cf和item-cf这种经典算法，还有KNN、SVD，Slope one这些，可谓随意挑选，简繁由君。ALS算法是2008年以来，用的比较多的协同过滤算法。它已经集成到Spark的Mllib库中，使用起来比较方便。 ALS算法算法介绍ALS的意思是交替最小二乘法（Alternating Least Squares），它只是一种优化算法的名字，被用在求解spark中所提供的推荐系统模型的最优解。spark中协同过滤的文档中一开始就说了。从协同过滤的分类来说，ALS算法属于User-Item CF，也叫做混合CF。它同时考虑了User和Item两个方面。用户和商品的关系，可以抽象为如下的三元组：&lt;User,Item,Rating&gt;。其中，Rating是用户对商品的评分，表征用户对该商品的喜好程度。 这是一个基于模型的协同过滤（model-based CF），其实它是一种近几年推荐系统界大火的隐语义模型中的一种。它的基本思想是对稀疏矩阵进行模型分解，评估出缺失项的值，以此来得到一个基本的训练模型。然后依照此模型可以针对新的用户和物品数据进行评估。ALS是采用交替的最小二乘法来算出缺失项的。交替的最小二乘法是在最小二乘法的基础上发展而来的。 隐语义模型又叫潜在因素模型，它试图通过数量相对少的未被观察到的底层原因，来解释大量用户和产品之间可观察到的交互。 原理SVD基于模型的协同过滤算法操作起来就是通过降维的方法来补全用户-物品矩阵，对矩阵中没有出现的值进行估计。基于这种思想的早期推荐系统常用的一种方法是SVD（奇异值分解）。该方法在矩阵分解之前需要先把评分矩阵R缺失值补全，补全之后稀疏矩阵R表示成稠密矩阵R’，然后将R’分解成如下形式： R’ = UTSV然后再选取U中的K列和V中的S行作为隐特征的个数，达到降维的目的。K的选取通常用启发式策略。 这种方法有两个缺点 补全成稠密矩阵之后需要耗费巨大的存储空间，在实际中，用户对物品的行为信息何止千万，对这样的稠密矩阵的存储是不现实的。 SVD的计算复杂度很高，更不用说这样的大规模稠密矩阵了。所以关于SVD的研究很多都是在小数据集上进行的。 ALS隐语义模型也是基于矩阵分解的，但是和SVD不同，它是把原始矩阵分解成两个矩阵相乘而不是三个。 假设我们有一批用户数据，其中包含m个User和n个Item，则我们定义Rating矩阵，其中的元素表示第u个User对第i个Item的评分。 在实际使用中，由于n和m的数量都十分巨大，因此R矩阵的规模很容易就会突破1亿项。这时候，传统的矩阵分解方法对于这么大的数据量已经是很难处理了。 另一方面，一个用户也不可能给所有商品评分，因此，R矩阵注定是个稀疏矩阵。矩阵中所缺失的评分，又叫做missing item。 针对这样的特点，我们可以假设用户和商品之间存在若干关联维度（比如用户年龄、性别、受教育程度和商品的外观、价格等），我们只需要将R矩阵投射到这些维度上即可。这个投射的数学表示是： Rm×n=Um×k×Vk×n 这里的表明这个投射只是一个近似的空间变换。 不懂这个空间变换的同学，可参见《机器学习（十二）》中的“奇异值分解”的内容，或是本节中的“主成分分析”的内容。 现在的问题就变成了确定U和V ，我们把 U叫做用户因子矩阵V叫做物品因子矩阵 一般情况下，k的值远小于n和m的值，从而达到了数据降维的目的。 幸运的是，我们并不需要显式的定义这些关联维度，而只需要假定它们存在即可，因此这里的关联维度又被称为Latent factor。k的典型取值一般是20～200。 这种方法被称为概率矩阵分解算法(probabilistic matrix factorization，PMF)。ALS算法是PMF在数值计算方面的应用。 通常上式不能达到精确相等的程度，我们要做的就是要最小化他们之间的差距，从而又变成了一个最优化问题。 求解最优化问题我们很容易就想到了随机梯度下降，其中有一种方法就是这样，通过优化如下损失函数来找到X和Y中合适的参数： 其中puk就是U矩阵中u行k列的参数，度量了用户u和第k个隐类的关系；qik是V矩阵中i行k列的参数，度量了物品i和第k个隐类的关系。这种方式也是一种很流行的方法，有很多对它的相关扩展，比如加上偏置项的LFM。 然而ALS用的是另一种求解方法，它先用随机初始化的方式固定一个矩阵，例如Y然后通过最小化等式两边差的平方来更新另一个矩阵X。 得到X之后，又可以固定X用相同的方法求Y，如此交替进行，直到最后收敛或者达到用户指定的迭代次数为止，是为“交替”是也。 因为这个迭代过程，交替优化X和Y，因此又被称作交替最小二乘算法（Alternating Least Squares，ALS）。 从上式可以看出，X的第i行是A的第i行和Y的函数，因此可以很容易地分开计算X的每一行，这就为并行计算提供了很大的便捷，也正是如此，Spark这种面向大规模计算的平台选择了这个算法。 在Intro to Implicit Matrix Factorization: Classic ALS with Sketchfab Models中，作者用了embarrassingly parallel来形容这个算法，意思是高度易并行化的——它的每个子任务之间没有什么依赖关系。 在现实中，不可能每个用户都和所有的物品都有行为关系，事实上，有交互关系的用户-物品对只占很小的一部分，换句话说，用户-物品关系列表是非常稀疏的。和SVD这种矩阵分解不同，ALS所用的矩阵分解技术在分解之前不用把系数矩阵填充成稠密矩阵之后再分解，这不但大大减少了存储空间，而且spark可以利用这种稀疏性用简单的线性代数计算求解。这几点使得本算法在大规模数据上计算非常快，解释了为什么spark mllib目前只有ALS一种推荐算法。 ALS算法的缺点 它是一个离线算法。 无法准确评估新加入的用户或商品。这个问题也被称为Cold Start问题。 显性反馈和隐性反馈基于矩阵分解的协同过滤的标准方法将用户项矩阵中的条目视为由用户给予该项的明确偏好，例如，给予电影评级的用户。在许多真实世界的用例中，通常只能访问隐式反馈（例如查看，点击，购买，喜欢，共享等）。 隐式反馈：用户给商品评分是个非常简单粗暴的用户行为。在实际的电商网站中，还有大量的用户行为，同样能够间接反映用户的喜好，比如用户的购买记录、搜索关键字，甚至是鼠标的移动。我们将这些间接用户行为称之为隐式反馈（implicit feedback），以区别于评分这样的显式反馈（explicit feedback）。 隐式反馈有以下几个特点：1.没有负面反馈（negative feedback）。用户一般会直接忽略不喜欢的商品，而不是给予负面评价。2.隐式反馈包含大量噪声。比如，电视机在某一时间播放某一节目，然而用户已经睡着了，或者忘了换台。3.显式反馈表现的是用户的喜好（preference），而隐式反馈表现的是用户的信任（confidence）。比如用户最喜欢的一般是电影，但观看时间最长的却是连续剧。大米购买的比较频繁，量也大，但未必是用户最想吃的食物。4.隐式反馈非常难以量化。 用于spark.ml处理这些数据的方法取自隐式反馈数据集的协作过滤。本质上，这种方法不是直接对收视率矩阵进行建模，而是将数据视为代表实力的数字观察用户操作（例如点击次数或某人观看电影的累计持续时间）。然后，这些数字与观察到的用户偏好的信心水平相关，而不是给予项目的明确评分。该模型然后试图找出可用于预测用户对物品的预期偏好的潜在因素。 在推荐系统中用户和物品的交互数据分为显性反馈和隐性反馈数据。在ALS中这两种情况也是被考虑了进来的，分别可以训练如下两种模型： 显性反馈模型val model1 = ALS.train(ratings, rank, numIterations, lambda) 隐性反馈模型val model2 = ALS.trainImplicit(ratings, rank, numIterations, lambda, alpha) 参数 numBlocks是为了并行化计算而将用户和项目划分到的块的数量（默认为10）。rank是模型中潜在因素的数量（默认为10）。maxIter是要运行的最大迭代次数（默认为10）。regParam指定ALS中的正则化参数（默认为1.0）。implicitPrefs指定是使用显式反馈 ALS变体还是使用 隐式反馈数据（默认为false使用显式反馈的手段）。alpha是一个适用于ALS的隐式反馈变量的参数，该变量管理偏好观察值的 基线置信度（默认值为1.0)nonnegative指定是否对最小二乘使用非负约束（默认为false）。注意： ALS的基于DataFrame的API目前仅支持用户和项目ID的整数。用户和项目ID列支持其他数字类型，但ID必须在整数值范围内。 从上面可以看到，隐式模型多了一个置信参数，这就涉及到ALS中对于隐式反馈模型的处理方式了——有的文章称为“加权的正则化矩阵分解”，它的损失函数如下： 我们知道，在隐反馈模型中是没有评分的，所以在式子中rui被pui所取代，pui是偏好的表示，仅仅表示用户和物品之间有没有交互，而不表示评分高低或者喜好程度。比如用户和物品之间有交互就让pui等于1，没有就等于0。函数中还有一个cui的项，它用来表示用户偏爱某个商品的置信程度，比如交互次数多的权重就会增加。如果我们用dui来表示交互次数的话，那么就可以把置信程度表示成如下公式： 参考：【推荐系统系列6】ALS推荐算法原理ALS算法解析]]></content>
      <categories>
        <category>推荐算法</category>
      </categories>
      <tags>
        <tag>ALS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql最左匹配原则]]></title>
    <url>%2F2019%2F06%2F27%2Fmysql-leftmost-matching-principle%2F</url>
    <content type="text"><![CDATA[转载自：mysql索引最左匹配原则的理解?作者：沈杰 下面是一个表结构，有三个字段，分别是id,name,cid 1234567CREATE TABLE `student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `cid` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_cid_INX` (`name`,`cid`),) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8 索引方面：id是主键，(name,cid)是一个多列索引。 先看下面两条SQL语句的执行计划： 1EXPLAIN SELECT * FROM student WHERE cid=1; 1EXPLAIN SELECT * FROM student WHERE cid=1 AND name=&apos;小红&apos;; sql查询用到索引的条件是必须要遵守最左前缀原则，为什么上面两个查询还能用到索引？-————————————————————————————————————————– 讲上面问题之前，我先补充一些知识，因为我觉得你对索引理解是狭隘的：上述你的两个查询的explain结果中显示用到索引的情况类型是不一样的。,可观察explain结果中的type字段。你的查询中分别是： type: indextype: ref 解释： index：这种类型表示是mysql会对整个该索引进行扫描。要想用到这种类型的索引，对这个索引并无特别要求，只要是索引，或者某个复合索引的一部分，mysql都可能会采用index类型的方式扫描。但是呢，缺点是效率不高，mysql会从索引中的第一个数据一个个的查找到最后一个数据，直到找到符合判断条件的某个索引。 所以：对于第一条语句： 1EXPLAIN SELECT * FROM student WHERE cid=1; 判断条件是cid=1,而cid是(name,cid)复合索引的一部分，没有问题，可以进行index类型的索引扫描方式。explain显示结果使用到了索引，是index类型的方式。 -————————————————————————————————————————– ref：这种类型表示mysql会根据特定的算法快速查找到某个符合条件的索引，而不是会对索引中每一个数据都进行一 一的扫描判断，也就是所谓你平常理解的使用索引查询会更快的取出数据。而要想实现这种查找，索引却是有要求的，要实现这种能快速查找的算法，索引就要满足特定的数据结构。简单说，也就是索引字段的数据必须是有序的，才能实现这种类型的查找，才能利用到索引。 有些了解的人可能会问，索引不都是一个有序排列的数据结构么。不过答案说的还不够完善，那只是针对单个索引，而复合索引的情况有些同学可能就不太了解了。 下面就说下复合索引： 以该表的(name,cid)复合索引为例,它内部结构简单说就是下面这样排列的： mysql创建复合索引的规则是首先会对复合索引的最左边的，也就是第一个name字段的数据进行排序，在第一个字段的排序基础上，然后再对后面第二个的cid字段进行排序。其实就相当于实现了类似 order by name cid这样一种排序规则。 所以：第一个name字段是绝对有序的，而第二字段就是无序的了。所以通常情况下，直接使用第二个cid字段进行条件判断是用不到索引的，当然，可能会出现上面的使用index类型的索引。这就是所谓的mysql为什么要强调最左前缀原则的原因。 那么什么时候才能用到呢? 当然是cid字段的索引数据也是有序的情况下才能使用咯，什么时候才是有序的呢？观察可知，当然是在name字段是等值匹配的情况下，cid才是有序的。发现没有，观察两个name名字为 c 的cid字段是不是有序的呢。从上往下分别是4 5。 这也就是mysql索引规则中要求复合索引要想使用第二个索引，必须先使用第一个索引的原因。（而且第一个索引必须是等值匹配）。 -————————————————————————————————————————– 所以对于这条sql查询： 1EXPLAIN SELECT * FROM student WHERE cid=1 AND name=&apos;小红&apos;; 没有错，而且复合索引中的两个索引字段都能很好的利用到了！因为语句中最左面的name字段进行了等值匹配，所以cid是有序的，也可以利用到索引了。 你可能会问：我建的索引是(name,cid)。而我查询的语句是cid=1 AND name=’小红’; 我是先查询cid，再查询name的，不是先从最左面查的呀？ 好吧，我再解释一下这个问题：首先可以肯定的是把条件判断反过来变成这样 name=’小红’ and cid=1; 最后所查询的结果是一样的。那么问题产生了？既然结果是一样的，到底以何种顺序的查询方式最好呢？ 所以，而此时那就是我们的mysql查询优化器该登场了，mysql查询优化器会判断纠正这条sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。所以，当然是我们能尽量的利用到索引时的查询顺序效率最高咯，所以mysql查询优化器会最终以这种顺序进行查询执行。 在最左前缀匹配原则，有一个非常重要的原则：mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 作者：Jokerone_ 链接：https://www.jianshu.com/p/b7911e0394b0 来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>最左匹配原则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Collaborative-Filtering协同过滤详解]]></title>
    <url>%2F2019%2F06%2F27%2FCollaborative-Filtering%2F</url>
    <content type="text"><![CDATA[基本思想基于用户的协同过滤算法是通过用户的历史行为数据发现用户对商品或内容的喜欢(如商品购买，收藏，内容评论或分享)，并对这些喜好进行度量和打分。根据不同用户对相同商品或内容的态度和偏好程度计算用户之间的关系。在有相同喜好的用户间进行商品推荐。简单的说就是如果A,B两个用户都购买了x、y、z三本图书，并且给出了5星的好评。那么A和B就属于同一类用户。可以将A看过的图书w也推荐给用户B。 基于用户协同过滤算法的原理图 所以，协同过滤算法主要分为两个步骤： 1、寻找相似的用户集合； 2、寻找集合中用户喜欢的且目标用户没有的进行推荐。 具体实现寻找用户间的相似度Jaccard公式Jaccard系数主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以Jaccard系数只关心个体间共同具有的特征是否一致这个问题。如果比较X与Y的Jaccard相似系数，只比较xn和yn中相同的个数。 Jaccard公式 皮尔逊相关系数皮尔逊相关系统是比欧几里德距离更加复杂的可以判断人们兴趣相似度的一种方法。它在数据不是很规范时，会倾向于给出更好的结果。假定有两个变量X、Y，那么两变量间的皮尔逊相关系数可通过以下公式计算： 公式一： 皮尔逊相关系数公式一公式二： 皮尔逊相关系数公式二公式三： 皮尔逊相关系数公式三公式四： 皮尔逊相关系数公式四 上述四个公式等价，其中E是数学期望，cov表示协方差，N表示变量取值的个数。 欧几里德距离假定两个用户X、Y，均为n维向量，表示用户对n个商品的评分，那么X与Y的欧几里德距离就是： 多维欧几里德距离公式数值越小则代表相似度越高，但是对于不同的n，计算出来的距离不便于控制，所以需要进行如下转换： 相似度公式使得结果分布在(0,1]上，数值越大，相似度越高。 余弦距离余弦距离，也称为余弦相似度，是用向量空间中两个向量余弦值作为衡量两个个体间差异大小的度量值。 与前面的欧几里德距离相似，用户X、Y为两个n维向量，套用余弦公式，其余弦距离表示为： 余弦距离公式即两个向量夹角的余弦值。但是相比欧式距离，余弦距离更加注意两个向量在方向上的相对差异，而不是在空间上的绝对距离，具体可以借助下图来感受两者间的区别： 余弦距离与欧式距离的区别 推荐物品在选取上述方法中的一种得到各个用户相似度后，针对目标用户u，我们选出最相似的k个用户，用集合S(u,k)表示，将S中所有用户喜欢的物品提取出来并去除目标用户u已经喜欢的物品。然后对余下的物品进行评分与相似度加权，得到的结果进行排序。最后由排序结果对目标用户u进行推荐。其中，对于每个可能推荐的物品i，用户u对其的感兴趣的程度可以用如下公式计算： 用户u对物品i感兴趣的程度rvi表示用户v对i的喜欢程度，即对i的评分，wuv表示用户u和v之间的相似度。 收集用户偏好要从用户的行为和偏好中发现规律，并基于此给予推荐，如何收集用户的偏好信息成为系统推荐效果最基础的决定因素。用户有很多方式向系统提供自己的偏好信息，而且不同的应用也可能大不相同，下面举例进行介绍： 表 1 用户行为和用户偏好 用户行为 类型 特征 作用 评分 显式 整数量化的偏好，可能的取值是 [0, n]；n 一般取值为 5 或者是 10 通过用户对物品的评分，可以精确的得到用户的偏好 投票 显式 布尔量化的偏好，取值是 0 或 1 通过用户对物品的投票，可以较精确的得到用户的偏好 转发 显式 布尔量化的偏好，取值是 0 或 1 通过用户对物品的投票，可以精确的得到用户的偏好,如果是站内，同时可以推理得到被转发人的偏好（不精确） 保存书签 显示 布尔量化的偏好，取值是 0 或 1 通过用户对物品的投票，可以精确的得到用户的偏好 标记标签(Tag) 显示 一些单词，需要对单词进行分析，得到偏好 通过分析用户的标签，可以得到用户对项目的理解，同时可以分析出用户的情感：喜欢还是讨厌 评论 显示 一段文字，需要进行文本分析，得到偏好 通过分析用户的评论，可以得到用户的情感：喜欢还是讨厌 点击流( 查看 ) 隐式 一组用户的点击，用户对物品感兴趣，需要进行分析，得到偏好 用户的点击一定程度上反映了用户的注意力，所以它也可以从一定程度上反映用户的喜好 页面停留时间 隐式 一组时间信息，噪音大，需要进行去噪，分析，得到偏好 用户的页面停留时间一定程度上反映了用户的注意力和喜好，但噪音偏大，不好利用 购买 隐式 布尔量化的偏好，取值是 0 或 1 用户的购买是很明确的说明这个项目它感兴趣 以上列举的用户行为都是比较通用的，推荐引擎设计人员可以根据自己应用的特点添加特殊的用户行为，并用他们表示用户对物品的喜好| 在一般应用中，我们提取的用户行为一般都多于一种，关于如何组合这些不同的用户行为，基本上有以下两种方式： 将不同的行为分组：一般可以分为“查看”和“购买”等等，然后基于不同的行为，计算不同的用户 / 物品相似度。类似于当当网或者 Amazon 给出的“购买了该图书的人还购买了 …”，“查看了图书的人还查看了 …” 根据不同行为反映用户喜好的程度将它们进行加权，得到用户对于物品的总体喜好。一般来说，显式的用户反馈比隐式的权值大，但比较稀疏，毕竟进行显示反馈的用户是少数；同时相对于“查看”，“购买”行为反映用户喜好的程度更大，但这也因应用而异。 收集了用户行为数据，我们还需要对数据进行一定的预处理，其中最核心的工作就是：减噪和归一化。 减噪：用户行为数据是用户在使用应用过程中产生的，它可能存在大量的噪音和用户的误操作，我们可以通过经典的数据挖掘算法过滤掉行为数据中的噪音，这样可以是我们的分析更加精确。 归一化：如前面讲到的，在计算用户对物品的喜好程度时，可能需要对不同的行为数据进行加权。但可以想象，不同行为的数据取值可能相差很大，比如，用户的查看数据必然比购买数据大的多，如何将各个行为的数据统一在一个相同的取值范围中，从而使得加权求和得到的总体喜好更加精确，就需要我们进行归一化处理。最简单的归一化处理，就是将各类数据除以此类中的最大值，以保证归一化后的数据取值在 [0,1] 范围中 进行了预处理后，根据不同应用的行为分析方法，可以选择分组或者加权处理，之后我们可以得到一个用户偏好的二维矩阵，一维是用户列表，另一维是物品列表，值是用户对物品的偏好，一般是 [0,1] 或者 [-1, 1] 的浮点数值。 基于用户的 CF（User CF）基于用户的协同过滤(user-basedCF)是基于这样一个事实：每个用户都有与其具有相似兴趣爱好和购买行为的用户群，这些相似用户(邻居用户)的购买项目可以作为对当前用户(目标用户)进行项目推荐的基础。因此，这种方法也被称为基于邻居的协同过滤或最近邻居算法。 基于用户的 CF 的基本思想相当简单，基于用户对物品的偏好找到相邻邻居用户，然后将邻居用户喜欢的推荐给当前用户。计算上，就是将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，找到 K 邻居后，根据邻居的相似度权重以及他们对物品的偏好，预测当前用户没有偏好的未涉及物品，计算得到一个排序的物品列表作为推荐。图 2 给出了一个例子，对于用户 A，根据用户的历史偏好，这里只计算得到一个邻居 – 用户 C，然后将用户 C 喜欢的物品 D 推荐给用户 A。 图 2 基于用户的 CF 的基本原理 基于物品的 CF（Item CF）基于物品的 CF 的原理和基于用户的 CF 类似，只是在计算邻居时采用物品本身，而不是从用户的角度，即基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。从计算的角度看，就是将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的偏好预测当前用户还没有表示偏好的物品，计算得到一个排序的物品列表作为推荐。图 3 给出了一个例子，对于物品 A，根据所有用户的历史偏好，喜欢物品 A 的用户都喜欢物品 C，得出物品 A 和物品 C 比较相似，而用户 C 喜欢物品 A，那么可以推断出用户 C 可能也喜欢物品 C。 基于物品的协同过滤的一个优点是容易解释推荐原因，第二个是电商网站中物品的相似度是相对不变的，物品相似度的矩阵维护起来相对容易。 图 3 基于物品的 CF 的基本原理 User CF vs. Item CF前面介绍了User CF 和Item CF 的基本原理，下面我们分几个不同的角度深入看看它们各自的优缺点和适用场景： 计算复杂度Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。但我们往往忽略了这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。 适用场景在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。 相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。 推荐多样性和精度研究推荐引擎的学者们在相同的数据集合上分别用 User CF 和Item CF计算推荐结果，发现推荐列表中，只有 50% 是一样的，还有 50% 完全不同。但是这两个算法确有相似的精度，所以可以说，这两个算法是很互补的。 关于推荐的多样性，有两种度量方法： 第一种度量方法是从单个用户的角度度量，就是说给定一个用户，查看系统给出的推荐列表是否多样，也就是要比较推荐列表中的物品之间两两的相似度，不难想到，对这种度量方法，Item CF 的多样性显然不如 User CF 的好，因为 Item CF 的推荐就是和以前看的东西最相似的。 第二种度量方法是考虑系统的多样性，也被称为覆盖率 (Coverage)，它是指一个推荐系统是否能够提供给所有用户丰富的选择。在这种指标下，Item CF 的多样性要远远好于 User CF, 因为 User CF 总是倾向于推荐热门的，从另一个侧面看，也就是说，Item CF 的推荐有很好的新颖性，很擅长推荐长尾里的物品。所以，尽管大多数情况，Item CF 的精度略小于 User CF， 但如果考虑多样性，Item CF 却比 User CF 好很多。 如果你对推荐的多样性还心存疑惑，那么下面我们再举个实例看看 User CF 和 Item CF 的多样性到底有什么差别。首先，假设每个用户兴趣爱好都是广泛的，喜欢好几个领域的东西，不过每个用户肯定也有一个主要的领域，对这个领域会比其他领域更加关心。给定一个用户，假设他喜欢 3 个领域 A,B,C，A 是他喜欢的主要领域，这个时候我们来看 User CF 和 Item CF 倾向于做出什么推荐：如果用 User CF, 它会将 A,B,C 三个领域中比较热门的东西推荐给用户；而如果用 ItemCF，它会基本上只推荐 A 领域的东西给用户。所以我们看到因为 User CF 只推荐热门的，所以它在推荐长尾里项目方面的能力不足；而 Item CF 只推荐 A 领域给用户，这样他有限的推荐列表中就可能包含了一定数量的不热门的长尾物品，同时 Item CF 的推荐对这个用户而言，显然多样性不足。但是对整个系统而言，因为不同的用户的主要兴趣点不同，所以系统的覆盖率会比较好。 从人们需求的角度来看，大多数的需求会集中在头部，而这部分我们可以称之为流行，而分布在尾部的需求是个性化的，零散的小量的需求。而这部分差异化的、少量的需求会在需求曲线上面形成一条长长的“尾巴”，而所谓长尾效应就在于它的数量上，将所有非流行的市场累加起来就会形成一个比流行市场还大的市场。 从上面的分析，可以很清晰的看到，这两种推荐都有其合理性，但都不是最好的选择，因此他们的精度也会有损失。其实对这类系统的最好选择是，如果系统给这个用户推荐 30 个物品，既不是每个领域挑选 10 个最热门的给他，也不是推荐 30 个 A 领域的给他，而是比如推荐 15 个 A 领域的给他，剩下的 15 个从 B,C 中选择。所以结合 User CF 和 Item CF 是最优的选择，结合的基本原则就是当采用 Item CF 导致系统对个人推荐的多样性不足时，我们通过加入 User CF 增加个人推荐的多样性，从而提高精度，而当因为采用 User CF 而使系统的整体多样性不足时，我们可以通过加入 Item CF 增加整体的多样性，同样同样可以提高推荐的精度。 用户对推荐算法的适应度前面我们大部分都是从推荐引擎的角度考虑哪个算法更优，但其实我们更多的应该考虑作为推荐引擎的最终使用者 — 应用用户对推荐算法的适应度。 对于 User CF，推荐的原则是假设用户会喜欢那些和他有相同喜好的用户喜欢的东西，但如果一个用户没有相同喜好的朋友，那 User CF 的算法的效果就会很差，所以一个用户对的 CF 算法的适应度是和他有多少共同喜好用户成正比的。 Item CF 算法也有一个基本假设，就是用户会喜欢和他以前喜欢的东西相似的东西，那么我们可以计算一个用户喜欢的物品的自相似度。一个用户喜欢物品的自相似度大，就说明他喜欢的东西都是比较相似的，也就是说他比较符合 Item CF 方法的基本假设，那么他对 Item CF 的适应度自然比较好；反之，如果自相似度小，就说明这个用户的喜好习惯并不满足 Item CF 方法的基本假设，那么对于这种用户，用 Item CF 方法做出好的推荐的可能性非常低。 总结 UserCF ItemCF 性能 适用于用户较少的场合，如果用户很多，计算用户相似度矩阵代价很大 适用于物品数明显小于用户数的场合，如果物品很多（网页），计算物品相似度矩阵代价很大 领域 时效性较强，用户个性化兴趣不太明显的领域 长尾物品丰富，用户个性化需求强烈的领域 实时性 用户有新行为，不一定造成推荐结果的立即变化 用户有新行为，一定会导致推荐结果的实时变化 冷启动 在新用户对很少的物品产生行为后，不能立即对他进行个性化推荐，因为用户相似度表是每隔一段时间离线计算的 新用户只要对一个物品产生行为，就可以给他推荐和该物品相关的其他物品 新物品上线后一段时间，一旦有用户对物品产生行为，就可以将新物品推荐给和对它产生行为的用户兴趣相似的其他用户 但没有办法在不离线更新物品相似度表的情况下将新物品推荐给用户 推荐理由 很难提供令用户信服的推荐解释 利用用户的历史行为给用户做推荐解释，可以令用户比较信服 矩阵分解 Spark推荐模型库当前只包含基于矩阵分解（matrix factorization）的实现，由此我们也将重点关注这类模型。它们有吸引人的地方。首先，这些模型在协同过滤中的表现十分出色。而在Netflix Prize等知名比赛中的表现也很拔尖 显式矩阵分解 要找到和“用户物品”矩阵近似的k维（低阶）矩阵，最终要求出如下两个矩阵：一个用于表示用户的U × k维矩阵，以及一个表征物品的I × k维矩阵。 这两个矩阵也称作因子矩阵。它们的乘积便是原始评级矩阵的一个近似。值得注意的是，原始评级矩阵通常很稀疏，但因子矩阵却是稠密的。 特点：因子分解类模型的好处在于，一旦建立了模型，对推荐的求解便相对容易。但也有弊端，即当用户和物品的数量很多时，其对应的物品或是用户的因子向量可能达到数以百万计。 这将在存储和计算能力上带来挑战。另一个好处是，这类模型的表现通常都很出色。 隐式矩阵分解（关联因子分确定，可能随时会变化）隐式模型仍然会创建一个用户因子矩阵和一个物品因子矩阵。但是，模型所求解的是偏好矩阵而非评级矩阵的近似。类似地，此时用户因子向量和物品因子向量的点积所得到的分数 也不再是一个对评级的估值，而是对某个用户对某一物品偏好的估值（该值的取值虽并不严格地处于0到1之间，但十分趋近于这个区间） 最小二乘法（Alternating Least Squares ALS）：解决矩阵分解的最优化方法ALS的实现原理是迭代式求解一系列最小二乘回归问题。在每一次迭代时，固定用户因子矩阵或是物品因子矩阵中的一个，然后用固定的这个矩阵以及评级数据来更新另一个矩阵。 之后，被更新的矩阵被固定住，再更新另外一个矩阵。如此迭代，直到模型收敛（或是迭代了预设好的次数）。 参考：Collaborative Filtering(协同过滤)算法详解协同过滤算法：基于用户和基于物品的优缺点比较]]></content>
      <categories>
        <category>推荐算法</category>
      </categories>
      <tags>
        <tag>协同过滤</tag>
        <tag>推荐算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识ElasticSearch]]></title>
    <url>%2F2019%2F06%2F27%2Fes-introduce%2F</url>
    <content type="text"><![CDATA[没有ES之前思考：大规模数据如何检索？当系统数据量上了10亿、100亿条的时候，我们在做系统架构的时候通常会从以下角度去考虑问题：1）用什么数据库好？(mysql、sybase、oracle、达梦、神通、mongodb、hbase…)2）如何解决单点故障；(lvs、F5、A10、Zookeeper、MQ)3）如何保证数据安全性；(热备、冷备、异地多活)4）如何解决检索难题；(数据库代理中间件：mysql-proxy、Cobar、MaxScale等;)5）如何解决统计分析问题；(离线、近实时) 传统数据库的应对解决方案对于关系型数据，我们通常采用以下或类似架构去解决查询瓶颈和写入瓶颈：解决要点：1）通过主从备份解决数据安全性问题；2）通过数据库代理中间件心跳监测，解决单点故障问题；3）通过代理中间件将查询语句分发到各个slave节点进行查询，并汇总结果 非关系型数据库的解决方案对于Nosql数据库，以mongodb为例，其它原理类似：解决要点：1）通过副本备份保证数据安全性；2）通过节点竞选机制解决单点问题；3）先从配置库检索分片信息，然后将请求分发到各个节点，最后由路由节点合并汇总结果 另辟蹊径——完全把数据放入内存怎么样？我们知道，完全把数据放在内存中是不可靠的，实际上也不太现实，当我们的数据达到PB级别时，按照每个节点96G内存计算，在内存完全装满的数据情况下，我们需要的机器是：1PB=1024T=1048576G节点数=1048576/96=10922个实际上，考虑到数据备份，节点数往往在2.5万台左右。成本巨大决定了其不现实！ 从前面讨论我们了解到，把数据放在内存也好，不放在内存也好，都不能完完全全解决问题。全部放在内存速度问题是解决了，但成本问题上来了。为解决以上问题，从源头着手分析，通常会从以下方式来寻找方法：1、存储数据时按有序存储；2、将数据和索引分离；3、压缩数据；这就引出了Elasticsearch。 Elasticsearch的概述Elasticsearch 是什么Elasticsearch（ES）是一个基于Lucene构建的开源、分布式、RESTful接口的全文搜索引擎。Elasticsearch还是一个分布式文档数据库，其中每个字段均可被索引，而且每个字段的数据均可被搜索，ES能够横向扩展至数以百计的服务器存储以及处理PB级的数据。可以在极短的时间内存储、搜索和分析大量的数据。通常作为具有复杂搜索场景情况下的核心发动机。Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 ElasticSearch就是为高可用和可扩展而生的。可以通过购置性能更强的服务器或者升级硬件来完成系统扩展，称为垂直或向上扩展（Vertical Scale/Scaling Up）。另一方面，增加更多的服务器来完成系统扩展，称为水平扩展或者向外扩展（Horizontal Scale/Scaling Out）。尽管ES能够利用更强劲的硬件，垂直扩展毕竟还是有它的极限。真正的可扩展性来自于水平扩展，通过向集群中添加更多的节点来分担负载，增加可靠性。ES天生就是分布式的：它知道如何管理多个节点来完成扩展和实现高可用性。这也意味你的应用不需要做任何的改动。 Lucene与ES关系 Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。 Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 ES主要解决问题1）检索相关数据；2）返回统计结果；3）速度要快。 ES工作原理当ElasticSearch的节点启动后，它会利用多播(multicast)(或者单播，如果用户更改了配置)寻找集群中的其它节点，并与之建立连接。这个过程如下图所示： ES数据架构的主要概念（与关系数据库Mysql对比）（1）关系型数据库中的数据库（DataBase），等价于ES中的索引（Index）（2）一个数据库下面有N张表（Table），等价于1个索引Index下面有N多类型（Type），（3）一个数据库表（Table）下的数据由多行（ROW）多列（column，属性）组成，等价于1个Type由多个文档（Document）和多Field组成。（4）在一个关系型数据库里面，schema定义了表、每个表的字段，还有表和字段之间的关系。 与之对应的，在ES中：Mapping定义索引下的Type的字段处理规则，即索引如何建立、索引类型、是否保存原始索引JSON文档、是否压缩原始JSON文档、是否需要分词处理、如何进行分词处理等。（5）在数据库中的增insert、删delete、改update、查search操作等价于ES中的增PUT/POST、删Delete、改_update、查GET. ES核心概念Near Realtime(NRT) 几乎实时Elasticsearch是一个几乎实时的搜索平台。意思是，从索引一个文档到这个文档可被搜索只需要一点点的延迟，这个时间一般为毫秒级。 Cluster：集群ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。 群集是一个或多个节点（服务器）的集合， 这些节点共同保存整个数据，并在所有节点上提供联合索引和搜索功能。一个集群由一个唯一集群ID确定，并指定一个集群名（默认为“elasticsearch”）。该集群名非常重要，因为节点可以通过这个集群名加入群集，一个节点只能是群集的一部分。 确保在不同的环境中不要使用相同的群集名称，否则可能会导致连接错误的群集节点。例如，你可以使用logging-dev、logging-stage、logging-prod分别为开发、阶段产品、生产集群做记录。 Node：节点形成集群的每个服务器称为节点。 节点是单个服务器实例，它是群集的一部分，可以存储数据，并参与群集的索引和搜索功能。就像一个集群，节点的名称默认为一个随机的通用唯一标识符（UUID），确定在启动时分配给该节点。如果不希望默认，可以定义任何节点名。这个名字对管理很重要，目的是要确定你的网络服务器对应于你的ElasticSearch群集节点。 我们可以通过群集名配置节点以连接特定的群集。默认情况下，每个节点设置加入名为“elasticSearch”的集群。这意味着如果你启动多个节点在网络上，假设他们能发现彼此都会自动形成和加入一个名为“elasticsearch”的集群。 在单个群集中，您可以拥有尽可能多的节点。此外，如果“elasticsearch”在同一个网络中，没有其他节点正在运行，从单个节点的默认情况下会形成一个新的单节点名为”elasticsearch”的集群。 Shard：分片 &amp; Replia：副本索引可以存储大量的数据，这些数据可能超过单个节点的硬件限制。例如，十亿个文件占用磁盘空间1TB的单指标可能不适合对单个节点的磁盘或可能太慢服务仅从单个节点的搜索请求。 为了解决这一问题，Elasticsearch提供细分你的指标分成多个块称为分片的能力。当你创建一个索引，你可以简单地定义你想要的分片数量。每个分片本身是一个全功能的、独立的“指数”，可以托管在集群中的任何节点。 Shards分片的重要性主要体现在以下两个特征： 分片允许您水平拆分或缩放内容的大小 分片允许你分配和并行操作的碎片（可能在多个节点上）从而提高性能/吞吐量 这个机制中的碎片是分布式的以及其文件汇总到搜索请求是完全由ElasticSearch管理，对用户来说是透明的。 在同一个集群网络或云环境上，故障是任何时候都会出现的，拥有一个故障转移机制以防分片和结点因为某些原因离线或消失是非常有用的，并且被强烈推荐。为此，Elasticsearch允许你创建一个或多个拷贝，你的索引分片进入所谓的副本或称作复制品的分片，简称Replicas。 Replicas的重要性主要体现在以下两个特征： 副本为分片或节点失败提供了高可用性。为此，需要注意的是，一个副本的分片不会分配在同一个节点作为原始的或主分片，副本是从主分片那里复制过来的。 副本允许用户扩展你的搜索量或吞吐量，因为搜索可以在所有副本上并行执行。 当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。 为提高查询吞吐量或实现高可用性，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。 全文检索全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。 Index索引索引是具有相似特性的文档集合。例如，可以为客户数据提供索引，为产品目录建立另一个索引，以及为订单数据建立另一个索引。索引由名称（必须全部为小写）标识，该名称用于在对其中的文档执行索引、搜索、更新和删除操作时引用索引。在单个群集中，您可以定义尽可能多的索引。 Type类型在索引中，可以定义一个或多个类型。类型是索引的逻辑类别/分区，其语义完全取决于您。一般来说，类型定义为具有公共字段集的文档。例如，假设你运行一个博客平台，并将所有数据存储在一个索引中。在这个索引中，您可以为用户数据定义一种类型，为博客数据定义另一种类型，以及为注释数据定义另一类型。 Document文档文档是可以被索引的信息的基本单位。例如，您可以为单个客户提供一个文档，单个产品提供另一个文档，以及单个订单提供另一个文档。本文件的表示形式为JSON（JavaScript Object Notation）格式，这是一种非常普遍的互联网数据交换格式。 在索引/类型中，您可以存储尽可能多的文档。请注意，尽管文档物理驻留在索引中，文档实际上必须索引或分配到索引中的类型。 Elasticsearch可以做什么?当你经营一家网上商店，你可以让你的客户搜索你卖的商品。在这种情况下，你可以使用ElasticSearch来存储你的整个产品目录和库存信息，为客户提供精准搜索，可以为客户推荐相关商品。 当你想收集日志或者交易数据的时候，需要分析和挖掘这些数据，寻找趋势，进行统计，总结，或发现异常。在这种情况下，你可以使用Logstash或者其他工具来进行收集数据，当这引起数据存储到ElasticsSearch中。你可以搜索和汇总这些数据，找到任何你感兴趣的信息。 对于程序员来说，比较有名的案例是GitHub，GitHub的搜索是基于ElasticSearch构建的，在github.com/search页面，你可以搜索项目、用户、issue、pull request，还有代码。共有40~50个索引库，分别用于索引网站需要跟踪的各种数据。虽然只索引项目的主分支（master），但这个数据量依然巨大，包括20亿个索引文档，30TB的索引文件。 ELK是什么？ELK=elasticsearch+Logstash+kibanaelasticsearch：后台分布式存储以及全文检索logstash: 日志加工、“搬运工”kibana：数据可视化展示。ELK架构为数据分布式存储、可视化查询和日志解析创建了一个功能强大的管理链。 三者相互配合，取长补短，共同完成分布式大数据处理工作。 ES特点和优势1）分布式实时文件存储，可将每一个字段存入索引，使其可以被检索到。2）实时分析的分布式搜索引擎。 分布式：索引分拆成多个分片，每个分片可有零个或多个副本。集群中的每个数据节点都可承载一个或多个分片，并且协调和处理各种操作；负载再平衡和路由在大多数情况下自动完成。 3）可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。也可以运行在单台PC上（已测试）4）支持插件机制，分词插件、同步插件、Hadoop插件、可视化插件等。 参考：Elasticsearch学习，请先看这一篇！ElasticSearch介绍和基本概念]]></content>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中遍历HashMap的5种方式]]></title>
    <url>%2F2019%2F06%2F27%2Fjava-HashMap-access%2F</url>
    <content type="text"><![CDATA[本文转载自：Java中遍历HashMap的5种方式 本教程将为你展示Java中HashMap的几种典型遍历方式。 如果你使用Java8，由于该版本JDK支持lambda表达式，可以采用第5种方式来遍历。 如果你想使用泛型，可以参考方法3。如果你使用旧版JDK不支持泛型可以参考方法4。 通过ForEach循环进行遍历1234567891011121314151617mport java.io.IOException;import java.util.HashMap;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); map.put(1, 10); map.put(2, 20); // Iterating entries using a For Each loop for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue()); &#125; &#125;&#125; ForEach迭代键值对方式如果你只想使用键或者值，推荐使用如下方式 123456789101112131415161718192021import java.io.IOException;import java.util.HashMap;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); map.put(1, 10); map.put(2, 20); // 迭代键 for (Integer key : map.keySet()) &#123; System.out.println("Key = " + key); &#125; // 迭代值 for (Integer value : map.values()) &#123; System.out.println("Value = " + value); &#125; &#125;&#125; 使用带泛型的迭代器进行遍历123456789101112131415161718import java.io.IOException;import java.util.HashMap;import java.util.Iterator;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); map.put(1, 10); map.put(2, 20); Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; entries = map.entrySet().iterator(); while (entries.hasNext()) &#123; Map.Entry&lt;Integer, Integer&gt; entry = entries.next(); System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue()); &#125; &#125;&#125; 使用不带泛型的迭代器进行遍历12345678910111213141516171819202122import java.io.IOException;import java.util.HashMap;import java.util.Iterator;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map map = new HashMap(); map.put(1, 10); map.put(2, 20); Iterator&lt;Map.Entry&gt; entries = map.entrySet().iterator(); while (entries.hasNext()) &#123; Map.Entry entry = (Map.Entry) entries.next(); Integer key = (Integer) entry.getKey(); Integer value = (Integer) entry.getValue(); System.out.println("Key = " + key + ", Value = " + value); &#125; &#125;&#125; 通过Java8 Lambda表达式遍历1234567891011121314import java.io.IOException;import java.util.HashMap;import java.util.Map; public class Test &#123; public static void main(String[] args) throws IOException &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); map.put(1, 10); map.put(2, 20); map.forEach((k, v) -&gt; System.out.println("key: " + k + " value:" + v)); &#125;&#125; 输出 12key: 1 value:10key: 2 value:20 英文原文：https://www.javatips.net/blog/iterate-hashmap-using-java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库范式]]></title>
    <url>%2F2019%2F06%2F26%2Fdatabase-paradigms%2F</url>
    <content type="text"><![CDATA[转载自：[数据库] 理解数据库范式-通俗易懂 ​ 数据库范式是数据库设计中必不可少的知识，没有对范式的理解，就无法设计出高效率、优雅的数据库。甚至设计出错误的数据库。而想要理解并掌握范式却并不是那么容易。教科书中一般以关系代数的方法来解释数据库范式。这样做虽然能够十分准确的表达数据库范式，但比较抽象，不太直观，不便于理解，更难以记忆。 本文用较为直白的语言介绍范式，旨在便于理解和记忆，这样做可能会出现一些不精确的表述。但对于初学者应该是个不错的入门。我写下这些的目的主要是为了加强记忆，其实我也比较菜，我希望当我对一些概念生疏的时候，回过头来看看自己写的笔记，可以快速地进入状态。如果你发现其中用错误，请指正。 下面开始进入正题： 基础概念要理解范式，首先必须对知道什么是关系数据库，如果你不知道，我可以简单的不能再简单的说一下：关系数据库就是用二维表来保存数据。表和表之间可以……（省略10W字）。 然后你应该理解以下概念： 实体：现实世界中客观存在并可以被区别的事物。比如“一个学生”、“一本书”、“一门课”等等。值得强调的是这里所说的“事物”不仅仅是看得见摸得着的“东西”，它也可以是虚拟的，比如说“老师与学校的关系”。 属性：教科书上解释为：“实体所具有的某一特性”，由此可见，属性一开始是个逻辑概念，比如说，“性别”是“人”的一个属性。在关系数据库中，属性又是个物理概念，属性可以看作是“表的一列”。 元组：表中的一行就是一个元组。 分量：元组的某个属性值。在一个关系数据库中，它是一个操作原子，即关系数据库在做任何操作的时候，属性是“不可分的”。否则就不是关系数据库了。 码：表中可以唯一确定一个元组的某个属性（或者属性组），如果这样的码有不止一个，那么大家都叫候选码，我们从候选码中挑一个出来做老大，它就叫主码。 全码：如果一个码包含了所有的属性，这个码就是全码。 主属性：一个属性只要在任何一个候选码中出现过，这个属性就是主属性。 非主属性：与上面相反，没有在任何候选码中出现过，这个属性就是非主属性。 外码：一个属性（或属性组），它不是码，但是它别的表的码，它就是外码。 六个范式好了，上面已经介绍了我们掌握范式所需要的全部基础概念，下面我们就来讲范式。首先要明白，范式的包含关系。一个数据库设计如果符合第二范式，一定也符合第一范式。如果符合第三范式，一定也符合第二范式…… 第一范式（1NF）：属性不可分在前面已经介绍了属性值的概念，我们说，它是“不可分的”。而第一范式要求属性也不可分。那么它和属性值不可分有什么区别呢？给一个例子： 这个表中，属性值“分”了。“电话”这个属性里对于“小明”属性值分成了两个。 这两种情况都不满足第一范式。不满足第一范式的数据库，不是关系数据库！所以，我们在任何关系数据库管理系统中，做不出这样的“表”来。针对上述情况可以做成这样的表：这个表中，属性 “分”了。也就是“电话”分为了“手机”和“座机”两个属性。 第二范式（2NF）：符合1NF，并且，非主属性完全依赖于码。（注意是完全依赖不能是部分依赖，设有函数依赖W→A，若存在XW，有X→A成立，那么称W→A是局部依赖，否则就称W→A是完全函数依赖） 一个学生上一门课，一定是特定某个老师教。所以有（学生，课程）－&gt;老师； 一个学生上一门课，一定在特定某个教室。所以有（学生，课程）－&gt;教室； 一个学生上一门课，他老师的职称可以确定。所以有（学生，课程）－&gt;老师职称； 一个学生上一门课，一定是特定某个教材。所以有（学生，课程）－&gt;教材 一个学生上一门课，一定在特定时间。所以有（学生，课程）－&gt;上课时间 因此（学生，课程）是一个码。 然而，一个课程，一定指定了某个教材，一年级语文肯定用的是《小学语文1》，那么就有课程－&gt;教材。（学生，课程）是个码，课程却决定了教材，这就叫做不完全依赖，或者说部分依赖。出现这样的情况，就不满足第二范式！ 有什么不好吗？你可以想想： 1、校长要新增加一门课程叫“微积分”，教材是《大学数学》，怎么办？学生还没选课，而学生又是主属性，主属性不能空，课程怎么记录呢，教材记到哪呢? ……郁闷了吧?(插入异常) 2、下学期没学生学一年级语文（上）了，学一年级语文（下）去了，那么表中将不存在一年级语文（上），也就没了《小学语文1》。这时候，校长问：一年级语文（上）用的什么教材啊？……郁闷了吧?(删除异常) 3、校长说：一年级语文（上）换教材，换成《大学语文》。有10000个学生选了这门课，改动好大啊！改累死了……郁闷了吧？（修改/更新异常，在这里你可能觉得直接把教材《小学语文1》替换成《大学语文》不就可以了，但是替换操作虽然计算机运行速度很快，但是毕竟也要替换10000次，造成了很大的时间开销） 那应该怎么解决呢？投影分解，将一个表分解成两个或若干个表 第三范式（3NF）：符合2NF，并且，消除传递依赖（也就是每个非主属性都不传递依赖于候选键，判断传递函数依赖，指的是如果存在”A → B → C”的决定关系，则C传递函数依赖于A。）上面的“学生上课新表”符合2NF，但是它有传递依赖！在哪呢？问题就出在“老师”和“老师职称”这里。一个老师一定能确定一个老师职称。（学生，课程）-&gt;老师-&gt;职称。 有什么问题吗？想想： 1、老师升级了，变教授了，要改数据库，表中有N条，改了N次……（修改异常）2、没人选这个老师的课了，老师的职称也没了记录……（删除异常）3、新来一个老师，还没分配教什么课，他的职称记到哪？……（插入异常）那应该怎么解决呢？和上面一样，投影分解： BC范式（BCNF）：符合3NF，并且，主属性不依赖于主属性(也就是不存在任何字段对任一候选关键字段的传递函数依赖)BC范式既检查非主属性，又检查主属性。当只检查非主属性时，就成了第三范式。满足BC范式的关系都必然满足第三范式。 还可以这么说：若一个关系达到了第三范式，并且它只有一个候选码，或者它的每个候选码都是单属性，则该关系自然达到BC范式。 给你举个例子：假设仓库管理关系表 (仓库ID, 存储物品ID, 管理员ID, 数量)，且有一个管理员只在一个仓库工作；一个仓库可以存储多种物品。 这个数据库表中存在如下决定关系： (仓库ID, 存储物品ID) →(管理员ID, 数量) (管理员ID, 存储物品ID) → (仓库ID, 数量) 所以，(仓库ID, 存储物品ID)和(管理员ID, 存储物品ID)都是StorehouseManage的候选关键字，表中的唯一非关键字段为数量，它是符合第三范式的。但是，由于存在如下决定关系： (仓库ID) → (管理员ID) (管理员ID) → (仓库ID) 即存在关键字段决定关键字段的情况，所以其不符合BCNF范式。它会出现如下异常情况： (1) 删除异常： 当仓库被清空后，所有”存储物品ID”和”数量”信息被删除的同时，”仓库ID”和”管理员ID”信息也被删除了。 (2) 插入异常： 当仓库没有存储任何物品时，无法给仓库分配管理员。 (3) 更新异常： 如果仓库换了管理员，则表中所有行的管理员ID都要修改。 把仓库管理关系表分解为二个关系表： 仓库管理：StorehouseManage(仓库ID, 管理员ID)； 仓库：Storehouse(仓库ID, 存储物品ID, 数量)。 这样的数据库表是符合BCNF范式的，消除了删除异常、插入异常和更新异常。 一般，一个数据库设计符合3NF或BCNF就可以了。在BC范式以上还有第四范式、第五范式。 第四范式：要求把同一表内的多对多关系删除第五范式：从最终结构重新建立原始结构其实数据库设计范式这方面重点掌握的就是1NF、2NF、3NF、BCNF 四种范式之间存在如下关系： 这里主要区别3NF和BCNF，一句话就是3NF是要满足不存在非主属性对候选码的传递函数依赖，BCNF是要满足不存在任一属性（包含非主属性和主属性）对候选码的传递函数依赖。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql的优化问题]]></title>
    <url>%2F2019%2F06%2F26%2Fmysql-optimize%2F</url>
    <content type="text"><![CDATA[MySQL表设计原则表结构设计满足三大范式三大范式◆ 第一范式（1NF）：强调的是列的原子性，即列不能够再分成其他几列。 ◆ 第二范式（2NF）：首先是 1NF，另外包含两部分内容，一是表必须有一个主键；二是没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。 ◆ 第三范式（3NF）：首先是 2NF，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。 第二范式（2NF）和第三范式（3NF）的概念很容易混淆，区分它们的关键点在于 2NF：非主键列是否完全依赖于主键，还是依赖于主键的一部分； 3NF：非主键列是直接依赖于主键，还是直接依赖于非主键列。 范式的优点 范式化的数据库更新起来更加快； 范式化之后，只有很少的重复数据，只需要修改更少的数据； 范式化的表更小，可以在内存中执行； 很少的冗余数据，在查询的时候需要更少的distinct或者group by语句。 范式的缺点范式化的表，在查询的时候经常需要很多的关联，因为单独一个表内不存在冗余和重复数据。这导致，稍微复杂一些的查询语句在查询范式的schema上都可能需要较多次的关联。这会增加让查询的代价，也可能使一些索引策略无效。因为范式化将列存放在不同的表中，而这些列在一个表中本可以属于同一个索引。 适度冗余， 让query尽量减少join虽然optimizer会对query进行一定的优化，但有时候遇见复杂的join，优化效果并不令人满意，再加上本来join的性能开销，所以需要尽量的减少join，而需要通过冗余来实现。比如：有两个数据表分别为用户信息表和用户发帖表，在展示发帖列表时，如果没有冗余的话，两个表要join以取得想要的发帖信息和用户昵称，但如果考虑冗余，用户昵称占用空间不大，如果在发帖表里增加这么一个字段的话，在展示列表时就不用做join操作了，性能会得到很大的改善。 但冗余也会带来一些问题，比如在发帖表里增加了用户昵称字段，就得维护两份用户昵称数据，为了保证数据的一致性，在用户昵称发生改变时，就得向两个表做更新操作，程序中就得做更多的处理。但相比的话，更新频率显然不及查询频率，这样通过增加少量的更新操作会换来更大的性能提升，这也是在项目中经常采用的优化手段。 大字段垂直分拆所谓的大字段，没有一个很严格的标准，常用的是如果一个字段的大小占整条记录的50%以上，我们就视为其为大字段。大字段垂直分拆相比适度冗余是完全相反的操作，适度冗余是将别的表中的字段放进一个表中，而大字段分拆是将自身的大字段拆分出去放进另一个表中。 这两个优化策略貌似是矛盾的，但要根据具体的应用场景来分析，适度冗余是因为在频率较高的查询中要使用该字段，为了减少join的性能开销。而大字段垂直分拆是将在查询中不使用的大字段拿出去，虽然不使用该字段但mysql在查询时并不是只需要访问需要查询的那几个字段，而是读取所有的字段，所以即使不使用字段，mysql也会读取该字段，为了节省IO开销，所以将查询中不常使用的大字段分拆出去。比如：拿博客系统为例，常用的作法是将博客内容从博客列表里分拆出去建立一个博客内容表，因为访问博客列表时并不需要读取博客内容，分拆出去之后，访问博客列表的性能将会大大的提升。但同时访问博客内容时就得做一次join操作了，性能对比的话，join操作两个表是一对一的关系，性能开销会很低。 大表水平分拆举例说明：在一个论坛系统里，管理员经常会发一些帖子，这些帖子要求在每个分类列表里都要置顶。 设计方案一：在发帖表里增加一列用来标示是否是管理员发帖，这样在每个分类列表展示时就需要对发帖表查询两次，一次是置顶帖，一次是普通帖，然后将两次结果合并。如果发帖表内容较大时，查询置顶帖的性能开销会比较大。 设计方案二：将置顶帖存放在一个单独的置顶表里。因为置顶帖数量相比会很少，但访问频率很高，这样从发帖表里分拆开来，访问的性能开销会少很多。 选择合适的数据类型要选择合适的数据类型必须要先了解不同数据类型间的差异。 数字类型有整数类型和浮点数类型，还有一类是通过二进制格式以字符串来存放的数字类型，如DECIMAL(size,d)，其存放长度主要通过定义的size决定，size定义多大，则实际存放就有多长。默认的size为10，d为0。这种类型的存放长度较长而且完全可以用整形来代替实现，所以不推荐使用。 时间类型主要使用DATE，DATETIME和TIMESTAMP三种类型，TIMESTAMP占用存储空间最少，只要4个字节，其它两种类型都要占用8个字节。从存储内容来看，TIMESTAMP只能存储1970年之后的时间，另外两种都能存储从1001开始的时间。 特别要说明的是varchar类型，varchar(size)，在mysql5.0.3之前size表示的是字节数，mysql5.0.3之后size表示的是字符数。这里我们只关注mysql5.0.3之后的表示，size表示的字符数最大限制和字符集有关，如果是gbk编码，最大长度为(65535-1-2)/2=32766，减1的原因是实际行存储从第二个字节开始，减2的原因是varchar头部的2个字节表示长度，除2因为是gbk编码；如果是utf8编码，最大长度为(65535-1-2)/3=21844。 如果数据量一样，但数据类型更小的话，数据存放同样的数据就会占用更少的空间，这样检索同样的数据所带来的IO消耗自然会降低，性能也就很自然的得到提升。此外，mysql对不同类型的数据，处理方式也不一样，比如在运算或者排序操作中，越简单的数据类型操作性能越高，所以对于要频繁进行运算或者排序的字段尽量选择简单的数据类型。 参考：mysql表设计原则]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis内部数据结构详解——skiplist]]></title>
    <url>%2F2019%2F06%2F26%2Fredis-skiplist%2F</url>
    <content type="text"><![CDATA[本文是《Redis内部数据结构详解》系列的第六篇。在本文中，我们围绕一个Redis的内部数据结构——skiplist展开讨论。 Redis里面使用skiplist是为了实现sorted set这种对外的数据结构。sorted set提供的操作非常丰富，可以满足非常多的应用场景。这也意味着，sorted set相对来说实现比较复杂。同时，skiplist这种数据结构对于很多人来说都比较陌生，因为大部分学校里的算法课都没有对这种数据结构进行过详细的介绍。因此，为了介绍得足够清楚，本文会比这个系列的其它几篇花费更多的篇幅。 我们将大体分成三个部分进行介绍： 介绍经典的skiplist数据结构，并进行简单的算法分析。这一部分的介绍，与Redis没有直接关系。我会尝试尽量使用通俗易懂的语言进行描述。 讨论Redis里的skiplist的具体实现。为了支持sorted set本身的一些要求，在经典的skiplist基础上，Redis里的相应实现做了若干改动。 讨论sorted set是如何在skiplist, dict和ziplist基础上构建起来的。 我们在讨论中还会涉及到两个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 我们在讨论中会详细解释这两个配置的含义。 注：本文讨论的代码实现基于Redis源码的3.2分支。 skiplist数据结构简介skiplist本质上也是一种查找结构，用于解决算法中的查找问题（Searching），即根据给定的key，快速查到它所在的位置（或者对应的value）。 我们在《Redis内部数据结构详解》系列的第一篇中介绍dict的时候，曾经讨论过：一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。但skiplist却比较特殊，它没法归属到这两大类里面。 这种数据结构是由William Pugh发明的，最早出现于他在1990年发表的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。对细节感兴趣的同学可以下载论文原文来阅读。 skiplist，顾名思义，首先它是一个list。实际上，它是在有序链表的基础上发展起来的。 我们先来看一个有序链表，如下图（最左侧的灰色节点表示一个空的头结点）： 在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。 假如我们每相邻两个节点增加一个指针，让指针指向下下个节点，如下图： 这样所有新增加的指针连成了一个新的链表，但它包含的节点个数只有原来的一半（上图中是7, 19, 26）。现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如，我们想查找23，查找的路径是沿着下图中标红的指针所指向的方向进行的： 23首先和7比较，再和19比较，比它们都大，继续向后比较。 但23和26比较的时候，比26要小，因此回到下面的链表（原链表），与22比较。 23比22要大，沿下面的指针继续向后和26比较。23比26小，说明待查数据23在原链表中不存在，而且它的插入位置应该在22和26之间。 在这个查找过程中，由于新增加的指针，我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。 利用同样的方式，我们可以在上层新产生的链表上，继续为每相邻的两个节点增加一个指针，从而产生第三层链表。如下图： 在这个新的三层链表结构上，如果我们还是查找23，那么沿着最上层链表首先要比较的是19，发现23比19大，接下来我们就知道只需要到19的后面去继续查找，从而一下子跳过了19前面的所有节点。可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度。 skiplist正是受这种多层链表的想法的启发而设计出来的。实际上，按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到O(log n)。但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新蜕化成O(n)。删除数据也有同样的问题。 skiplist为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程： 从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。 根据上图中的skiplist结构，我们很容易理解这种数据结构的名字的由来。skiplist，翻译成中文，可以翻译成“跳表”或“跳跃表”，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度。 刚刚创建的这个skiplist总共包含4层链表，现在假设我们在它里面依然查找23，下图给出了查找路径： 需要注意的是，前面演示的各个节点的插入过程，实际上在插入之前也要先经历一个类似的查找过程，在确定插入位置后，再完成插入操作。 至此，skiplist的查找和插入操作，我们已经很清楚了。而删除操作与插入操作类似，我们也很容易想象出来。这些操作我们也应该能很容易地用代码实现出来。 当然，实际应用中的skiplist每个节点应该包含key和value两部分。前面的描述中我们没有具体区分key和value，但实际上列表中是按照key进行排序的，查找过程也是根据key在比较。 但是，如果你是第一次接触skiplist，那么一定会产生一个疑问：节点插入时随机出一个层数，仅仅依靠这样一个简单的随机数操作而构建出来的多层链表结构，能保证它有一个良好的查找性能吗？为了回答这个疑问，我们需要分析skiplist的统计性能。 在分析之前，我们还需要着重指出的是，执行插入操作时计算随机数的过程，是一个很关键的过程，它对skiplist的统计特性有着很重要的影响。这并不是一个普通的服从均匀分布的随机数，它的计算过程如下： 首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）。 如果一个节点有第i层(i&gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的概率为p。 节点最大的层数不允许超过一个最大值，记为MaxLevel。 这个计算随机层数的伪码如下所示： 123456randomLevel() level := 1 // random()返回一个[0...1)的随机数 while random() &lt; p and level &lt; MaxLevel do level := level + 1 return level randomLevel()的伪码中包含两个参数，一个是p，一个是MaxLevel。在Redis的skiplist实现中，这两个参数的取值为： 12p = 1/4MaxLevel = 32 skiplist的算法性能分析在这一部分，我们来简单分析一下skiplist的时间复杂度和空间复杂度，以便对于skiplist的性能有一个直观的了解。如果你不是特别偏执于算法的性能分析，那么可以暂时跳过这一小节的内容。 我们先来计算一下每个节点所包含的平均指针数目（概率期望）。节点包含的指针数目，相当于这个算法在空间上的额外开销(overhead)，可以用来度量空间复杂度。 根据前面randomLevel()的伪码，我们很容易看出，产生越高的节点层数，概率越低。定量的分析如下： 节点层数至少为1。而大于1的节点层数，满足一个概率分布。 节点层数恰好等于1的概率为1-p。 节点层数大于等于2的概率为p，而节点层数恰好等于2的概率为p(1-p)。 节点层数大于等于3的概率为p2，而节点层数恰好等于3的概率为p2(1-p)。 节点层数大于等于4的概率为p3，而节点层数恰好等于4的概率为p3(1-p)。 …… 因此，一个节点的平均层数（也即包含的平均指针数目），计算如下： 现在很容易计算出： 当p=1/2时，每个节点所包含的平均指针数目为2； 当p=1/4时，每个节点所包含的平均指针数目为1.33。这也是Redis里的skiplist实现在空间上的开销。 接下来，为了分析时间复杂度，我们计算一下skiplist的平均查找长度。查找长度指的是查找路径上跨越的跳数，而查找过程中的比较次数就等于查找长度加1。以前面图中标出的查找23的查找路径为例，从左上角的头结点开始，一直到结点22，查找长度为6。 为了计算查找长度，这里我们需要利用一点小技巧。我们注意到，每个节点插入的时候，它的层数是由随机函数randomLevel()计算出来的，而且随机的计算不依赖于其它节点，每次插入过程都是完全独立的。所以，从统计上来说，一个skiplist结构的形成与节点的插入顺序无关。 这样的话，为了计算查找长度，我们可以将查找过程倒过来看，从右下方第1层上最后到达的那个节点开始，沿着查找路径向左向上回溯，类似于爬楼梯的过程。我们假设当回溯到某个节点的时候，它才被插入，这虽然相当于改变了节点的插入顺序，但从统计上不影响整个skiplist的形成结构。 现在假设我们从一个层数为i的节点x出发，需要向左向上攀爬k层。这时我们有两种可能： 如果节点x有第(i+1)层指针，那么我们需要向上走。这种情况概率为p。 如果节点x没有第(i+1)层指针，那么我们需要向左走。这种情况概率为(1-p)。 这两种情形如下图所示： 用C(k)表示向上攀爬k个层级所需要走过的平均查找路径长度（概率期望），那么： 12C(0)=0C(k)=(1-p)×(上图中情况b的查找长度) + p×(上图中情况c的查找长度) 代入，得到一个差分方程并化简： 123C(k)=(1-p)(C(k)+1) + p(C(k-1)+1)C(k)=1/p+C(k-1)C(k)=k/p 这个结果的意思是，我们每爬升1个层级，需要在查找路径上走1/p步。而我们总共需要攀爬的层级数等于整个skiplist的总层数-1。 那么接下来我们需要分析一下当skiplist中有n个节点的时候，它的总层数的概率均值是多少。这个问题直观上比较好理解。根据节点的层数随机算法，容易得出： 第1层链表固定有n个节点； 第2层链表平均有n*p个节点； 第3层链表平均有n*p2个节点； … 所以，从第1层到最高层，各层链表的平均节点数是一个指数递减的等比数列。容易推算出，总层数的均值为log1/pn，而最高层的平均节点数为1/p。 综上，粗略来计算的话，平均查找长度约等于： C(log1/pn-1)=(log1/pn-1)/p 即，平均时间复杂度为O(log n)。 当然，这里的时间复杂度分析还是比较粗略的。比如，沿着查找路径向左向上回溯的时候，可能先到达左侧头结点，然后沿头结点一路向上；还可能先到达最高层的节点，然后沿着最高层链表一路向左。但这些细节不影响平均时间复杂度的最后结果。另外，这里给出的时间复杂度只是一个概率平均值，但实际上计算一个精细的概率分布也是有可能的。详情还请参见William Pugh的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。 skiplist与平衡树、哈希表的比较 skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。 在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 Redis中的skiplist实现在这一部分，我们讨论Redis中的skiplist实现。 在Redis中，skiplist被用于实现暴露给外部的一个数据结构：sorted set。准确地说，sorted set底层不仅仅使用了skiplist，还使用了ziplist和dict。这几个数据结构的关系，我们下一章再讨论。现在，我们先花点时间把sorted set的关键命令看一下。这些命令对于Redis里skiplist的实现，有重要的影响。 sorted set的命令举例sorted set是一个有序的数据集合，对于像类似排行榜这样的应用场景特别适合。 现在我们来看一个例子，用sorted set来存储代数课（algebra）的成绩表。原始数据如下： Alice 87.5 Bob 89.0 Charles 65.5 David 78.0 Emily 93.5 Fred 87.5 这份数据给出了每位同学的名字和分数。下面我们将这份数据存储到sorted set里面去： [ 对于上面的这些命令，我们需要的注意的地方包括： 前面的6个zadd命令，将6位同学的名字和分数(score)都输入到一个key值为algebra的sorted set里面了。注意Alice和Fred的分数相同，都是87.5分。 zrevrank命令查询Alice的排名（命令中的rev表示按照倒序排列，也就是从大到小），返回3。排在Alice前面的分别是Emily、Bob、Fred，而排名(rank)从0开始计数，所以Alice的排名是3。注意，其实Alice和Fred的分数相同，这种情况下sorted set会把分数相同的元素，按照字典顺序来排列。按照倒序，Fred排在了Alice的前面。 zscore命令查询了Charles对应的分数。 zrevrange命令查询了从大到小排名为0~3的4位同学。 zrevrangebyscore命令查询了分数在80.0和90.0之间的所有同学，并按分数从大到小排列。 总结一下，sorted set中的每个元素主要表现出3个属性： 数据本身（在前面的例子中我们把名字存成了数据）。 每个数据对应一个分数(score)。 根据分数大小和数据本身的字典排序，每个数据会产生一个排名(rank)。可以按正序或倒序。 Redis中skiplist实现的特殊性我们简单分析一下前面出现的几个查询命令： zrevrank由数据查询它对应的排名，这在前面介绍的skiplist中并不支持。 zscore由数据查询它对应的分数，这也不是skiplist所支持的。 zrevrange根据一个排名范围，查询排名在这个范围内的数据。这在前面介绍的skiplist中也不支持。 zrevrangebyscore根据分数区间查询数据集合，是一个skiplist所支持的典型的范围查找（score相当于key）。 实际上，Redis中sorted set的实现是这样的： 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个dict + 一个skiplist来实现的。简单来讲，dict用来查询数据到分数的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 这里sorted set的构成我们在下一章还会再详细地讨论。现在我们集中精力来看一下sorted set与skiplist的关系，： zscore的查询，不是由skiplist来提供的，而是由那个dict来提供的。 为了支持排名(rank)，Redis里对skiplist做了扩展，使得根据排名能够快速查到数据，或者根据分数查到数据之后，也同时很容易获得排名。而且，根据排名的查找，时间复杂度也为O(log n)。 zrevrange的查询，是根据排名查数据，由扩展后的skiplist来提供。 zrevrank是先在dict中由数据查到分数，再拿分数到skiplist中去查找，查到后也同时获得了排名。 前述的查询过程，也暗示了各个操作的时间复杂度： zscore只用查询一个dict，所以时间复杂度为O(1) zrevrank, zrevrange, zrevrangebyscore由于要查询skiplist，所以zrevrank的时间复杂度为O(log n)，而zrevrange, zrevrangebyscore的时间复杂度为O(log(n)+M)，其中M是当前查询返回的元素个数。 总结起来，Redis中的skiplist跟前面介绍的经典的skiplist相比，有如下不同： 分数(score)允许重复，即skiplist的key允许重复。这在最开始介绍的经典skiplist中是不允许的。 在比较时，不仅比较分数（相当于skiplist的key），还比较数据本身。在Redis的skiplist实现中，数据本身的内容唯一标识这份数据，而不是由key来唯一标识。另外，当多个元素分数相同的时候，还需要根据数据内容来进字典排序。 第1层链表不是一个单向链表，而是一个双向链表。这是为了方便以倒序方式获取一个范围内的元素。 在skiplist中可以很方便地计算出每个元素的排名(rank)。 skiplist的数据结构定义1234567891011121314151617#define ZSKIPLIST_MAXLEVEL 32#define ZSKIPLIST_P 0.25typedef struct zskiplistNode &#123; robj *obj; double score; struct zskiplistNode *backward; struct zskiplistLevel &#123; struct zskiplistNode *forward; unsigned int span; &#125; level[];&#125; zskiplistNode; typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; unsigned long length; int level;&#125; zskiplist; 这段代码出自server.h，我们来简要分析一下： 开头定义了两个常量，ZSKIPLIST_MAXLEVEL和ZSKIPLIST_P，分别对应我们前面讲到的skiplist的两个参数：一个是MaxLevel，一个是p。 zskiplistNode定义了skiplist的节点结构。 obj字段存放的是节点数据，它的类型是一个string robj。本来一个string robj可能存放的不是sds，而是long型，但zadd命令在将数据插入到skiplist里面之前先进行了解码，所以这里的obj字段里存储的一定是一个sds。有关robj的详情可以参见系列文章的第三篇：《Redis内部数据结构详解(3)——robj》。这样做的目的应该是为了方便在查找的时候对数据进行字典序的比较，而且，skiplist里的数据部分是数字的可能性也比较小。 score字段是数据对应的分数。 backward字段是指向链表前一个节点的指针（前向指针）。节点只有1个前向指针，所以只有第1层链表是一个双向链表。 level[]存放指向各层链表后一个节点的指针（后向指针）。每层对应1个后向指针，用forward字段表示。另外，每个后向指针还对应了一个span值，它表示当前的指针跨越了多少个节点。span用于计算元素排名(rank)，这正是前面我们提到的Redis对于skiplist所做的一个扩展。需要注意的是，level[]是一个柔性数组（flexible array member），因此它占用的内存不在zskiplistNode结构里面，而需要插入节点的时候单独为它分配。也正因为如此，skiplist的每个节点所包含的指针数目才是不固定的，我们前面分析过的结论——skiplist每个节点包含的指针数目平均为1/(1-p)——才能有意义。 zskiplist定义了真正的skiplist结构，它包含： 头指针header和尾指针tail。 链表长度length，即链表包含的节点总数。注意，新创建的skiplist包含一个空的头指针，这个头指针不包含在length计数中。 level表示skiplist的总层数，即所有节点层数的最大值。 下图以前面插入的代数课成绩表为例，展示了Redis中一个skiplist的可能结构： 注意：图中前向指针上面括号中的数字，表示对应的span的值。即当前指针跨越了多少个节点，这个计数不包括指针的起点节点，但包括指针的终点节点。 假设我们在这个skiplist中查找score=89.0的元素（即Bob的成绩数据），在查找路径中，我们会跨域图中标红的指针，这些指针上面的span值累加起来，就得到了Bob的排名(2+2+1)-1=4（减1是因为rank值以0起始）。需要注意这里算的是从小到大的排名，而如果要算从大到小的排名，只需要用skiplist长度减去查找路径上的span累加值，即6-(2+2+1)=1。 可见，在查找skiplist的过程中，通过累加span值的方式，我们就能很容易算出排名。相反，如果指定排名来查找数据（类似zrange和zrevrange那样），也可以不断累加span并时刻保持累加值不超过指定的排名，通过这种方式就能得到一条O(log n)的查找路径。 Redis中的sorted set我们前面提到过，Redis中的sorted set，是在skiplist, dict和ziplist基础上构建起来的: 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个叫zset的数据结构来实现的，这个zset包含一个dict + 一个skiplist。dict用来查询数据到分数(score)的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 在这里我们先来讨论一下前一种情况——基于ziplist实现的sorted set。在本系列前面关于ziplist的文章里，我们介绍过，ziplist就是由很多数据项组成的一大块连续内存。由于sorted set的每一项元素都由数据和score组成，因此，当使用zadd命令插入一个(数据, score)对的时候，底层在相应的ziplist上就插入两个数据项：数据在前，score在后。 ziplist的主要优点是节省内存，但它上面的查找操作只能按顺序查找（可以正序也可以倒序）。因此，sorted set的各个查询操作，就是在ziplist上从前向后（或从后向前）一步步查找，每一步前进两个数据项，跨域一个(数据, score)对。 随着数据的插入，sorted set底层的这个ziplist就可能会转成zset的实现（转换过程详见t_zset.c的zsetConvert）。那么到底插入多少才会转呢？ 还记得本文开头提到的两个Redis配置吗？ 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 这个配置的意思是说，在如下两个条件之一满足的时候，ziplist会转成zset（具体的触发条件参见t_zset.c中的zaddGenericCommand相关代码）： 当sorted set中的元素个数，即(数据, score)对的数目超过128的时候，也就是ziplist数据项超过256的时候。 当sorted set中插入的任意一个数据的长度超过了64的时候。 最后，zset结构的代码定义如下： 1234typedef struct zset &#123; dict *dict; zskiplist *zsl;&#125; zset; Redis为什么用skiplist而不用平衡树？在前面我们对于skiplist和平衡树、哈希表的比较中，其实已经不难看出Redis里使用skiplist而不用平衡树的原因了。现在我们看看，对于这个问题，Redis的作者 @antirez 是怎么说的： There are a few reasons: 1) They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. 2) A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. 3) They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 这段话原文出处： https://news.ycombinator.com/item?id=1171423 这里从内存占用、对范围查找的支持和实现难易程度这三方面总结的原因，我们在前面其实也都涉及到了。 系列下一篇我们将介绍intset，以及它与Redis对外暴露的数据类型set的关系，敬请期待。 其它精选文章： Redis内部数据结构详解(5)——quicklist Redis内部数据结构详解(4)——ziplist Redis内部数据结构详解(3)——robj Redis内部数据结构详解(2)——sds Redis内部数据结构详解(1)——dict 你需要了解深度学习和神经网络这项技术吗？ 技术的正宗与野路子 论人生之转折 编程世界的熵增原理 Android端外推送到底有多烦？ Android和iOS开发中的异步处理（四）——异步任务和队列 用树型模型管理App数字和红点提示 原创文章，转载请注明出处，并包含下面的二维码！否则拒绝转载！http://zhangtielei.com/posts/blog-redis-skiplist.html]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BitMap对海量无重复的整数排序]]></title>
    <url>%2F2019%2F06%2F26%2Fbitmap%2F</url>
    <content type="text"><![CDATA[转载自：bitmap对海量无重复的整数排序 现在有n个无重复的正整数（n 小于10的7次方），如果内存限制在1.5M以内，要求对着n个数进行排序。【编程珠玑第一章题目】 很显然，10的7次方个整数占用的空间为10 ^ 7 * 4字节，大约等于40M，而内存限制为1.5M，因此，无法将所有数字加载到内存，所以快速排序、堆排序等高效的排序算法就没法使用。这里可以使用bitmap方式，用1bit表示一个整数，那么，10^7个整数需要10^7位，也就是大约1.25M空间。 如下是bitmap对无重复整数的排序过程。 一次bitmap就可以将所有数据排完如果每个整数占一位，可以将所有的整数在内存中表示（如上述提到的那样），那么可以直接一次bitmap排序就完成了，时间复杂度为O(n)，空间复杂度为O(n位)。下面分别给出C和C++的bitset方式： C语言方式 下面代码以n = 100为例子；n是海量时，只要每个整数1bit可以一次在内存中表示所有整数的情况下，方法一样，将宏定义N的值改为海量数据的上限（比如10^7）即可： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//位图排序#include &lt;iostream&gt;#include &lt;bitset&gt;#define WIDTHWORD 32 //一个整数的宽度是32bit#define SHIFT 5 #define MASK 0x1F //0x1f == 31#define N 100 //对100个无重复的整数排序using namespace std; //申请一个N位的bitmapint bitmap[1 + N / WIDTHWORD]; //将bitmap的第value设置为1void set(int value) &#123; bitmap[value &gt;&gt; SHIFT] |= (1 &lt;&lt; (value &amp; MASK));&#125; //清除bitmap第value位上的1:设置为0void clear(int value) &#123; bitmap[value &gt;&gt; SHIFT] &amp;= ~(1 &lt;&lt; (value &amp; MASK));&#125; //测试bitmap第value位是否为1int test(int value) &#123; return bitmap[value &gt;&gt; SHIFT] &amp; (1 &lt;&lt; (value &amp; MASK));&#125; int main() &#123; int a[] = &#123;12, 5, 1, 89, 64, 49, 77, 91, 3, 0, 32, 50, 99&#125;; int length = sizeof(a) / sizeof(int); //将bitmap所有位设置为0 for (int i = 0; i &lt; N; ++i) &#123; clear(i); &#125; //bitmap中将待排序数组中值所在的位设置为1 for (int i = 0; i &lt; length; i++) set(a[i]); //输出排序后的结果 for (int i = 0; i &lt; N; ++i) &#123; if (test(i)) cout &lt;&lt; i &lt;&lt; " "; &#125;&#125; 如上代码中：N表示待排序整数的上限，例如本题要求的10^7。那么申请一个N位大小的bitmap： int bitmap[1 + N / WIDTHWORD];设置、清除、测试函数的含义可以参考文章：http://blog.163.com/xb_stone_yinyang/blog/static/2118160372013625112558579/，下面给出这几个函数的简要解释： 对于一个整数value，要将其对应到bitmap中的第value位，如果设置第value位为1呢？ 看设置函数：value &gt;&gt; SHIFT 是找到value在bitmap中对应的是第几个int型数的位置，例如整数100，它对应的是int数组（也就是bitmap）的第 100 &gt;&gt; 5 == 100 / 32 == 3个int型的位置（从0开始计数，每个int型占据32位）；然后再在int数组（也就是bitmap）的第3个位置中寻找需要将第几位设置为1： 1 &lt;&lt; (value &amp; 0x1f) == 1 &lt;&lt; 100 &amp; 31 == 1 &lt;&lt; 4，即要将1左移四位就是要设置为1的那一位；bitmap[value &gt;&gt; SHIFT] |= (1 &lt;&lt; (value &amp; MASK)); 最终完成将bitmap的第100位设置为1。 对于一个整数value，如何将其对应到bitmap中的那位的上的1清除掉呢？ 看清除函数，和设置函数一样，value &gt;&gt; SHIFT 是找到value在bitmap中对应的是第几个整型的位置；然后，1 &lt;&lt; (value &amp; 0x1f)在找到的那个整型的位置中判断要将该字节的哪一位设置为0；bitmap[value &gt;&gt; SHIFT] &amp;= ~(1 &lt;&lt; (value &amp; MASK));完成最终清除工作。 对于一个整数value，如何测试在bitmap中是否包含该数，也就是bitmap中第value位上是否为1？ 也是先找到value对应bitmap中第几个整型位置，然后在该位置中找到对应的位，再看该位上是否为1，为1表示bitmap中包含value。 程序排序结果： 使用C++的bitset1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;bitset&gt;#define N 100using namespace std; int main() &#123; int a[] = &#123;12, 5, 1, 89, 64, 49, 77, 91, 3, 0, 32, 50, 99&#125;; int length = sizeof(a) / sizeof(int); //直接使用C++bitset，申请Nbit的空间，每一位均设置为0 bitset&lt;N&gt; bitmap; //遍历待排序数组，将bitmap中对应位设置为1 for (int i = 0; i &lt; length; i++) bitmap.set(a[i], 1); //输入排序结果 for (int i = 0; i &lt; N; ++i) &#123; if (bitmap[i]) cout &lt;&lt; i &lt;&lt; " "; &#125;&#125; 需要多次bitmap排序 如果上限N更大或者进一步限制内存大小（例如，将内存限制在0.5M之内），那么一次bitmap就不能将所有数据排序。需要多次bitmap排序。 例如上面排序小于100的一些数，我们上面的一次bitmap，是申请100位的bitmap；但是，如果限制我们只能使用30位bitmap，那么就需要排序100 / 30 + 1次：第一次排序0 ~ 29之间的数，第二次排序30 ~ 59之间的数，第三次排序60 ~ 89之间的数，第四次排序90 ~ 100之间的数。如果是k次bitmap排序，那么时间复杂度为O(kn)，空间开销为O(n / k 位). 12345678910111213141516171819202122232425262728293031下面只给出C++方式，C方式类似：```cint main() &#123; int a[] = &#123;12, 5, 1, 89, 64, 49, 77, 91, 3, 0, 32, 50, 99&#125;; int length = sizeof(a) / sizeof(int); //假设还是有小于100的不重复整数需要排序，但是 //不能申请100位空间，只能申请30位空间，那么，需要 //排序的次数如下： int sort_times = N / 30 + 1; //那么，第一趟先排序0-29，第二趟排序30-59， //第三趟排序60-89，第四趟排序剩下的 bitset&lt;30&gt; bitmap; //只能申请30位的bitmap for (int times = 0; times &lt; sort_times; ++times) &#123; //一共进行四趟排序 bitmap.reset(); //记得每次排序前将bitmap清空为0 for (int i = 0; i &lt; length; i++) &#123; if (a[i] &gt;= 30 * times &amp;&amp; a[i] &lt; 30 * (times + 1)) bitmap.set(a[i] - 30 * times); &#125; for (int i = 0; i &lt; 30; ++i) &#123; if (bitmap[i]) cout &lt;&lt; i + 30 * times &lt;&lt; " "; &#125; &#125;&#125; 如果每个整数最多出现m次，如何排序?上述两部分讨论的是如果整数是不重复时的排序，那么，如果海量整数，每个整数允许重复，但是重复次数不超过m（例如m == 10），如何排序？方法：如果每个整数重复出现次数不超过10次，那么，可以用4位表示一个整数，用这四位统计该数出现次数，然后排序后输出该数时，输出m次即可。 除了排序，bitmap的其他用途如上，bitmap可以用于不重复正整数排序，那么，bitmap其他用途：1、找出不重复数：在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数。2、判断某数是否存在于海量整数中：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>BitMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中创建实例对象的方式]]></title>
    <url>%2F2019%2F06%2F26%2Fjava-create-instance%2F</url>
    <content type="text"><![CDATA[1、关键字 new。工厂模式是对这种方式的包装； 2、类实现克隆接口，克隆一个实例。原型模式是一个应用实例； 3、用该类的加载器，newInstance。java的反射，反射使用实例：Spring的依赖注入、切面编程中动态代理 4、sun.misc.Unsafe类，allocateInstance方法创建一个实例。（Java官方也不建议直接使用的Unsafe类，据说Oracle正在计划从Java 9中去掉Unsafe类） 5、实现序列化接口的类，通过IO流反序列化读取一个类，获得实例。 参考：Java创建类的实例的几种方法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis中基本数据类型与内部存储结构]]></title>
    <url>%2F2019%2F06%2F26%2Fredis-storage-structure%2F</url>
    <content type="text"><![CDATA[本文转载自：Redis-基本数据类型与内部存储结构 概览Redis是典型的Key-Value类型数据库，Key为字符类型，Value的类型常用的为五种类型：String、Hash 、List 、 Set 、 Ordered Set Redis内部内存管理 redisObject 核心对象 Redis 内部使用一个 redisObject 对象来表示所有的 key 和 value。 type ：代表一个 value 对象具体是何种数据类型。 encoding ：是不同数据类型在 redis 内部的存储方式，比如：type=string 代表 value 存储的是一个普通字符串，那么对应的 encoding 可以是 raw 或者是 int，如果是 int 则代表实际 redis 内部是按数值型类存储和表示这个字符串的，当然前提是这个字符串本身可以用数值表示，比如：”123” “456”这样的字符串。 vm 字段：只有打开了 Redis 的虚拟内存功能，此字段才会真正的分配内存，该功能默认是关闭状态的。 Redis 使用 redisObject 来表示所有的 key/value 数据是比较浪费内存的，当然这些内存管理成本的付出主要也是为了给 Redis 不同数据类型提供一个统一的管理接口，实际作者也提供了多种方法帮助我们尽量节省内存使用。 Key（键值） 官网Key链接：https://redis.io/commands#generic 过期删除过期数据的清除从来不容易，为每一条key设置一个timer，到点立刻删除的消耗太大，每秒遍历所有数据消耗也大，Redis使用了一种相对务实的做法： 当client主动访问key会先对key进行超时判断，过时的key会立刻删除。 如果clien永远都不再get那条key呢？ 它会在Master的后台，每秒10次的执行如下操作： 随机选取100个key校验是否过期，如果有25个以上的key过期了，立刻额外随机选取下100个key(不计算在10次之内)。可见，如果过期的key不多，它最多每秒回收200条左右，如果有超过25%的key过期了，它就会做得更多，但只要key不被主动get，它占用的内存什么时候最终被清理掉只有天知道。 常用操作 Key的长度限制：Key的最大长度不能超过1024字节，在实际开发时不要超过这个长度，但是Key的命名不能太短，要能唯一标识缓存的对，作者建议按照在关系型数据库中的库表唯一标识字段的方式来命令Key的值，用分号分割不同数据域，用点号作为单词连接。 Key的查询：Keys，返回匹配的key，支持通配符如 “keys a*” 、 “keys a?c”，但不建议在生产环境大数据量下使用。 对Key对应的Value进行的排序：Sort命令对集合按数字或字母顺序排序后返回或另存为list，还可以关联到外部key等。因为复杂度是最高的O(N+Mlog(M))*(N是集合大小，M 为返回元素的数量)，有时会安排到slave上执行。官网链接https://redis.io/commands/sort Key的超时操作：Expire（指定失效的秒数）/ExpireAt（指定失效的时间戳）/Persist（持久化）/TTL（返回还可存活的秒数），关于Key超时的操作。默认以秒为单位，也有p字头的以毫秒为单位的版本 String（字符串类型的Value） 可以是String，也可是是任意的byte[]类型的数组，如图片等。String 在 redis 内部存储默认就是一个字符串，被 redisObject 所引用，当遇到 incr,decr 等操作时会转成数值型进行计算，此时 redisObject 的 encoding 字段为int。 https://redis.io/commands#string 大小限制：最大为512Mb，基本可以存储任意图片啦。 常用命令的时间复杂度为O(1)，读写一样的快。 对String代表的数字进行增减操作（没有指定的Key则设置为0值，然后在进行操作）：Incr/IncrBy/IncrByFloat/Decr/DecrBy（原子性），** 可以用来做计数器，做自增序列，也可以用于限流，令牌桶计数等**。key不存在时会创建并贴心的设原值为0。IncrByFloat专门针对float。。 设置Value的安全性：SetNx命令仅当key不存在时才Set（原子性操作）。可以用来选举Master或做分布式锁：所有Client不断尝试使用SetNx master myName抢注Master，成功的那位不断使用Expire刷新它的过期时间。如果Master倒掉了key就会失效，剩下的节点又会发生新一轮抢夺。SetEx， Set + Expire 的简便写法，p字头版本以毫秒为单位。 获取：GetSet（原子性）， 设置新值，返回旧值。比如一个按小时计算的计数器，可以用GetSet获取计数并重置为0。这种指令在服务端做起来是举手之劳，客户端便方便很多。MGet/MSet/MSetNx， 一次get/set多个key。 其他操作：Append/SetRange/GetRange/StrLen，对文本进行扩展、替换、截取和求长度，只对特定数据格式如字段定长的有用，json就没什么用。 BitMap的用法：GetBit/SetBit/BitOp,与或非/BitCount， BitMap的玩法，比如统计今天的独立访问用户数时，每个注册用户都有一个offset，他今天进来的话就把他那个位设为1，用BitCount就可以得出今天的总人数。 Hash（HashMap，哈希映射表） Redis 的 Hash 实际是内部存储的 Value 为一个 HashMap，并提供了直接存取这个 Map 成员的接口。Hash将对象的各个属性存入Map里，可以只读取/更新对象的某些属性。另外不同的模块可以只更新自己关心的属性而不会互相并发覆盖冲突。 不同程序通过 key（用户 ID） + field（属性标签）就可以并发操作各自关心的属性数据 https://redis.io/commands#hash 实现原理Redis Hash 对应 Value 内部实际就是一个 HashMap，实际这里会有2种不同实现，** 这个 Hash 的成员比较少时 Redis 为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的 HashMap 结构，对应的 value redisObject 的 encoding 为 zipmap，当成员数量增大时会自动转成真正的 HashMap，此时 encoding 为 ht**。一般操作复杂度是O(1)，要同时操作多个field时就是O(N)，N是field的数量。 常用操作 O(1)操作：hget、hset等等 O(n)操作：hgetallRedis 可以直接取到全部的属性数据，但是如果内部 Map 的成员很多，那么涉及到遍历整个内部 Map 的操作， 由于 Redis 单线程模型的缘故，这个遍历操作可能会比较耗时，而另其它客户端的请求完全不响应，这点需要格外注意。 List（双向链表） Redis list 的应用场景非常多，也是 Redis 最重要的数据结构之一，比如 twitter 的关注列表，粉丝列表等都可以用 Redis 的 list 结构来实现，还提供了生产者消费者阻塞模式（B开头的命令,block,表示阻塞），常用于任务队列，消息队列等。 实现方式Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis 内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 用作消息队列中防止数据丢失的解决方法 如果消费者把job给Pop走了又没处理完就死机了怎么办？ 消息生产者保证不丢失 加多一个sorted set，分发的时候同时发到list与sorted set，以分发时间为score，用户把job做完了之后要用ZREM消掉sorted set里的job，并且定时从sorted set中取出超时没有完成的任务，重新放回list。 如果发生重复可以在sorted set中在查询确认一遍，或者将消息的消费接口设计成幂等性。 消息消费者保证不丢失 为每个worker多加一个的list，弹出任务时改用RPopLPush，将job同时放到worker自己的list中，完成时用LREM消掉。如果集群管理(如zookeeper)发现worker已经挂掉，就将worker的list内容重新放回主list 常用操作 复合操作：RPopLPush/ BRPopLPush，弹出来返回给client的同时，把自己又推入另一个list，是原子操作。 RPUSH key value [value …] 将一个或多个值 value 插入到列表 key 的表尾(最右边)。 12&gt; LRANGE KEY_NAME START END&gt; Redis Lrange 返回列表中指定区间内的元素，区间以偏移量 START 和 END(从0开始) 指定。 其中 0 表示列表的第一个元素， 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 按值进行的操作：LRem(按值删除元素)、LInsert(插在某个值的元素的前后)，复杂度是O(N)，N是List长度，因为List的值不唯一，所以要遍历全部元素，而Set只要O(log(N))。 按下表进行操作（下标从0开始，队列从左到右算，下标为负数时则从右到左，-1为右端第一个元素） 时间复杂度为O(N) LSet ：按下标设置元素值。（N为List的长度） LIndex：按下标返回元素。（N为index的值） LTrim：限制List的大小，保留指定范围的元素。（N是移除元素的个数） LRange：返回列表内指定范围下标的元素，常用于分页。（N = start+range） set（HashSet） Set就是HashSet，可以将重复的元素随便放入而Set会自动去重，底层实现也是HashMap，并且 set 提供了判断某个成员是否在一个 set 集合内的重要接口，这个也是 list 所不能提供的。 实现原理set 的内部实现是一个 value 永远为 null 的 HashMap，实际就是通过计算 hash 的方式来快速排重的，这也是 set 能提供判断一个成员是否在集合内的原因。 常用操作 增删改查：SAdd/SRem/SIsMember/SCard/SMove/SMembers等等。除了SMembers都是O(1)。 集合操作：SInter/SInterStore/SUnion/SUnionStore/SDiff/SDiffStore，各种集合操作。交集运算可以用来显示在线好友(在线用户 交集 好友列表)，共同关注(两个用户的关注列表的交集)。O(N)，并集和差集的N是集合大小之和，交集的N是小的那个集合的大小的2倍。 Sorted Set（插入有序Set集合） set 不是自动有序的，而** sorted set 可以通过用户额外提供一个优先级（score）的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么可以选择 sorted set 数据结构，比如 twitter 的 public **timeline 可以以发表时间作为 score 来存储，这样获取时就是自动按时间排好序的。 实现方式 内部使用 HashMap 和跳跃表（SkipList）来保证数据的存储和有序 Sorted Set的实现是HashMap(element-&gt;score, 用于实现ZScore及判断element是否在集合内)，和SkipList(score-&gt;element,按score排序)的混合体。SkipList有点像平衡二叉树那样，不同范围的score被分成一层一层，每层是一个按score排序的链表。 常用操作 ZAdd/ZRem是O(log(N))；ZRangeByScore/ZRemRangeByScore是O(log(N)+M)，N是Set大小，M是结果/操作元素的个数。复杂度的log取对数很关键，可以使，1000万大小的Set，复杂度也只是几十不到。但是，如果一次命中很多元素M很大则复杂度很高。 ZRange/ZRevRange，按排序结果的范围返回元素列表，可以为正数与倒数。 ZRangeByScore/ZRevRangeByScore，按score的范围返回元素，可以为正数与倒数。 ZRemRangeByRank/ZRemRangeByScore，按排序/按score的范围限删除元素。 ZCount，统计按score的范围的元素个数。 ZRank/ZRevRank ，显示某个元素的正/倒序的排名。 ZScore/ZIncrby，显示元素的Score值/增加元素的Score。 ZAdd(Add)/ZRem(Remove)/ZCard(Count)，ZInsertStore(交集)/ZUnionStore(并集)，与Set相比，少了IsMember和差集运算。 Redis使用与内存优化 上面的一些实现上的分析可以看出 redis 实际上的内存管理成本非常高，即占用了过多的内存，属于用空间换时间。作者对这点也非常清楚，所以提供了一系列的参数和手段来控制和节省内存 建议不要开启VM（虚拟内存）选项 VM 选项是作为 Redis 存储超出物理内存数据的一种数据在内存与磁盘换入换出的一个持久化策略，将严重地拖垮系统的运行速度，所以要关闭 VM 功能，请检查你的 redis.conf 文件中 vm-enabled 为 no。 设置最大内存选项最好设置下 redis.conf 中的 maxmemory 选项，该选项是告诉 Redis 当使用了多少物理内存后就开始拒绝后续的写入请求，该参数能很好的保护好你的 Redis 不会因为使用了过多的物理内存而导致 swap，最终严重影响性能甚至崩溃。 一般还需要设置内存饱和回收策略 volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 控制内存使用的参数 Redis 为不同数据类型分别提供了一组参数来控制内存使用 Hash redis.conf 配置文件中下面2项 *hash-max-zipmap-entries 64 * 含义是当 value 这个 Map 内部不超过多少个成员时会采用线性紧凑格式存储，默认是64，即 value 内部有64个以下的成员就是使用线性紧凑存储zipmap，超过该值自动转成真正的 HashMap(ht)。 hash-max-zipmap-value 512 hash-max-zipmap-value 含义是当 value 这个 Map 内部的每个成员值长度不超过 多少字节就会采用线性紧凑存储zipmap来节省空间。 以上2个条件任意一个条件超过设置值都会转换成真正的 HashMap，也就不会再节省内存了，但是也不是越大越好（空间和查改效率需要根据实际情况来权衡） List list-max-ziplist-entries 512list 数据类型多少节点以下会采用去指针的紧凑存储格式ziplist list-max-ziplist-value 64list 数据类型节点值大小小于多少字节会采用紧凑存储格式ziplist。 Set set-max-intset-entries 512set 数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储 Redis内部的优化 Redis 内部实现没有对内存分配方面做过多的优化，在一定程度上会存在内存碎片，不过大多数情况下这个不会成为 Redis 的性能瓶颈。 Redis 缓存了一定范围的常量数字作为资源共享，在很多数据类型是数值型则能极大减少内存开销，默认为1-10000，可以重新编译配置修改源代码中的一行宏定义 REDIS_SHARED_INTEGERS。 总结 根据业务需要选择合适的数据类型，并为不同的应用场景设置相应的紧凑存储参数。 当业务场景不需要数据持久化时，关闭所有的持久化方式可以获得最佳的性能以及最大的内存使用量。 如果需要使用持久化，根据是否可以容忍重启丢失部分数据在快照方式与语句追加方式之间选择其一，不要使用虚拟内存以及 diskstore 方式。 不要让你的 Redis 所在机器物理内存使用超过实际内存总量的3/5。 Redis 的持久化使用了 Buffer IO ，所谓 Buffer IO 是指 Redis 对持久化文件的写入和读取操作都会使用物理内存的 Page Cache，而当 Redis 的持久化文件过大操作系统会进行Swap，这时你的系统就会有内存还有余量但是系统不稳定或者崩溃的风险。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nagle算法]]></title>
    <url>%2F2019%2F06%2F26%2Fnagle%2F</url>
    <content type="text"><![CDATA[Nagle算法用于对缓冲区内的一定数量的消息进行自动连接。该处理过程(称为Nagling)，通过减少必须发送的封包的数量，提高了网络应用程序系统的效率。 Nagle算法的规则 （可参考tcp_output.c文件里tcp_nagle_check函数注释）： 如果包长度达到MSS，则允许发送； MSS是最大分段大小Maxitum Segment Size ，TCP双方建立连接的时候可以协商MTU是最大传输单元Maxitum Transmission Unit 如果该包含有FIN，则允许发送； 设置了TCP_NODELAY选项，则允许发送； 未设置TCP_CORK选项时，若所有发出去的包均被确认，或所有发出去的小数据包(包长度小于MSS)均被确认，则允许发送。 对于规则4，就是说一个TCP连接上最多只能有一个未被确认的小数据包，在该分组的确认到达之前，不能发送其他的小数据包。如果某个小分组的确认被延迟了，那么后续小分组的发送就会相应的延迟。也就是说延迟确认影响的并不只是被延迟确认的那个数据包，而是后续所有的应答包。 Nagle算法的门槛实际上Nagle算法并不是很复杂，他的主要职责是数据的累积，实际上有三个门槛： 缓冲区中的字节数达到了一定量； 等待了一定的时间（一般的Nagle算法都是等待200ms）； 紧急数据发送。 这三个门槛的任何一个达到都必须发送数据了。一般情况下，如果数据流量很大，第二个条件是永远不会起作用的，但当发送小的数据包时，第二个门槛就发挥作用了，防止数据被无限的缓存在缓冲区不是好事情哦。 Nagle算法的选项配置TCP_NODELAY和TCP_CORK都是禁用Nagle算法，只不过NODELAY完全关闭而TCP_CORK完全由自己决定发送时机。Linux文档上说两者不要同时设置。 TCP_NODELAY 选项设置该选项: setsockopt(s,IPPRO_TCP,TCP_NODELAY,(const char*)&amp;on,sizeof(int));读取该选项: getsockopt(s,IPPRO_TCP,TCP_NODELAY,(char*)&amp;on,&amp;optlen); 默认情况下, 发送数据采用Nagle 算法。Nagle 算法是指发送方发送的数据不会立即发出,而是先放在缓冲区, 等缓存区满了再发出. 发送完一批数据后, 会等待接收方对这批数据的回应,然后再发送下一批数据。 Nagle 算法适用于发送方需要发送大批量数据, 并且接收方会及时作出回应的场合, 这种算法通过减少传输数据的次数来提高通信效率。如果发送方持续地发送小批量的数据, 并且接收方不一定会立即发送响应数据, 那么Nagle算法会使发送方运行很慢。 TCP_CORK选项 TCP链接的过程中，默认开启Nagle算法，进行小包发送的优化。优化网络传输，兼顾网络延时和网络拥塞。这个时候可以置位TCP_NODELAY关闭Nagle算法，有数据包的话直接发送保证网络时效性。 在进行大量数据发送的时候可以置位TCP_CORK关闭Nagle算法保证网络利用性。尽可能的进行数据的组包，以最大mtu传输，如果发送的数据包大小过小则如果在0.6 到 0.8S范围内都没能组装成一个MTU时，直接发送。如果发送的数据包大小足够间隔在0.45内时，每次组装一个MTU进行发送。如果间隔大于0.4 到 0.8S则，每过来一个数据包就直接发送。 Nagle组织包的长度是由系统决定的，有时候我们知道我们会每个1分钟产生1字节，共1000字节。如果完全由Nagle算法来发送的话，可能还是会1字节1字节发送[这是一种极端情况，假设返回ACK时间不是很长]。这个时候首先设置TCP_CORK能够阻塞住TCP[尽量阻塞住]，等我们write完1000字节之后，取消TCP_CORK，这个时候就能够将1000字节一次发出。 总结 TCP_CORK选项与TCP_NODELAY一样，是控制Nagle化的。 打开TCP_NODELAY选项，则意味着无论数据包是多么的小，都立即发送（不考虑拥塞窗口）。 如果将TCP连接比喻为一个管道，那TCP_CORK选项的作用就像一个塞子。 设置TCP_CORK选项，就是用塞子塞住管道，而取消TCP_CORK选项，就是将塞子拔掉。当TCP_CORK选项被设置时，TCP链接不会发送任何的小包，即只有当数据量达到MSS时，才会被发送。一般当数据传输完成时，通常需要取消该选项，以防被塞住，这样才可以让不够MSS大小的包能及时发出去。 参考：Nagle算法–TCP缓冲区管理算法]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>Nagle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么单线程的redis这么快]]></title>
    <url>%2F2019%2F06%2F25%2Fwhy-redis-quick%2F</url>
    <content type="text"><![CDATA[纯内存访问，redis将所有数据都放在内存中，内存响应时间大约为100纳秒，这是redis达到每秒万级级别访问的重要基础。完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 非阻塞IO，redis使用epoll作为IO多路复用技术的实现，再加上redis自身事件处理模型将epoll中的链接、读写、关闭都转换为事件，不在网络IO上浪费过多的事件。 123多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快。 单线程避免了线程切换和竟态产生的消耗。避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；1.单线程简化数据结构和算法的实现。2.单线程避免线程切换和竟态产生的消耗。缺点：如果命令执行时间过程，会导致其它命令阻塞。 参考： 单线程的redis为什么达到每秒万级的处理速度？为什么说Redis是单线程的以及Redis为什么这么快！]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MSS和MTU的关系]]></title>
    <url>%2F2019%2F06%2F24%2Fmss-mtu%2F</url>
    <content type="text"><![CDATA[MSS，Maxitum Segment Size 最大分段大小，是TCP协议定义的一个选项。MSS选项用于在TCP连接建立时，收发双方协商通信时每一个报文段所能承载的最大数据长度。建立tcp连接的两端在三次握手时会协商tcp mss大小。 MTU，Maxitum Transmission Unit ，最大传输单元，是指ip层数据包的最大字节数，因为ip数据报要在数据链路层传输，所以也是数据链路层所能传输的的最大字节数，不同链路层的MTU可能不同。 举个例子：如果你要搬家，需要把东西打包，用车运走。这样的情况下，车的大小受路的宽度限制；箱子的大小受车限制；能够搬运的东西的大小受箱子的限制。这时可以将路的宽度理解成第二层的MTU，车的大小理解成第三层的MTU，箱子的大小理解成第四层的MTU，搬运的东西理解成MSS。 MTU= MSS+TCP层头部长度+IP层头部长度 参考: MSS和MTU的关系]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>MSS</tag>
        <tag>MTU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP方法GET和POST的区别]]></title>
    <url>%2F2019%2F06%2F24%2Fget-post%2F</url>
    <content type="text"><![CDATA[当第一次面试的时候被问到说一说get和post有什么区别。当时就说了一大堆很普遍很基础的答案，什么post比get安全，get比post传的少什么的。然而，面试官问，还有呢？ 好家伙，面试完回去百度，整理了网上一堆的get和post的区别。整理如下： 从http标准来看，get比post安全，这里的安全是针对服务器而言的，get用于获取数据，不会引起数据的变化，并且是安全和幂等(多次请求，效果一致)的。而post是有可能引起数据的变化。 从应用角度来看，get参数是放在URL后面(通过URL传递)，参数直接暴露在URL上，所以不能用来传递敏感信息，而post提交的数据会放置在Request body中 get请求会被浏览器主动缓存post不会，除非手动设置。从这里看，post安全性比get高。 GET请求只能进行url编码，而POST支持多种编码方式。 GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。 GET在浏览器回退时是无害的，而POST会再次提交请求。 GET产生的URL地址可以被Bookmark，而POST不可以。 对参数的数据类型，GET只接受ASCII字符，而POST没有限制。 Get方式的提交顶多是1024字节，理论上post没有限制，可以传较大量的数据。 Get一般是获取数据，post是向服务器提交修改的数据。 再然后呢，去面试的时候自然自信满满。结果，还有没有点别的，感觉有种答非所问的赶脚。好吧，回去继续百度深造。结果是这样的： GET和POST是什么?HTTP协议中的两种发送请求的方法。HTTP是什么?HTTP是基于TCP/IP的关于数据如何在万维网中如何通信的协议。HTTP的底层是TCP/IP。所以GET和POST的底层也是TCP/IP，也就是说，GET/POST都是TCP链接 。GET和POST能做的事情是一样一样的。你要给GET加上request body，给POST带上url参数，技术上是完全行的通的。 那所谓的区别呢？ 运输快递需要车辆，而TCP就像车辆，但如果车辆全部按自己的想法走，交通就会瘫痪，所以交通规则HTTP诞生了。HTTP规定了运输方式：get、post、head、options、put、delete等。当执行get请求时，车上贴get标签，货物放在上层运输。如果是post请求，车上贴post标签，货物放在下层运输。当然，get方式也可以把货物放在下层，但是这样是算get还是post呢？所以，HTTP只是个行为准则，而TCP才是GET和POST怎么实现的基本。 而关于传递的参数大小问题，是这样的。 过大的数据量会增加运输成本，超出的部分，概不处理，那还不如乖按照规定走。所以，浏览器通常都会限制url长度在2K个字节，而(大多数)服务器最多处理64K大小的url。如果你用GET服务，在request body偷偷藏了数据，不同服务器的处理方式也是不同的，有些服务器会帮你卸货，读出数据，有些服务器直接忽略，所以，虽然GET可以带request body，也不能保证一定能被接收到哦。 GET和POST还有一个重大区别，简单的说： GET产生一个TCP数据包;POST产生两个TCP数据包。对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200(返回数据);而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok(返回数据) 总结： 语义不同：get获取数据；post提交数据 GET是用来向获取服务器信息的，请求报文传输的信息只是用于描述所需资源的参数，返回的信息才是数据本身；POST是用来向服务器传递数据的，其请求报文传递的信息就是数据本身，返回的报文只是操作的结果。这是GET和POST最重要的区别，没有之一。 get在浏览器回退时是无害的，而post会再次请求 get产生的url地址可以被收藏，而post不会 get请求会被浏览器主动缓存，而post不会，除非手动设置 使用GET，你可能会有意无意就享受到从浏览器到代理到网络服务商再到服务器各个网络组件一层一层的透明缓存；而POST默认是不可缓存的,需要你显式使用缓存相关Header。 get请求只能进行url编码，而post支持多种编码方式 get请求参数会被完整保留在浏览历史记录里，而post中的参数不会被保留 get 请求在url中传送的参数有长度限制，而post没有 使用载荷的POST比使用URI的GET可以传递更多数据。HTTP协议本身没有限制传递信息的最大值，但具体的限制受各个网络组件的实现影响，一般而言，GET的长度限制更短。当然用来获取数据的GET方式，绝大多数情况也用不着传递那么多的数据。 对参数的数据类型，get只接受ascll字符，而post没有限制 get比post更安全，因为参数直接暴露在url上，所以不能用来传递敏感信息 简单来说HTTP的安全(Safe)指的是是否改变服务器资源的状态，即是我们平常说的有无副作用。因为提交数据的目的往往是为了改变服务器状态，所以POST不是安全的(Safe)；而GET是为了获取数据，所以它不应该改变服务器的状态，是安全的(Safe)。HTTP中的安全Safe(副作用)和大多数人平时想的安全Security(例如数据安全)，仅仅是共用一个中文词汇，实质上就是雷峰和雷峰塔的关系，从这个角度上来说GET反而比POST安全多了。为此我建议大家把安全留给更加常用的Security,中文使用一个更加少歧义的说法来描述GET和POST的第二个区别:POST是一个可能有副作用的方法，但GET应是没有副作用的的。如果要保证传输的安全，请使用HTTPS。 get参数通过url传递，poet放在request body中 GET产生一个TCP数据包;POST产生两个TCP数据包。 有些浏览器在发送POST的Ajax请求时会在发送POST请求前先发送一个HEAD，而GET请求则是直接发送。 get是幂等的，post是非幂等的 ‘POST是非幂等的‘就是你提交完表单后，按F5后浏览器会弹框要求你重复确认是否刷新的原因。 参考：关于面试被问到post和get的区别？？？[原][经典面试题]带你深入理解HTTP中GET和POST的区别]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>get</tag>
        <tag>post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的CopyOnWriteArraylist]]></title>
    <url>%2F2019%2F06%2F24%2Fjava-CopyOnWriteArraylist%2F</url>
    <content type="text"><![CDATA[转载自：https://www.xttblog.com/?p=4006 CopyOnWriteArrayList简介CopyOnWriteArrayList是一个并发容器。有很多人称它是线程安全的，我认为这句话不严谨，缺少一个前提条件，那就是非复合场景下操作它是线程安全的。 Copy-On-Write 简称 COW，是一种用于程序设计中的优化策略。其基本思路是，从一开始大家都在共享同一个内容，当某个人想要修改这个内容的时候，才会真正把内容 Copy 出去形成一个新的内容然后再改，这是一种延时懒惰策略。 Java 并发包提供了很多线程安全的集合，有了他们的存在，使得我们在多线程开发下，大大简化了多线程开发的难度，但是如果不知道其中的原理，可能会引发意想不到的问题，所以知道其中的原理还是很有必要的。 CopyOnWriteArrayList原理CopyOnWrite 容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行 Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对 CopyOnWrite 容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以 CopyOnWrite 容器也是一种读写分离的思想，读和写不同的容器。 add 方法下面我们看看它的 add 方法的源码： 1`public` `boolean` `add(E e) &#123;`` ``//1.获得独占锁`` ``final` `ReentrantLock lock = ``this``.lock;`` ``lock.lock();`` ``try` `&#123;`` ``Object[] elements = getArray();``//2.获得Object[]`` ``int` `len = elements.length;``//3.获得elements的长度`` ``Object[] newElements = Arrays.copyOf(elements, len + ``1``);``//4.复制到新的数组`` ``newElements[len] = e;``//5.将add的元素添加到新元素`` ``setArray(newElements);``//6.替换之前的数据`` ``return` `true``;`` ``&#125; ``finally` `&#123;`` ``lock.unlock();``//7.释放独占锁`` ``&#125;``&#125;``final` `Object[] getArray() &#123;`` ``return` `array;``&#125;` CopyOnWrite的名字就是这样来的。在写的时候，先 copy 一个，操作新的对象。然后在覆盖旧的对象，保证 volatile语义。 看完这个源码后，我们来看几个常见的面试题。 CopyOnWriteArrayList 有什么优点？ 读写分离，适合写少读多的场景。使用了独占锁，支持多线程下的并发写。 ** CopyOnWriteArrayList 是如何保证写时线程安全的？** 因为用了ReentrantLock独占锁，保证同时只有一个线程对集合进行修改操作。 CopyOnWrite 怎么理解？ 写时复制。就是在写的时候，先 copy 一个，操作新的对象。然后在覆盖旧的对象，保证 volatile 语义。新数组的长度等于旧数组的长度 + 1。 从 add 方法的源码中你可以看出 CopyOnWriteArrayList 的缺点是什么？ 占用内存，写时 copy 效率低。因为 CopyOnWrite 的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（注意:在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说 200M 左右，那么再写入 100M 数据进去，内存就会占用 300M，那么这个时候很有可能造成频繁的 Yong GC 和 Full GC。 get 方法get 的源码分析。 123456public E get(int index) &#123; return get(getArray(), index);&#125;final Object[] getArray() &#123; return array;&#125; get 方法很简单。但是会出现一个很致命的问题，那就是一致性问题。 当我们获得了 array 后，由于整个 get 方法没有独占锁，所以另外一个线程还可以继续执行修改的操作，比如执行了 remove 的操作，remove 和 add 一样，也会申请独占锁，并且复制出新的数组，删除元素后，替换掉旧的数组。而这一切 get 方法是不知道的，它不知道 array 数组已经发生了天翻地覆的变化。就像微信一样，虽然对方已经把你给删了，但是你不知道。这就是一个一致性问题。 CopyOnWrite 容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用 CopyOnWrite 容器。 set方法set 方法解读。 1234567891011121314151617181920212223public E set(int index, E element) &#123; //(1)获得独占锁 final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray();//(2)获得array E oldValue = get(elements, index);//(3)根据下标,获得旧的元素 if (oldValue != element) &#123;//(4)如果旧的元素不等于新的元素 int len = elements.length;//(5)获得旧数组的长度 Object[] newElements = Arrays.copyOf(elements, len);//(6)复制出新的数组 newElements[index] = element;//(7)修改 setArray(newElements);//(8)替换 &#125; else &#123; //(9)为了保证volatile 语义，即使没有修改，也要替换成新的数组 // Not quite a no-op; ensures volatile write semantics setArray(elements); &#125; return oldValue; &#125; finally &#123; lock.unlock();//(10)释放独占锁 &#125;&#125; 上面的代码，我写的都有注释，相信大家都能看明白。 set 的时候，同样的会获得一个独占锁，来保证写的线程安全。修改操作，实际上操作的是 array 的一个副本，最后才把 array 给替换掉。所以，修改和 add 很相似。set、add、remove 是互斥的。 removeremove 方法解读。 1234567891011121314151617181920212223 final ReentrantLock lock = this.lock;//获得独占锁 lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length;// 旧数组的长度 E oldValue = get(elements, index); int numMoved = len - index - 1; if (numMoved == 0)//判断是否是删除数组尾部的最后一个元素 //则复制出一个长度为【旧数组的长度-1】的新数组 setArray(Arrays.copyOf(elements, len - 1)); else &#123; //如果要删除的元素不是最后一个，则分两次复制，随之替换。 Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); &#125; return oldValue; &#125; finally &#123; lock.unlock(); &#125;&#125; 研究 java 自带的一些数据结构，你会发现设计的都很巧妙。大师就是大师啊。 迭代器迭代器的主要源码如下： 12345678910111213141516171819202122public Iterator&lt;E&gt; iterator() &#123; return new COWIterator&lt;E&gt;(getArray(), 0);&#125;static final class COWIterator&lt;E&gt; implements ListIterator&lt;E&gt; &#123; private final Object[] snapshot; private int cursor; private COWIterator(Object[] elements, int initialCursor) &#123; cursor = initialCursor; snapshot = elements; &#125; // 判断是否还有下一个元素 public boolean hasNext() &#123; return cursor &lt; snapshot.length; &#125; //获取下个元素 @SuppressWarnings("unchecked") public E next() &#123; if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++]; &#125;&#125; 调用iterator 方法获取迭代器，内部会调用 COWIterator 的构造方法，此构造方法有两个参数，第一个参数就是 array 数组，第二个参数是下标，就是 0。随后构造方法中会把 array 数组赋值给snapshot变量。snapshot 是“快照”的意思，如果 Java 基础尚可的话，应该知道数组是引用类型，传递的是指针，如果有其他地方修改了数组，这里应该马上就可以反应出来，那为什么又会是 snapshot这样的命名呢？没错，如果其他线程没有对 CopyOnWriteArrayList 进行增删改的操作，那么 snapshot 就是本身的 array，但是如果其他线程对 CopyOnWriteArrayList 进行了增删改的操作，旧的数组会被新的数组给替换掉，但是 snapshot 还是原来旧的数组的引用。也就是说 当我们使用迭代器便利 CopyOnWriteArrayList 的时候，不能保证拿到的数据是最新的，这也是一致性问题。 CopyOnWriteArrayList 的使用场景通过源码分析，我们看出它的优缺点比较明显，所以使用场景也就比较明显。就是合适读多写少的场景。 CopyOnWriteArrayList 的缺点 由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致young gc或者 full gc。 不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个 set 操作后，读取到数据可能还是旧的，虽然CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求。 由于实际使用中可能没法保证 CopyOnWriteArrayList 到底要放置多少数据，万一数据稍微有点多，每次 add/set 都要重新复制数组，这个代价实在太高昂了。在高性能的互联网应用中，这种操作分分钟引起故障。 CopyOnWriteArrayList 的设计思想 读写分离，读和写分开 最终一致性 使用另外开辟空间的思路，来解决并发冲突]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>CopyOnWriteArrayList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中常用的线程安全的类]]></title>
    <url>%2F2019%2F06%2F24%2Fjava-Thread-safe-classes%2F</url>
    <content type="text"><![CDATA[概念线程安全就是当多线程访问时，采用了加锁的机制；即当一个线程访问该类的某个数据时，会对这个数据进行保护，其他线程不能对其访问，直到该线程读取完之后，其他线程才可以使用。防止出现数据不一致或者数据被污染的情况。线程不安全：就是不提供数据访问时的数据保护，多个线程能够同时操作某个数据，从而出现数据不一致或者数据污染的情况。 对于线程不安全的问题，一般会使用synchronized关键字加锁同步控制。 工作原理：jvm中有一个main memory(主内存)对象，每一个线程也有自己的working memory(工作内存)，一个线程对于一个变量variable进行操作的时候， 都需要在自己的working memory里创建一个copy,操作完之后再写入main memory。 当多个线程操作同一个变量variable，就可能出现不可预知的结果。 而用synchronized的关键是建立一个监控monitor，这个monitor可以是要修改的变量，也可以是其他自己认为合适的对象(方法)，然后通过给这个monitor加锁来实现线程安全，每个线程在获得这个锁之后，要执行完加载load到working memory 到 use &amp;&amp; 指派assign 到 存储store 再到 main memory的过程。才会释放它得到的锁。这样就实现了所谓的线程安全。 线程安全(Thread-safe)的集合对象： Vector HashTable StringBuffer 非线程安全的集合对象： ArrayList LinkedList HashMap HashSet TreeMap TreeSet StringBulider 相关集合对象比较Vector、ArrayList、LinkedList Vector：Vector与ArrayList一样，也是通过数组实现的，不同的是它支持线程的同步，即某一时刻只有一个线程能够写Vector，避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此，访问它比访问ArrayList慢。 ArrayList：a. 当操作是在一列数据的后面添加数据而不是在前面或者中间，并需要随机地访问其中的元素时，使用ArrayList性能比较好。b. ArrayList是最常用的List实现类，内部是通过数组实现的，它允许对元素进行快速随机访问。数组的缺点是每个元素之间不能有间隔，当数组大小不满足时需要增加存储能力，就要讲已经有数组的数据复制到新的存储空间中。当从ArrayList的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。因此，它适合随机查找和遍历，不适合插入和删除。 LinkedList：a. 当对一列数据的前面或者中间执行添加或者删除操作时，并且按照顺序访问其中的元素时，要使用LinkedList。b. LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，他还提供了List接口中没有定义的方法，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。 Vector和ArrayList在使用上非常相似，都可以用来表示一组数量可变的对象应用的集合，并且可以随机的访问其中的元素。 HashTable、HashMap、HashSetHashTable和HashMap采用的存储机制是一样的，不同的是： HashMap：a. 采用数组方式存储key-value构成的Entry对象，无容量限制；b. 基于key hash查找Entry对象存放到数组的位置，对于hash冲突采用链表的方式去解决；c. 在插入元素时，可能会扩大数组的容量，在扩大容量时须要重新计算hash，并复制对象到新的数组中；d. 是非线程安全的；e. 遍历使用的是Iterator迭代器； f. 键和值都允许为null，最多有一个键为null的元素 HashTable：a. 是线程安全的；b. 无论是key还是value都不允许有null值的存在；在HashTable中调用Put方法时，如果key为null，直接抛出NullPointerException异常；c. 遍历使用的是Enumeration列举； HashSet：a. 基于HashMap实现，无容量限制；b. 是非线程安全的；c. 不保证数据的有序； TreeSet、TreeMapTreeSet和TreeMap都是完全基于Map来实现的。 TreeSet：a. 基于TreeMap实现的，支持排序；b. 是非线程安全的； TreeMap：a. 典型的基于红黑树的Map实现，因此它要求一定要有key比较的方法，要么传入Comparator比较器实现，要么key对象实现Comparator接口；b. 是非线程安全的； StringBuffer和StringBuliderStringBuilder与StringBuffer都继承自AbstractStringBuilder类，在AbstractStringBuilder中也是使用字符数组保存字符串。 在执行速度方面的比较：StringBuilder &gt; StringBuffer ； 他们都是字符串变量，是可改变的对象，每当我们用它们对字符串做操作时，实际上是在一个对象上操作的，不像String一样创建一些对象进行操作，所以速度快； StringBuilder：线程非安全的； StringBuffer：线程安全的； 对于String、StringBuffer和StringBulider三者使用的总结：1.如果要操作少量的数据用 = String2.单线程操作字符串缓冲区 下操作大量数据 = StringBuilder3.多线程操作字符串缓冲区 下操作大量数据 = StringBuffer Java中常见的线程安全的类 通过synchronized 关键字给方法加上内置锁来实现线程安全 Timer，TimerTask，Vector，Stack，HashTable，StringBuffer 原子类Atomicxxx—包装类的线程安全类如AtomicLong，AtomicInteger等等 Atomicxxx 是通过Unsafe 类的native(CAS)方法实现线程安全的 BlockingQueue 和BlockingDequeBlockingDeque接口继承了BlockingQueue接口，BlockingQueue 接口的实现类有ArrayBlockingQueue ，LinkedBlockingQueue ，PriorityBlockingQueue, 而BlockingDeque接口的实现类有LinkedBlockingDeque, BlockingQueue和BlockingDeque 都是通过使用定义为final的ReentrantLock作为类属性显式加锁实现同步的 CopyOnWriteArrayList和 CopyOnWriteArraySet CopyOnWriteArraySet的内部实现是在其类内部声明一个final的CopyOnWriteArrayList属性，并在调用其构造函数时实例化该CopyOnWriteArrayList，CopyOnWriteArrayList采用的是显式地加上ReentrantLock实现同步，而CopyOnWriteArrayList(读写分离)容器的线程安全性在于在每次修改时都会创建并重新发布一个新的容器副本，从而实现可变性。 Concurrentxxx最常用的就是ConcurrentHashMap，当然还有ConcurrentSkipListSet和ConcurrentSkipListMap等等。 ConcurrentHashMap使用了一种完全不同的加锁策略来提供更高的并发性和伸缩性。ConcurrentHashMap并不是将每个方法都在同一个锁上同步并使得每次只能有一个线程访问容器，而是使用一种粒度更细的加锁机制——分段锁来实现更大程度的共享。在这种机制中，任意数量的读取线程可以并发访问Map，执行读取操作的线程和执行写入操作的线程可以并发地访问Map，并且一定数量的写入线程可以并发地修改Map，这使得在并发环境下吞吐量更高，而在单线程环境中只损失非常小的性能。 ThreadPoolExecutor ThreadPoolExecutor也是使用了ReentrantLock显式加锁同步。 Collections中的synchronizedCollection(Collection c)方法可将一个集合变为线程安全，其内部通过synchronized关键字加锁同步。 参考Java常见的线程安全的类]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP协议详解]]></title>
    <url>%2F2019%2F06%2F23%2FTCP%2F</url>
    <content type="text"><![CDATA[前言小到基于应用层做网络开发，大到生活中无处不在的网络。我们在享受这个便利的时候，没有人会关心它如此牢固的底层基石是如何搭建的。而这些基石中很重要的一环就是tcp协议。翻看一下“三次握手”和“四次挥手”，本以为这就是tcp了，其实不然。它仅仅解决了连接和关闭的问题，传输的问题才是tcp协议更重要，更难，更复杂的问题。回头看tcp协议的原理，会发现它为了承诺上层数据传输的“可靠”，不知要应对多少网络中复杂多变的情况。简单直白列举一下： 怎么保证数据都是可靠呢？—连接确认！关闭确认！收到数据确认！各种确认！！ 因为网络或其他原因，对方收不到数据怎么办？–超时重试 网络情况千变万化，超时时间怎么确定？–根据RTT动态计算 反反复复，不厌其烦的重试，导致网络拥塞怎么办？—慢启动，拥塞避免，快速重传，快速恢复 发送速度和接收速度不匹配怎么办？–滑动窗口 滑动窗口滑的过程中，他一直告诉我处理不过来了，不让传数据了怎么办？–ZWP 滑动窗口滑的过程中，他处理得慢，就理所当然的每次让我发很少的数据，导致网络利用率很低怎么办？—Nagle 其中任何一个小环节，都凝聚了无数的算法，我们没有能力理解各个算法的实现，但是需要了解下tcp实现者的思路历程。 梳理完所有内容，大概可以知道： tcp提供哪些机制保证了数据传输的可靠性？ tcp连接的“三次握手”和关闭的“四次挥手”流程是怎么样的？ tcp连接和关闭过程中，状态是如何变化的？ tcp头部有哪些字段，分别用来做什么的？ tcp的滑动窗口协议是什么？ 超时重传的机制是什么？ 如何避免传输拥塞？ 概述tcp连接的特点 提供面向连接的，1可靠1的1字节流1服务 为上层应用层提供服务，不关心具体传输的内容是什么，也不知道是二进制流，还是ascii字符。 tcp的可靠性如何保证 分块传送：数据被分割成最合适的数据块（UDP的数据报长度不变） 等待确认：通过定时器等待接收端发送确认请求，收不到确认则重发 确认回复：收到确认后发送确认回复(不是立即发送，通常推迟几分之一秒) 数据校验：保持首部和数据的校验和，检测数据传输过程有无变化 乱序排序：接收端能重排序数据，以正确的顺序交给应用端 重复丢弃：接收端能丢弃重复的数据包 流量缓冲：两端有固定大小的缓冲区（滑动窗口），防止速度不匹配丢数据 tcp的首部格式宏观位置 从应用层-&gt;传输层-&gt;网络层-&gt;链路层，每经过一次都会在报文中增加相应的首部。 TCP数据被封装在IP数据报中 首部格式 tcp首部数据通常包含20个字节（不包括任选字段） 第1-2两个字节：源端口号 第3-4两个字节：目的端口号 源端口号+ip首部中的源ip地址+目的端口号+ip首部中的目的ip地址，唯一的确定了一个tcp连接。对应编码级别的socket。 第5-8四个字节：32位序号。tcp提供全双工服务，两端都有各自的序号。 编号：解决网络包乱序的问题 序号如何生成：不能是固定写死的，否则断网重连时序号重复使用会乱套。tcp基于时钟生成一个序号，每4微秒加一，到2^32-1时又从0开始 第9-12四个字节：32位确认序列号。上次成功收到数据字节序号加1，ack为1才有效。确认号：解决丢包的问题 第13位字节：首部长度。因为任选字段长度可变 后面6bite：保留 随后6bite：标识位。控制各种状态 第15-16两个字节：窗口大小。接收端期望接收的字节数。解决流量控制的问题 第17-18两个字节：校验和。由发送端计算和存储，由接收端校验。解决数据正确性问题 第19-20两个字节：紧急指针 标识位说明 URG：为1时，表示紧急指针有效 ACK：确认标识，连接建立成功后，总为1。为1时确认号有效 PSH：接收方应尽快把这个报文交给应用层 RST：复位标识，重建连接 SYN：建立新连接时，该位为0 FIN：关闭连接标识 tcp选项格式 每个选项开始是1字节kind字段，说明选项的类型 kind为0和1的选项，只占一个字节 其他kind后有一字节len，表示该选项总长度（包括kind和len） kind为11，12，13表示tcp事务 MSS 最长报文大小 最常见的可选字段 MSS只能出现在SYN时传过来（第一次握手和第二次握手时） 指明本端能接收的最大长度的报文段 建立连接时，双方都要发送MSS 如果不发送，默认为536字节 连接的建立与释放连接建立的“三次握手”三次握手流程TCP协议中，主动发起请求的一端称为『客户端』，被动连接的一端称为『服务端』。不管是客户端还是服务端，TCP连接建立完后都能发送和接收数据。起初，服务器和客户端都为CLOSED状态。在通信开始前，双方都得创建各自的传输控制块（TCB）。 服务器创建完TCB后遍进入LISTEN状态，此时准备接收客户端发来的连接请求。 第一次握手客户端向服务端发送连接请求报文段。该报文段的头部中SYN=1，ACK=0，seq=x。请求发送后，客户端便进入SYN-SENT状态。 123PS1：SYN=1，ACK=0表示该报文段为连接请求报文。PS2：x为本次TCP通信的字节流的初始序号。TCP规定：SYN=1的报文段不能有数据部分，但要消耗掉一个序号。 第二次握手服务端收到连接请求报文段后，如果同意连接，则会发送一个应答：SYN=1，ACK=1，seq=y，ack=x+1。该应答发送完成后便进入SYN-RCVD状态。 123PS1：SYN=1，ACK=1表示该报文段为连接同意的应答报文，ACK为1表示ack字段有效。PS2：seq=y表示服务端作为发送者时，发送字节流的初始序号。PS3：ack=x+1表示服务端希望下一个数据报发送序号从x+1开始的字节。 第三次握手当客户端收到连接同意的应答后，还要向服务端发送一个确认报文段，表示：服务端发来的连接同意应答已经成功收到。该报文段的头部为：ACK=1，seq=x+1，ack=y+1。客户端发完这个报文段后便进入ESTABLISHED状态，服务端收到这个应答后也进入ESTABLISHED状态，此时连接的建立完成！ 为什么连接建立需要三次握手，而不是两次握手？防止失效的连接请求报文段被服务端接收，从而产生错误。 1PS：失效的连接请求：若客户端向服务端发送的连接请求丢失，客户端等待应答超时后就会再次发送连接请求，此时，上一个连接请求就是『失效的』。 若建立连接只需两次握手，客户端并没有太大的变化，仍然需要获得服务端的应答后才进入ESTABLISHED状态，而服务端在收到连接请求后就进入ESTABLISHED状态。此时如果网络拥塞，客户端发送的连接请求迟迟到不了服务端，客户端便超时重发请求，如果服务端正确接收并确认应答，双方便开始通信，通信结束后释放连接。此时，如果那个失效的连接请求抵达了服务端，由于只有两次握手，服务端收到请求就会进入ESTABLISHED状态，等待发送数据或主动发送数据。但此时的客户端早已进入CLOSED状态，服务端将会一直等待下去，这样浪费服务端连接资源。 之所以存在 3-way hanshake 的说法，是因为 TCP 是双向通讯协议，作为响应一方(Responder) 要想初始化发送通道，必须也进行一轮 SYN + ACK。由于 SYN ACK 在 TCP 分组头部是两个标识位，因此处于优化目的被合并了。所以达到双方都能进行收发的状态只需要 3 个分组。 在谢希仁著《计算机网络》第四版中讲“三次握手”的目的是“为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误”。在另一部经典的《计算机网络》一书中讲“三次握手”的目的是为了解决“网络中存在延迟的重复分组”的问题。这两种不用的表述其实阐明的是同一个问题。谢希仁版《计算机网络》中的例子是这样的，“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。 连接关闭的“四次挥手”四次挥手流程 TCP连接的释放一共需要四步，因此称为『四次挥手』。我们知道，TCP连接是双向的，因此在四次挥手中，前两次挥手用于断开一个方向的连接，后两次挥手用于断开另一方向的连接。 第一次挥手若A认为数据发送完成，则它需要向B发送连接释放请求。该请求只有报文头，头中携带的主要参数为：FIN=1，seq=u。此时，A将进入FIN-WAIT-1状态。 12PS1：FIN=1表示该报文段是一个连接释放请求。PS2：seq=u，u-1是A向B发送的最后一个字节的序号。 第二次挥手B收到连接释放请求后，会通知相应的应用程序，告诉它A向B这个方向的连接已经释放。此时B进入CLOSE-WAIT状态，并向A发送连接释放的应答，其报文头包含：ACK=1，seq=v，ack=u+1。 123PS1：ACK=1：除TCP连接请求报文段以外，TCP通信过程中所有数据报的ACK都为1，表示应答。PS2：seq=v，v-1是B向A发送的最后一个字节的序号。PS3：ack=u+1表示希望收到从第u+1个字节开始的报文段，并且已经成功接收了前u个字节。 A收到该应答，进入FIN-WAIT-2状态，等待B发送连接释放请求。 第二次挥手完成后，A到B方向的连接已经释放，B不会再接收数据，A也不会再发送数据。但B到A方向的连接仍然存在，B可以继续向A发送数据。 第三次挥手当B向A发完所有数据后，向A发送连接释放请求，请求头：FIN=1，ACK=1，seq=w，ack=u+1。B便进入LAST-ACK状态。 第四次挥手A收到释放请求后，向B发送确认应答，此时A进入TIME-WAIT状态。该状态会持续2MSL时间，若该时间段内没有B的重发请求的话，就进入CLOSED状态，撤销TCB。当B收到确认应答后，也便进入CLOSED状态，撤销TCB。 为什么A要先进入TIME-WAIT状态，等待2MSL时间后才进入CLOSED状态？为了保证B能收到A的确认应答。若A发完确认应答后直接进入CLOSED状态，那么如果该应答丢失，B等待超时后就会重新发送连接释放请求，但此时A已经关闭了，不会作出任何响应，因此B永远无法正常关闭。 time_wait状态 也称为2MSL等待状态，MSL=Maximum Segment LifetIme，报文段最大生存时间，根据不同的tcp实现自行设定。常用值为30s，1min，2min。linux一般为30s。 12345MSL是Maximum Segment Lifetime英文的缩写，中文可以译为“报文最大生存时间”，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为tcp报文（segment）是ip数据报（datagram）的数据部分，具体称谓请参见《数据在网络各层中的称呼》一文，而ip头中有一个TTL域，TTL是time to live的缩写，中文可以译为“生存时间”，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个ip数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减1，当此值为0则数据报将被丢弃，同时发送ICMP报文通知源主机。RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。 2MSL即两倍的MSL，TCP的TIME_WAIT状态也称为2MSL等待状态，当TCP的一端发起主动关闭，在发出最后一个ACK包后，即第3次握手完成后发送了第四次握手的ACK包后就进入了TIME_WAIT状态，必须在此状态上停留两倍的MSL时间，等待2MSL时间主要目的是怕最后一个ACK包对方没收到，那么对方在超时后将重发第三次握手的FIN包，主动关闭端接到重发的FIN包后可以再发一个ACK应答包。在TIME_WAIT状态时两端的端口不能使用，要等到2MSL时间结束才可继续使用。当连接处于2MSL等待阶段时任何迟到的报文段都将被丢弃。不过在实际应用中可以通过设置SO_REUSEADDR选项达到不必等待2MSL时间结束再使用此端口。TTL与MSL是有关系的但不是简单的相等的关系，MSL要大于等于TTL。 主动关闭的一方发送最后一个ack所处的状态 这个状态必须维持2MSL等待时间 复位报文段一个报文段从源地址发往目的地址，只要出现错误，都会发出复位的报文段，首部字段的RST是用于“复位”的。这些错误包括以下情况 端口没有在监听 异常中止：通过发送RST而不是fin来中止连接 同时打开 两个应用程序同时执行主动打开，称为“同时打开“ 这种情况极少发生 两端同时发送SYN，同时进入SYN_SENT状态 打开一条连接而不是两条 要进行四次报文交换过程，“四次握手” 同时关闭 双方同时执行主动关闭 进行四次报文交换 状态和正常关闭不一样 服务器对于并发请求的处理 正等待连接的一端有一个固定长度的队列（长度叫做“积压值”，大多数情况长度为5） 该队列中的连接为：已经完成了三次握手，但还没有被应用层接收（应用层需要等待最后一个ack收到后才知道这个连接） 应用层接收请求的连接，将从该队列中移除 当新的请求到来时，先判断队列情况来决定是否接收这个连接 积压值的含义：tcp监听的端点已经被tcp接收，但是等待应用层接收的最大值。与系统允许的最大连接数，服务器接收的最大并发数无关 数据的传输tcp传输的数据分类 成块数据传输：量大，报文段常常满 交互数据传输：量小，报文段为微小分组，大量微小分组，在广域网传输会增加拥堵的出现 tcp处理的数据包括两类，有不同的特点，需要不同的传输技术 交互数据的传输技术经受时延的确认 概念：tcp收到数据时，并不立马发送ack确认，而是稍后发送 目的：将ack与需要沿该方向发送的数据一起发送，以减少开销 特点：接收方不必确认每一个收到的分组，ACK是累计的，它表示接收方已经正确收到了一直到确认序号-1的所有字节 延时时间：绝大多数为200ms。不能超过500ms Nagle算法 解决什么问题：微小分组导致在广域网出现的拥堵问题 核心：减少了通过广域网传输的小分组数目 原理：要求一个tcp连接上最多只能有一个未被确认的未完成的分组，该分组的确认到达之前，不能发送其他分组。tcp收集这些分组，确认到来之前以一个分组的形式发出去 优点：自适应。确认到达的快，数据发送越快。确认慢，发送更少的组。 使用注意：局域网很少使用该算法。且有些特殊场景需要禁用该算法 成块数据的传输 主要使用滑动窗口协议 滑动窗口协议概述 解决了什么问题：发送方和接收方速率不匹配时，保证可靠传输和包乱序的问题 机制：接收方根据目前缓冲区大小，通知发送方目前能接收的最大值。发送方根据接收方的处理能力来发送数据。通过这种协调机制，防止接收端处理不过来。 窗口大小：接收方发给发送端的这个值称为窗口大小 tcp缓冲区的数据结构 接收端： LastByteRead: 缓冲区读取到的位置 NextByteExpected：收到的连续包的最后一个位置 LastByteRcvd：收到的包的最后一个位置 中间空白区：数据没有到达 发送端： LastByteAcked: 被接收端ack的位置，表示成功发送确认 LastByteSent：发出去了，还没有收到成功确认的Ack LastByteWritten：上层应用正在写的地方 滑动窗口示意图初始时示意图 黑框表示滑动窗口 #1表示收到ack确认的数据 #2表示还没收到ack的数据 #3表示在窗口中还没有发出的（接收方还有空间） #4窗口以外的数据（接收方没空间） 滑动过程示意图 收到36的ack，并发出46-51的字节 拥塞窗口 解决什么问题：发送方发送速度过快，导致中转路由器拥堵的问题 机制：发送方增加一个拥塞窗口（cwnd），每次收到ack，窗口值加1。发送时，取拥塞窗口和接收方发来的窗口大小取最小值发送 起到发送方流量控制的作用 滑动窗口会引发的问题零窗口 如何发生： 接收端处理速度慢，发送端发送速度快。窗口大小慢慢被调为0 如何解决：ZWP技术。发送zwp包给接收方，让接收方ack他的窗口大小。 糊涂窗口综合征 如何发生：接收方太忙，取不完数据，导致发送方越来越小。最后只让发送方传几字节的数据。 缺点：数据比tcp和ip头小太多，网络利用率太低。 如何解决：避免对小的窗口大小做响应。 发送端：前面说到的Nagle算法。 接收端：窗口大小小于某个值，直接ack（0），阻止发送数据。窗口变大后再发。 超时与重传概述 tcp提供可靠的运输层，使用的方法是确认机制。 但是数据和确认都有可能丢失 tcp通过在发送时设置定时器解决这种问题 定时器时间到了还没收到确认，就重传该数据 tcp管理的定时器类型 重传定时器：等待收到确认 坚持定时器：使窗口大小信息保持不断流动 保活定时器：检测空闲连接崩溃或重启 2MSL定时器：检测time_wait状态 超时重传机制背景 接收端给发送端的Ack确认只会确认最后一个连续的包 比如发送1,2,3,4,5共五份数据，接收端收到1,2，于是回ack3，然后收到4（还没收到3），此时tcp不会跳过3直接确认4，否则发送端以为3也收到了。这时你能想到的方法是什么呢？tcp又是怎么处理的呢？ 被动等待的超时重传策略 直观的方法是：接收方不做任何处理，等待发送方超时，然后重传。 缺点：发送端不知道该重发3，还是重发3,4,5 如果发送方如果只发送3：节省宽度，但是慢 如果发送方如果发送3,4,5：快，但是浪费宽带 总之，都在被动等待超时，超时可能很长。所以tcp不采用此方法 主动的快速重传机制概述 名称为：Fast Retransmit 不以实际驱动，而以数据驱动重传 实现原理 如果包没有送达，就一直ack最后那个可能被丢的包 发送方连续收到3相同的ack，就重传。不用等待超时 图中发生1,2,3,4,5数据 数据1到达，发生ack2 数据2因为某些原因没有送到 后续收到3的时候，接收端并不是ack4，也不是等待。而是主动ack2 收到4,5同理，一直主动ack2 客户端收到三次ack2，就重传2 2收到后，结合之前收到的3,4,5，直接ack6 快速重传的利弊 解决了被动等待timeout的问题 无法解决重传之前的一个，还是所有的问题。 上面的例子中是重传2，还是重传2,3,4,5。因为并不清楚ack2是谁传回来的 SACK方法概述 为了解决快速重传的缺点，一种更好的SACK重传策略被提出 基于快速重传，同时在tcp头里加了一个SACK的东西 解决了什么问题：客户端应该发送哪些超时包的问题 实现原理 SACK记录一个数值范围，表示哪些数据收到了 linux2.4后默认打开该功能，之前版本需要配置tcp-sack参数 SACK只是一种辅助的方式，发送方不能完全依赖SACK。主要还是依赖ACK和timout Duplicate SACK(D-SACK) 使用SACK标识的范围，还可以知道告知发送方，有哪些数据被重复接收了 可以让发送方知道：是发出去的包丢了，还是回来的ack包丢了 超时时间的确定背景 路由器和网络流量均会变化 所以超时时间肯定不能设置为一个固定值 超时长：重发慢，效率低，性能差 超时短：并没有丢就重发，导致网络拥塞，导致更多超时和更多重发 tcp会追踪这些变化，并相应的动态改变超时时间（RTO） 如何动态改变 每次重传的时间间隔为上次的一倍，直到最大间隔为64s，称为“指数退避” 首次重传到最后放弃重传的时间间隔一般为9min 依赖以往的往返时间计算（RTT）动态的计算 往返时间（RTT）的计算方法 并不是简单的ack时间和发送时间的差值。因为有重传，网络阻塞等各种变化的因素。 而是通过采样多次数值，然后做估算 tcp使用的方法有： 被平滑的RTT估计器 被平滑的均值偏差估计器 重传时间的具体计算 计算往返时间（RTT），保存测量结果 通过测量结果维护一个被平滑的RTT估计器和被平滑的均值偏差估计器 根据这两个估计器计算下一次重传时间 超时重传引发的问题-拥塞为什么重传会引发拥塞 当网络延迟突然增加时，tcp会重传数据 但是过多的重传会导致网络负担加重，从而导致更大的延时和丢包，进入恶性循环 也就是tcp的拥塞问题 解决拥塞-拥塞控制的算法 慢启动：降低分组进入网络的传输速率 拥塞避免：处理丢失分组的算法 快速重传 快速恢复 其他定时器坚持定时器坚持定时器存在的意义 当窗口大小为0时，接收方会发送一个没有数据，只有窗口大小的ack 但是，如果这个ack丢失了会出现什么问题？双方可能因为等待而中止连接 坚持定时器周期性的向接收方查询窗口是否被增大。这些发出的报文段称为窗口探查 坚持定时器启动时机 发送方被通告接收方窗口大小为0时 与超时重传的相同和不同 相同：同样的重传时间间隔 不同：窗口探查从不放弃发送，直到窗口被打开或者进程被关闭。而超时重传到一定时间就放弃发送 保活定时器保活定时器存在的意义 当tcp上没有数据传输时，服务器如何检测到客户端是否还存活 参考：TCP协议详解TCP系列01—概述及协议头格式TCP三次握手 四次挥手什么是2MSL]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8中的HashMap]]></title>
    <url>%2F2019%2F06%2F21%2Fjava8-HashMap%2F</url>
    <content type="text"><![CDATA[本文转载自：Java 8系列之重新认识HashMap 摘要HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap，还可以使用HashTable。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 12简单来说，Hashtable通过给方法加synchronized实现线程安全。而ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素， 每个Segment守护一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。 分段锁可理解为，把整个Map分成了N个Segment，put和get的时候，根据key.hashCode()找到该使用哪个Segment，这个Segment做到了类似于Hashtable的线程安全，分段锁就是说用到哪部分就锁哪部分。ConcurrentHashMap键值不能为null。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现搞清楚HashMap，首先需要知道HashMap是什么，即它的存储结构-字段；其次弄明白它能干什么，即它的功能实现-方法。下面我们针对这两个方面详细展开讲解。 存储结构-字段从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： 1map.put(&quot;美团&quot;,&quot;小美&quot;); 系统将调用”美团”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考http://blog.csdn.net/liuqiyao_01/article/details/14475159，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考http://blog.csdn.net/v_july_v/article/details/6105630。 功能实现-方法HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 1. 确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 1234567891011//方法一：static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;//方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 2. 分析HashMap的put方法HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 3. 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213 void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值&#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125;&#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;，&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： 1234567891011121314151617181920public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， &quot;C&quot;); new Thread(&quot;Thread1&quot;) &#123; public void run() &#123; map.put(7, &quot;B&quot;); System.out.println(map); &#125;; &#125;.start(); new Thread(&quot;Thread2&quot;) &#123; public void run() &#123; map.put(3, &quot;A); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。 线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 JDK1.8与JDK1.7的性能对比HashMap中，如果key经过hash算法得出的数组索引位置全部不相同，即Hash算法非常好，那样的话，getKey方法的时间复杂度就是O(1)，如果Hash算法技术的结果碰撞非常多，假如Hash算极其差，所有的Hash算法结果得出的索引位置一样，那样所有的键值对都集中到一个桶中，或者在一个链表中，或者在一个红黑树中，时间复杂度分别为O(n)和O(lgn)。 鉴于JDK1.8做了多方面的优化，总体性能优于JDK1.7，下面我们从两个方面用例子证明这一点。 Hash较均匀的情况为了便于测试，我们先写一个类Key，如下： 123456789101112131415161718192021222324252627class Key implements Comparable&lt;Key&gt; &#123; private final int value; Key(int value) &#123; this.value = value; &#125; @Override public int compareTo(Key o) &#123; return Integer.compare(this.value, o.value); &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Key key = (Key) o; return value == key.value; &#125; @Override public int hashCode() &#123; return value; &#125;&#125; 这个类复写了equals方法，并且提供了相当好的hashCode函数，任何一个值的hashCode都不会相同，因为直接使用value当做hashcode。为了避免频繁的GC，我将不变的Key实例缓存了起来，而不是一遍一遍的创建它们。代码如下： 123456789101112131415public class Keys &#123; public static final int MAX_KEY = 10_000_000; private static final Key[] KEYS_CACHE = new Key[MAX_KEY]; static &#123; for (int i = 0; i &lt; MAX_KEY; ++i) &#123; KEYS_CACHE[i] = new Key(i); &#125; &#125; public static Key of(int value) &#123; return KEYS_CACHE[value]; &#125;&#125; 现在开始我们的试验，测试需要做的仅仅是，创建不同size的HashMap（1、10、100、……10000000），屏蔽了扩容的情况，代码如下： 1234567891011121314151617181920static void test(int mapSize) &#123; HashMap&lt;Key, Integer&gt; map = new HashMap&lt;Key,Integer&gt;(mapSize); for (int i = 0; i &lt; mapSize; ++i) &#123; map.put(Keys.of(i), i); &#125; long beginTime = System.nanoTime(); //获取纳秒 for (int i = 0; i &lt; mapSize; i++) &#123; map.get(Keys.of(i)); &#125; long endTime = System.nanoTime(); System.out.println(endTime - beginTime); &#125; public static void main(String[] args) &#123; for(int i=10;i&lt;= 1000 0000;i*= 10)&#123; test(i); &#125; &#125; 在测试中会查找不同的值，然后度量花费的时间，为了计算getKey的平均时间，我们遍历所有的get方法，计算总的时间，除以key的数量，计算一个平均值，主要用来比较，绝对值可能会受很多环境因素的影响。结果如下： 通过观测测试结果可知，JDK1.8的性能要高于JDK1.7 15%以上，在某些size的区域上，甚至高于100%。由于Hash算法较均匀，JDK1.8引入的红黑树效果不明显，下面我们看看Hash不均匀的的情况。 Hash极不均匀的情况假设我们有一个非常差的Key，它们所有的实例都返回相同的hashCode值。这是使用HashMap最坏的情况。代码修改如下： 123456789class Key implements Comparable&lt;Key&gt; &#123; //... @Override public int hashCode() &#123; return 1; &#125;&#125; 仍然执行main方法，得出的结果如下表所示： 从表中结果中可知，随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表太长的时候，HashMap会动态的将它替换成一个红黑树，这话的话会将时间复杂度从O(n)降为O(logn)。hash算法均匀和不均匀所花费的时间明显也不相同，这两种情况的相对比较，可以说明一个好的hash算法的重要性。 测试环境：处理器为2.2 GHz Intel Core i7，内存为16 GB 1600 MHz DDR3，SSD硬盘，使用默认的JVM参数，运行在64位的OS X 10.10.1上。 遍历Map对象既然java中的所有map都实现了Map接口，以下方法适用于任何map实现（HashMap, TreeMap, LinkedHashMap, Hashtable, 等等）： 方法一： 在for-each循环中使用entries来遍历这是最常见的并且在大多数情况下也是最可取的遍历方式。在键值都需要时使用。但是如果你遍历的是一个空的map对象，for-each循环将抛出NullPointerException，因此在遍历前你总是应该检查空引用。 1234Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue());&#125; 方法二：在for-each循环中遍历keys或values如果只需要map中的键或者值，你可以通过性能稍好的keySet()或values()来实现遍历，而不是用entrySet()。 123456789Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();//遍历map中的键for (Integer key : map.keySet()) &#123; System.out.println("Key = " + key);&#125;//遍历map中的值for (Integer value : map.values()) &#123; System.out.println("Value = " + value);&#125; 方法三：使用Iterator遍历1234567Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();// 使用泛型Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; entries = map.entrySet().iterator();while (entries.hasNext()) &#123; Map.Entry&lt;Integer, Integer&gt; entry = entries.next(); System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue());&#125;1234567 你也可以在keySet和values上应用同样的方法。该种方式看起来冗余却有其优点所在。首先，在老版本java中这是惟一遍历map的方式。另一个好处是，你可以在遍历时调用iterator.remove()来**删除** entries，另两个方法则不能。 小结(1) 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 (3) HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 (4) JDK1.8引入红黑树大程度优化了HashMap的性能。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统之页面置换算法]]></title>
    <url>%2F2019%2F06%2F21%2Fpage-replacement-algorithm%2F</url>
    <content type="text"><![CDATA[操作系统为何要进行页面置换呢？这是由于操作系统给用户态的应用程序提供了一个虚拟的“大容量”内存空间，而实际的物理内存空间又没有那么大。所以操作系统就就“瞒着”应用程序，只把应用程序中“常用”的数据和代码放在物理内存中，而不常用的数据和代码放在了硬盘这样的存储介质上。如果应用程序访问的是“常用”的数据和代码，那么操作系统已经放置在内存中了，不会出现什么问题。但当应用程序访问它认为应该在内存中的的数据或代码时，如果这些数据或代码不在内存中，会产生缺页异常。这时，操作系统必须能够应对这种缺页异常，即尽快把应用程序当前需要的数据或代码放到内存中来，然后重新执行应用程序产生异常的访存指令。如果在把硬盘中对应的数据或代码调入内存前，操作系统发现物理内存已经没有空闲空间了，这时操作系统必须把它认为“不常用”的页换出到磁盘上去，以腾出内存空闲空间给应用程序所需的数据或代码。 操作系统迟早会碰到没有内存空闲空间而必须要置换出内存中某个“不常用”的页的情况。如何判断内存中哪些是“常用”的页，哪些是“不常用”的页，把“常用”的页保持在内存中，在物理内存空闲空间不够的情况下，把“不常用”的页置换到硬盘上就是页面置换算法着重考虑的问题。容易理解，一个好的页面置换算法会使得缺页异常次数少，也就意味着访问硬盘的次数也少，从而使得应用程序执行的效率就高。 从操作系统原理的角度看，有如下一些页面置换算法： 最优 (Optimal) 页面置换算法：由Belady于1966年提出的一种理论上的算法。其所选择的被淘汰页面，将是以后永不使用的或许是在最长的未来时间内不再被访问的页面。采用最佳置换算法，通常可保证获得最低的缺页率。但由于操作系统其实无法预知一个应用程序在执行过程中访问到的若干页中，哪一个页是未来最长时间内不再被访问的，因而该算法是无法实际实现，但可以此算法作为上限来评价其它的页面置换算法。 先进先出(First In First Out, FIFO)页面置换算法：该算法总是淘汰最先进入内存的页，即选择在内存中驻留时间最久的页予以淘汰。只需把一个应用程序在执行过程中已调入内存的页按先后次序链接成一个队列，队列头指向内存中驻留时间最久的页，队列尾指向最近被调入内存的页(队列尾是插入的位置)。这样需要淘汰页时，从队列头很容易查找到需要淘汰的页。FIFO算法只是在应用程序按线性顺序访问地址空间时效果才好，否则效率不高。因为那些常被访问的页，往往在内存中也停留得最久，结果它们因变“老”而不得不被置换出去。FIFO算法的另一个缺点是，它有一种异常现象（Belady现象），即在增加放置页的页帧的情况下，反而使缺页异常次数增多。 二次机会（Second Chance）页面置换算法：为了克服FIFO算法的缺点，人们对它进行了改进。此算法在页表项（PTE）中设置了一位访问位来表示此页表项对应的页当前是否被访问过。当该页被访问时，CPU中的MMU硬件将把访问位置“1”。当需要找到一个页淘汰时，对于最“老”的那个页面，操作系统去检查它的访问位。如果访问位是0，说明这个页面老且无用，应该立刻淘汰出局；如果访问位是1，这说明该页面曾经被访问过，因此就再给它一次机会。具体来说，先把访问位位清零，然后把这个页面放到队列的尾端，并修改它的装入时间，就好像它刚刚进入系统一样，然后继续往下搜索。二次机会算法的实质就是寻找一个比较古老的、而且从上一次缺页异常以来尚未被访问的页面。如果所有的页面都被访问过了，它就退化为纯粹的FIFO算法。 LRU(Least Recently Used，LRU)页面置换算法： FIFO置换算法性能之所以较差，是因为它所依据的条件是各个页调入内存的时间，而页调入的先后顺序并不能反映页是否“常用”的使用情况。最近最久未使用（LRU）置换算法，是根据页调入内存后的使用情况进行决策页是否“常用”。由于无法预测各页面将来的使用情况，只能利用“最近的过去”作为“最近的将来”的近似，因此，LRU置换算法是选择最近最久未使用的页予以淘汰。该算法赋予每个页一个访问字段，用来记录一个页面自上次被访问以来所经历的时间t,当须淘汰一个页面时，选择现有页面中其t值最大的，即最近最久未使用的页面予以淘汰。 时钟（Clock）页面置换算法：也称最近未使用 (Not Used Recently, NUR) 页面置换算法。虽然二次机会算法是一个较合理的算法，但它经常需要在链表中移动页面，这样做既降低了效率，又是不必要的。一个更好的办法是把各个页面组织成环形链表的形式，类似于一个钟的表面。然后把一个指针指向最古老的那个页面，或者说，最先进来的那个页面。时钟算法和第二次机会算法的功能是完全一样的，只是在具体实现上有所不同。时钟算法需要在页表项（PTE）中设置了一位访问位来表示此页表项对应的页当前是否被访问过。当该页被访问时，CPU中的MMU硬件将把访问位置“1”。然后将内存中所有的页都通过指针链接起来并形成一个循环队列。初始时，设置一个当前指针指向某页（比如最古老的那个页面）。操作系统需要淘汰页时，对当前指针指向的页所对应的页表项进行查询，如果访问位为“0”，则淘汰该页，把它换出到硬盘上；如果访问位为“1”，这将该页表项的此位置“0”，继续访问下一个页。该算法近似地体现了LRU的思想，且易于实现，开销少。但该算法需要硬件支持来设置访问位，且该算法在本质上与FIFO算法是类似的，惟一不同的是在clock算法中跳过了访问位为1的页。 改进的时钟（Enhanced Clock）页面置换算法：在时钟置换算法中，淘汰一个页面时只考虑了页面是否被访问过，但在实际情况中，还应考虑被淘汰的页面是否被修改过。因为淘汰修改过的页面还需要写回硬盘，使得其置换代价大于未修改过的页面。改进的时钟置换算法除了考虑页面的访问情况，还需考虑页面的修改情况。即该算法不但希望淘汰的页面是最近未使用的页，而且还希望被淘汰的页是在主存驻留期间其页面内容未被修改过的。这需要为每一页的对应页表项内容中增加一位引用位和一位修改位。当该页被访问时，CPU中的MMU硬件将把访问位置“1”。当该页被“写”时，CPU中的MMU硬件将把修改位置“1”。这样这两位就存在四种可能的组合情况：（0，0）表示最近未被引用也未被修改，首先选择此页淘汰；（0，1）最近未被使用，但被修改，其次选择；（1，0）最近使用而未修改，再次选择；（1，1）最近使用且修改，最后选择。该算法与时钟算法相比，可进一步减少磁盘的I/O操作次数，但为了查找到一个尽可能适合淘汰的页面，可能需要经过多次扫描，增加了算法本身的执行开销。 以上文章来自：https://github.com/chyyuu/simple_os_book]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的Future和FutureTask]]></title>
    <url>%2F2019%2F06%2F21%2Fjava-future-futuretask%2F</url>
    <content type="text"><![CDATA[Callable 接口线程的创建方式中有两种，一种是实现Runnable接口，另一种是继承Thread，但是这两种方式都有个缺点，那就是在任务执行完成之后无法获取返回结果，于是就有了Callable接口，Future接口与FutureTask类的配和取得返回的结果。 我们先回顾一下java.lang.Runnable接口，就声明了run(),其返回值为void，当然就无法获取结果。 123public interface Runnable &#123; public abstract void run(); &#125; 而Callable的接口定义如下 123public interface Callable&lt;V&gt; &#123; V call() throws Exception; &#125; 该接口声明了一个名称为call()的方法，同时这个方法可以有返回值V，也可以抛出异常。无论是Runnable接口的实现类还是Callable接口的实现类，都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行，ThreadPoolExecutor或ScheduledThreadPoolExecutor都实现了ExcutorService接口，而因此Callable需要和Executor框架中的ExcutorService结合使用，我们先看看ExecutorService提供的方法： &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future&lt;?&gt; submit(Runnable task); 第一个方法：submit提交一个实现Callable接口的任务，并且返回封装了异步计算结果的Future。 第二个方法：submit提交一个实现Runnable接口的任务，并且指定了在调用Future的get方法时返回的result对象。（不常用）第三个方法：submit提交一个实现Runnable接口的任务，并且返回封装了异步计算结果的Future。因此我们只要创建好我们的线程对象（实现Callable接口或者Runnable接口），然后通过上面3个方法提交给线程池去执行即可。还有点要注意的是，除了我们自己实现Callable对象外，我们还可以使用工厂类Executors来把一个Runnable对象包装成Callable对象。Executors工厂类提供的方法如下： 12public static Callable&lt;Object&gt; callable(Runnable task) public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) Future接口Future接口是用来获取异步计算结果的，说白了就是对具体的Runnable或者Callable对象任务执行的结果进行获取(get()),取消(cancel()),判断是否完成等操作。我们看看Future接口的源码： 1234567public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; &#125; 方法解析：V get() ：获取异步执行的结果，如果没有结果可用，此方法会阻塞直到异步计算完成。V get(Long timeout , TimeUnit unit) ：获取异步执行结果，如果没有结果可用，此方法会阻塞，但是会有时间限制，如果阻塞时间超过设定的timeout时间，该方法将抛出异常。 boolean isDone() ：如果任务执行结束，无论是正常结束或是中途取消还是发生异常，都返回true。boolean isCanceller() ：如果任务完成前被取消，则返回true。boolean cancel(boolean mayInterruptRunning)： 如果任务还没开始，执行cancel(…)方法将返回false； 如果任务已经启动，执行cancel(true)方法将以中断执行此任务线程的方式来试图停止任务，如果停止成功，返回true； 当任务已经启动，执行cancel(false)方法将不会对正在执行的任务线程产生影响(让线程正常执行到完成)，此时返回false； 当任务已经完成，执行cancel(…)方法将返回false。 mayInterruptRunning参数表示是否中断执行中的线程。 通过方法分析我们也知道实际上Future提供了3种功能： 能够中断执行中的任务 判断任务是否执行完成 获取任务执行完成后额结果。但是我们必须明白Future只是一个接口，我们无法直接创建对象，因此就需要其实现类FutureTask登场啦。FutureTask类我们先来看看FutureTask的实现1public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; FutureTask类实现了RunnableFuture接口，我们看一下RunnableFuture接口的实现： 123public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run(); &#125; 分析：FutureTask除了实现了Future接口外还实现了Runnable接口（即可以通过Runnable接口实现线程，也可以通过Future取得线程执行完后的结果），因此FutureTask也可以直接提交给Executor执行。 最后我们给出FutureTask的两种构造函数： 1234public FutureTask(Callable&lt;V&gt; callable) &#123; &#125; public FutureTask(Runnable runnable, V result) &#123; &#125; Callable/Future/FutureTask的使用(封装了异步获取结果的Future!!!)通过上面的介绍，我们对Callable，Future，FutureTask都有了比较清晰的了解了，那么它们到底有什么用呢？我们前面说过通过这样的方式去创建线程的话，最大的好处就是能够返回结果，加入有这样的场景，我们现在需要计算一个数据，而这个数据的计算比较耗时，而我们后面的程序也要用到这个数据结果，那么这个时Callable岂不是最好的选择？我们可以开设一个线程去执行计算，而主线程继续做其他事，而后面需要使用到这个数据时，我们再使用Future获取不就可以了吗？下面我们就来编写一个这样的实例 使用Callable+Future获取执行结果Callable实现类如下： com.zejian.Executor; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.concurrent.Callable; /** * @author zejian * @time 2016年3月15日 下午2:02:42 * @decrition Callable接口实例 */ public class CallableDemo implements Callable&lt;Integer&gt; &#123; private int sum; @Override public Integer call() throws Exception &#123; System.out.println(&quot;Callable子线程开始计算啦！&quot;); Thread.sleep(2000); for(int i=0 ;i&lt;5000;i++)&#123; sum=sum+i; &#125; System.out.println(&quot;Callable子线程计算结束！&quot;); return sum; &#125; &#125; Callable执行测试类如下： package com.zejian.Executor; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.Future; /** * @author zejian * @time 2016年3月15日 下午2:05:43 * @decrition callable执行测试类 */ public class CallableTest &#123; public static void main(String[] args) &#123; //创建线程池 ExecutorService es = Executors.newSingleThreadExecutor(); //创建Callable对象任务 CallableDemo calTask=new CallableDemo(); //提交任务并获取执行结果 Future&lt;Integer&gt; future =es.submit(calTask); //关闭线程池 es.shutdown(); try &#123; Thread.sleep(2000); System.out.println(&quot;主线程在执行其他任务&quot;); if(future.get()!=null)&#123; //输出获取到的结果 System.out.println(&quot;future.get()--&gt;&quot;+future.get()); &#125;else&#123; //输出获取到的结果 System.out.println(&quot;future.get()未获取到结果&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;主线程在执行完成&quot;); &#125; &#125; 执行结果： 12345Callable子线程开始计算啦！主线程在执行其他任务Callable子线程计算结束！future.get()--&gt;12497500主线程在执行完成 使用Callable+FutureTask获取执行结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.zejian.Executor; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.Future; import java.util.concurrent.FutureTask; /** * @author zejian * @time 2016年3月15日 下午2:05:43 * @decrition callable执行测试类 */ public class CallableTest &#123; public static void main(String[] args) &#123; // //创建线程池 // ExecutorService es = Executors.newSingleThreadExecutor(); // //创建Callable对象任务 // CallableDemo calTask=new CallableDemo(); // //提交任务并获取执行结果 // Future&lt;Integer&gt; future =es.submit(calTask); // //关闭线程池 // es.shutdown(); //创建线程池 ExecutorService es = Executors.newSingleThreadExecutor(); //创建Callable对象任务 CallableDemo calTask=new CallableDemo(); //创建FutureTask FutureTask&lt;Integer&gt; futureTask=new FutureTask&lt;&gt;(calTask); //执行任务 es.submit(futureTask); //关闭线程池 es.shutdown(); try &#123; Thread.sleep(2000); System.out.println("主线程在执行其他任务"); if(futureTask.get()!=null)&#123; //输出获取到的结果 System.out.println("futureTask.get()--&gt;"+futureTask.get()); &#125;else&#123; //输出获取到的结果 System.out.println("futureTask.get()未获取到结果"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println("主线程在执行完成"); &#125; &#125; 执行结果： 12345Callable子线程开始计算啦！主线程在执行其他任务Callable子线程计算结束！futureTask.get()--&gt;12497500主线程在执行完成 作者：LittleCadet来源：CSDN原文：https://blog.csdn.net/sx1119183530/article/details/79735348版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LRU原理和Redis实现]]></title>
    <url>%2F2019%2F06%2F21%2Flur-redis%2F</url>
    <content type="text"><![CDATA[文章转载自：https://zhuanlan.zhihu.com/p/34133067 LRU简介LRU是内存不够的场景下，淘汰旧内容的一种策略。LRU ,Least Recent Used，淘汰掉最不经常使用的。可以稍微多补充两句，因为计算机体系结构中，最大的最可靠的存储是硬盘，它容量很大，并且内容可以固化，但是访问速度很慢，所以需要把使用的内容载入内存中；内存速度很快，但是容量有限，并且断电后内容会丢失，并且为了进一步提升性能，还有CPU内部的 L1 Cache，L2 Cache等概念。因为速度越快的地方，它的单位成本越高，容量越小，新的内容不断被载入，旧的内容肯定要被淘汰，所以就有这样的使用背景。 LRU原理在一般标准的操作系统教材里，会用下面的方式来演示LRU原理，假设内存只能容纳3个页大小，按照 7 0 1 2 0 3 0 4的次序访问页。假设内存按照栈的方式来描述访问时间，在上面的，是最近访问的，在下面的是，最远时间访问的，LRU就是这样工作的。 但是如果让我们自己设计一个基于 LRU 的缓存，这样设计可能问题很多，这段内存按照访问时间进行了排序，会有大量的内存拷贝操作，所以性能肯定是不能接受的。 那么如何设计一个LRU缓存，使得放入和移除都是O(1) 的，我们需要把访问次序维护起来，但是不能通过内存中的真实排序来反应，有一种方案就是使用双向链表。 基于 HashMap 和 双向链表实现 LRU整体的设计思路是，可以使用 HashMap存储 key，这样可以做到 save 和 get key的时间都是 O(1)，而 HashMap的 Value 指向双向链表实现的 LRU的 Node 节点，如图所示。 LRU 存储是基于双向链表实现的，下面的图演示了它的原理。其中head 代表双向链表的表头，tail 代表尾部。首先预先设置 LRU 的容量，如果存储满了，可以通过 O(1) 的时间淘汰掉双向链表的尾部，每次新增和访问数据，都可以通过 O(1)的效率把新的节点增加到对头，或者把已经存在的节点移动到队头。 下面展示了，预设大小是 3 的，LRU存储的在存储和访问过程中的变化。为了简化图复杂度，图中没有展示 HashMap部分的变化，仅仅演示了上图LRU双向链表的变化。我们对这个LRU缓存的操作序列如下： 123456789101112131415save("key1", 7)save("key2", 0)save("key3", 1)save("key4", 2)get("key2")save("key5", 3)get("key2")save("key6", 4) 相应的 LRU 双向链表部分变化如下： s = save, g = get 总结一下核心操作的步骤: save(key, value)，首先在HashMap 找到 Key对应的节点，如果节点存在，更新节点的值，并把这个节点移动队头。如果不存在，需要构造新的节点，并且尝试把节点塞到队头，如果LRU空间不足，则通过tail 淘汰掉队尾的节点，同时在 HashMap中移除 Key。 get(key)，通过 HashMap 找到LRU链表节点，因为根据LRU 原理，这个节点是最新访问的，所以要把节点插入到队头，然后返回缓存的值。 完整基于 Java 的代码参考如下 123456class DLinkedNode &#123; String key; int value; DLinkedNode pre; DLinkedNode post;&#125; LRU Cache 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class LRUCache &#123; private Hashtable&lt;Integer, DLinkedNode&gt; cache = new Hashtable&lt;Integer, DLinkedNode&gt;(); private int count; private int capacity; private DLinkedNode head, tail; public LRUCache(int capacity) &#123; this.count = 0; this.capacity = capacity; head = new DLinkedNode(); head.pre = null; tail = new DLinkedNode(); tail.post = null; head.post = tail; tail.pre = head; &#125; public int get(String key) &#123; DLinkedNode node = cache.get(key); if(node == null)&#123; return -1; // should raise exception here. &#125; // move the accessed node to the head; this.moveToHead(node); return node.value; &#125; public void set(String key, int value) &#123; DLinkedNode node = cache.get(key); if(node == null)&#123; DLinkedNode newNode = new DLinkedNode(); newNode.key = key; newNode.value = value; this.cache.put(key, newNode); this.addNode(newNode); ++count; if(count &gt; capacity)&#123; // pop the tail DLinkedNode tail = this.popTail(); this.cache.remove(tail.key); --count; &#125; &#125;else&#123; // update the value. node.value = value; this.moveToHead(node); &#125; &#125; /** * Always add the new node right after head; */ private void addNode(DLinkedNode node)&#123; node.pre = head; node.post = head.post; head.post.pre = node; head.post = node; &#125; /** * Remove an existing node from the linked list. */ private void removeNode(DLinkedNode node)&#123; DLinkedNode pre = node.pre; DLinkedNode post = node.post; pre.post = post; post.pre = pre; &#125; /** * Move certain node in between to the head. */ private void moveToHead(DLinkedNode node)&#123; this.removeNode(node); this.addNode(node); &#125; // pop the current tail. private DLinkedNode popTail()&#123; DLinkedNode res = tail.pre; this.removeNode(res); return res; &#125;&#125; Redis 如何实现？这个问题这么问肯定是有坑的，那就是redis肯定不是这样实现的。 Redis的LRU实现如果按照HashMap和双向链表实现，需要额外的存储存放 next 和 prev 指针，牺牲比较大的存储空间，显然是不划算的。所以Redis采用了一个近似的做法，就是随机取出若干个key，然后按照访问时间排序后，淘汰掉最不经常使用的，具体分析如下： 为了支持LRU，Redis 2.8.19中使用了一个全局的LRU时钟，server.lruclock，定义如下， 12#define REDIS_LRU_BITS 24unsigned lruclock:REDIS_LRU_BITS; /* Clock for LRU eviction */ 默认的LRU时钟的分辨率是1秒，可以通过改变REDIS_LRU_CLOCK_RESOLUTION宏的值来改变，Redis会在serverCron()中调用updateLRUClock定期的更新LRU时钟，更新的频率和hz参数有关，默认为100ms一次，如下， 1234567#define REDIS_LRU_CLOCK_MAX ((1&lt;&lt;REDIS_LRU_BITS)-1) /* Max value of obj-&gt;lru */#define REDIS_LRU_CLOCK_RESOLUTION 1 /* LRU clock resolution in seconds */void updateLRUClock(void) &#123; server.lruclock = (server.unixtime / REDIS_LRU_CLOCK_RESOLUTION) &amp; REDIS_LRU_CLOCK_MAX;&#125; server.unixtime是系统当前的unix时间戳，当 lruclock 的值超出REDIS_LRU_CLOCK_MAX时，会从头开始计算，所以在计算一个key的最长没有访问时间时，可能key本身保存的lru访问时间会比当前的lrulock还要大，这个时候需要计算额外时间，如下， 12345678910/* Given an object returns the min number of seconds the object was never * requested, using an approximated LRU algorithm. */unsigned long estimateObjectIdleTime(robj *o) &#123; if (server.lruclock &gt;= o-&gt;lru) &#123; return (server.lruclock - o-&gt;lru) * REDIS_LRU_CLOCK_RESOLUTION; &#125; else &#123; return ((REDIS_LRU_CLOCK_MAX - o-&gt;lru) + server.lruclock) * REDIS_LRU_CLOCK_RESOLUTION; &#125;&#125; Redis支持和LRU相关淘汰策略包括， volatile-lru 设置了过期时间的key参与近似的lru淘汰策略 allkeys-lru 所有的key均参与近似的lru淘汰策略 当进行LRU淘汰时，Redis按如下方式进行的， 123456789101112131415161718192021222324252627...... /* volatile-lru and allkeys-lru policy */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) &#123; for (k = 0; k &lt; server.maxmemory_samples; k++) &#123; sds thiskey; long thisval; robj *o; de = dictGetRandomKey(dict); thiskey = dictGetKey(de); /* When policy is volatile-lru we need an additional lookup * to locate the real key, as dict is set to db-&gt;expires. */ if (server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) de = dictFind(db-&gt;dict, thiskey); o = dictGetVal(de); thisval = estimateObjectIdleTime(o); /* Higher idle time is better candidate for deletion */ if (bestkey == NULL || thisval &gt; bestval) &#123; bestkey = thiskey; bestval = thisval; &#125; &#125; &#125; ...... Redis会基于server.maxmemory_samples配置选取固定数目的key，然后比较它们的lru访问时间，然后淘汰最近最久没有访问的key，maxmemory_samples的值越大，Redis的近似LRU算法就越接近于严格LRU算法，但是相应消耗也变高，对性能有一定影响，样本值默认为5。 总结看来，虽然一个简单的概念，在工业界的产品中，为了追求空间的利用率，也会采用权衡的实现方案。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>LRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的WeakHashMap]]></title>
    <url>%2F2019%2F06%2F21%2Fjava-WeakHashMap%2F</url>
    <content type="text"><![CDATA[什么是WeakHashMap？WeakHashMap 继承于AbstractMap，实现了Map接口。 1public class WeakHashMap&lt;K,V&gt;extends AbstractMap&lt;K,V&gt;implements Map&lt;K,V&gt; 和HashMap一样，WeakHashMap 也是一个散列表，它存储的内容也是键值对(key-value)映射，而且键和值都可以是null。 不过WeakHashMap的键是“弱键”。在 WeakHashMap 中，当某个键不再正常使用时，会被从WeakHashMap中被自动移除。更精确地说，对于一个给定的键，其映射的存在并不阻止垃圾回收器对该键的丢弃，这就使该键成为可终止的，被终止，然后被回收。某个键被终止时，它对应的键值对也就从映射中有效地移除了。 这个“弱键”的原理呢？大致上就是，通过WeakReference和ReferenceQueue实现的。 WeakHashMap的key是“弱键”，即是WeakReference类型的；ReferenceQueue是一个队列，它会保存被GC回收的“弱键”。实现步骤是： 新建WeakHashMap，将“键值对”添加到WeakHashMap中。实际上，WeakHashMap是通过数组table保存Entry(键值对)；每一个Entry实际上是一个单向链表，即Entry是键值对链表。 当某“弱键”不再被其它对象引用，并被GC回收时。在GC回收该“弱键”时，这个“弱键”也同时会被添加到ReferenceQueue(queue)队列中。 当下一次我们需要操作WeakHashMap时，会先同步table和queue。table中保存了全部的键值对，而queue中保存被GC回收的键值对；同步它们，就是删除table中被GC回收的键值对。 这就是“弱键”如何被自动从WeakHashMap中删除的步骤了。 和HashMap一样，WeakHashMap是不同步的。可以使用 Collections.synchronizedMap 方法来构造同步的 WeakHashMap。 在Java8中，当冲突的key变多时，HashMap引入了二叉树（红黑树）进行存储，而WeakHashMap则一直使用链表进行存储。 而WeakHashMap的特点，这里也有总结： 基于Map接口，是一种弱键相连，WeakHashMap里面的键会自动回收 支持 null值和null键。和HashMap有些相似 fast-fail机制 不允许重复 WeakHashMap经常用作缓存 Java里面引用分为4中类型,而在WeakHashMap则主要用到了WeakReference这个引用。 WeakHashMap数据结构1234567java.lang.Object ↳ java.util.AbstractMap&lt;K, V&gt; ↳ java.util.WeakHashMap&lt;K, V&gt;public class WeakHashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123;&#125; WeakHashMap与Map关系如下图：从图中可以看出： WeakHashMap继承于AbstractMap，并且实现了Map接口。 WeakHashMap是哈希表，但是它的键是”弱键”。WeakHashMap中保护几个重要的成员变量：table, size, threshold, loadFactor, modCount, queue。123456table是一个Entry[]数组类型，而Entry实际上就是一个单向链表。哈希表的"key-value键值对"都是存储在Entry数组中的。 size是map的大小，它是map保存的键值对的数量。 threshold是map的阈值，用于判断是否需要调整map的容量。threshold的值="容量*加载因子"。loadFactor就是加载因子。 modCount是用来实现fail-fast机制的queue保存的是“已被GC清除”的“弱引用的键” 关于Entry&lt;K,V&gt;和HashMap一样，WeakHashMap也是用一个Entry实体来构造里面所有的元素的，但是这个Entry却和HashMap的不同，他是弱引用。 1private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; 如上，Entry还继承了WeakReference，所以Entry是个弱引用。何为弱引用呢？就是就是每当进行一次GC,你这个对象就会被清除，当然如果这个对象还存在着软引用或者强引用，就可能不会被清除。 ReferenceQueue queue作用queue是用来存放那些，被jvm清除的entry的引用，因为WeakHashMap使用的是弱引用，所以一旦gc，就会有key键被清除，所以会把entry加入到queue中。在WeakHashMap中加入queue的目的，就是为expungeStaleEntries所用。 12345678Entry(Object key, V value, ReferenceQueue&lt;Object&gt; queue, int hash, Entry&lt;K,V&gt; next) &#123; super(key, queue); this.value = value; this.hash = hash; this.next = next;&#125; 在构造每一个Entry时，都将它与queue绑定，从而一旦被jvm回收，那么这个Entry就会添加到queue中。 expungeStaleEntries方法这个方法里面就仅仅是释放value值。由前面的Entry的构造方法可知， super(key, queue); 传入父类的仅仅是key，所以经过仔细阅读jdk源码开始部分分析后，得出结论，在WeakHashMap中，有jvm回收的，仅仅是Entry的key部分，所以一旦jvm强制回收，那么这些key都会为null，再通过私有的expungeStaleEntries 方法，把value也制null，并且把size--。 首先看代码： 12345678910111213141516171819202122232425262728293031323334353637/** * 从ReferenceQueue中取出过期的entry，从WeakHashMap找到对应的entry，逐一删除 * 注意，只会把value置为null。 */private void expungeStaleEntries() &#123; for (Object x; (x = queue.poll()) != null; ) &#123; //遍历queue synchronized (queue) &#123; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x; int i = indexFor(e.hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; p = prev; while (p != null) &#123; //遍历table[i]所在链表 Entry&lt;K,V&gt; next = p.next; if (p == e) &#123; //queue里面有e，那就删了。 if (prev == e) //e就是当前的p.next table[i] = next; else prev.next = next; // Must not null out e.next; // stale entries may be in use by a HashIterator //置为null，帮助gc。只制null了value。 e.value = null; // Help GC //设置e的value，但是没看到设置e的key。 size--; break; &#125; prev = p; p = next; &#125; &#125; &#125;&#125; 上面代码逻辑为，当在table中找到queue中存在元素时，就把value制空，然后size--。所以在WeakHashMap中，就只有key被回收。下面看一个实例验证。 首先需要了解一点：expungeStaleEntries方法在哪些方面会被调用？经过阅读源码，发现expungeStaleEntries方法只在以下几个地方被调用： 1private Entry&lt;K,V&gt;[] getTable() 里面，而这个getTable则在下列方法被调用: 123456789101112`public V get(Object key)``Entry&lt;K,V&gt; getEntry(Object key)``public V put(K key, V value)``void resize(int newCapacity)``public V remove(Object key)``boolean removeMapping(Object o)``public boolean containsValue(Object value)``private boolean containsNullValue()``public void forEach(BiConsumer&lt;? super K, ? super V&gt; action)``public void replaceAll(BiFunction&lt;? super K, ? super V, ? extends V&gt; function)``public int size()``void resize(int newCapacity)` 应用场景tomcat的源码里，实现缓存时会用到WeakHashMap。 还有 ThreadLocal。 12345678910111213141516171819202122232425262728293031323334353637383940414243package org.apache.tomcat.util.collections;import java.util.Map;import java.util.WeakHashMap;import java.util.concurrent.ConcurrentHashMap;public final class ConcurrentCache&lt;K,V&gt; &#123; private final int size; private final Map&lt;K,V&gt; eden; private final Map&lt;K,V&gt; longterm; public ConcurrentCache(int size) &#123; this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); &#125; public V get(K k) &#123; V v = this.eden.get(k); if (v == null) &#123; synchronized (longterm) &#123; v = this.longterm.get(k); &#125; if (v != null) &#123; this.eden.put(k, v); &#125; &#125; return v; &#125; public void put(K k, V v) &#123; if (this.eden.size() &gt;= size) &#123; synchronized (longterm) &#123; this.longterm.putAll(this.eden); &#125; this.eden.clear(); &#125; this.eden.put(k, v); &#125;&#125; 源码中有eden和longterm的两个map，对jvm堆区有所了解的话，可以猜测出tomcat在这里是使用ConcurrentHashMap和WeakHashMap做了分代的缓存。在put方法里，在插入一个k-v时，先检查eden缓存的容量是不是超了。没有超就直接放入eden缓存，如果超了则锁定longterm将eden中所有的k-v都放入longterm。再将eden清空并插入k-v。在get方法中，也是优先从eden中找对应的v，如果没有则进入longterm缓存中查找，找到后就加入eden缓存并返回。经过这样的设计，相对常用的对象都能在eden缓存中找到，不常用（有可能被销毁的对象）的则进入longterm缓存。而longterm的key的实际对象没有其他引用指向它时，gc就会自动回收heap中该弱引用指向的实际对象，弱引用进入引用队列。longterm调用expungeStaleEntries()方法，遍历引用队列中的弱引用，并清除对应的Entry，不会造成内存空间的浪费。 参考：Java8中的WeakHashMapJava 集合系列13之 WeakHashMap详细介绍(源码解析)和使用示例WeakHashMap的使用场景]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>WeakHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java四种引用类型及其应用场景]]></title>
    <url>%2F2019%2F06%2F20%2Fjava-four-reference%2F</url>
    <content type="text"><![CDATA[java内存管理分为内存分配和内存回收，都不需要程序员负责，垃圾回收的机制主要是看对象是否有引用指向该对象。 java对象的引用包括强引用，软引用，弱引用，虚引用 Java中提供这四种引用类型主要有两个目的： 第一是可以让程序员通过代码的方式决定某些对象的生命周期； 第二是有利于JVM进行垃圾回收。 强引用 - Strong Reference功能：强引用不会被GC回收，并且在java.lang.ref里也没有实际的对应类型，平时工作接触的最多的就是强引用。一个对象如果具有强引用，那么垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。使用场景：我们平常大部分使用的场景都是使用了强引用，比如new创建对象，反射获得一个对象等。 1String str = new String("str"); 这个str就是强引用。 软引用 - Soft Reference功能： 如果一个对象只具有软引用，则内存空间足够时，垃圾回收器就不会去回收它；如果内存空间不足时，就会回收这些对象的内存。软引用还可以和一个引用队列进行关联，如果这个软引用的对象被垃圾回收，那么VM就会将这个软引用加入到关联的队列中去。 使用场景： 软引用可用来实现内存敏感的高速缓存,比如网页缓存、图片缓存等。使用软引用能防止内存泄露，增强程序的健壮性。 SoftReference的特点是它的一个实例保存对一个Java对象的软引用， 该软引用的存在不妨碍垃圾收集线程对该Java对象的回收。也就是说，一旦SoftReference保存了对一个Java对象的软引用后，在垃圾线程对 这个Java对象回收前，SoftReference类所提供的get()方法返回Java对象的强引用。 另外，一旦垃圾线程回收该Java对象之 后，get()方法将返回null。 12MyObject aRef = new MyObject(); SoftReference aSoftRef=new SoftReference(aRef); 此时，对于这个MyObject对象，有两个引用路径，一个是来自SoftReference对象的软引用，一个来自变量aRef的强引用，所以这个MyObject对象是强可及对象。 1aRef = null; 此后，这个MyObject对象成为了软引用对象。如果垃圾收集线程进行内存垃圾收集，并不会因为有一个SoftReference对该对象的引用而始终保留该对象。Java虚拟机的垃圾收集线程对软可及对象和其他一般Java对象进行了区别对待:软可及对象的清理是由垃圾收集线程根据其特定算法按照内存需求决定的。也就是说，垃圾收集线程会在虚拟机抛出OutOfMemoryError之前回收软可及对象，而且虚拟机会尽可能优先回收长时间闲置不用的软可及对象，对那些刚刚构建的或刚刚使用过的“新”软可反对象会被虚拟机尽可能保留。在回收这些对象之前，我们可以通过: 1MyObject anotherRef=(MyObject)aSoftRef.get(); 重新获得对该实例的强引用。而回收之后，调用get()方法就只能得到null了。 使用ReferenceQueue清除失去了软引用对象的SoftReference： 作为一个Java对象，SoftReference对象除了具有保存软引用的特殊性之外，也具有Java对象的一般性。所以，当软可及对象被回收之后，虽然这个SoftReference对象的get()方法返回null,但这个SoftReference对象已经不再具有存在的价值，需要一个适当的清除机制，避免大量SoftReference对象带来的内存泄漏。在java.lang.ref包里还提供了ReferenceQueue。如果在创建SoftReference对象的时候，使用了一个ReferenceQueue对象作为参数提供给SoftReference的构造方法，如: 12ReferenceQueue queue = new ReferenceQueue(); SoftReference ref=new SoftReference(aMyObject, queue); 那么当这个SoftReference所软引用的MyObject被垃圾收集器回收的同时，ref所强引用的SoftReference对象被加入ReferenceQueue。也就是说，ReferenceQueue中保存的对象是Reference对象，而且是已经失去了它所软引用的对象的Reference对象。另外从ReferenceQueue这个名字也可以看出，它是一个队列，当我们调用它的poll()方法的时候，如果这个队列中不是空队列，那么将返回队列前面的那个Reference对象。 在任何时候，我们都可以调用ReferenceQueue的poll()方法来检查是否有它所关心的非强可及对象被回收。如果队列为空，将返回一个null,否则该方法返回队列中前面的一个Reference对象。利用这个方法，我们可以检查哪个SoftReference所软引用的对象已经被回收。于是我们可以把这些失去所软引用的对象的SoftReference对象清除掉。常用的方式为: 1234SoftReference ref = null; while ((ref = (EmployeeRef) q.poll()) != null) &#123; // 清除ref &#125; PS：图片编辑器，视频编辑器之类的软件可以使用这种思路。 弱引用 - Weak Reference功能： 被弱引用关联的对象，在垃圾回收时，如果这个对象只被弱引用关联（没有任何强引用关联它），那么这个对象就会被回收。弱引用和软引用的区别在于，只具有弱引用的对象拥有更短暂的生命周期，在垃圾回收器线程扫描它管辖的内存区域的过程中，一旦发现对象只具有弱引用，不管当前内存空间是否足够，都会回收它的内存。它比软引用的生命周期更短，和软引用相似，它同样可以和引用队列关联，如果被垃圾回收了，就会加入到这个关联队列中。**使用场景：WeakHashMap、ThreadLocal。 下面是使用示例： 1234567891011121314151617181920212223public class test &#123; public static void main(String[] args) &#123; WeakReference&lt;People&gt;reference=new WeakReference&lt;People&gt;(new People("zhouqian",20)); System.out.println(reference.get()); System.gc();//通知GVM回收资源 System.out.println(reference.get()); &#125; &#125; class People&#123; public String name; public int age; public People(String name,int age) &#123; this.name=name; this.age=age; &#125; @Override public String toString() &#123; return "[name:"+name+",age:"+age+"]"; &#125; &#125; 输出结果： [name:zhouqian,age:20]null 第二个输出结果是null，这说明只要JVM进行垃圾回收，被弱引用关联的对象必定会被回收掉。不过要注意的是，这里所说的被弱引用关联的对象是指只有弱引用与之关联，如果存在强引用同时与之关联，则进行垃圾回收时也不会回收该对象（软引用也是如此）。 比如：将代码做一点小更改： 12345678910111213141516171819202122232425import java.lang.ref.WeakReference; public class test &#123; public static void main(String[] args) &#123; People people=new People("zhouqian",20); WeakReference&lt;People&gt;reference=new WeakReference&lt;People&gt;(people);//&lt;span style="color:#FF0000;"&gt;关联强引用&lt;/span&gt; System.out.println(reference.get()); System.gc(); System.out.println(reference.get()); &#125; &#125; class People&#123; public String name; public int age; public People(String name,int age) &#123; this.name=name; this.age=age; &#125; @Override public String toString() &#123; return "[name:"+name+",age:"+age+"]"; &#125; &#125;//结果发生了很大的变化 [name:zhouqian,age:20] [name:zhouqian,age:20] 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被JVM回收，这个软引用就会被加入到与之关联的引用队列中。 虚引用 - Phantom Reference功能： “虚引用”形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期，如果一个对象仅持有虚引用的话，那么它就和没有任何的引用一样，在任何时候都可能被垃圾回收器回收。 虚引用必须和引用队列联合使用，引用队列的作用和软弱引用一样。 虚引用的回收机制跟弱引用差不多，但是它被回收之前，会被放入ReferenceQueue中。其它引用是被JVM回收后才被传入ReferenceQueue中的。由于这个机制，所以虚引用大多被用于引用销毁前的处理工作。 虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。在java中用java.lang.ref.PhantomReference类表示。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。要注意的是，虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之 关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来获取一个对象的实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。虚引用和弱引用对关联对象的回收都不会产生影响，如果只有虚引用活着弱引用关联着对象，那么这个对象就会被回收。它们的不同之处在于弱引用的get方法，虚引用的get方法始终返回null,弱引用可以使用ReferenceQueue,虚引用必须配合ReferenceQueue使用。 使用场景：程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。还可以做对象销毁前的一些操作，比如说资源释放等。Object.finalize()虽然也可以做这类动作，但是这个方式即不安全又低效。 jdk中直接内存的回收就用到虚引用，由于jvm自动内存管理的范围是堆内存，而直接内存是在堆内存之外（其实是内存映射文件，自行去理解虚拟内存空间的相关概念），所以直接内存的分配和回收都是有Unsafe类去操作，java在申请一块直接内存之后，会在堆内存分配一个对象保存这个堆外内存的引用，这个对象被垃圾收集器管理，一旦这个对象被回收，相应的用户线程会收到通知并对直接内存进行清理工作。 1234String name = "a";ReferenceQueue&lt;String&gt; prq = new ReferenceQueue&lt;&gt;();PhantomReference&lt;String&gt; nameRf = new PhantomReference&lt;&gt;(name, prq);System.out.println(prq.poll()); 参考：Java 四种引用介绍及使用场景java中的4种reference的差别和使用场景（含理论、代码和执行结果）Java中的四种引用介绍和使用场景软引用、弱引用、虚引用-他们的特点及应用场景Java的四种引用方式]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中ThreadLocal原理]]></title>
    <url>%2F2019%2F06%2F20%2Fjava-ThreadLocal%2F</url>
    <content type="text"><![CDATA[本文转发自技术世界，原文链接 http://www.jasongj.com/java/threadlocal/ ThreadLocal解决什么问题由于 ThreadLocal 支持范型，如 ThreadLocal&lt; StringBuilder &gt;，为表述方便，后文用 变量 代表 ThreadLocal 本身，而用 实例 代表具体类型（如 StringBuidler ）的实例。 不恰当的理解写这篇文章的一个原因在于，网上很多博客关于 ThreadLocal 的适用场景以及解决的问题，描述的并不清楚，甚至是错的。下面是常见的对于 ThreadLocal的介绍 ThreadLocal为解决多线程程序的并发问题提供了一种新的思路ThreadLocal的目的是为了解决多线程访问资源时的共享问题 还有很多文章在对比 ThreadLocal 与 synchronize 的异同。既然是作比较，那应该是认为这两者解决相同或类似的问题。 上面的描述，问题在于，ThreadLocal 并不解决多线程 共享 变量的问题。既然变量不共享，那就更谈不上同步的问题。 合理的理解ThreadLoal 变量，它的基本原理是，同一个 ThreadLocal 所包含的对象（对ThreadLocal&lt; String &gt;而言即为 String 类型变量），在不同的 Thread 中有不同的副本（实际是不同的实例，后文会详细阐述）。这里有几点需要注意 因为每个 Thread 内有自己的实例副本，且该副本只能由当前 Thread 使用。这是也是 ThreadLocal 命名的由来 既然每个 Thread 有自己的实例副本，且其它 Thread 不可访问，那就不存在多线程间共享的问题 既无共享，何来同步问题，又何来解决同步问题一说？ 那 ThreadLocal 到底解决了什么问题，又适用于什么样的场景？ This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID).Each thread holds an implicit reference to its copy of a thread-local variable as long as the thread is alive and the ThreadLocal instance is accessible; after a thread goes away, all of its copies of thread-local instances are subject to garbage collection (unless other references to these copies exist). 核心意思是 ThreadLocal 提供了线程本地的实例。它与普通变量的区别在于，每个使用该变量的线程都会初始化一个完全独立的实例副本。ThreadLocal 变量通常被private static修饰。当一个线程结束时，它所使用的所有 ThreadLocal 相对的实例副本都可被回收。 总的来说，ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也即变量在线程间隔离而在方法或类间共享的场景。后文会通过实例详细阐述该观点。另外，该场景下，并非必须使用 ThreadLocal ，其它方式完全可以实现同样的效果，只是 ThreadLocal 使得实现更简洁。 ThreadLocal用法实例代码下面通过如下代码说明 ThreadLocal 的使用方式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class ThreadLocalDemo &#123; public static void main(String[] args) throws InterruptedException &#123; int threads = 3; CountDownLatch countDownLatch = new CountDownLatch(threads); InnerClass innerClass = new InnerClass(); for(int i = 1; i &lt;= threads; i++) &#123; new Thread(() -&gt; &#123; for(int j = 0; j &lt; 4; j++) &#123; innerClass.add(String.valueOf(j)); innerClass.print(); &#125; innerClass.set("hello world"); countDownLatch.countDown(); &#125;, "thread - " + i).start(); &#125; countDownLatch.await(); &#125; private static class InnerClass &#123; public void add(String newStr) &#123; StringBuilder str = Counter.counter.get(); Counter.counter.set(str.append(newStr)); &#125; public void print() &#123; System.out.printf("Thread name:%s , ThreadLocal hashcode:%s, Instance hashcode:%s, Value:%s\n", Thread.currentThread().getName(), Counter.counter.hashCode(), Counter.counter.get().hashCode(), Counter.counter.get().toString()); &#125; public void set(String words) &#123; Counter.counter.set(new StringBuilder(words)); System.out.printf("Set, Thread name:%s , ThreadLocal hashcode:%s, Instance hashcode:%s, Value:%s\n", Thread.currentThread().getName(), Counter.counter.hashCode(), Counter.counter.get().hashCode(), Counter.counter.get().toString()); &#125; &#125; private static class Counter &#123; private static ThreadLocal&lt;StringBuilder&gt; counter = new ThreadLocal&lt;StringBuilder&gt;() &#123; @Override protected StringBuilder initialValue() &#123; return new StringBuilder(); &#125; &#125;; &#125;&#125; 实例分析ThreadLocal本身支持范型。该例使用了 StringBuilder 类型的 ThreadLocal 变量。可通过 ThreadLocal 的 get() 方法读取 StringBuidler 实例，也可通过 set(T t) 方法设置 StringBuilder。 上述代码执行结果如下 123456789101112131415Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:418873098, Value:0Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1609588821, Value:0Thread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1780437710, Value:0Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1609588821, Value:01Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:418873098, Value:01Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1609588821, Value:012Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1609588821, Value:0123Set, Thread name:thread - 3 , ThreadLocal hashcode:372282300, Instance hashcode:1362597339, Value:hello worldThread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1780437710, Value:01Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:418873098, Value:012Thread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1780437710, Value:012Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:418873098, Value:0123Thread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1780437710, Value:0123Set, Thread name:thread - 1 , ThreadLocal hashcode:372282300, Instance hashcode:482932940, Value:hello worldSet, Thread name:thread - 2 , ThreadLocal hashcode:372282300, Instance hashcode:1691922941, Value:hello world 从上面的输出可看出 从第1-3行输出可见，每个线程通过 ThreadLocal 的 get() 方法拿到的是不同的 StringBuilder 实例 第1-3行输出表明，每个线程所访问到的是同一个 ThreadLocal 变量 从7、12、13行输出以及第30行代码可见，虽然从代码上都是对 Counter 类的静态 counter 字段进行 get() 得到 StringBuilder 实例并追加字符串，但是这并不会将所有线程追加的字符串都放进同一个 StringBuilder 中，而是每个线程将字符串追加进各自的 StringBuidler 实例内 对比第1行与第15行输出并结合第38行代码可知，使用 set(T t) 方法后，ThreadLocal 变量所指向的 StringBuilder 实例被替换 ThreadLocal原理ThreadLocal维护线程与实例的映射既然每个访问 ThreadLocal 变量的线程都有自己的一个“本地”实例副本。一个可能的方案是 ThreadLocal 维护一个 Map，键是 Thread，值是它在该 Thread 内的实例。线程通过该 ThreadLocal 的 get() 方案获取实例时，只需要以线程为键，从 Map 中找出对应的实例即可。该方案如下图所示 ] 该方案可满足上文提到的每个线程内一个独立备份的要求。每个新线程访问该 ThreadLocal 时，需要向 Map 中添加一个映射，而每个线程结束时，应该清除该映射。这里就有两个问题： 增加线程与减少线程均需要写 Map，故需保证该 Map 线程安全。虽然从ConcurrentHashMap的演进看Java多线程核心技术一文介绍了几种实现线程安全 Map 的方式，但它或多或少都需要锁来保证线程的安全性 线程结束时，需要保证它所访问的所有 ThreadLocal 中对应的映射均删除，否则可能会引起内存泄漏。（后文会介绍避免内存泄漏的方法） 其中锁的问题，是 JDK 未采用该方案的一个原因。 Thread维护ThreadLocal与实例的映射上述方案中，出现锁的问题，原因在于多线程访问同一个 Map。如果该 Map 由 Thread 维护，从而使得每个 Thread 只访问自己的 Map，那就不存在多线程写的问题，也就不需要锁。该方案如下图所示。 ] 该方案虽然没有锁的问题，但是由于每个线程访问某 ThreadLocal 变量后，都会在自己的 Map 内维护该 ThreadLocal 变量与具体实例的映射，如果不删除这些引用（映射），则这些 ThreadLocal 不能被回收，可能会造成内存泄漏。后文会介绍 JDK 如何解决该问题。 ThreadLocal 在 JDK 8 中的实现ThreadLocalMap与内存泄漏该方案中，Map 由 ThreadLocal 类的静态内部类 ThreadLocalMap 提供。该类的实例维护某个 ThreadLocal 与具体实例的映射。与 HashMap 不同的是，ThreadLocalMap 的每个 Entry 都是一个对 键 的弱引用，这一点从super(k)可看出。另外，每个 Entry 都包含了一个对 值 的强引用。 123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 使用弱引用的原因在于，当没有强引用指向 ThreadLocal 变量时，它可被回收，从而避免上文所述 ThreadLocal 不能被回收而造成的内存泄漏的问题。 但是，这里又可能出现另外一种内存泄漏的问题。ThreadLocalMap 维护 ThreadLocal 变量与具体实例的映射，当 ThreadLocal 变量被回收后，该映射的键变为 null，该 Entry 无法被移除。从而使得实例被该 Entry 引用而无法被回收造成内存泄漏。 注：Entry虽然是弱引用，但它是 ThreadLocal 类型的弱引用（也即上文所述它是对 键 的弱引用），而非具体实例的的弱引用，所以无法避免具体实例相关的内存泄漏。 读取实例读取实例方法如下所示 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 读取实例时，线程首先通过getMap(t)方法获取自身的 ThreadLocalMap。从如下该方法的定义可见，该 ThreadLocalMap 的实例是 Thread 类的一个字段，即由 Thread 维护 ThreadLocal 对象与具体实例的映射，这一点与上文分析一致。 123ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 获取到 ThreadLocalMap 后，通过map.getEntry(this)(this 指的是 ThreadLocal类对象)方法获取该 ThreadLocal 在当前线程的 ThreadLocalMap 中对应的 Entry。该方法中的 this 即当前访问的 ThreadLocal 对象。 如果获取到的 Entry 不为 null，从 Entry 中取出值即为所需访问的本线程对应的实例。如果获取到的 Entry 为 null，则通过setInitialValue()方法设置该 ThreadLocal 变量在该线程中对应的具体实例的初始值。 设置初始值设置初始值方法如下 12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 该方法为 private 方法，无法被重载。 首先，通过initialValue()方法获取初始值。该方法为 public 方法，且默认返回 null。所以典型用法中常常重载该方法。上例中即在内部匿名类中将其重载。 然后拿到该线程对应的 ThreadLocalMap 对象，若该对象不为 null，则直接将该 ThreadLocal 对象与对应实例初始值的映射添加进该线程的 ThreadLocalMap中。若为 null，则先创建该 ThreadLocalMap 对象再将映射添加其中。 这里并不需要考虑 ThreadLocalMap 的线程安全问题。因为每个线程有且只有一个 ThreadLocalMap 对象，并且只有该线程自己可以访问它，其它线程不会访问该 ThreadLocalMap，也即该对象不会在多个线程中共享，也就不存在线程安全的问题。 设置实例除了通过initialValue()方法设置实例的初始值，还可通过 set 方法设置线程内实例的值，如下所示。 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 该方法先获取该线程的 ThreadLocalMap 对象，然后直接将 ThreadLocal 对象（即代码中的 this）与目标实例的映射添加进 ThreadLocalMap 中。当然，如果映射已经存在，就直接覆盖。另外，如果获取到的 ThreadLocalMap 为 null，则先创建该 ThreadLocalMap 对象。 防止内存泄漏对于已经不再被使用且已被回收的 ThreadLocal 对象，它在每个线程内对应的实例由于被线程的 ThreadLocalMap 的 Entry 强引用，无法被回收，可能会造成内存泄漏。 ThreadLocal对象是弱引用，可以被回收，但是ThreadLocalMap的Entry可能会存在内存泄漏问题. 针对该问题，ThreadLocalMap 的 set 方法中，通过 replaceStaleEntry 方法将所有键为 null 的 Entry 的值设置为 null，从而使得该值可被回收。另外，会在 rehash 方法中通过 expungeStaleEntry 方法将键和值为 null 的 Entry 设置为 null 从而使得该 Entry 可被回收。通过这种方式，ThreadLocal 可防止内存泄漏。 123456789101112131415161718192021private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 适用场景如上文所述，ThreadLocal 适用于如下两种场景 每个线程需要有自己单独的实例 实例需要在多个方法中共享，但不希望被多线程共享 对于第一点，每个线程拥有自己实例，实现它的方式很多。例如可以在线程内部构建一个单独的实例。ThreadLocal 可以以非常方便的形式满足该需求。 对于第二点，可以在满足第一点（每个线程有自己的实例）的条件下，通过方法间引用传递的形式实现。ThreadLocal 使得代码耦合度更低，且实现更优雅。 案例对于 Java Web 应用而言，Session 保存了很多信息。很多时候需要通过 Session 获取信息，有些时候又需要修改 Session 的信息。一方面，需要保证每个线程有自己单独的 Session 实例。另一方面，由于很多地方都需要操作 Session，存在多方法共享 Session 的需求。如果不使用 ThreadLocal，可以在每个线程内构建一个 Session实例，并将该实例在多个方法间传递，如下所示。 123456789101112131415161718192021222324252627282930313233343536public class SessionHandler &#123; @Data public static class Session &#123; private String id; private String user; private String status; &#125; public Session createSession() &#123; return new Session(); &#125; public String getUser(Session session) &#123; return session.getUser(); &#125; public String getStatus(Session session) &#123; return session.getStatus(); &#125; public void setStatus(Session session, String status) &#123; session.setStatus(status); &#125; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; SessionHandler handler = new SessionHandler(); Session session = handler.createSession(); handler.getStatus(session); handler.getUser(session); handler.setStatus(session, "close"); handler.getStatus(session); &#125;).start(); &#125;&#125; 该方法是可以实现需求的。但是每个需要使用 Session 的地方，都需要显式传递 Session 对象，方法间耦合度较高。 这里使用 ThreadLocal 重新实现该功能如下所示。 123456789101112131415161718192021222324252627282930313233public class SessionHandler &#123; public static ThreadLocal&lt;Session&gt; session = ThreadLocal.&lt;Session&gt;withInitial(() -&gt; new Session()); @Data public static class Session &#123; private String id; private String user; private String status; &#125; public String getUser() &#123; return session.get().getUser(); &#125; public String getStatus() &#123; return session.get().getStatus(); &#125; public void setStatus(String status) &#123; session.get().setStatus(status); &#125; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; SessionHandler handler = new SessionHandler(); handler.getStatus(); handler.getUser(); handler.setStatus("close"); handler.getStatus(); &#125;).start(); &#125;&#125; 使用 ThreadLocal 改造后的代码，不再需要在各个方法间传递 Session 对象，并且也非常轻松的保证了每个线程拥有自己独立的实例。 如果单看其中某一点，替代方法很多。比如可通过在线程内创建局部变量可实现每个线程有自己的实例，使用静态变量可实现变量在方法间的共享。但如果要同时满足变量在线程间的隔离与方法间的共享，ThreadLocal再合适不过。 总结 ThreadLocal 并不解决线程间共享数据的问题 ThreadLocal 通过隐式的在不同线程内创建独立实例副本避免了实例线程安全的问题 每个线程持有一个 Map 并维护了 ThreadLocal 对象与具体实例的映射，该 Map 由于只被持有它的线程访问，故不存在线程安全以及锁的问题 ThreadLocalMap 的 Entry 对 ThreadLocal 的引用为弱引用，避免了 ThreadLocal 对象无法被回收的问题 ThreadLocalMap 的 set 方法通过调用 replaceStaleEntry 方法回收键为 null 的 Entry 对象的值（即为具体实例）以及 Entry 对象本身从而防止内存泄漏 ThreadLocal 适用于变量在线程间隔离且在方法间共享的场景]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java关键字synchronized]]></title>
    <url>%2F2019%2F06%2F20%2Fjava-synchronized%2F</url>
    <content type="text"><![CDATA[线程安全是并发编程中的重要关注点，应该注意到的是，造成线程安全问题的主要诱因有两点，一是存在共享数据(也称临界资源)，二是存在多条线程共同操作共享数据。因此为了解决这个问题，我们可能需要这样一个方案，当存在多个线程操作共享数据时，需要保证同一时刻有且只有一个线程在操作共享数据，其他线程必须等到该线程处理完数据后再进行，这种方式有个高大上的名称叫互斥锁，即能达到互斥访问目的的锁，也就是说当一个共享数据被当前正在访问的线程加上互斥锁后，在同一个时刻，其他线程只能处于等待的状态，直到当前线程处理完毕释放该锁。在 Java 中，关键字 synchronized可以保证在同一个时刻，只有一个线程可以执行某个方法或者某个代码块(主要是对方法或者代码块中存在共享数据的操作)，同时我们还应该注意到synchronized另外一个重要的作用，synchronized可保证一个线程的变化(主要是共享数据的变化)被其他线程所看到（保证可见性，完全可以替代Volatile功能），这点确实也是很重要的。 synchronized的三种应用方式synchronized关键字最主要有以下3种应用方式，下面分别介绍 12345修饰实例方法，作用于当前实例加锁，进入同步代码前要获得当前实例的锁修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 synchronized作用于实例方法所谓的实例对象锁就是用synchronized修饰实例对象中的实例方法，注意是实例方法不包括静态方法，如下 12345678910111213141516171819202122232425262728293031public class AccountingSync implements Runnable&#123; //共享资源(临界资源) static int i=0; /** * synchronized 修饰实例方法 */ public synchronized void increase()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; AccountingSync instance=new AccountingSync(); Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125; /** * 输出结果: * 2000000 */&#125; 上述代码中，我们开启两个线程操作同一个共享资源即变量i，由于i++;操作并不具备原子性，该操作是先读取值，然后写回一个新值，相当于原来的值加上1，分两步完成(应该是三步，读取，加1，写入)，如果第二个线程在第一个线程读取旧值和写回新值期间读取i的域值，那么第二个线程就会与第一个线程一起看到同一个值，并执行相同值的加1操作，这也就造成了线程安全失败，因此对于increase方法必须使用synchronized修饰，以便保证线程安全。此时我们应该注意到synchronized修饰的是实例方法increase，在这样的情况下，当前线程的锁便是实例对象instance，注意Java中的线程同步锁可以是任意对象。从代码执行结果来看确实是正确的，倘若我们没有使用synchronized关键字，其最终输出结果就很可能小于2000000，这便是synchronized关键字的作用。这里我们还需要意识到，当一个线程正在访问一个对象的 synchronized 实例方法，那么其他线程不能访问该对象的其他 synchronized 方法，毕竟一个对象只有一把锁，当一个线程获取了该对象的锁之后，其他线程无法获取该对象的锁，所以无法访问该对象的其他synchronized实例方法，但是其他线程还是可以访问该实例对象的其他非synchronized方法，当然如果是一个线程 A 需要访问实例对象 obj1 的 synchronized 方法 f1(当前对象锁是obj1)，另一个线程 B 需要访问实例对象 obj2 的 synchronized 方法 f2(当前对象锁是obj2)，这样是允许的，因为两个实例对象锁并不同相同，此时如果两个线程操作数据并非共享的，线程安全是有保障的，遗憾的是如果两个线程操作的是共享数据，那么线程安全就有可能无法保证了，如下代码将演示出该现象 123456789101112131415161718192021222324public class AccountingSyncBad implements Runnable&#123; static int i=0; public synchronized void increase()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //new新实例 Thread t1=new Thread(new AccountingSyncBad()); //new新实例 Thread t2=new Thread(new AccountingSyncBad()); t1.start(); t2.start(); //join含义:当前线程A等待thread线程终止之后才能从thread.join()返回 t1.join(); t2.join(); System.out.println(i); &#125;&#125; 上述代码与前面不同的是我们同时创建了两个新实例AccountingSyncBad，然后启动两个不同的线程对共享变量i进行操作，但很遗憾操作结果是1452317而不是期望结果2000000，因为上述代码犯了严重的错误，虽然我们使用synchronized修饰了increase方法，但却new了两个不同的实例对象，这也就意味着存在着两个不同的实例对象锁，因此t1和t2都会进入各自的对象锁，也就是说t1和t2线程使用的是不同的锁，因此线程安全是无法保证的。解决这种困境的的方式是将synchronized作用于静态的increase方法，这样的话，对象锁就当前类对象，由于无论创建多少个实例对象，但对于的类对象拥有只有一个，所有在这样的情况下对象锁就是唯一的。下面我们看看如何使用将synchronized作用于静态的increase方法。 synchronized作用于静态方法当synchronized作用于静态方法时，其锁就是当前类的class对象锁。由于静态成员不专属于任何一个实例对象，是类成员，因此通过class对象锁可以控制静态成员的并发操作。需要注意的是如果一个线程A调用一个实例对象的非static synchronized方法，而线程B需要调用这个实例对象所属类的静态 synchronized方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的class对象，而访问非静态 synchronized 方法占用的锁是当前实例对象锁，看如下代码 123456789101112131415161718192021222324252627282930313233343536public class AccountingSyncClass implements Runnable&#123; static int i=0; /** * 作用于静态方法,锁是当前class对象,也就是 * AccountingSyncClass类对应的class对象 */ public static synchronized void increase()&#123; i++; &#125; /** * 非静态,访问时锁不一样不会发生互斥 */ public synchronized void increase4Obj()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //new新实例 Thread t1=new Thread(new AccountingSyncClass()); //new心事了 Thread t2=new Thread(new AccountingSyncClass()); //启动线程 t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 由于synchronized关键字修饰的是静态increase方法，与修饰实例方法不同的是，其锁对象是当前类的class对象。注意代码中的increase4Obj方法是实例方法，其对象锁是当前实例对象，如果别的线程调用该方法，将不会产生互斥现象，毕竟锁对象不同，但我们应该意识到这种情况下可能会发现线程安全问题(操作了共享静态变量i)。 synchronized同步代码块除了使用关键字修饰实例方法和静态方法外，还可以修饰同步代码块，在某些情况下，我们编写的方法体可能比较大，同时存在一些比较耗时的操作，而需要同步的代码又只有一小部分，如果直接对整个方法进行同步操作，可能会得不偿失，此时我们可以使用同步代码块的方式对需要同步的代码进行包裹，这样就无需对整个方法进行同步操作了，同步代码块的使用示例如下： 123456789101112131415161718192021public class AccountingSync implements Runnable&#123; static AccountingSync instance=new AccountingSync(); static int i=0; @Override public void run() &#123; //省略其他耗时操作.... //使用同步代码块对变量i进行同步操作,锁对象为instance synchronized(instance)&#123; for(int j=0;j&lt;1000000;j++)&#123; i++; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 从代码看出，将synchronized作用于一个给定的实例对象instance，即当前实例对象就是锁对象，每次当线程进入synchronized包裹的代码块时就会要求当前线程持有instance实例对象锁，如果当前有其他线程正持有该对象锁，那么新到的线程就必须等待，这样也就保证了每次只有一个线程执行i++;操作。当然除了instance作为对象外，我们还可以使用this对象(代表当前实例)或者当前类的class对象作为锁，如下代码： 12345678910111213//this,当前实例对象锁synchronized(this)&#123; for(int j=0;j&lt;1000000;j++)&#123; i++; &#125;&#125;//class对象锁synchronized(AccountingSync.class)&#123; for(int j=0;j&lt;1000000;j++)&#123; i++; &#125;&#125; 了解完synchronized的基本含义及其使用方式后，下面我们将进一步深入理解synchronized的底层实现原理。 synchronized底层语义原理Java 虚拟机中的同步(Synchronization)基于进入和退出管程(Monitor)对象实现， 无论是显式同步(有明确的 monitorenter 和 monitorexit 指令,即同步代码块)还是隐式同步都是如此。在 Java 语言中，同步用的最多的地方可能是被 synchronized 修饰的同步方法。同步方法并不是由 monitorenter 和 monitorexit 指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的ACC_SYNCHRONIZED标志来隐式实现的，关于这点，稍后详细分析。下面先来了解一个概念Java对象头，这对深入理解synchronized实现原理非常关键。 理解Java对象头与Monitor在JVM中，对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充。如下： 实例变量：存放类的属性数据信息，包括父类的属性信息，如果是数组的实例部分还包括数组的长度，这部分内存按4字节对齐。 填充数据：由于虚拟机要求对象起始地址必须是8字节的整数倍。填充数据不是必须存在的，仅仅是为了字节对齐，这点了解即可。 而对于顶部，则是Java头对象，它是实现synchronized的锁对象的基础，这点我们重点分析它，一般而言，synchronized使用的锁对象是存储在Java对象头里的，jvm中采用2个字来存储对象头(如果对象是数组则会分配3个字，多出来的1个字记录的是数组长度)，其主要结构是由Mark Word 和Class Metadata Address 组成，其结构说明如下表： 虚拟机位数 头对象结构 说明 32/64bit Mark Word 存储对象的hashCode、锁信息或分代年龄或GC标志等信息 32/64bit Class Metadata Address 类型指针指向对象的类元数据，JVM通过这个指针确定该对象是哪个类的实例 其中Mark Word在默认情况下存储着对象的HashCode、分代年龄、锁标记位等. 以下是32位JVM的Mark Word默认存储结构 锁状态 25bit 4bit 1bit是否是偏向锁 2bit 锁标志位 无锁状态 对象HashCode 对象分代年龄 0 01 由于对象头的信息是与对象自身定义的数据没有关系的额外存储成本，因此考虑到JVM的空间效率，Mark Word 被设计成为一个非固定的数据结构，以便存储更多有效的数据，它会根据对象本身的状态复用自己的存储空间，如32位JVM下，除了上述列出的Mark Word默认存储结构外，还有如下可能变化的结构： 其中轻量级锁和偏向锁是Java 6对 synchronized 锁进行优化后新增加的，稍后我们会简要分析。这里我们主要分析一下重量级锁也就是通常说synchronized的对象锁，锁标识位为10，其中指针指向的是monitor对象（也称为管程或监视器锁）的起始地址。每个对象都存在着一个 monitor 与之关联，对象与其 monitor 之间的关系有存在多种实现方式，如monitor可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个 monitor 被某个线程持有后，它便处于锁定状态。在Java虚拟机(HotSpot)中，monitor是由ObjectMonitor实现的，其主要数据结构如下（位于HotSpot虚拟机源码ObjectMonitor.hpp文件，C++实现的） 123456789101112131415161718ObjectMonitor() &#123; _header = NULL; _count = 0; //记录个数 _waiters = 0, _recursions = 0; _object = NULL; _owner = NULL; _WaitSet = NULL; //处于wait状态的线程，会被加入到_WaitSet _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; //处于等待锁block状态的线程，会被加入到该列表 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; &#125; ObjectMonitor中有两个队列，_WaitSet 和 _EntryList，用来保存ObjectWaiter对象列表( 每个等待锁的线程都会被封装成ObjectWaiter对象)，_owner指向持有ObjectMonitor对象的线程，当多个线程同时访问一段同步代码时，首先会进入 _EntryList 集合，当线程获取到对象的monitor 后进入 _Owner 区域并把monitor中的owner变量设置为当前线程同时monitor中的计数器count加1，若线程调用 wait() 方法，将释放当前持有的monitor，owner变量恢复为null，count自减1，同时该线程进入 WaitSet集合中等待被唤醒。若当前线程执行完毕也将释放monitor(锁)并复位变量的值，以便其他线程进入获取monitor(锁)。如下图所示 由此看来，monitor对象存在于每个Java对象的对象头中(存储的指针的指向)，synchronized锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因，同时也是notify/notifyAll/wait等方法存在于顶级对象Object中的原因(关于这点稍后还会进行分析)，ok~，有了上述知识基础后，下面我们将进一步分析synchronized在字节码层面的具体语义实现。 synchronized代码块底层原理现在我们重新定义一个synchronized修饰的同步代码块，在代码块中操作共享变量i，如下 1234567891011public class SyncCodeBlock &#123; public int i; public void syncTask()&#123; //同步代码库 synchronized (this)&#123; i++; &#125; &#125;&#125; 编译上述代码并使用javap反编译后得到字节码如下(这里我们省略一部分没有必要的信息)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Classfile /Users/zejian/Downloads/Java8_Action/src/main/java/com/zejian/concurrencys/SyncCodeBlock.class Last modified 2017-6-2; size 426 bytes MD5 checksum c80bc322c87b312de760942820b4fed5 Compiled from "SyncCodeBlock.java"public class com.zejian.concurrencys.SyncCodeBlock minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: //........省略常量池中数据 //构造函数 public com.zejian.concurrencys.SyncCodeBlock(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return LineNumberTable: line 7: 0 //===========主要看看syncTask方法实现================ public void syncTask(); descriptor: ()V flags: ACC_PUBLIC Code: stack=3, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter //注意此处，进入同步方法 4: aload_0 5: dup 6: getfield #2 // Field i:I 9: iconst_1 10: iadd 11: putfield #2 // Field i:I 14: aload_1 15: monitorexit //注意此处，退出同步方法 16: goto 24 19: astore_2 20: aload_1 21: monitorexit //注意此处，退出同步方法 22: aload_2 23: athrow 24: return Exception table: //省略其他字节码.......&#125;SourceFile: "SyncCodeBlock.java" 我们主要关注字节码中的如下代码 1234563: monitorenter //进入同步方法//..........省略其他 15: monitorexit //退出同步方法16: goto 24//省略其他.......21: monitorexit //退出同步方法 从字节码中可知同步语句块的实现使用的是monitorenter 和 monitorexit 指令，其中monitorenter指令指向同步代码块的开始位置，monitorexit指令则指明同步代码块的结束位置，当执行monitorenter指令时，当前线程将试图获取 objectref(即对象锁) 所对应的 monitor 的持有权，当 objectref 的 monitor 的进入计数器为 0，那线程可以成功取得 monitor，并将计数器值设置为 1，取锁成功。如果当前线程已经拥有 objectref 的 monitor 的持有权，那它可以重入这个 monitor (关于重入性稍后会分析)，重入时计数器的值也会加 1。倘若其他线程已经拥有 objectref 的 monitor 的所有权，那当前线程将被阻塞，直到正在执行线程执行完毕，即monitorexit指令被执行，执行线程将释放 monitor(锁)并设置计数器值为0 ，其他线程将有机会持有 monitor 。值得注意的是编译器将会确保无论方法通过何种方式完成，方法中调用过的每条 monitorenter 指令都有执行其对应 monitorexit 指令，而无论这个方法是正常结束还是异常结束。为了保证在方法异常完成时 monitorenter 和 monitorexit 指令依然可以正确配对执行，编译器会自动产生一个异常处理器，这个异常处理器声明可处理所有的异常，它的目的就是用来执行 monitorexit 指令。从字节码中也可以看出多了一个monitorexit指令，它就是异常结束时被执行的释放monitor 的指令。 synchronized方法底层原理方法级的同步是隐式，即无需通过字节码指令来控制的，它实现在方法调用和返回操作之中。JVM可以从方法常量池中的方法表结构(method_info Structure) 中的 ACC_SYNCHRONIZED访问标志区分一个方法是否同步方法。当方法调用时，调用指令将会 检查方法的ACC_SYNCHRONIZED访问标志是否被设置，如果设置了，执行线程将先持有monitor（虚拟机规范中用的是管程一词）， 然后再执行方法，最后再方法完成(无论是正常完成还是非正常完成)时释放monitor。在方法执行期间，执行线程持有了monitor，其他任何线程都无法再获得同一个monitor。如果一个同步方法执行期间抛 出了异常，并且在方法内部无法处理此异常，那这个同步方法所持有的monitor将在异常抛到同步方法之外时自动释放。下面我们看看字节码层面如何实现： 12345678public class SyncMethod &#123; public int i; public synchronized void syncTask()&#123; i++; &#125;&#125; 使用javap反编译后的字节码如下： 123456789101112131415161718192021222324252627282930Classfile /Users/zejian/Downloads/Java8_Action/src/main/java/com/zejian/concurrencys/SyncMethod.class Last modified 2017-6-2; size 308 bytes MD5 checksum f34075a8c059ea65e4cc2fa610e0cd94 Compiled from "SyncMethod.java"public class com.zejian.concurrencys.SyncMethod minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool; //省略没必要的字节码 //==================syncTask方法====================== public synchronized void syncTask(); descriptor: ()V //方法标识ACC_PUBLIC代表public修饰，ACC_SYNCHRONIZED指明该方法为同步方法 flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=3, locals=1, args_size=1 0: aload_0 1: dup 2: getfield #2 // Field i:I 5: iconst_1 6: iadd 7: putfield #2 // Field i:I 10: return LineNumberTable: line 12: 0 line 13: 10&#125;SourceFile: "SyncMethod.java" 从字节码中可以看出，synchronized修饰的方法并没有monitorenter指令和monitorexit指令，取得代之的确实是ACC_SYNCHRONIZED标识，该标识指明了该方法是一个同步方法，JVM通过该ACC_SYNCHRONIZED访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。这便是synchronized锁在同步代码块和同步方法上实现的基本原理。同时我们还必须注意到的是在Java早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的Mutex Lock来实现的，而操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的synchronized效率低的原因。庆幸的是在Java 6之后Java官方对从JVM层面对synchronized较大优化，所以现在的synchronized锁效率也优化得很不错了，Java 6之后，为了减少获得锁和释放锁所带来的性能消耗，引入了轻量级锁和偏向锁，接下来我们将简单了解一下Java官方在JVM层面对synchronized锁的优化。 Java虚拟机对synchronized的优化锁的状态总共有四种，无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁，但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级，关于重量级锁，前面我们已详细分析过，下面我们将介绍偏向锁和轻量级锁以及JVM的其他优化手段，这里并不打算深入到每个锁的实现和转换过程更多地是阐述Java虚拟机所提供的每个锁的核心优化思想，毕竟涉及到具体过程比较繁琐，如需了解详细过程可以查阅《深入理解Java虚拟机原理》。 偏向锁偏向锁是Java 6之后加入的新锁，它是一种针对加锁操作的优化手段，经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了减少同一线程获取锁(会涉及到一些CAS操作,耗时)的代价而引入偏向锁。偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，当这个线程再次请求锁时，无需再做任何同步操作，即获取锁的过程，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。下面我们接着了解轻量级锁。 轻量级锁倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)，此时Mark Word 的结构也变为轻量级锁的结构。轻量级锁能够提升程序性能的依据是“对绝大部分的锁，在整个同步周期内都不存在竞争”，注意这是经验数据。需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合，就会导致轻量级锁膨胀为重量级锁。 自旋锁轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，因此自旋锁会假设在不久将来，当前的线程可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环(这也是称为自旋的原因)，一般不会太久，可能是50个循环或100循环，在经过若干次循环后，如果得到锁，就顺利进入临界区。如果还不能获得锁，那就会将线程在操作系统层面挂起，这就是自旋锁的优化方式，这种方式确实也是可以提升效率的。最后没办法也就只能升级为重量级锁了。 锁消除消除锁是虚拟机另外一种锁的优化，这种优化更彻底，Java虚拟机在JIT编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间，如下StringBuffer的append是一个同步方法，但是在add方法中的StringBuffer属于一个局部变量，并且不会被其他线程所使用，因此StringBuffer不可能存在共享资源竞争的情景，JVM会自动将其锁消除。 12345678910111213141516171819202122/** * Created by zejian on 2017/6/4. * Blog : http://blog.csdn.net/javazejian [原文地址,请尊重原创] * 消除StringBuffer同步锁 */public class StringBufferRemoveSync &#123; public void add(String str1, String str2) &#123; //StringBuffer是线程安全,由于sb只会在append方法中使用,不可能被其他线程引用 //因此sb属于不可能共享的资源,JVM会自动消除内部的锁 StringBuffer sb = new StringBuffer(); sb.append(str1).append(str2); &#125; public static void main(String[] args) &#123; StringBufferRemoveSync rmsync = new StringBufferRemoveSync(); for (int i = 0; i &lt; 10000000; i++) &#123; rmsync.add("abc", "123"); &#125; &#125;&#125; 关于synchronized 可能需要了解的关键点synchronized的可重入性从互斥锁的设计上来说，当一个线程试图操作一个由其他线程持有的对象锁的临界资源时，将会处于阻塞状态，但当一个线程再次请求自己持有对象锁的临界资源时，这种情况属于重入锁，请求将会成功，在java中synchronized是基于原子性的内部锁机制，是可重入的，因此在一个线程调用synchronized方法的同时在其方法体内部调用该对象另一个synchronized方法，也就是说一个线程得到一个对象锁后再次请求该对象锁，是允许的，这就是synchronized的可重入性。如下： 1234567891011121314151617181920212223242526272829public class AccountingSync implements Runnable&#123; static AccountingSync instance=new AccountingSync(); static int i=0; static int j=0; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; //this,当前实例对象锁 synchronized(this)&#123; i++; increase();//synchronized的可重入性 &#125; &#125; &#125; public synchronized void increase()&#123; j++; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 正如代码所演示的，在获取当前实例对象锁后进入synchronized代码块执行同步代码，并在代码块中调用了当前实例对象的另外一个synchronized方法，再次请求当前实例锁时，将被允许，进而执行方法体代码，这就是重入锁最直接的体现，需要特别注意另外一种情况，当子类继承父类时，子类也是可以通过可重入锁调用父类的同步方法。注意由于synchronized是基于monitor实现的，因此每次重入，monitor中的计数器仍会加1。 线程中断与synchronized线程中断正如中断二字所表达的意义，在线程运行(run方法)中间打断它，在Java中，提供了以下3个有关线程中断的方法 12345678//中断线程（实例方法）public void Thread.interrupt();//判断线程是否被中断（实例方法）public boolean Thread.isInterrupted();//判断是否被中断并清除当前中断状态（静态方法）public static boolean Thread.interrupted(); 当一个线程处于被阻塞状态或者试图执行一个阻塞操作时，使用Thread.interrupt()方式中断该线程，注意此时将会抛出一个InterruptedException的异常，同时中断状态将会被复位(由中断状态改为非中断状态)，如下代码将演示该过程： 12345678910111213141516171819202122232425262728293031public class InterruputSleepThread3 &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread() &#123; @Override public void run() &#123; //while在try中，通过异常中断就可以退出run循环 try &#123; while (true) &#123; //当前线程处于阻塞状态，异常必须捕捉处理，无法往外抛出 TimeUnit.SECONDS.sleep(2); &#125; &#125; catch (InterruptedException e) &#123; System.out.println("Interruted When Sleep"); boolean interrupt = this.isInterrupted(); //中断状态被复位 System.out.println("interrupt:"+interrupt); &#125; &#125; &#125;; t1.start(); TimeUnit.SECONDS.sleep(2); //中断处于阻塞状态的线程 t1.interrupt(); /** * 输出结果: Interruted When Sleep interrupt:false */ &#125;&#125; 如上述代码所示，我们创建一个线程，并在线程中调用了sleep方法从而使用线程进入阻塞状态，启动线程后，调用线程实例对象的interrupt方法中断阻塞异常，并抛出InterruptedException异常，此时中断状态也将被复位。这里有些人可能会诧异，为什么不用Thread.sleep(2000);而是用TimeUnit.SECONDS.sleep(2);其实原因很简单，前者使用时并没有明确的单位说明，而后者非常明确表达秒的单位，事实上后者的内部实现最终还是调用了Thread.sleep(2000);，但为了编写的代码语义更清晰，建议使用TimeUnit.SECONDS.sleep(2);的方式，注意TimeUnit是个枚举类型。ok~，除了阻塞中断的情景，我们还可能会遇到处于运行期且非阻塞的状态的线程，这种情况下，直接调用Thread.interrupt()中断线程是不会得到任响应的，如下代码，将无法中断非阻塞状态下的线程： 1234567891011121314151617181920212223public class InterruputThread &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread()&#123; @Override public void run()&#123; while(true)&#123; System.out.println("未被中断"); &#125; &#125; &#125;; t1.start(); TimeUnit.SECONDS.sleep(2); t1.interrupt(); /** * 输出结果(无限执行): 未被中断 未被中断 未被中断 ...... */ &#125;&#125; 虽然我们调用了interrupt方法，但线程t1并未被中断，因为处于非阻塞状态的线程需要我们手动进行中断检测并结束程序，改进后代码如下： 123456789101112131415161718192021222324252627public class InterruputThread &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread()&#123; @Override public void run()&#123; while(true)&#123; //判断当前线程是否被中断 if (this.isInterrupted())&#123; System.out.println("线程中断"); break; &#125; &#125; System.out.println("已跳出循环,线程中断!"); &#125; &#125;; t1.start(); TimeUnit.SECONDS.sleep(2); t1.interrupt(); /** * 输出结果: 线程中断 已跳出循环,线程中断! */ &#125;&#125; 是的，我们在代码中使用了实例方法isInterrupted判断线程是否已被中断，如果被中断将跳出循环以此结束线程,注意非阻塞状态调用interrupt()并不会导致中断状态重置。综合所述，可以简单总结一下中断两种情况， 一种是当线程处于阻塞状态或者试图执行一个阻塞操作时，我们可以使用实例方法interrupt()进行线程中断，执行中断操作后将会抛出interruptException异常(该异常必须捕捉无法向外抛出)并将中断状态复位 另外一种是当线程处于运行状态时，我们也可调用实例方法interrupt()进行线程中断，但同时必须手动判断中断状态，并编写中断线程的代码(其实就是结束run方法体的代码)。 有时我们在编码时可能需要兼顾以上两种情况，那么就可以如下编写： 12345678910public void run()&#123; try &#123; //判断当前线程是否已中断,注意interrupted方法是静态的,执行后会对中断状态进行复位 while (!Thread.interrupted()) &#123; TimeUnit.SECONDS.sleep(2); &#125; &#125; catch (InterruptedException e) &#123; &#125;&#125; 中断与synchronized事实上线程的中断操作对于正在等待获取的锁对象的synchronized方法或者代码块并不起作用，也就是对于synchronized来说，如果一个线程在等待锁，那么结果只有两种，要么它获得这把锁继续执行，要么它就保存等待，即使调用中断线程的方法，也不会生效。演示代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Created by zejian on 2017/6/2. * Blog : http://blog.csdn.net/javazejian [原文地址,请尊重原创] */public class SynchronizedBlocked implements Runnable&#123; public synchronized void f() &#123; System.out.println("Trying to call f()"); while(true) // Never releases lock Thread.yield(); // yield 让步，屈服，投降，让出CPU的意思，使当前线程从执行状态（运行状态）变为可执行态（就绪状态） &#125; /** * 在构造器中创建新线程并启动获取对象锁 */ public SynchronizedBlocked() &#123; //该线程已持有当前实例锁 new Thread() &#123; public void run() &#123; f(); // Lock acquired by this thread &#125; &#125;.start(); &#125; public void run() &#123; //中断判断 while (true) &#123; if (Thread.interrupted()) &#123; System.out.println("中断线程!!"); break; &#125; else &#123; f(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; SynchronizedBlocked sync = new SynchronizedBlocked(); Thread t = new Thread(sync); //启动后调用f()方法,无法获取当前实例锁处于等待状态 t.start(); TimeUnit.SECONDS.sleep(1); //中断线程,无法生效 t.interrupt(); &#125;&#125; 我们在SynchronizedBlocked构造函数中创建一个新线程并启动获取调用f()获取到当前实例锁，由于SynchronizedBlocked自身也是线程，启动后在其run方法中也调用了f()，但由于对象锁被其他线程占用，导致t线程只能等到锁，此时我们调用了t.interrupt();但并不能中断线程。 等待唤醒机制与synchronized所谓等待唤醒机制本篇主要指的是notify/notifyAll和wait方法，在使用这3个方法时，必须处于synchronized代码块或者synchronized方法中，否则就会抛出IllegalMonitorStateException异常，这是因为调用这几个方法前必须拿到当前对象的监视器monitor对象，也就是说notify/notifyAll和wait方法依赖于monitor对象，在前面的分析中，我们知道monitor 存在于对象头的Mark Word 中(存储monitor引用指针)，而synchronized关键字可以获取 monitor ，这也就是为什么notify/notifyAll和wait方法必须在synchronized代码块或者synchronized方法调用的原因。 12345synchronized (obj) &#123; obj.wait(); obj.notify(); obj.notifyAll(); &#125; 需要特别理解的一点是，与sleep方法不同的是wait方法调用完成后，线程将被暂停，但wait方法将会释放当前持有的监视器锁(monitor)，直到有线程调用notify/notifyAll方法后方能继续执行，而sleep方法只让线程休眠并不释放锁。同时notify/notifyAll方法调用后，并不会马上释放监视器锁，而是在相应的synchronized(){}/synchronized方法执行结束后才自动释放锁。 本篇的主要参考资料：《Java编程思想》《深入理解Java虚拟机》《实战Java高并发程序设计》 作者：zejian_来源：CSDN原文：https://blog.csdn.net/javazejian/article/details/72828483版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <tags>
        <tag>java</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java之AQS]]></title>
    <url>%2F2019%2F06%2F20%2Fjava-AQS%2F</url>
    <content type="text"><![CDATA[简单解释一下J.U.C，是JDK中提供的并发工具包,java.util.concurrent。里面提供了很多并发编程中很常用的实用工具类，比如atomic原子操作、比如lock同步锁、fork/join等。 概述谈到并发，不得不谈ReentrantLock；而谈到ReentrantLock，不得不AbstractQueuedSynchronizer（AQS）！ 类如其名，抽象的队列式的同步器，AQS定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch…。 它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。这里volatile是核心关键词，具体volatile的语义，在此不述。state的访问方式有三种: 123getState()setState()compareAndSetState() AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法： isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后续动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 从Lock作为切入点我想以lock作为切入点来讲解AQS，毕竟同步锁是解决线程安全问题的通用手段，也是我们工作中用得比较多的方式。 Lock APILock是一个接口，方法定义如下 12345void lock() // 如果锁可用就获得锁，如果锁不可用就阻塞直到锁释放void lockInterruptibly() // 和 lock()方法相似, 但阻塞的线程可中断，抛出 java.lang.InterruptedException异常boolean tryLock() // 非阻塞获取锁;尝试获取锁，如果成功返回trueboolean tryLock(long timeout, TimeUnit timeUnit) //带有超时时间的获取锁方法void unlock() // 释放锁 Lock的实现实现Lock接口的类有很多，以下为几个常见的锁实现 ReentrantLock：表示重入锁，它是唯一一个实现了Lock接口的类。重入锁指的是线程在获得锁之后，再次获取该锁不需要阻塞，而是直接关联一次计数器增加重入次数。 ReentrantReadWriteLock：重入读写锁，它实现了ReadWriteLock接口，在这个类中维护了两个锁，一个是ReadLock，一个是WriteLock，他们都分别实现了Lock接口。读写锁是一种适合读多写少的场景下解决线程安全问题的工具，基本原则是：读和读不互斥、读和写互斥、写和写互斥。也就是说涉及到影响数据变化的操作都会存在互斥。 StampedLock： stampedLock是JDK8引入的新的锁机制，可以简单认为是读写锁的一个改进版本，读写锁虽然通过分离读和写的功能使得读和读之间可以完全并发，但是读和写是有冲突的，如果大量的读线程存在，可能会引起写线程的饥饿。stampedLock是一种乐观的读策略，使得乐观锁完全不会阻塞写线程。 ReentrantLock的简单使用如何在实际应用中使用ReentrantLock呢？我们通过一个简单的demo来演示一下 1234567891011121314public class Demo &#123; private static int count=0; static Lock lock=new ReentrantLock(); public static void inc()&#123; lock.lock(); try &#123; Thread.sleep(1); count++; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally&#123; lock.unlock(); &#125; &#125; 这段代码主要做一件事，就是通过一个静态的incr()方法对共享变量count做连续递增，在没有加同步锁的情况下多线程访问这个方法一定会存在线程安全问题。所以用到了ReentrantLock来实现同步锁，并且在finally语句块中释放锁。那么我来引出一个问题，大家思考一下 多个线程通过lock竞争锁时，当竞争失败的锁是如何实现等待以及被唤醒的呢? 什么是AQSAQS全称为AbstractQueuedSynchronizer，它提供了一个FIFO队列，可以看成是一个用来实现同步锁以及其他涉及到同步功能的核心组件，常见的有:ReentrantLock、CountDownLatch等。AQS是一个抽象类，主要是通过继承的方式来使用，它本身没有实现任何的同步接口，仅仅是定义了同步状态的获取以及释放的方法来提供自定义的同步组件。可以这么说，只要搞懂了AQS，那么J.U.C中绝大部分的api都能轻松掌握。 AQS的两种功能从使用层面来说，AQS的功能分为两种：独占和共享 独占锁，每次只能有一个线程持有锁，比如前面给大家演示的ReentrantLock就是以独占方式实现的互斥锁 共享锁，允许多个线程同时获取锁，并发访问共享资源，比如ReentrantReadWriteLock ReentrantLock的类图仍然以ReentrantLock为例，来分析AQS在重入锁中的使用。毕竟单纯分析AQS没有太多的含义。先理解这个类图，可以方便我们理解AQS的原理 AQS的内部实现AQS的实现依赖内部的同步队列,也就是FIFO的双向队列（不是双端队列，双端队列是指允许两端都可以进行入队和出队操作的队列），如果当前线程竞争锁失败，那么AQS会把当前线程以及等待状态信息构造成一个Node加入到同步队列中，同时再阻塞该线程。当获取锁的线程释放锁以后，会从队列中唤醒一个阻塞的节点(线程)。 AQS队列内部维护的是一个FIFO的双向链表，这种结构的特点是每个数据结构都有两个指针，分别指向直接的后继节点和直接前驱节点。所以双向链表可以从任意一个节点开始很方便的访问前驱和后继。每个Node其实是对线程的封装，当线程争抢锁失败后会封装成Node加入到ASQ队列中去 Node类的组成如下 1234567891011121314151617181920212223242526272829303132333435363738static final class Node &#123; static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; //前驱节点 volatile Node next; //后继节点 volatile Thread thread;//当前线程 Node nextWaiter; //存储在condition队列中的后继节点 //是否为共享锁 final boolean isShared() &#123; return nextWaiter == SHARED; &#125; final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; //将线程构造成一个Node，添加到等待队列 Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; //这个方法会在Condition队列使用，后续单独写一篇文章分析condition Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125; &#125; 释放锁以及添加线程对于队列的变化添加节点当出现锁竞争以及释放锁的时候，AQS同步队列中的节点会发生变化，首先看一下添加节点的场景。这里会涉及到两个变化 新的线程封装成Node节点追加到同步队列中，设置prev节点以及修改当前节点的前置节点的next节点指向自己 通过CAS将tail重新指向新的尾部节点 释放锁移除节点head节点表示获取锁成功的节点，当头结点在释放同步状态时，会唤醒后继节点，如果后继节点获得锁成功，会把自己设置为头结点，节点的变化过程如下这个过程也是涉及到两个变化 修改head节点指向下一个获得锁的节点 新的获得锁的节点，将prev的指针指向null 这里有一个小的变化，就是设置head节点不需要用CAS，原因是设置head节点是由获得锁的线程来完成的，而同步锁只能由一个线程获得，所以不需要CAS保证，只需要把head节点设置为原首节点的后继节点，并且断开原head节点的next引用即可 synchronized 和 ReentrantLock 的区别共同点 都是用来协调多线程对共享对象、变量的访问 都是可重入锁，同一线程可以多次获得同一个锁 都保证了可见性和互斥性 不同点 ReentrantLock 显示的获得、释放锁，synchronized 隐式获得释放锁 ReentrantLock 可响应中断、可轮回，synchronized 是不可以响应中断的，为处理锁的不可用性提供了更高的灵活性 ReentrantLock 是 API 级别的，synchronized 是 JVM 级别的 Synchronized是Java语言的关键字，因此Synchronized的锁是原生语法层面的互斥，需要JVM来实现。具体是通过对象内部的一个叫做监视器锁（monitor）来实现的。ReentrantLock，字面意思可重入锁，它是JDK1.5之后提供的API层面的互斥锁，锁的功能主要由2个方法完成，即lock()和unlock()。 ReentrantLock 可以实现公平锁 ReentrantLock 通过 Condition 可以绑定多个条件 ReentrantLock可以同时绑定多个Condition对象，而synchronized中，锁对象的wait()和notify()或notifyAll()方法可以实现一个隐含的条件，如果要和多于一个条件关联时，只能再加一个额外的锁，而ReentrantLock只需要多次调用newCondition方法即可。 底层实现不一样， synchronized 是同步阻塞，使用的是悲观并发策略，lock 是同步非阻塞，采用的是乐观并发策略 Lock 是一个接口，而 synchronized 是 Java 中的关键字，synchronized 是内置的语言实现。 synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，因此使用 Lock 时需要在 finally 块中释放锁。 Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，等待的线程会一直等待下去，不能够响应中断。 ReentrantLock在等待锁时可以使用lockInterruptibly()方法选择中断， 改为处理其他事情，而synchronized关键字，线程需要一直等待下去。同样的，tryLock()方法可以设置超时时间，用于在超时时间内一直获取不到锁时进行中断。 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 Lock 可以提高多个线程进行读操作的效率，既就是实现读写锁等。 AQS的源码分析清楚了AQS的基本架构以后，我们来分析一下AQS的源码，仍然以ReentrantLock为模型。 ReentrantLock的时序图调用ReentrantLock中的lock()方法，源码的调用过程我使用了时序图来展现从图上可以看出来，当锁获取失败时，会调用addWaiter()方法将当前线程封装成Node节点加入到AQS队列，基于这个思路，我们来分析AQS的源码实现 分析源码ReentrantLock.lock()123public void lock() &#123; sync.lock();&#125; 这个是获取锁的入口，调用sync这个类里面的方法，sync是什么呢？ 1abstract static class Sync extends AbstractQueuedSynchronizer sync是一个静态内部类，它继承了AQS这个抽象类，前面说过AQS是一个同步工具，主要用来实现同步控制。我们在利用这个工具的时候，会继承它来实现同步控制功能。通过进一步分析，发现Sync这个类有两个具体的实现，分别是NofairSync(非公平锁),FailSync(公平锁). 公平锁：表示所有线程严格按照FIFO来获取锁 非公平锁： 表示可以存在抢占锁的功能，也就是说不管当前队列上是否存在其他线程等待，新线程都有机会抢占锁 公平锁和非公平锁的实现上的差异，我会在文章后面做一个解释，接下来的分析仍然以非公平锁作为主要分析逻辑。 NonfairSync.lock123456final void lock() &#123; if (compareAndSetState(0, 1)) //通过cas操作来修改state状态，表示争抢锁的操作 setExclusiveOwnerThread(Thread.currentThread());//设置当前获得锁状态的线程 else acquire(1); //尝试去获取锁&#125; 这段代码简单解释一下 由于这里是非公平锁，所以调用lock方法时，先去通过cas去抢占锁 如果抢占锁成功，保存获得锁成功的当前线程 抢占锁失败，调用acquire来走锁竞争逻辑 compareAndSetStatecompareAndSetState的代码实现逻辑如下 12// See below for intrinsics setup to support thisreturn unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 123&gt; 这段代码其实逻辑很简单，就是通过cas乐观锁的方式来做比较并替换。上面这段代码的意思是，如果当前内存中的state的值和预期值expect相等，则替换为update。更新成功返回true，否则返回false.&gt; 这个操作是原子的，不会出现线程安全问题，这里面涉及到Unsafe这个类的操作，一级涉及到state这个属性的意义。&gt; 当state=0时，表示无锁状态 当state&gt;0时，表示已经有线程获得了锁，也就是state = 1，但是因为ReentrantLock允许重入，所以同一个线程多次获得同步锁的时候，state会递增，比如重入5次，那么state=5。 而在释放锁的时候，同样需要释放5次直到state=0其他线程才有资格获得锁 12&gt; private volatile int state;&gt; 需要注意的是：不同的AQS实现，state所表达的含义是不一样的。UnsafeUnsafe类是在sun.misc包下，不属于Java标准。但是很多Java的基础类库，包括一些被广泛使用的高性能开发库都是基于Unsafe类开发的，比如Netty、Hadoop、Kafka等；Unsafe可认为是Java中留下的后门，提供了一些低层次操作，如直接内存访问、线程调度等 12&gt; public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);&gt; 这个是一个native方法， 第一个参数为需要改变的对象，第二个为偏移量(即之前求出来的headOffset的值)，第三个参数为期待的值，第四个为更新后的值整个方法的作用是如果当前时刻的值等于预期值var4相等，则更新为新的期望值 var5，如果更新成功，则返回true，否则返回false； acquireacquire是AQS中的方法，如果CAS操作未能成功，说明state已经不为0，此时继续acquire(1)操作,这里大家思考一下，acquire方法中的1的参数是用来做什么呢？如果没猜中，往前面回顾一下state这个概念 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 这个方法的主要逻辑是 通过tryAcquire尝试获取独占锁，如果成功返回true，失败返回false 如果tryAcquire失败，则会通过addWaiter方法将当前线程封装成Node添加到AQS队列尾部 acquireQueued，将Node作为参数，通过自旋去尝试获取锁。 如果大家看过我写的Synchronized源码分析的文章，就应该能够明白自旋存在的意义 NonfairSync.tryAcquire这个方法的作用是尝试获取锁，如果成功返回true，不成功返回false它是重写AQS类中的tryAcquire方法，并且大家仔细看一下AQS中tryAcquire方法的定义，并没有实现，而是抛出异常。按照一般的思维模式，既然是一个不实现的模版方法，那应该定义成abstract，让子类来实现呀？大家想想为什么 123protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125; nonfairTryAcquiretryAcquire(1)在NonfairSync中的实现代码如下 1234567891011121314151617181920212223final boolean nonfairTryAcquire(int acquires) &#123; //获得当前执行的线程 final Thread current = Thread.currentThread(); int c = getState(); //获得state的值 if (c == 0) &#123; //state=0说明当前是无锁状态 //通过cas操作来替换state的值改为1，大家想想为什么要用cas呢？ //理由是，在多线程环境中，直接修改state=1会存在线程安全问题，你猜到了吗？ if (compareAndSetState(0, acquires)) &#123; //保存当前获得锁的线程 setExclusiveOwnerThread(current); return true; &#125; &#125; //这段逻辑就很简单了。如果是同一个线程来获得锁，则直接增加重入次数 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; //增加重入次数 if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 获取当前线程，判断当前的锁的状态 如果state=0表示当前是无锁状态，通过cas更新state状态的值 如果当前线程是属于重入，则增加重入次数 addWaiter当tryAcquire方法获取锁失败以后，则会先调用addWaiter将当前线程封装成Node，然后添加到AQS队列 12345678910111213141516private Node addWaiter(Node mode) &#123; //mode=Node.EXCLUSIVE //将当前线程封装成Node，并且mode为独占锁 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // tail是AQS的中表示同步队列队尾的属性，刚开始为null，所以进行enq(node)方法 Node pred = tail; if (pred != null) &#123; //tail不为空的情况，说明队列中存在节点数据 node.prev = pred; //将当前线程的Node的prev节点指向tail if (compareAndSetTail(pred, node)) &#123;//通过cas将node添加到AQS队列 pred.next = node;//cas成功，把旧的tail的next指针指向新的tail return node; &#125; &#125; enq(node); //tail=null，将node添加到同步队列中 return node; &#125; 将当前线程封装成Node 判断当前链表中的tail节点是否为空，如果不为空，则通过cas操作把当前线程的node添加到AQS队列 如果为空或者cas失败，调用enq将节点添加到AQS队列 enqenq就是通过自旋操作把当前节点加入到队列中 1234567891011121314151617181920private Node enq(final Node node) &#123; //自旋，不做过多解释，不清楚的关注公众号[架构师修炼宝典] for (;;) &#123; Node t = tail; //如果是第一次添加到队列，那么tail=null if (t == null) &#123; // Must initialize //CAS的方式创建一个空的Node作为头结点 if (compareAndSetHead(new Node())) //此时队列中只一个头结点，所以tail也指向它 tail = head; &#125; else &#123;//进行第二次循环时，tail不为null，进入else区域。将当前线程的Node结点的prev指向tail，然后使用CAS将tail指向Node node.prev = t; if (compareAndSetTail(t, node)) &#123;//t此时指向tail,所以可以CAS成功，将tail重新指向Node。此时t为更新前的tail的值，即指向空的头结点，t.next=node，就将头结点的后续结点指向Node，返回头结点 t.next = node; return t; &#125; &#125; &#125; &#125; 假如有两个线程t1,t2同时进入enq方法，t==null表示队列是首次使用，需要先初始化另外一个线程cas失败，则进入下次循环，通过cas操作将node添加到队尾 到目前为止，通过addwaiter方法构造了一个AQS队列，并且将线程添加到了队列的节点中 acquireQueued将添加到队列中的Node作为参数传入acquireQueued方法，这里面会做抢占锁的操作 1234567891011121314151617181920212223final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor();// 获取prev节点,若为null即刻抛出NullPointException if (p == head &amp;&amp; tryAcquire(arg)) &#123;// 如果前驱为head才有资格进行锁的抢夺 setHead(node); // 获取锁成功后就不需要再进行同步操作了,获取锁成功的线程作为新的head节点//凡是head节点,head.thread与head.prev永远为null, 但是head.next不为null p.next = null; // help GC failed = false; //获取锁成功 return interrupted; &#125;//如果获取锁失败，则根据节点的waitStatus决定是否需要挂起线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())// 若前面为true,则执行挂起,待下次唤醒的时候检测中断的标志 interrupted = true; &#125; &#125; finally &#123; if (failed) // 如果抛出异常则取消锁的获取,进行出队(sync queue)操作 cancelAcquire(node); &#125;&#125; 获取当前节点的prev节点 如果prev节点为head节点，那么它就有资格去争抢锁，调用tryAcquire抢占锁 抢占锁成功以后，把获得锁的节点设置为head，并且移除原来的初始化head节点 如果获得锁失败，则根据waitStatus决定是否需要挂起线程 最后，通过cancelAcquire取消获得锁的操作 前面的逻辑都很好理解，主要看一下shouldParkAfterFailedAcquire这个方法和parkAndCheckInterrupt的作用 shouldParkAfterFailedAcquire从上面的分析可以看出，只有队列的第二个节点可以有机会争用锁，如果成功获取锁，则此节点晋升为头节点。对于第三个及以后的节点，if (p == head)条件不成立，首先进行shouldParkAfterFailedAcquire(p, node)操作shouldParkAfterFailedAcquire方法是判断一个争用锁的线程是否应该被阻塞。它首先判断一个节点的前置节点的状态是否为Node.SIGNAL，如果是，是说明此节点已经将状态设置-如果锁释放，则应当通知它，所以它可以安全的阻塞了，返回true。 123456789101112131415161718192021private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; //前继节点的状态 if (ws == Node.SIGNAL)//如果是SIGNAL状态，意味着当前线程需要被unpark唤醒 return true;//如果前节点的状态大于0，即为CANCELLED状态时，则会从前节点开始逐步循环找到一个没有被“CANCELLED”节点设置为当前节点的前节点，返回false。在下次循环执行shouldParkAfterFailedAcquire时，返回true。这个操作实际是把队列中CANCELLED的节点剔除掉。 if (ws &gt; 0) &#123;// 如果前继节点是“取消”状态，则设置 “当前节点”的 “当前前继节点” 为 “‘原前继节点'的前继节点”。 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 如果前继节点为“0”或者“共享锁”状态，则设置前继节点为SIGNAL状态。 /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; parkAndCheckInterrupt如果shouldParkAfterFailedAcquire返回了true，则会执行：parkAndCheckInterrupt()方法，它是通过LockSupport.park(this)将当前线程挂起到WATING状态，它需要等待一个中断、unpark方法来唤醒它，通过这样一种FIFO的机制的等待，来实现了Lock的操作。 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; LockSupportLockSupport类是Java6引入的一个类，提供了基本的线程同步原语。LockSupport实际上是调用了Unsafe类里的函数，归结到Unsafe里，只有两个函数： 123&gt; public native void unpark(Thread jthread); &gt; public native void park(boolean isAbsolute, long time); &gt; unpark函数为线程提供“许可(permit)”，线程调用park函数则等待“许可”。这个有点像信号量，但是这个“许可”是不能叠加的，“许可”是一次性的。permit相当于0/1的开关，默认是0，调用一次unpark就加1变成了1.调用一次park会消费permit，又会变成0。 如果再调用一次park会阻塞，因为permit已经是0了。直到permit变成1.这时调用unpark会把permit设置为1.每个线程都有一个相关的permit，permit最多只有一个，重复调用unpark不会累积 锁的释放ReentrantLock.unlock加锁的过程分析完以后，再来分析一下释放锁的过程，调用release方法，这个方法里面做两件事， 1，释放锁 ； 2，唤醒park的线程 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; tryRelease这个动作可以认为就是一个设置锁状态的操作，而且是将状态减掉传入的参数值（参数是1），如果结果状态为0，就将排它锁的Owner设置为null，以使得其它的线程有机会进行执行。在排它锁中，加锁的时候状态会增加1（当然可以自己修改这个值），在解锁的时候减掉1，同一个锁，在可以重入后，可能会被叠加为2、3、4这些值，只有unlock()的次数与lock()的次数对应才会将Owner线程设置为空，而且也只有这种情况下才会返回true。 1234567891011121314protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; // 这里是将锁的数量减1 if (Thread.currentThread() != getExclusiveOwnerThread())// 如果释放的线程和获取锁的线程不是同一个，抛出非法监视器状态异常 throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; // 由于重入的关系，不是每次释放锁c都等于0， // 直到最后一次释放锁时，才会把当前线程释放 free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; unparkSuccessor在方法unparkSuccessor(Node)中，就意味着真正要释放锁了，它传入的是head节点（head节点是占用锁的节点），当前线程被释放之后，需要唤醒下一个节点的线程 123456789101112131415private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123;//判断后继节点是否为空或者是否是取消状态, s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) //然后从队列尾部向前遍历找到最前面的一个waitStatus小于0的节点, 至于为什么从尾部开始向前遍历，因为在doAcquireInterruptibly.cancelAcquire方法的处理过程中只设置了next的变化，没有设置prev的变化，在最后有这样一行代码：node.next = node，如果这时执行了unparkSuccessor方法，并且向后遍历的话，就成了死循环了，所以这时只有prev是稳定的 s = t; &#125;//内部首先会发生的动作是获取head节点的next节点，如果获取到的节点不为空，则直接通过：“LockSupport.unpark()”方法来释放对应的被挂起的线程，这样一来将会有一个节点唤醒后继续进入循环进一步尝试tryAcquire()方法来获取锁 if (s != null) LockSupport.unpark(s.thread); //释放许可&#125; 总结通过这篇文章基本将AQS队列的实现过程做了比较清晰的分析，主要是基于非公平锁的独占锁实现。在获得同步锁时，同步器维护一个同步队列，获取状态失败的线程都会被加入到队列中并在队列中进行自旋；移出队列（或停止自旋）的条件是前驱节点为头节点且成功获取了同步状态。在释放同步状态时，同步器调用tryRelease(int arg)方法释放同步状态，然后唤醒头节点的后继节点。 参考文章：深入分析AQS实现原理Java并发之AQS详解]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>AQS</tag>
        <tag>ReentrantLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis数据类型]]></title>
    <url>%2F2019%2F06%2F20%2Fredis-data-type%2F</url>
    <content type="text"><![CDATA[Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 String（字符串）string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。 string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。 实例1234redis 127.0.0.1:6379&gt; SET name "runoob"OKredis 127.0.0.1:6379&gt; GET name"runoob" 在以上实例中我们使用了 Redis 的 SET 和 GET 命令。键为 name，对应的值为 runoob。 注意：一个键最大能存储512MB。 Hash（哈希）Redis hash 是一个键值(key=&gt;value)对集合。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 实例DEL runoob 用于删除前面测试用过的 key，不然会报错：(error) WRONGTYPE Operation against a key holding the wrong kind of value 1234567redis 127.0.0.1:6379&gt; DEL runoobredis 127.0.0.1:6379&gt; HMSET myhash field1 "Hello" field2 "World""OK"redis 127.0.0.1:6379&gt; HGET myhash field1"Hello"redis 127.0.0.1:6379&gt; HGET myhash field2"World" 实例中我们使用了 Redis HMSET, HGET 命令，HMSET 设置了两个 field=&gt;value 对, HGET 获取对应 field 对应的 value。 每个 hash 可以存储 232-1 键值对（40多亿）。 List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 实例123456789101112redis 127.0.0.1:6379&gt; DEL runoobredis 127.0.0.1:6379&gt; lpush runoob redis(integer) 1redis 127.0.0.1:6379&gt; lpush runoob mongodb(integer) 2redis 127.0.0.1:6379&gt; lpush runoob rabitmq(integer) 3redis 127.0.0.1:6379&gt; lrange runoob 0 101) "rabitmq"2) "mongodb"3) "redis"redis 127.0.0.1:6379&gt; 列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。 Set（集合）Redis的Set是string类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 sadd 命令添加一个 string 元素到 key 对应的 set 集合中，成功返回1，如果元素已经在集合中返回 0，如果 key 对应的 set 不存在则返回错误。 1sadd key member 实例1234567891011121314redis 127.0.0.1:6379&gt; DEL runoobredis 127.0.0.1:6379&gt; sadd runoob redis(integer) 1redis 127.0.0.1:6379&gt; sadd runoob mongodb(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 0redis 127.0.0.1:6379&gt; smembers runoob1) "redis"2) "rabitmq"3) "mongodb" 注意：以上实例中 rabitmq 添加了两次，但根据集合内元素的唯一性，第二次插入的元素将被忽略。 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令添加元素到集合，元素在集合中存在则更新对应score 1zadd key score member 实例12345678910111213redis 127.0.0.1:6379&gt; DEL runoobredis 127.0.0.1:6379&gt; zadd runoob 0 redis(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 mongodb(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 0redis 127.0.0.1:6379&gt; &gt; ZRANGEBYSCORE runoob 0 10001) "mongodb"2) "rabitmq"3) "redis" 参考文章：Redis 数据类型]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis持久化]]></title>
    <url>%2F2019%2F06%2F19%2Fredis-persistence%2F</url>
    <content type="text"><![CDATA[Redis持久化概述Redis是一种高级key-value数据库。它跟memcached类似，不过数据可以持久化，而且支持的数据类型很丰富。有字符串，链表，集合和有序集合和map。支持在服务器端计算集合的并，交和补集(difference)等，还支持多种排序功能。所以Redis也可以被看成是一个数据结构服务器。 Redis的所有数据都是保存在内存中，然后不定期的通过异步方式保存到磁盘上(这称为“半持久化模式”)；也可以把每一次数据变化都写入到一个append only file(aof)里面(这称为“全持久化模式”)。 由于Redis的数据都存放在内存中，如果没有配置持久化，redis重启后数据就全丢失了，于是需要开启redis的持久化功能，将数据保存到磁盘上，当redis重启后，可以从磁盘中恢复数据。redis提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化），另外一种是AOF（append only file）持久化（原理是将Reids的操作日志以追加的方式写入文件）。 持久化的功能：Redis是内存数据库，数据都是存储在内存中，为了避免进程退出导致数据的永久丢失，需要定期将Redis中的数据以某种形式(数据或命令)从内存保存到硬盘；当下次Redis重启时，利用持久化文件实现数据恢复。除此之外，为了进行灾难备份，可以将持久化文件拷贝到一个远程位置。 Redis持久化分为RDB持久化和AOF持久化：前者将当前数据保存到硬盘，后者则是将每次执行的写命令保存到硬盘（类似于MySQL的binlog）；由于AOF持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此AOF是目前主流的持久化方式，不过RDB持久化仍然有其用武之地。 Redis持久化的两种方式： RDB：在指定的时间间隔能对数据进行快照存储。 AOF：记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据。 持久化的配置为了使用持久化的功能，我们需要先知道该如何开启持久化的功能。 RDB的持久化配置12345678910111213141516171819# 时间策略save 900 1save 300 10save 60 10000# 文件名称dbfilename dump.rdb# 文件保存路径dir /home/work/app/redis/data/# 如果持久化出错，主进程是否停止写入stop-writes-on-bgsave-error yes# 是否压缩rdbcompression yes# 导入时是否检查rdbchecksum yes 配置其实非常简单，这里说一下持久化的时间策略具体是什么意思。 save 900 1 表示900s内如果有1条是写入命令，就触发产生一次快照，可以理解为就进行一次备份 save 300 10 表示300s内有10条写入，就产生快照 下面的类似，那么为什么需要配置这么多条规则呢？因为Redis每个时段的读写请求肯定不是均衡的，为了平衡性能与数据安全，我们可以自由定制什么情况下触发备份。所以这里就是根据自身Redis写入情况来进行合理配置。 stop-writes-on-bgsave-error yes 这个配置也是非常重要的一项配置，这是当备份进程出错时，主进程就停止接受新的写入操作，是为了保护持久化的数据一致性问题。如果自己的业务有完善的监控系统，可以禁止此项配置， 否则请开启。 关于压缩的配置 rdbcompression yes ，建议没有必要开启，毕竟Redis本身就属于CPU密集型服务器，再开启压缩会带来更多的CPU消耗，相比硬盘成本，CPU更值钱。 当然如果你想要禁用RDB配置，也是非常容易的，只需要在save的最后一行写上：`save “”` AOF的配置123456789101112131415161718192021# 是否开启aofappendonly yes# 文件名称appendfilename "appendonly.aof"# 同步方式appendfsync everysec# aof重写期间是否同步no-appendfsync-on-rewrite no# 重写触发配置auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# 加载aof时如果有错如何处理aof-load-truncated yes# 文件重写策略aof-rewrite-incremental-fsync yes 还是重点解释一些关键的配置： appendfsync everysec 它其实有三种模式: always：把每个写命令都立即同步到aof，很慢，但是很安全 everysec：每秒同步一次，是折中方案 no：redis不处理交给OS来处理，非常快，但是也最不安全 一般情况下都采用 everysec 配置，这样可以兼顾速度与安全，最多损失1s的数据。 aof-load-truncated yes 如果该配置启用，在加载时发现aof尾部不正确时，会向客户端写入一个log，但是会继续执行，如果设置为 no ，发现错误就会停止，必须修复后才能重新加载。 工作原理关于原理部分，我们主要来看RDB与AOF是如何完成持久化的，它们的过程是如何。 在介绍原理之前先说下Redis内部的定时任务机制，定时任务执行的频率可以在配置文件中通过 hz 10 来设置（这个配置表示1s内执行10次，也就是每100ms触发一次定时任务）。该值最大能够设置为：500，但是不建议超过：100，因为值越大说明执行频率越频繁越高，这会带来CPU的更多消耗，从而影响主进程读写性能。 定时任务使用的是Redis自己实现的 TimeEvent，它会定时去调用一些命令完成定时任务，这些任务可能会阻塞主进程导致Redis性能下降。因此我们在配置Redis时，一定要整体考虑一些会触发定时任务的配置，根据实际情况进行调整。 RDB的原理RDB持久化是将当前进程中的数据生成快照保存到硬盘(因此也称作快照持久化)，保存的文件后缀是rdb；当Redis重新启动时，可以读取快照文件恢复数据。 在Redis中RDB持久化的触发分为两种：自己手动触发与Redis定时触发。 手动触发save命令和bgsave命令都可以生成RDB文件。save命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在Redis服务器阻塞期间，服务器不能处理任何命令请求。 1save 而bgsave命令会创建一个子进程，由子进程来负责创建RDB文件，父进程(即Redis主进程)则继续处理请求。 1bgsave bgsave命令执行过程中，只有fork子进程时会阻塞服务器，而对于save命令，整个过程都会阻塞服务器，因此save已基本被废弃，线上环境要杜绝save的使用。 自动触发1234# 时间策略save 900 1save 300 10save 60 10000 其中save 900 1的含义是：当时间到900秒时，如果redis数据发生了至少1次变化，则执行bgsave；save 300 10和save 60 10000同理。当三个save条件满足任意一个时，都会引起bgsave的调用。 save m n的实现原理Redis的save m n，是通过serverCron函数、dirty计数器、和lastsave时间戳来实现的。 serverCron是Redis服务器的周期性操作函数，默认每隔100ms执行一次；该函数对服务器的状态进行维护，其中一项工作就是检查 save m n 配置的条件是否满足，如果满足就执行bgsave。 dirty计数器是Redis服务器维持的一个状态，记录了上一次执行bgsave/save命令后，服务器状态进行了多少次修改(包括增删改)；而当save/bgsave执行完成后，会将dirty重新置为0。 例如，如果Redis执行了set mykey helloworld，则dirty值会+1；如果执行了sadd myset v1 v2 v3，则dirty值会+3；注意dirty记录的是服务器进行了多少次修改，而不是客户端执行了多少修改数据的命令。 astsave时间戳也是Redis服务器维持的一个状态，记录的是上一次成功执行save/bgsave的时间。save m n的原理如下： 123每隔100ms，执行serverCron函数；在serverCron函数中，遍历save m n配置的保存条件，只要有一个条件满足，就进行bgsave。对于每一个save m n条件，只有下面两条同时满足时才算满足： （1）当前时间-lastsave &gt; m （2）dirty &gt;= n 其他自动触发机制除了save m n 以外，还有一些其他情况会触发bgsave： 12在主从复制场景下，如果从节点执行全量复制操作，则主节点会执行bgsave命令，并将rdb文件发送给从节点 执行shutdown命令时，自动执行rdb持久化 自动触发的场景主要是有以下几点： 根据我们的 save m n 配置规则自动触发； 从节点全量复制时，主节点发送rdb文件给从节点完成复制操作，主节点会触发 bgsave； 执行 debug reload 时； 执行 shutdown时，如果没有开启aof，也会触发。bgsave执行流程图片中的5个步骤所进行的操作如下： Redis父进程首先判断：当前是否在执行save，或bgsave/bgrewriteaof（后面会详细介绍该命令）的子进程，如果在执行则bgsave命令直接返回。bgsave/bgrewriteaof 的子进程不能同时执行，主要是基于性能方面的考虑：两个并发的子进程同时执行大量的磁盘写操作，可能引起严重的性能问题。 父进程执行fork操作创建子进程，这个过程中父进程是阻塞的，Redis不能执行来自客户端的任何命令 父进程fork后，bgsave命令返回”Background saving started”信息并不再阻塞父进程，并可以响应其他命令 子进程创建RDB文件，根据父进程内存快照生成临时快照文件，完成后对原有文件进行原子替换 子进程发送信号给父进程表示完成，父进程更新统计信息 这里注意的是 fork 操作会阻塞，导致Redis读写性能下降。我们可以控制单个Redis实例的最大内存，来尽可能降低Redis在fork时的事件消耗。以及上面提到的自动触发的频率减少fork次数，或者使用手动触发，根据自己的机制来完成持久化。 RDB文件RDB文件是经过压缩的二进制文件，下面介绍关于RDB文件的一些细节。 RDB文件的存储路径既可以在启动前配置，也可以通过命令动态设定。 配置：dir配置指定目录，dbfilename指定文件名。默认是Redis根目录下的dump.rdb文件。 动态设定：Redis启动后也可以动态修改RDB存储路径，在磁盘损害或空间不足时非常有用；执行命令为config set dir {newdir}和config set dbfilename {newFileName} REDIS：常量，保存着”REDIS”5个字符。 db_version：RDB文件的版本号，注意不是Redis的版本号。 SELECTDB 0 pairs：表示一个完整的数据库(0号数据库)，同理SELECTDB 3 pairs表示完整的3号数据库；只有当数据库中有键值对时，RDB文件中才会有该数据库的信息(上图所示的Redis中只有0号和3号数据库有键值对)；如果Redis中所有的数据库都没有键值对，则这一部分直接省略。其中：SELECTDB是一个常量，代表后面跟着的是数据库号码；0和3是数据库号码；pairs则存储了具体的键值对信息，包括key、value值，及其数据类型、内部编码、过期时间、压缩信息等等。 EOF：常量，标志RDB文件正文内容结束。 check_sum：前面所有内容的校验和；Redis在载入RBD文件时，会计算前面的校验和并与check_sum值比较，判断文件是否损坏。Redis默认采用LZF算法对RDB文件进行压缩。虽然压缩耗时，但是可以大大减小RDB文件的体积，因此压缩默认开启；可以通过命令关闭：1config set rdbcompression no 需要注意的是，RDB文件的压缩并不是针对整个文件进行的，而是对数据库中的字符串进行的，且只有在字符串达到一定长度(20字节)时才会进行。 加载RDB文件的载入工作是在服务器启动时自动执行的，并没有专门的命令。但是由于AOF的优先级更高，因此当AOF开启时，Redis会优先载入AOF文件来恢复数据；只有当AOF关闭时，才会在Redis服务器启动时检测RDB文件，并自动载入。服务器载入RDB文件期间处于阻塞状态，直到载入完成为止。Redis载入RDB文件时，会对RDB文件进行校验，如果文件损坏，则日志中会打印错误，Redis启动失败。 RDB优势 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 RDB劣势 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 AOF的原理AOF的整个流程大体来看可以分为两步，一步是命令的实时写入（如果是 appendfsync everysec 配置，会有1s损耗），第二步是对aof文件的重写。 对于增量追加到文件这一步主要的流程是：命令写入=》追加到aof_buf =》同步到aof磁盘。那么这里为什么要先写入buf在同步到磁盘呢？如果实时写入磁盘会带来非常高的磁盘IO，影响整体性能。 aof重写是为了减少aof文件的大小，可以手动或者自动触发，关于自动触发的规则请看上面配置部分。fork的操作也是发生在重写这一步，也是这里会对主进程产生阻塞。 手动触发： bgrewriteaof，自动触发 就是根据配置规则来触发，当然自动触发的整体时间还跟Redis的定时任务频率有关系。 下面来看看重写的一个流程图： 对于上图有四个关键点补充一下： 在重写期间，由于主进程依然在响应命令，为了保证最终备份的完整性；因此它依然会写入旧的AOF file中，如果重写失败，能够保证数据不丢失。 为了把重写期间响应的写入信息也写入到新的文件中，因此也会为子进程保留一个buf，防止新写的file丢失数据。 重写是直接把当前内存的数据生成对应命令，并不需要读取老的AOF文件进行分析、命令合并。 AOF文件直接采用的文本协议，主要是兼容性好、追加方便、可读性高可认为修改修复。 不管是RDB还是AOF都是先写入一个临时文件，然后通过 rename 完成文件的替换工作。 AOF优势 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 AOF劣势 Redis会不断地将被执行的命令记录到AOF文件里面，所以随着Redis不断运行，AOF文件的体积也会不断增长。在极端情况下，体积不断增大的AOF文件甚至可能会用完硬盘的所有可用空间。对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 Redis在重启之后需要通过重新执行AOF文件记录的所有写命令来还原数据集，所以如果AOF文件的体积非常大，那么还原操作执行的时间就可能会非常长。 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。解决办法： 1231.为了解决AOF文件体积不断增大的问题，用户可以向Redis发送BGREWRITEAOF命令，这个命令会通过移除AOF文件中的冗余命令来重写（rewrite）AOF文件，使AOF文件的体积变得尽可能地小。BGREWRITEAOF的工作原理和BGSAVE创建快照的工作原理非常相似：Redis会创建一个子进程，然后由子进程负责对AOF文件进行重写。因为AOF文件重写也需要用到子进程，所以快照持久化因为创建子进程而导致的性能问题和内存占用问题，在AOF持久化中也同样存在。2.跟快照持久化可以通过设置save选项来自动执行BGSAVE一样，AOF持久化也可以通过设置auto-aof-rewrite-percentage选项和auto-aof-rewrite-min-size选项来自动执行BGREWRITEAOF。举个例子，假设用户对Redis设置了配置选项auto-aof-rewrite-percentage 100和auto-aof-rewrite-min-size 64mb，并且启动了AOF持久化，那么当AOF文件的体积大于64MB，并且AOF文件的体积比上一次重写之后的体积大了至少一倍（100%）的时候，Redis将执行BGREWRITEAOF命令。如果AOF重写执行得过于频繁的话，用户可以考虑将auto-aof-rewrite-percentage选项的值设置为100以上，这种做法可以让Redis在AOF文件的体积变得更大之后才执行重写操作，不过也会让Redis在启动时还原数据集所需的时间变得更长。 从持久化中恢复数据数据的备份、持久化做完了，我们如何从这些持久化文件中恢复数据呢？如果一台服务器上有既有RDB文件，又有AOF文件，该加载谁呢？ 其实想要从这些文件中恢复数据，只需要重新启动Redis即可。我们还是通过图来了解这个流程： 启动时会先检查AOF文件是否存在，如果不存在就尝试加载RDB。那么为什么会优先加载AOF呢？因为AOF保存的数据更完整，通过上面的分析我们知道AOF基本上最多损失1s的数据。 性能与实践通过上面的分析，我们都知道RDB的快照、AOF的重写都需要fork，这是一个重量级操作，会对Redis造成阻塞。因此为了不影响Redis主进程响应，我们需要尽可能降低阻塞。 降低fork的频率，比如可以手动来触发RDB生成快照与AOF重写； 控制Redis最大使用内存，防止fork耗时过长； 使用更牛逼的硬件； 合理配置Linux的内存分配策略，避免因为物理内存不足导致fork失败。 在线上我们到底该怎么做？我提供一些自己的实践经验。 如果Redis中的数据并不是特别敏感或者可以通过其它方式重写生成数据，可以关闭持久化，如果丢失数据可以通过其它途径补回； 自己制定策略定期检查Redis的情况，然后可以手动触发备份、重写数据； 单机如果部署多个实例，要防止多个机器同时运行持久化、重写操作，防止出现内存、CPU、IO资源竞争，让持久化变为串行； 可以加入主从机器，利用一台从机器进行备份处理，其它机器正常响应客户端的命令； RDB持久化与AOF持久化可以同时存在，配合使用。 参考文章：一起看懂Redis两种持久化方式的原理Redis重写/压缩AOF文件redis持久化的几种方式]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>持久化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消失的U盘内存]]></title>
    <url>%2F2019%2F06%2F19%2Fudisk-capacity-small%2F</url>
    <content type="text"><![CDATA[今天为了给mac book pro安装双系统ubuntu 16.04，用U盘制作安装启动盘。安装的过程还算顺利，唯一让人难受的地方进入安装界面之后 mbp的触摸板不能用，只能用有线的鼠标和键盘。安装好了之后发现32G容量的U盘只有2M多了。应该是制作安装启动盘的时候，把很大一部分内存隐藏了起来，所以导致U盘可见容量减小。 下面记录了找回U盘容量的过程： 右键我的电脑 -&gt; 管理 -&gt; 磁盘管理，发现U盘内存属于未分配的状态。 在黑色未分配区域点击右键选择新建简单卷 接下来一路 next 然后就发现消失的U盘内存回来了。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>U盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql分片、分区、分表、分库]]></title>
    <url>%2F2019%2F06%2F19%2Fmysql-partition%2F</url>
    <content type="text"><![CDATA[一、Scale Out（横向扩展）/Scale Up（纵向扩展）Mysql的扩展方案包括Scale Out和Scale Up两种。 Scale Out（横向扩展）Scale Out（横向扩展）：是指Application可以在水平方向上扩展。一般对数据中心的应用而言，Scale out指的是当添加更多的机器时，应用仍然可以很好的利用这些机器的资源来提升自己的效率从而达到很好的扩展性。 Scale Up（纵向扩展）Scale Up（纵向扩展）：是指Application可以在垂直方向上扩展。一般对单台机器而言，Scale Up指的是当某个计算节点（机器）添加更多的CPU Cores，存储设备，使用更大的内存时，应用可以很充分的利用这些资源来提升自己的效率从而达到很好的扩展性。 二、Sharding（属于横向扩展）Sharding 是把数据库横向扩展（Scale Out）到多个物理节点上的一种有效的方式，其主要目的是为突破单节点数据库服务器的 I/O 能力限制，解决数据库扩展性问题。Shard这个词的意思是“碎片”。如果将一个数据库当作一块大玻璃，将这块玻璃打碎，那么每一小块都称为数据库的碎片（Database Shard）。将整个数据库打碎的过程就叫做sharding，可以翻译为分片。 形式上，Sharding可以简单定义为将大数据库分布到多个物理节点上的一个分区方案。每一个分区包含数据库的某一部分，称为一个shard，分区方式可以是任意的，并不局限于传统的水平分区和垂直分区。一个shard可以包含多个表的内容甚至可以包含多个数据库实例中的内容。每个shard被放置在一个数据库服务器上。一个数据库服务器可以处理一个或多个shard的数据。系统中需要有服务器进行查询路由转发，负责将查询转发到包含该查询所访问数据的shard或shards节点上去执行。 Sharding 策略 哈希取模：hash(key) % N； 范围：可以是 ID 范围也可以是时间范围； 映射表：使用单独的一个数据库来存储映射关系。 MySql的Sharding策略包括垂直切分和水平切分两种。 垂直(纵向)拆分垂直(纵向)拆分：是指按功能模块拆分，以解决表与表之间的io竞争。即将表按照功能模块、关系密切程度划分出来，部署到不同的库上。例如，我们会建立定义数据库workDB、商品数据库payDB、用户数据库userDB、日志数据库logDB等，分别用于存储项目数据定义表、商品定义表、用户数据表、日志数据表等。 如userid,name,addr一个表，为了防止表过大，分成2个表。 123userid,nameuserid,addr 表结构设计垂直切分。常见的一些场景包括: 大字段的垂直切分。单独将大字段建在另外的表中，提高基础表的访问性能，原则上在性能关键的应用中应当避免数据库的大字段 按照使用用途垂直切分。例如企业物料属性，可以按照基本属性、销售属性、采购属性、生产制造属性、财务会计属性等用途垂直切分. 按照访问频率垂直切分。例如电子商务、Web 2.0系统中，如果用户属性设置非常多，可以将基本、使用频繁的属性和不常用的属性垂直切分开 水平(横向)拆分 水平(横向)拆分：将同一个表的数据进行分块保存到不同的数据库中，来解决单表中数据量增长出现的压力。这些数据库中的表结构完全相同。当一个表中的数据量过大时，我们可以把该表的数据按照某种规则，例如userID散列、按性别、按省，进行划分，然后存储到多个结构相同的表，和不同的库上。例如，我们的userDB中的用户数据表中，每一个表的数据量都很大，就可以把userDB切分为结构相同的多个userDB：part0DB、part1DB等，再将userDB上的用户数据表userTable，切分为很多userTable：userTable0、userTable1等，然后将这些表按照一定的规则存储到多个userDB上。 表结构设计水平切分。常见的一些场景包括: 比如在线电子商务网站，订单表数据量过大，按照年度、月度水平切分。 Web 2.0网站注册用户、在线活跃用户过多，按照用户ID范围等方式，将相关用户以及该用户紧密关联的表做水平切分。 例如论坛的置顶帖子，因为涉及到分页问题，每页都需要显示置顶贴，这种情况可以把置顶贴水平切分开来，避免取置顶帖子时从所有帖子的表中读取 三、分表和分区分表从表面意思说就是把一张表分成多个小表，把一张表按一定的规则分解成N个具有独立存储空间的实体表。系统读写时需要根据定义好的规则得到对应的字表明，然后操作它。 分区则是把一张表的数据分成N多个区块，这些区块可以在同一个磁盘上，也可以在不同的磁盘上，在逻辑上看最终只是一张表，但底层是由N个物理区块组成的，分区实现比较简单，数据库mysql、oracle等很容易就可支持。分区对业务透明，分区只不过把存放数据的文件分成了许多小块，根据一定的规则把数据文件(MYD)和索引文件（MYI）进行了分割，分区后的表呢，还是一张表。 分表和分区的区别： 实现方式上mysql的分表是真正的分表，一张表分成很多表后，每一个小表都是完整的一张表，都对应三个文件（MyISAM引擎：一个.MYD数据文件，.MYI索引文件，.frm表结构文件）。 数据处理上分表后数据都是存放在分表里，总表只是一个外壳，存取数据发生在一个一个的分表里面。分区则不存在分表的概念，分区只不过把存放数据的文件分成了许多小块，分区后的表还是一张表，数据处理还是由自己来完成。 提高性能上 （1）分表后，单表的并发能力提高了，磁盘I/O性能也提高了。并发能力为什么提高了呢，因为查询一次所花的时间变短了，如果出现高并发的话，总表可以根据不同的查询，将并发压力分到不同的小表里面。磁盘I/O性能高了，本来一个非常大的.MYD文件现在也分摊到各个小表的.MYD中去了。 （2）mysql提出了分区的概念，我觉得就想突破磁盘I/O瓶颈，想提高磁盘的读写能力，来增加mysql性能。 在这一点上，分区和分表的测重点不同，分表重点是存取数据时，如何提高mysql并发能力上；而分区呢，如何突破磁盘的读写能力，从而达到提高mysql性能的目的。 实现的难易度上分表的方法有很多，用merge来分表，是最简单的一种方式。这种方式和分区难易度差不多，并且对程序代码来说可以做到透明的。如果是用其他分表方式就比分区麻烦了。 分区实现是比较简单的，建立分区表，跟建平常的表没什么区别，并且对代码端来说是透明的。 分区的适用场景: 1231. 一张表的查询速度已经慢到影响使用的时候。2. 表中的数据是分段的3. 对数据的操作往往只涉及一部分数据，而不是所有的数据 1234567891011 CREATE TABLE sales ( id INT AUTO_INCREMENT, amount DOUBLE NOT NULL, order_day DATETIME NOT NULL, PRIMARY KEY(id, order_day) ) ENGINE=Innodb PARTITION BY RANGE(YEAR(order_day)) ( PARTITION p_2010 VALUES LESS THAN (2010), PARTITION p_2011 VALUES LESS THAN (2011), PARTITION p_2012 VALUES LESS THAN (2012), PARTITION p_catchall VALUES LESS THAN MAXVALUE); 分表的适用场景 121. 一张表的查询速度已经慢到影响使用的时候。2. 当频繁插入或者联合查询时，速度变慢。 分表的实现需要业务结合实现和迁移，较为复杂。 如何选择？应该使用哪一种方式来实施数据库分库分表，这要看数据库中数据量的瓶颈所在，并综合项目的业务类型进行考虑。 如果数据库是因为表太多而造成海量数据，并且项目的各项业务逻辑划分清晰、低耦合，那么规则简单明了、容易实施的垂直切分必是首选。 而如果数据库中的表并不多，但单表的数据量很大、或数据热度很高，这种情况之下就应该选择水平切分，水平切分比垂直切分要复杂一些，它将原本逻辑上属于一体的数据进行了物理分割，除了在分割时要对分割的粒度做好评估，考虑数据平均和负载平均，后期也将对项目人员及应用程序产生额外的数据管理负担。 在现实项目中，往往是这两种情况兼而有之，这就需要做出权衡，甚至既需要垂直切分，又需要水平切分。 四、分表和分库分表能够解决单表数据量过大带来的查询效率下降的问题，但是，却无法给数据库的并发处理能力带来质的提升。面对高并发的读写访问，当数据库master服务器无法承载写操作压力时，不管如何扩展slave服务器，此时都没有意义了。因此，我们必须换一种思路，对数据库进行拆分，从而提高数据库写入能力，这就是所谓的分库。 分表和分区都是基于同一个数据库里的数据分离技巧，对数据库性能有一定提升，但是随着业务数据量的增加，原来所有的数据都是在一个数据库上的，网络IO及文件IO都集中在一个数据库上的，因此CPU、内存、文件IO、网络IO都可能会成为系统瓶颈。当业务系统的数据容量接近或超过单台服务器的容量、QPS/TPS接近或超过单个数据库实例的处理极限等此时，往往是采用垂直和水平结合的数据拆分方法，把数据服务和数据存储分布到多台数据库服务器上。 与分表策略相似，分库可以采用通过一个关键字取模的方式，来对数据访问进行路由，如下图所示： 五、分库分表存在的问题事务问题在执行分库分表之后，由于数据存储到了不同的库上，数据库事务管理出现了困难。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价；如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库跨表的join问题在执行了分库分表之后，难以避免会将原本逻辑关联性很强的数据划分到不同的表、不同的库上，这时，表的关联操作将受到限制，我们无法join位于不同分库的表，也无法join分表粒度不同的表，结果原本一次查询能够完成的业务，可能需要多次查询才能完成。 额外的数据管理负担和数据运算压力。额外的数据管理负担，最显而易见的就是数据的定位问题和数据的增删改查的重复执行问题，这些都可以通过应用程序解决，但必然引起额外的逻辑运算，例如，对于一个记录用户成绩的用户数据表userTable，业务要求查出成绩最好的100位，在进行分表之前，只需一个order by语句就可以搞定，但是在进行分表之后，将需要n个order by语句，分别查出每一个分表的前100名用户数据，然后再对这些数据进行合并计算，才能得出结果。 六、分片（Sharding）和分区（Partition）sharding和partition的区别： 参考文章：mysql分片、分区、分表、分库Mysql分表和分区的区别、分库和分表区别阿里P8架构师谈：数据库分库分表、读写分离的原理实现，使用场景]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>分片</tag>
        <tag>分区</tag>
        <tag>分表</tag>
        <tag>分库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[桥梁模式bridge]]></title>
    <url>%2F2019%2F06%2F19%2Fbridge%2F</url>
    <content type="text"><![CDATA[桥梁模式的定义定义: 将抽象和实现解耦, 使得两者可以独立的变化 通俗的说, 就是一个类调用另一个类中的方法, 需要一个桥梁, 通过聚合的关系调用 其类图如下: 其中角色说明如下: 1234567Abstraction 抽象化角色: 它的主要职责是定义出该角色的行为, 同时保存一个对实现化角色的引用, 一般是抽象类Implementor 实现化角色: 接口或抽象类, 定义角色必须的行为和属性RefinedAbstraction 修正抽象化角色: 它引用实现化角色对抽象化角色进行修正ConcreteImplementor 具体实现化角色: 它实现接口或抽象类定义的方法和属性 桥梁模式实现抽象角色的部分实现是由实现角色完成的 实现化角色代码: 具体实现化角色代码: 抽象化角色代码: 具体抽象化角色代码: 场景类代码: 桥梁模式优点桥梁模式是一个很简单的模式, 它只是使用了类间的聚合关系、继承、覆写等常用功能, 但是它却提供了一个非常清晰、稳定的架构。 桥梁模式的优点: 123抽象和实现分离. 这是桥梁模式的主要特点, 它完全是为了解决继承的缺点而提出的设计模式. 在该模式下,实现可以不受抽象的约束,不用再绑定在一个固定的抽象层次上,具有优秀的扩充能力.实现细节对客户透明. 客户不用关心细节的实现, 它已经由抽象层通过聚合关系完成了封装 桥梁模式的使用场景:12345不希望或不适用使用继承的场景. 例如继承层次过滤、无法更细化设计颗粒等场景接口或抽象类不稳定的场景.重用性要求较高的场景. 设计的颗粒度越细,则被重用的可能性就越大, 而采用继承则受父类的限制, 不可能出现太细的颗粒度 使用桥梁模式主要考虑如何拆分抽象和实现,并不是一设计继承就要考虑使用该模式. 桥梁模式的意图还是对变化的封装, 尽量把可能变化的因素封装到最细、最小的逻辑单元中,避免风险扩散.因此在进行系统设计时,发现类的继承有N层时,可以考虑使用桥梁模式. 桥梁模式在Java应用中的一个非常典型的例子就是JDBC驱动器。JDBC为所有的关系型数据库提供一个通用的界面。一个应用系统动态地选择一个合适的驱动器，然后通过驱动器向数据库引擎发出指令。这个过程就是将抽象角色的行为委派给实现角色的过程。 抽象角色可以针对任何数据库引擎发出查询指令，因为抽象角色并不直接与数据库引擎打交道，JDBC驱动器负责这个底层的工作。由于JDBC驱动器的存在，应用系统可以不依赖于数据库引擎的细节而独立地演化；同时数据库引擎也可以独立于应用系统的细节而独立的演化。两个独立的等级结构如下图所示，左边是JDBC API的等级结构，右边是JDBC驱动器的等级结构。应用程序是建立在JDBC API的基础之上的。 应用系统作为一个等级结构，与JDBC驱动器这个等级结构是相对独立的，它们之间没有静态的强关联。应用系统通过委派与JDBC驱动器相互作用，这是一个桥梁模式的例子。 JDBC的这种架构，把抽象部分和具体部分分离开来，从而使得抽象部分和具体部分都可以独立地扩展。对于应用程序而言，只要选用不同的驱动，就可以让程序操作不同的数据库，而无需更改应用程序，从而实现在不同的数据库上移植；对于驱动程序而言，为数据库实现不同的驱动程序，并不会影响应用程序。 以上文章来自：23种设计模式之桥梁模式]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>桥接模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql之explain详解]]></title>
    <url>%2F2019%2F06%2F18%2Fmysql-explain%2F</url>
    <content type="text"><![CDATA[原文：MySQL 性能优化神器 Explain 使用分析 简介MySQL 提供了一个 EXPLAIN 命令, 它可以对 SELECT 语句进行分析, 并输出 SELECT 执行的详细信息, 以供开发人员针对性优化.EXPLAIN 命令用法十分简单, 在 SELECT 语句前加上 Explain 就可以了, 例如: 1EXPLAIN SELECT * from user_info WHERE id &lt; 300; 准备为了接下来方便演示 EXPLAIN 的使用, 首先我们需要建立两个测试用的表, 并添加相应的数据: 1234567891011121314151617181920CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT '', `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB DEFAULT CHARSET = utf8INSERT INTO user_info (name, age) VALUES ('xys', 20);INSERT INTO user_info (name, age) VALUES ('a', 21);INSERT INTO user_info (name, age) VALUES ('b', 23);INSERT INTO user_info (name, age) VALUES ('c', 50);INSERT INTO user_info (name, age) VALUES ('d', 15);INSERT INTO user_info (name, age) VALUES ('e', 20);INSERT INTO user_info (name, age) VALUES ('f', 21);INSERT INTO user_info (name, age) VALUES ('g', 23);INSERT INTO user_info (name, age) VALUES ('h', 50);INSERT INTO user_info (name, age) VALUES ('i', 15); 12345678910111213141516171819CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT '', `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`)) ENGINE = InnoDB DEFAULT CHARSET = utf8INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p1', 'WHH');INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p2', 'WL');INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p1', 'DX');INSERT INTO order_info (user_id, product_name, productor) VALUES (2, 'p1', 'WHH');INSERT INTO order_info (user_id, product_name, productor) VALUES (2, 'p5', 'WL');INSERT INTO order_info (user_id, product_name, productor) VALUES (3, 'p3', 'MA');INSERT INTO order_info (user_id, product_name, productor) VALUES (4, 'p1', 'WHH');INSERT INTO order_info (user_id, product_name, productor) VALUES (6, 'p1', 'WHH');INSERT INTO order_info (user_id, product_name, productor) VALUES (9, 'p8', 'TE'); EXPLAIN 输出格式EXPLAIN 命令的输出内容大致如下: 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 各列的含义如下: id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type: SELECT 查询的类型. table: 查询的是哪个表 partitions: 匹配的分区 type: join 类型 possible_keys: 此次查询中可能选用的索引 key: 此次查询中确切使用到的索引. ref: 哪个字段或常数与 key 一起被使用 rows: 显示此查询一共扫描了多少行. 这个是一个估计值. filtered: 表示此查询条件所过滤的数据的百分比 extra: 额外的信息 接下来我们来重点看一下比较重要的几个字段. select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 最常见的查询类别应该是 SIMPLE 了, 比如当我们的查询没有子查询, 也没有 UNION 查询时, 那么通常就是 SIMPLE 类型, 例如: 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 如果我们使用了 UNION 查询, 那么 EXPLAIN 输出 的结果类似如下: 1234567891011mysql&gt; EXPLAIN (SELECT * FROM user_info WHERE id IN (1, 2, 3)) -&gt; UNION -&gt; (SELECT * FROM user_info WHERE id IN (3, 4, 5));+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+| 1 | PRIMARY | user_info | NULL | range | PRIMARY | PRIMARY | 8 | NULL | 3 | 100.00 | Using where || 2 | UNION | user_info | NULL | range | PRIMARY | PRIMARY | 8 | NULL | 3 | 100.00 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+----+--------------+------------+------------+-------+---------------+---------+---------+------+------+----------+-----------------+3 rows in set, 1 warning (0.00 sec) table表示查询涉及的表或衍生表 typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. type 常用类型type 常用的取值有: system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可.例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的. 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如: 12345678910111213141516171819202122232425262728mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 314 ref: NULL rows: 9 filtered: 100.00 Extra: Using where; Using index*************************** 2. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: test.order_info.user_id rows: 1 filtered: 100.00 Extra: NULL2 rows in set, 1 warning (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询.例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678910111213141516171819202122232425262728mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: refpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 9 ref: const rows: 1 filtered: 100.00 Extra: Using index2 rows in set, 1 warning (0.01 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. 例如下面的例子就是一个范围查询: 1234567891011121314151617mysql&gt; EXPLAIN SELECT * -&gt; FROM user_info -&gt; WHERE id BETWEEN 2 AND 8 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: rangepossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: NULL rows: 7 filtered: 100.00 Extra: Using where1 row in set, 1 warning (0.00 sec) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据.index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index. 例如: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: indexpossible_keys: NULL key: name_index key_len: 152 ref: NULL rows: 10 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免.下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的. 123456789101112131415mysql&gt; EXPLAIN SELECT age FROM user_info WHERE age = 20 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 10 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) type 类型的性能比较通常来说, 不同的 type 类型的性能关系如下:ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; systemALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的.而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快.后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key此字段是 MySQL 在当前查询时所真正使用到的索引. key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的栗子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id &lt; 3 AND product_name = 'p1' AND productor = 'WHH' \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: rangepossible_keys: user_product_detail_index key: user_product_detail_index key_len: 9 ref: NULL rows: 5 filtered: 11.11 Extra: Using where; Using index1 row in set, 1 warning (0.00 sec) 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 不过此查询语句 WHERE user_id &lt; 3 AND product_name = &#39;p1&#39; AND productor = &#39;WHH&#39; 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT &#39;0&#39;, 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = 'p1' \G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: refpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 161 ref: const,const rows: 2 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = &#39;p1&#39; 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 rowsrows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY product_name \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: NULL key: user_product_detail_index key_len: 253 ref: NULL rows: 9 filtered: 100.00 Extra: Using index; Using filesort1 row in set, 1 warning (0.00 sec) 我们的索引是 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort. 如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: NULL key: user_product_detail_index key_len: 253 ref: NULL rows: 9 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java动态代理]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-dynamic-proxy%2F</url>
    <content type="text"><![CDATA[以下文章来自：java动态代理实现与原理详细分析 关于Java中的动态代理，我们首先需要了解的是一种常用的设计模式–代理模式，而对于代理，根据创建代理类的时间点，又可以分为静态代理和动态代理。 一、代理模式 代理模式是常用的java设计模式，他的特征是代理类与委托类有同样的接口，代理类主要负责为委托类预处理消息、过滤消息、把消息转发给委托类，以及事后处理消息等。代理类与委托类之间通常会存在关联关系，一个代理类的对象与一个委托类的对象关联，代理类的对象本身并不真正实现服务，而是通过调用委托类的对象的相关方法，来提供特定的服务。简单的说就是，我们在访问实际对象时，是通过代理对象来访问的，代理模式就是在访问实际对象时引入一定程度的间接性，因为这种间接性，可以附加多种用途。在后面我会解释这种间接性带来的好处。代理模式结构图（图片来自《大话设计模式》）： 二、静态代理静态代理静态代理：由程序员创建或特定工具自动生成源代码，也就是在编译时就已经将接口，被代理类，代理类等确定下来。在程序运行之前，代理类的.class文件就已经生成。 静态代理简单实现 根据上面代理模式的类图，来写一个简单的静态代理的例子。我这儿举一个比较粗糙的例子，假如一个班的同学要向老师交班费，但是都是通过班长把自己的钱转交给老师。这里，班长就是代理学生上交班费， 班长就是学生的代理。 ​ 首先，我们创建一个Person接口。这个接口就是学生（被代理类），和班长（代理类）的公共接口，他们都有上交班费的行为。这样，学生上交班费就可以让班长来代理执行。 12345678/** * 创建Person接口 * @author Gonjan */public interface Person &#123; //上交班费 void giveMoney();&#125; Student类实现Person接口。Student可以具体实施上交班费的动作。 1234567891011public class Student implements Person &#123; private String name; public Student(String name) &#123; this.name = name; &#125; @Override public void giveMoney() &#123; System.out.println(name + "上交班费50元"); &#125;&#125; StudentsProxy类，这个类也实现了Person接口，但是还另外持有一个学生类对象，由于实现了Peson接口，同时持有一个学生对象，那么他可以代理学生类对象执行上交班费（执行giveMoney()方法）行为。 123456789101112131415161718192021/** * 学生代理类，也实现了Person接口，保存一个学生实体，这样既可以代理学生产生行为 * @author Gonjan * */public class StudentsProxy implements Person&#123; //被代理的学生 Student stu; public StudentsProxy(Person stu) &#123; // 只代理学生对象 if(stu.getClass() == Student.class) &#123; this.stu = (Student)stu; &#125; &#125; //代理上交班费，调用被代理学生的上交班费行为 public void giveMoney() &#123; stu.giveMoney(); &#125;&#125; 下面测试一下，看如何使用代理模式： 123456789101112public class StaticProxyTest &#123; public static void main(String[] args) &#123; //被代理的学生张三，他的班费上交有代理对象monitor（班长）完成 Person zhangsan = new Student("张三"); //生成代理对象，并将张三传给代理对象 Person monitor = new StudentsProxy(zhangsan); //班长代理上交班费 monitor.giveMoney(); &#125;&#125; 运行结果： 这里并没有直接通过张三（被代理对象）来执行上交班费的行为，而是通过班长（代理对象）来代理执行了。这就是代理模式。 代理模式最主要的就是有一个公共接口（Person），一个具体的类（Student），一个代理类（StudentsProxy）,代理类持有具体类的实例，代为执行具体类实例方法。上面说到，代理模式就是在访问实际对象时引入一定程度的间接性，因为这种间接性，可以附加多种用途。这里的间接性就是指不直接调用实际对象的方法，那么我们在代理过程中就可以加上一些其他用途。就这个例子来说，加入班长在帮张三上交班费之前想要先反映一下张三最近学习有很大进步，通过代理模式很轻松就能办到： 1234567891011121314151617public class StudentsProxy implements Person&#123; //被代理的学生 Student stu; public StudentsProxy(Person stu) &#123; // 只代理学生对象 if(stu.getClass() == Student.class) &#123; this.stu = (Student)stu; &#125; &#125; //代理上交班费，调用被代理学生的上交班费行为 public void giveMoney() &#123; System.out.println("张三最近学习有进步！"); stu.giveMoney(); &#125;&#125; 运行结果： 可以看到，只需要在代理类中帮张三上交班费之前，执行其他操作就可以了。这种操作，也是使用代理模式的一个很大的优点。最直白的就是在Spring中的面向切面编程（AOP），我们能在一个切点之前执行一些操作，在一个切点之后执行一些操作，这个切点就是一个个方法。这些方法所在类肯定就是被代理了，在代理过程中切入了一些其他操作。 三、动态代理动态代理代理类在程序运行时创建的代理方式被成为动态代理。 我们上面静态代理的例子中，代理类(studentProxy)是自己定义好的，在程序运行之前就已经编译完成。然而动态代理，代理类并不是在Java代码中定义的，而是在运行时根据我们在Java代码中的“指示”动态生成的。相比于静态代理， 动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类中的方法。 比如说，想要在每个代理的方法前都加上一个处理方法： 12345public void giveMoney() &#123; //调用被代理方法前加入处理方法 beforeMethod(); stu.giveMoney(); &#125; 这里只有一个giveMoney方法，就写一次beforeMethod方法，但是如果出了giveMonney还有很多其他的方法，那就需要写很多次beforeMethod方法，麻烦。那看看下面动态代理如何实现。 动态代理简单实现在java的java.lang.reflect包下提供了一个Proxy类和一个InvocationHandler接口，通过这个类和这个接口可以生成JDK动态代理类和动态代理对象。 创建一个动态代理对象步骤，具体代码见后面： 创建一个InvocationHandler对象 12//创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new MyInvocationHandler&lt;Person&gt;(stu); 使用Proxy类的getProxyClass静态方法生成一个动态代理类stuProxyClass 1Class&lt;?&gt; stuProxyClass = Proxy.getProxyClass(Person.class.getClassLoader(), new Class&lt;?&gt;[] &#123;Person.class&#125;); 获得stuProxyClass 中一个带InvocationHandler参数的构造器constructor 1Constructor&lt;?&gt; constructor = PersonProxy.getConstructor(InvocationHandler.class); 通过构造器constructor来创建一个动态实例stuProxy 1Person stuProxy = (Person) cons.newInstance(stuHandler); 就此，一个动态代理对象就创建完毕，当然，上面四个步骤可以通过Proxy类的newProxyInstances方法来简化： 1234 //创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new MyInvocationHandler&lt;Person&gt;(stu);//创建一个代理对象stuProxy，代理对象的每个执行方法都会替换执行Invocation中的invoke方法 Person stuProxy= (Person) Proxy.newProxyInstance(Person.class.getClassLoader(), new Class&lt;?&gt;[]&#123;Person.class&#125;, stuHandler); 到这里肯定都会很疑惑，这动态代理到底是如何执行的，是如何通过代理对象来执行被代理对象的方法的，先不急，我们先看看一个简单的完整的动态代理的例子。还是上面静态代理的例子，班长需要帮学生代交班费。首先是定义一个Person接口: 12345678/** * 创建Person接口 * @author Gonjan */public interface Person &#123; //上交班费 void giveMoney();&#125; 创建需要被代理的实际类： 1234567891011121314151617public class Student implements Person &#123; private String name; public Student(String name) &#123; this.name = name; &#125; @Override public void giveMoney() &#123; try &#123; //假设数钱花了一秒时间 Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(name + "上交班费50元"); &#125;&#125; 再定义一个检测方法执行时间的工具类，在任何方法执行前先调用start方法，执行后调用finsh方法，就可以计算出该方法的运行时间，这也是一个最简单的方法执行时间检测工具。 1234567891011121314public class MonitorUtil &#123; private static ThreadLocal&lt;Long&gt; tl = new ThreadLocal&lt;&gt;(); public static void start() &#123; tl.set(System.currentTimeMillis()); &#125; //结束时打印耗时 public static void finish(String methodName) &#123; long finishTime = System.currentTimeMillis(); System.out.println(methodName + "方法耗时" + (finishTime - tl.get()) + "ms"); &#125;&#125; 创建StuInvocationHandler类，实现InvocationHandler接口，这个类中持有一个被代理对象的实例target。InvocationHandler中有一个invoke方法，所有执行代理对象的方法都会被替换成执行invoke方法。 再再invoke方法中执行被代理对象target的相应方法。当然，在代理过程中，我们在真正执行被代理对象的方法前加入自己其他处理。这也是Spring中的AOP实现的主要原理，这里还涉及到一个很重要的关于java反射方面的基础知识。 123456789101112131415161718192021222324public class StuInvocationHandler&lt;T&gt; implements InvocationHandler &#123; //invocationHandler持有的被代理对象 T target; public StuInvocationHandler(T target) &#123; this.target = target; &#125; /** * proxy:代表动态代理对象 * method：代表正在执行的方法 * args：代表调用目标方法时传入的实参 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("代理执行" +method.getName() + "方法"); */ //代理过程中插入监测方法,计算该方法耗时 MonitorUtil.start(); Object result = method.invoke(target, args); MonitorUtil.finish(method.getName()); return result; &#125;&#125; 做完上面的工作后，我们就可以具体来创建动态代理对象了，上面简单介绍了如何创建动态代理对象，我们使用简化的方式创建动态代理对象： 12345678910111213141516public class ProxyTest &#123; public static void main(String[] args) &#123; //创建一个实例对象，这个对象是被代理的对象 Person zhangsan = new Student("张三"); //创建一个与代理对象相关联的InvocationHandler InvocationHandler stuHandler = new StuInvocationHandler&lt;Person&gt;(zhangsan); //创建一个代理对象stuProxy来代理zhangsan，代理对象的每个执行方法都会替换执行Invocation中的invoke方法 Person stuProxy = (Person) Proxy.newProxyInstance(Person.class.getClassLoader(), new Class&lt;?&gt;[]&#123;Person.class&#125;, stuHandler)； //代理执行上交班费的方法 stuProxy.giveMoney(); &#125;&#125; 我们执行这个ProxyTest类，先想一下，我们创建了一个需要被代理的学生张三，将zhangsan对象传给了stuHandler中，我们在创建代理对象stuProxy时，将stuHandler作为参数了的，上面也有说到所有执行代理对象的方法都会被替换成执行invoke方法，也就是说，最后执行的是StuInvocationHandler中的invoke方法。所以在看到下面的运行结果也就理所当然了。 运行结果： 上面说到，动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类中的方法。是因为所有被代理执行的方法，都是通过在InvocationHandler中的invoke方法调用的，所以我们只要在invoke方法中统一处理，就可以对所有被代理的方法进行相同的操作了。例如，这里的方法计时，所有的被代理对象执行的方法都会被计时，然而我只做了很少的代码量。 动态代理的过程，代理对象和被代理对象的关系不像静态代理那样一目了然，清晰明了。因为动态代理的过程中，我们并没有实际看到代理类，也没有很清晰地的看到代理类的具体样子，而且动态代理中被代理对象和代理对象是通过InvocationHandler来完成的代理过程的，其中具体是怎样操作的，为什么代理对象执行的方法都会通过InvocationHandler中的invoke方法来执行。带着这些问题，我们就需要对java动态代理的源码进行简要的分析，弄清楚其中缘由。 四、动态代理原理分析​ 1、Java动态代理创建出来的动态代理类 上面我们利用Proxy类的newProxyInstance方法创建了一个动态代理对象，查看该方法的源码，发现它只是封装了创建动态代理类的步骤(红色标准部分)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException&#123; Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); //红色部分 final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * Look up or generate the designated proxy class. */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams);//红色部分 final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; return cons.newInstance(new Object[]&#123;h&#125;); //红色部分 &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125;&#125; 其实，我们最应该关注的是Class&lt;?&gt; cl = getProxyClass0(loader, intfs);这句，这里产生了代理类，后面代码中的构造器也是通过这里产生的类来获得，可以看出，这个类的产生就是整个动态代理的关键，由于是动态生成的类文件，我这里不具体进入分析如何产生的这个类文件，只需要知道这个类文件时缓存在java虚拟机中的，我们可以通过下面的方法将其打印到文件里面，一睹真容： 123456789byte[] classFile = ProxyGenerator.generateProxyClass("$Proxy0", Student.class.getInterfaces());String path = "G:/javacode/javase/Test/bin/proxy/StuProxy.class";try(FileOutputStream fos = new FileOutputStream(path)) &#123; fos.write(classFile); fos.flush(); System.out.println("代理类class文件写入成功");&#125; catch (Exception e) &#123; System.out.println("写文件错误");&#125; 对这个class文件进行反编译，我们看看jdk为我们生成了什么样的内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;import proxy.Person;public final class $Proxy0 extends Proxy implements Person&#123; private static Method m1; private static Method m2; private static Method m3; private static Method m0; /** *注意这里是生成代理类的构造方法，方法参数为InvocationHandler类型，看到这，是不是就有点明白 *为何代理对象调用方法都是执行InvocationHandler中的invoke方法，而InvocationHandler又持有一个 *被代理对象的实例，不禁会想难道是....？ 没错，就是你想的那样。 * *super(paramInvocationHandler)，是调用父类Proxy的构造方法。 *父类持有：protected InvocationHandler h; *Proxy构造方法： * protected Proxy(InvocationHandler h) &#123; * Objects.requireNonNull(h); * this.h = h; * &#125; * */ public $Proxy0(InvocationHandler paramInvocationHandler) throws &#123; super(paramInvocationHandler); &#125; //这个静态块本来是在最后的，我把它拿到前面来，方便描述 static &#123; try &#123; //看看这儿静态块儿里面有什么，是不是找到了giveMoney方法。请记住giveMoney通过反射得到的名字m3，其他的先不管 m1 = Class.forName("java.lang.Object").getMethod("equals", new Class[] &#123; Class.forName("java.lang.Object") &#125;); m2 = Class.forName("java.lang.Object").getMethod("toString", new Class[0]); m3 = Class.forName("proxy.Person").getMethod("giveMoney", new Class[0]); m0 = Class.forName("java.lang.Object").getMethod("hashCode", new Class[0]); return; &#125; catch (NoSuchMethodException localNoSuchMethodException) &#123; throw new NoSuchMethodError(localNoSuchMethodException.getMessage()); &#125; catch (ClassNotFoundException localClassNotFoundException) &#123; throw new NoClassDefFoundError(localClassNotFoundException.getMessage()); &#125; &#125; /** * *这里调用代理对象的giveMoney方法，直接就调用了InvocationHandler中的invoke方法，并把m3传了进去。 *this.h.invoke(this, m3, null);这里简单，明了。 *来，再想想，代理对象持有一个InvocationHandler对象，InvocationHandler对象持有一个被代理的对象， *再联系到InvacationHandler中的invoke方法。嗯，就是这样。 */ public final void giveMoney() throws &#123; try &#123; this.h.invoke(this, m3, null); return; &#125; catch (Error|RuntimeException localError) &#123; throw localError; &#125; catch (Throwable localThrowable) &#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; //注意，这里为了节省篇幅，省去了toString，hashCode、equals方法的内容。原理和giveMoney方法一毛一样。&#125; jdk为我们的生成了一个叫$Proxy0（这个名字后面的0是编号，有多个代理类会一次递增）的代理类，这个类文件是放在内存中的，我们在创建代理对象时，就是通过反射获得这个类的构造方法，然后创建的代理实例。通过对这个生成的代理类源码的查看，我们很容易能看出，动态代理实现的具体过程。 我们可以把InvocationHandler看做一个中介类，中介类持有一个被代理对象，在invoke方法中调用了被代理对象的相应方法。通过聚合方式持有被代理对象的引用，把外部对invoke的调用最终都转为对被代理对象的调用。 代理类调用自己方法时，通过自身持有的中介类对象来调用中介类对象的invoke方法，从而达到代理执行被代理对象的方法。也就是说，动态代理通过中介类实现了具体的代理功能。 五、总结生成的代理类：$Proxy0 extends Proxy implements Person，我们看到代理类继承了Proxy类，所以也就决定了java动态代理只能对接口进行代理，Java的继承机制注定了这些动态代理类们无法实现对class的动态代理。 上面的动态代理的例子，其实就是AOP的一个简单实现了，在目标对象的方法执行之前和执行之后进行了处理，对方法耗时统计。Spring的AOP实现其实也是用了Proxy和InvocationHandler这两个东西的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql主从复制和读写分离]]></title>
    <url>%2F2019%2F06%2F18%2Fmysql-master-slave-replication%2F</url>
    <content type="text"><![CDATA[以下文章参考自：：MySql 主从复制及配置实现 Mysql 主从复制的原理和kafka的partition的replication机制很类似，原理互通，大概是因为这种做法确实可以保证分布式系统的可靠性。 一、什么是Mysql主从复制MySQL主从复制是其最重要的功能之一。主从复制是指一台服务器充当主数据库服务器，另一台或多台服务器充当从数据库服务器，主服务器中的数据自动复制到从服务器之中。对于多级复制，数据库服务器即可充当主机，也可充当从机。MySQL主从复制的基础是主服务器对数据库修改记录二进制日志(bin log)，从服务器通过主服务器的二进制日志自动执行更新。 二、Mysq主从复制的类型基于语句的复制：主服务器上面执行的语句在从服务器上面再执行一遍，在MySQL-3.23版本以后支持。 存在的问题：时间上可能不完全同步造成偏差，执行语句的用户也可能不是同一个用户。 基于行的复制：把主服务器上面改变后的内容直接复制过去，而不关心到底改变该内容是由哪条语句引发的，在MySQL-5.0版本以后引入。 存在的问题：比如一个工资表中有一万个用户，我们把每个用户的工资+1000，那么基于行的复制则要复制一万行的内容，由此造成的开销比较大，而基于语句的复制仅仅一条语句就可以了。 混合类型的复制：MySQL默认使用基于语句的复制，当基于语句的复制会引发问题的时候就会使用基于行的复制，MySQL会自动进行选择。 在MySQL主从复制架构中，读操作可以在所有的服务器上面进行，而写操作只能在主服务器上面进行。主从复制架构虽然给读操作提供了扩展，可如果写操作也比较多的话（多台从服务器还要从主服务器上面同步数据），单主模型的复制中主服务器势必会成为性能瓶颈。 三、主从复制的作用1、主数据库出现问题，可以切换到从数据库。2、可以进行数据库层面的读写分离。读写分离就是在主服务器上修改，数据会同步到从服务器，从服务器只能提供读取数据，不能写入，实现备份的同时也实现了数据库性能的优化，以及提升了服务器安全。 1234567891011121314151）基于程序代码内部实现在代码中根据select 、insert进行路由分类，这类方法也是目前生产环境下应用最广泛的。优点是性能较好，因为程序在代码中实现，不需要增加额外的硬件开支，缺点是需要开发人员来实现，运维人员无从下手。2）基于中间代理层实现代理一般介于应用服务器和数据库服务器之间，代理数据库服务器接收到应用服务器的请求后根据判断后转发到，后端数据库，有以下代表性的程序。（1）MySQL_proxy。MySQL_proxy是MySQL的一个开源项目，通过其自带的lua脚本进行sql判断。（2）Atlas。是由 Qihoo 360, Web平台部基础架构团队开发维护的一个基于MySQL协议的数据中间层项目。它是在MySQL-proxy 0.8.2版本的基础上，对其进行了优化，增加了一些新的功能特性。360内部使用Atlas运行的MySQL业务，每天承载的读写请求数达几十亿条。支持事物以及存储过程。（3）Amoeba。由阿里巴巴集团在职员工陈思儒使用java语言进行开发，阿里巴巴集团将其用户生产环境下，但是它并不支持事物以及存储过程。不是所有的应用都能够在基于程序代码中实现读写分离，像一些大型的java应用，如果在程序代码中实现读写分离对代码的改动就较大，所以，像这种应用一般会考虑使用代理层来实现。 3、可以在从数据库上进行日常备份 四、Mysql主从复制的工作原理如下图所示： [ 主服务器上面的任何修改都会保存在二进制日志Binary log里面，从服务器上面启动一个I/O thread（实际上就是一个主服务器的客户端进程），连接到主服务器上面请求读取二进制日志，然后把读取到的二进制日志写到本地的一个Realy log（中继日志）里面。从服务器上面开启一个SQL thread定时检查Realy log，如果发现有更改立即把更改的内容在本机上面执行一遍。 如果一主多从的话，这时主库既要负责写又要负责为几个从库提供二进制日志。此时可以稍做调整，将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从。或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。工作原理图如下： [ 实际上在老版本的MySQL主从复制中Slave端并不是两个进程完成的，而是由一个进程完成。但是后来发现这样做存在较大的风险和性能问题，主要如下： 123首先，一个进程会使复制bin-log日志和解析日志并在自身执行的过程成为一个串行的过程，性能受到了一定的限制，异步复制的延迟也会比较长。另外，Slave端从Master端获取bin-log过来之后，需要接着解析日志内容，然后在自身执行。在这个过程中，Master端可能又产生了大量变化并新增了大量的日志。如果在这个阶段Master端的存储出现了无法修复的错误，那么在这个阶段所产生的所有变更都将永远无法找回。如果在Slave端的压力比较大的时候，这个过程的时间可能会比较长。 为了提高复制的性能并解决存在的风险，后面版本的MySQL将Slave端的复制动作交由两个进程来完成。提出这个改进方案的人是Yahoo!的一位工程师“Jeremy Zawodny”。这样既解决了性能问题，又缩短了异步的延时时间，同时也减少了可能存在的数据丢失量。 当然，即使是换成了现在这样两个线程处理以后，同样也还是存在slave数据延时以及数据丢失的可能性的，毕竟这个复制是异步的。只要数据的更改不是在一个事务中，这些问题都是会存在的。如果要完全避免这些问题，就只能用MySQL的cluster来解决了。不过MySQL的cluster是内存数据库的解决方案，需要将所有数据都load到内存中，这样就对内存的要求就非常大了，对于一般的应用来说可实施性不是太大。 还有一点要提的是MySQL的复制过滤(Replication Filters)，复制过滤可以让你只复制服务器中的一部分数据。有两种复制过滤：在Master上过滤二进制日志中的事件；在Slave上过滤中继日志中的事件。如下： [ 配置Master的my.cnf文件(关键性的配置)/etc/my.cnf 1234567891011log-bin=mysql-binserver-id = 1binlog-do-db=icingabinlog-do-db=DB2 //如果备份多个数据库，重复设置这个选项即可binlog-do-db=DB3 //需要同步的数据库，如果没有本行，即表示同步所有的数据库binlog-ignore-db=mysql //被忽略的数据库 配置Slave的my.cnf文件(关键性的配置)/etc/my.cnf 12345678910111213141516171819log-bin=mysql-binserver-id=2master-host=10.1.68.110master-user=backupmaster-password=1234qwermaster-port=3306replicate-do-db=icingareplicate-do-db=DB2replicate-do-db=DB3 //需要同步的数据库，如果没有本行，即表示同步所有的数据库replicate-ignore-db=mysql //被忽略的数据库 网友说replicate-do-db的使用中可能会出些问题（http://blog.knowsky.com/19696…），自己没有亲自去测试。猜想binlog-do-db参数用于主服务器中，通过过滤Binary Log来过滤掉配置文件中不允许复制的数据库，也就是不向Binary Log中写入不允许复制数据的操作日志；而replicate-do-db用于从服务器中，通过过滤Relay Log来过滤掉不允许复制的数据库或表，也就是执行Relay Log中的动作时不执行那些不被允许的修改动作。这样的话，多个从数据库服务器的情况：有的从服务器既从主服务器中复制数据，又做为主服务器向另外的从服务器复制数据，那它的配置文件中应该可以同时存在binlog-do-db、replicate-do-db这两个参数才对。一切都是自己的预测，关于binlog-do-db、replicate-do-db的具体使用方法还得在实际开发中一点点摸索才可以。 网上有说，复制时忽略某些数据库或者表的操作最好不要在主服务器上面进行，因为主服务器忽略之后就不会再往二进制文件中写了，但是在从服务器上面虽然忽略了某些数据库但是主服务器上面的这些操作信息依然会被复制到从服务器上面的relay log里面，只是不会在从服务器上面执行而已。我想这个意思应该是建议在从服务器中设置replicate-do-db，而不要在主服务器上设置binlog-do-db。 另外，不管是黑名单（binlog-ignore-db、replicate-ignore-db）还是白名单（binlog-do-db、replicate-do-db）只写一个就行了，如果同时使用那么只有白名单生效。 五、Mysql主从复制的过程MySQL主从复制的两种情况：同步复制和异步复制，实际复制架构中大部分为异步复制。 复制的基本过程如下： Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容。 Master接收到来自Slave的IO进程的请求后，负责复制的IO进程会根据请求信息读取日志指定位置之后的日志信息，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置。 Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的 bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”。 Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行。 六、Mysql主从复制的具体配置复制通常用来创建主节点的副本，通过添加冗余节点来保证高可用性，当然复制也可以用于其他用途，例如在从节点上进行数据读、分析等等。在横向扩展的业务中，复制很容易实施，主要表现在在利用主节点进行写操作，多个从节点进行读操作，MySQL复制的异步性是指：事物首先在主节点上提交，然后复制给从节点并在从节点上应用，这样意味着在同一个时间点主从上的数据可能不一致。异步复制的好处在于它比同步复制要快，如果对数据的一致性要求很高，还是采用同步复制较好。 最简单的复制模式就是一主一从的复制模式了，这样一个简单的架构只需要三个步骤即可完成： （1）建立一个主节点，开启binlog，设置服务器id； （2）建立一个从节点，设置服务器id； （3）将从节点连接到主节点上。 下面我们开始操作，以MySQL 5.5为例，操作系统Ubuntu12.10，Master 10.1.6.159 Slave 10.1.6.191。 1apt-get install mysql-server Master机器Master上面开启binlog日志，并且设置一个唯一的服务器id，在局域网内这个id必须唯一。二进制的binlog日志记录master上的所有数据库改变，这个日志会被复制到从节点上，并且在从节点上回放。修改my.cnf文件，在mysqld模块下修改如下内容： 123[mysqld]server-id = 1log_bin = /var/log/mysql/mysql-bin.log log_bin设置二进制日志所产生文件的基本名称，二进制日志由一系列文件组成，log_bin的值是可选项，如果没有为log_bin设置值，则默认值是：主机名-bin。如果随便修改主机名，则binlog日志的名称也会被改变的。server-id是用来唯一标识一个服务器的，每个服务器的server-id都不一样。这样slave连接到master后，会请求master将所有的binlog传递给它，然后将这些binlog在slave上回放。为了防止权限混乱，一般都是建立一个单独用于复制的账户。 binlog是复制过程的关键，它记录了数据库的所有改变，通常即将执行完毕的语句会在binlog日志的末尾写入一条记录，binlog只记录改变数据库的语句，对于不改变数据库的语句则不进行记录。这种情况叫做基于语句的复制，前面提到过还有一种情况是基于行的复制，两种模式各有各的优缺点。 Slave机器slave机器和master一样，需要一个唯一的server-id。 12[mysqld]server-id = 2 连接Slave到Master 在Master和Slave都配置好后，只需要把slave只想master即可 123change master to master_host='10.1.6.159',master_port=3306,master_user='rep',master_password='123456';start slave; 接下来在master上做一些针对改变数据库的操作，来观察slave的变化情况。在修改完my.cnf配置重启数据库后，就开始记录binlog了。可以在/var/log/mysql目录下看到一个mysql-bin.000001文件，而且还有一个mysql-bin.index文件，这个mysql-bin.index文件是什么？这个文件保存了所有的binlog文件列表，但是我们在配置文件中并没有设置改值，这个可以通过log_bin_index进行设置，如果没有设置改值，则默认值和log_bin一样。在master上执行show binlog events命令，可以看到第一个binlog文件的内容。 注意：上面的sql语句是从头开始复制第一个binlog，如果想从某个位置开始复制binlog，就需要在change master to时指定要开始的binlog文件名和语句在文件中的起点位置，参数如下：master_log_file和master_log_pos。 1234567891011121314151617181920212223mysql&gt; show binlog events\G*************************** 1. row *************************** Log_name: mysql-bin.000001 Pos: 4 Event_type: Format_desc Server_id: 1End_log_pos: 107 Info: Server ver: 5.5.28-0ubuntu0.12.10.2-log, Binlog ver: 4*************************** 2. row *************************** Log_name: mysql-bin.000001 Pos: 107 Event_type: Query Server_id: 1End_log_pos: 181 Info: create user rep*************************** 3. row *************************** Log_name: mysql-bin.000001 Pos: 181 Event_type: Query Server_id: 1End_log_pos: 316 Info: grant replication slave on *.* to rep identified by '123456'3 rows in set (0.00 sec) Log_name 是二进制日志文件的名称，一个事件不能横跨两个文件 Pos 这是该事件在文件中的开始位置 Event_type 事件的类型，事件类型是给slave传递信息的基本方法，每个新的binlog都已Format_desc类型开始，以Rotate类型结束 Server_id 创建该事件的服务器id End_log_pos 该事件的结束位置，也是下一个事件的开始位置，因此事件范围为Pos~End_log_pos-1 Info 事件信息的可读文本，不同的事件有不同的信息 示例 在master的test库中创建一个rep表，并插入一条记录。 123create table rep(name var);insert into rep values ("guol");flush logs; flush logs命令强制轮转日志，生成一个新的二进制日志，可以通过show binlog events in ‘xxx’来查看该二进制日志。可以通过show master status查看当前正在写入的binlog文件。这样就会在slave上执行相应的改变操作。 上面就是最简单的主从复制模式，不过有时候随着时间的推进，binlog会变得非常庞大，如果新增加一台slave，从头开始复制master的binlog文件是非常耗时的，所以我们可以从一个指定的位置开始复制binlog日志，可以通过其他方法把以前的binlog文件进行快速复制，例如copy物理文件。在change master to中有两个参数可以实现该功能，master_log_file和master_log_pos，通过这两个参数指定binlog文件及其位置。我们可以从master上复制也可以从slave上复制，假如我们是从master上复制，具体操作过程如下： （1）为了防止在操作过程中数据更新，导致数据不一致，所以需要先刷新数据并锁定数据库：flush tables with read lock。 （2）检查当前的binlog文件及其位置：show master status。 1234567mysql&gt; show master status\G*************************** 1. row ***************************File: mysql-bin.000003Position: 107Binlog_Do_DB:Binlog_Ignore_DB:1 row in set (0.00 sec) （3）通过mysqldump命令创建数据库的逻辑备分：mysqldump –all-databases -hlocalhost -p &gt;back.sql。 （4）有了master的逻辑备份后，对数据库进行解锁：unlock tables。 （5）把back.sql复制到新的slave上，执行：mysql -hlocalhost -p 把master的逻辑备份插入slave的数据库中。 （6）现在可以把新的slave连接到master上了，只需要在change master to中多设置两个参数master_log_file=’mysql-bin.000003’和master_log_pos=’107’即可，然后启动slave：start slave，这样slave就可以接着107的位置进行复制了。 123change master to master_host='10.1.6.159',master_port=3306,master_user='rep',master_password='123456',master_log_file='mysql-bin.000003',master_log_pos='107';start slave; 有时候master并不能让你锁住表进行复制，因为可能跑一些不间断的服务，如果这时master已经有了一个slave，我们则可以通过这个slave进行再次扩展一个新的slave。原理同在master上进行复制差不多，关键在于找到binlog的位置，你在复制的同时可能该slave也在和master进行同步，操作如下： （1）为了防止数据变动，还是需要停止slave的同步：stop slave。 （2）然后刷新表，并用mysqldump逻辑备份数据库。 （3）使用show slave status查看slave的相关信息，记录下两个字段的值Relay_Master_Log_File和Exec_Master_Log_Pos，这个用来确定从后面哪里开始复制。 （4）对slave解锁，把备份的逻辑数据库导入新的slave的数据库中，然后设置change master to，这一步和复制master一样。 七、深入了解Mysql主从配置一主多从由一个master和一个slave组成复制系统是最简单的情况。Slave之间并不相互通信，只能与master进行通信。在实际应用场景中，MySQL复制90%以上都是一个Master复制到一个或者多个Slave的架构模式，主要用于读压力比较大的应用的数据库端廉价扩展解决方案。 [ 在上图中，是我们开始时提到的一主多从的情况，这时主库既要负责写又要负责为几个从库提供二进制日志。这种情况将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从，或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。 主主复制[ 上图中，Master-Master复制的两台服务器，既是master，又是另一台服务器的slave。这样，任何一方所做的变更，都会通过复制应用到另外一方的数据库中。在这种复制架构中，各自上运行的不是同一db，比如左边的是db1,右边的是db2，db1的从在右边反之db2的从在左边，两者互为主从，再辅助一些监控的服务还可以实现一定程度上的高可以用。 主动—被动模式的Master-Master(Master-Master in Active-Passive Mode)[ 上图中，这是由master-master结构变化而来的，它避免了M-M的缺点，实际上，这是一种具有容错和高可用性的系统。它的不同点在于其中只有一个节点在提供读写服务，另外一个节点时刻准备着，当主节点一旦故障马上接替服务。比如通过corosync+pacemaker+drbd+MySQL就可以提供这样一组高可用服务，主备模式下再跟着slave服务器，也可以实现读写分离。 带从服务器的Master-Master结构(Master-Master with Slaves)[ 这种结构的优点就是提供了冗余。在地理上分布的复制结构，它不存在单一节点故障问题，而且还可以将读密集型的请求放到slave上。 MySQL-5.5支持半同步复制 早前的MySQL复制只能是基于异步来实现，从MySQL-5.5开始，支持半自动复制。在以前的异步（asynchronous）复制中，主库在执行完一些事务后，是不会管备库的进度的。如果备库处于落后，而更不幸的是主库此时又出现Crash（例如宕机），这时备库中的数据就是不完整的。简而言之，在主库发生故障的时候，我们无法使用备库来继续提供数据一致的服务了。Semisynchronous Replication(半同步复制)则一定程度上保证提交的事务已经传给了至少一个备库。Semi synchronous中，仅仅保证事务的已经传递到备库上，但是并不确保已经在备库上执行完成了。 此外，还有一种情况会导致主备数据不一致。在某个session中，主库上提交一个事务后，会等待事务传递给至少一个备库，如果在这个等待过程中主库Crash，那么也可能备库和主库不一致，这是很致命的。如果主备网络故障或者备库挂了，主库在事务提交后等待10秒（rpl_semi_sync_master_timeout的默认值）后，就会继续。这时，主库就会变回原来的异步状态。 MySQL在加载并开启Semi-sync插件后，每一个事务需等待备库接收日志后才返回给客户端。如果做的是小事务，两台主机的延迟又较小，则Semi-sync可以实现在性能很小损失的情况下的零数据丢失。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中clone常见的三种方式]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-clone-threeMethod%2F</url>
    <content type="text"><![CDATA[在 JAVA 中克隆一个对象常见的有三种形式 : 通过自己写一个克隆方法，里面 new 一个同样的对象来进行 get、set 依次赋值实现深度克隆（很繁琐且易出错）； 通过实现 Cloneable 接口并重写 Object 类的 clone() 方法（分为深浅两种方式）； 通过实现 Serializable 接口并用对象的序列化和反序列化来实现真正的深度克隆； 下面介绍第二、第三种方法。 Cloneable 接口实现克隆Cloneable 接口实现浅克隆12345678910111213141516171819202122232425262728public class People implements Cloneable &#123; private String name = "ilt"; private Hand hand = new Hand(); public Hand getHand() &#123; return hand; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub return super.clone(); &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; People p1 = new People(); People p2 = (People) p1.clone(); System.out.println("第一个对象的hash值:"+p1.hashCode()); System.out.println("第二个对象的hash值:"+p2.hashCode()); System.out.println("分割线-----------"); System.out.println("p1中的hand对象的hash值:"+p1.getHand().hashCode()); System.out.println("p2中的hand对象的hash值:"+p2.getHand().hashCode()); &#125;&#125;class Hand implements Cloneable &#123;&#125; 上面代码输出的结果如下，根据hash值相等能确定两个对象是否相等的原则，发现p1和p2不等，但p1中的hand对象与p2中的hand对象是相等的。Cloneable 接口实现克隆是先在内存中开辟一块和原始对象一样的空间，然后原样拷贝原始对象中的内容，对基本数据类型就是值复制，而对非基本类型变量保存的仅仅是对象的引用，所以会导致 clone 后的非基本类型变量和原始对象中相应的变量指向的是同一个对象。 12345第一个对象的hash值:1408448235第二个对象的hash值:77244764分割线-----------p1中的hand对象的hash值:1172625760p2中的hand对象的hash值:1172625760 Cloneable 接口实现深克隆在浅度克隆的基础上对于要克隆对象中的非基本数据类型的属性对应的类也实现克隆，这样对于非基本数据类型的属性复制的不是一份引用。 123456789101112131415161718192021222324252627282930313233343536373839public class People implements Cloneable &#123; private String name = "ilt"; private Hand hand = new Hand(); public Hand getHand() &#123; return hand; &#125; public void setHand(Hand hand) &#123; this.hand = hand; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub People p2 = (People) super.clone(); p2.setHand((Hand) hand.clone()); return p2; &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; People p1 = new People(); People p2 = (People) p1.clone(); System.out.println("第一个对象的hash值:" + p1.hashCode()); System.out.println("第二个对象的hash值:" + p2.hashCode()); System.out.println("分割线-----------"); System.out.println("p1中的hand对象的hash值:" + p1.getHand().hashCode()); System.out.println("p2中的hand对象的hash值:" + p2.getHand().hashCode()); &#125;&#125;class Hand implements Cloneable &#123; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub return super.clone(); &#125;&#125; 结果如下，证明已经进行深克隆 12345第一个对象的hash值:1172625760第二个对象的hash值:863719801分割线-----------p1中的hand对象的hash值:1696725334p2中的hand对象的hash值:427340025 序列化与反序列化实现深克隆对象序列化操作可以将对象的状态转换成字节流传输或者存储再生，我们可以借用这一特点实现对象的深度克隆，特别是当我们的对象嵌套非常复杂且想实现深度克隆时如果使用序列化方式会大大减少代码量。 1234567891011121314151617181920212223242526272829public class TestClone implements Serializable&#123; private static final long serialVersionUID = 1L; public String name = "ilt"; public static void main(String[] args) throws Exception &#123; TestClone t1 = new TestClone(); byte[] b = ObjectUtil.objectToBytes(t1);//序列化 TestClone t2 = (TestClone) ObjectUtil.bytesToObject(b);//反序列化 System.out.println("t1对象的name："+t1.name); System.out.println("t2对象的name："+t2.name); System.out.println("分割线-------------"); System.out.println("t1对象的hash值为："+t1.hashCode()); System.out.println("t2对象的hash值为："+t2.hashCode()); System.out.println("分割线-------------"); System.out.println("t1中的obj对象的hash值为："+t1.obj.hashCode()); System.out.println("t2中的obj对象的hash值为："+t2.obj.hashCode()); &#125; class Bean implements Serializable&#123; private static final long serialVersionUID = 1L; &#125;&#125; 结果如下，证明对象的属性被深克隆下来了 12345678t1对象的name：iltt2对象的name：ilt分割线-------------t1对象的hash值为：1847546936t2对象的hash值为：812610706分割线-------------t1中的obj对象的hash值为：1164730192t2中的obj对象的hash值为：1699624469 作者：youngerTree来源：CSDN原文：https://blog.csdn.net/syilt/article/details/78482927版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>clone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中volatile关键字的作用]]></title>
    <url>%2F2019%2F06%2F18%2Fjava-volatile%2F</url>
    <content type="text"><![CDATA[以下文章来源于：Java并发：volatile内存可见性和指令重排 volatile两大作用1、保证内存可见性 2、防止指令重排 此外需注意volatile并不保证操作的原子性。 （一）内存可见性1 概念JVM内存模型：主内存和线程独立的工作内存 Java内存模型规定，对于多个线程共享的变量，存储在主内存当中，每个线程都有自己独立的工作内存（比如CPU的寄存器），线程只能访问自己的工作内存，不可以访问其它线程的工作内存。 工作内存中保存了主内存共享变量的副本，线程要操作这些共享变量，只能通过操作工作内存中的副本来实现，操作完毕之后再同步回到主内存当中。 如何保证多个线程操作主内存的数据完整性是一个难题，Java内存模型也规定了工作内存与主内存之间交互的协议，定义了8种原子操作： 123456789101112131415(1) lock:将主内存中的变量锁定，为一个线程所独占(2) unclock:将lock加的锁定解除，此时其它的线程可以有机会访问此变量(3) read:将主内存中的变量值读到工作内存当中(4) load:将read读取的值保存到工作内存中的变量副本中。(5) use:将值传递给线程的代码执行引擎(6) assign:将执行引擎处理返回的值重新赋值给变量副本(7) store:将变量副本的值存储到主内存中。(8) write:将store存储的值写入到主内存的共享变量当中。 通过上面Java内存模型的概述，我们会注意到这么一个问题，每个线程在获取锁之后会在自己的工作内存来操作共享变量，操作完成之后将工作内存中的副本回写到主内存，并且在其它线程从主内存将变量同步回自己的工作内存之前，共享变量的改变对其是不可见的。即其他线程的本地内存中的变量已经是过时的，并不是更新后的值。 2 内存可见性带来的问题很多时候我们需要一个线程对共享变量的改动，其它线程也需要立即得知这个改动该怎么办呢？下面举两个例子说明内存可见性的重要性： 例子1有一个全局的状态变量open: 1`boolean` `open=``true``;` 这个变量用来描述对一个资源的打开关闭状态，true表示打开，false表示关闭，假设有一个线程A,在执行一些操作后将open修改为false: 123//线程Aresource.close();open = false; 线程B随时关注open的状态，当open为true的时候通过访问资源来进行一些操作: 1234//线程Bwhile(open) &#123;doSomethingWithResource(resource);&#125; 当A把资源关闭的时候，open变量对线程B是不可见的，如果此时open变量的改动尚未同步到线程B的工作内存中,那么线程B就会用一个已经关闭了的资源去做一些操作，因此产生错误。 例子2下面是一个通过布尔标志判断线程是否结束的例子： 12345678910111213141516171819202122232425public class CancelThreadTest &#123; publicstatic void main(String[] args) throws Exception&#123; PrimeGeneratorgen = new PrimeGenerator(); newThread(gen).start(); try &#123; Thread.sleep(3000); &#125;finally&#123; gen.cancel(); &#125; &#125;&#125; class PrimeGenerator implements Runnable&#123; privateboolean cancelled; @Override publicvoid run() &#123; while(!cancelled) &#123; System.out.println("Running..."); //doingsomething here... &#125; &#125; publicvoid cancel()&#123;cancelled = true;&#125;&#125; 主线程中设置PrimeGenerator线程的是否取消标识，PrimeGenerator线程检测到这个标识后就会结束线程，由于主线程修改cancelled变量的内存可见性，主线程修改cancelled标识后并不马上同步回主内存，所以PrimeGenerator线程结束的时间难以把控（最终是一定会同步回主内存，让PrimeGenerator线程结束）。 如果PrimeGenerator线程执行一些比较关键的操作，主线程希望能够及时终止它，这时将cenceled用volatile关键字修饰就是必要的。 特别注意：上面演示这个并不是正确的取消线程的方法，因为一旦PrimeGenerator线程中包含BolckingQueue.put()等阻塞方法，那么将可能永远不会去检查cancelled标识，导致线程永远不会退出。正确的方法参见另外一篇关于如何正确终止线程的方法。 3 提供内存可见性volatile保证可见性的原理是在每次访问变量时都会进行一次刷新，因此每次访问都是主内存中最新的版本。所以volatile关键字的作用之一就是保证变量修改的实时可见性。 针对上面的例子1： 要求一个线程对open的改变，其他的线程能够立即可见，Java为此提供了volatile关键字，在声明open变量的时候加入volatile关键字就可以保证open的内存可见性，即open的改变对所有的线程都是立即可见的。 针对上面的例子2： 将cancelled标志设置的volatile保证主线程针对cancelled标识的修改能够让PrimeGenerator线程立马看到。 备注：也可以通过提供synchronized同步的open变量的Get/Set方法解决此内存可见性问题，因为要Get变量open，必须等Set方完全释放锁之后。后面将介绍到两者的区别。 （二）指令重排1 概念指令重排序是JVM为了优化指令，提高程序运行效率，在不影响单线程程序执行结果的前提下，尽可能地提高并行度。编译器、处理器也遵循这样一个目标。注意是单线程。多线程的情况下指令重排序就会给程序员带来问题。 不同的指令间可能存在数据依赖。比如下面计算圆的面积的语句： 123double r = 2.3d;//(1)double pi =3.1415926; //(2)double area = pi* r * r; //(3) area的计算依赖于r与pi两个变量的赋值指令。而r与pi无依赖关系。 as-if-serial语义是指：不管如何重排序（编译器与处理器为了提高并行度），（单线程）程序的结果不能被改变。这是编译器、Runtime、处理器必须遵守的语义。 虽然，（1） – happensbefore -&gt; （2）,（2） – happens before -&gt; （3），但是计算顺序(1)(2)(3)与(2)(1)(3) 对于r、pi、area变量的结果并无区别。编译器、Runtime在优化时可以根据情况重排序（1）与（2），而丝毫不影响程序的结果。 指令重排序包括编译器重排序和运行时重排序。 2 指令重排带来的问题如果一个操作不是原子的，就会给JVM留下重排的机会。下面看几个例子： 例子1：A线程指令重排导致B线程出错对于在同一个线程内，这样的改变是不会对逻辑产生影响的，但是在多线程的情况下指令重排序会带来问题。看下面这个情景: 在线程A中: 12context = loadContext();inited = true; 在线程B中: 1234while(!inited )&#123; //根据线程A中对inited变量的修改决定是否使用context变量 sleep(100);&#125;doSomethingwithconfig(context); 假设线程A中发生了指令重排序: 12inited = true;context = loadContext(); 那么B中很可能就会拿到一个尚未初始化或尚未初始化完成的context,从而引发程序错误。 例子2：指令重排导致单例模式失效我们都知道一个经典的懒加载方式的双重判断单例模式： 1234567891011121314public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance == null) &#123; synchronzied(Singleton.class) &#123; if(instance == null) &#123; &lt;strong&gt;instance = new Singleton(); //非原子操作 &#125; &#125; &#125; return instance; &#125;&#125; 看似简单的一段赋值语句：instance= new Singleton()，但是很不幸它并不是一个原子操作，其实际上可以抽象为下面几条JVM指令： 123memory =allocate(); //1：分配对象的内存空间 ctorInstance(memory); //2：初始化对象 instance =memory; //3：设置instance指向刚分配的内存地址 上面操作2依赖于操作1，但是操作3并不依赖于操作2，所以JVM是可以针对它们进行指令的优化重排序的，经过重排序后如下： 123memory =allocate(); //1：分配对象的内存空间 instance =memory; //3：instance指向刚分配的内存地址，此时对象还未初始化ctorInstance(memory); //2：初始化对象 可以看到指令重排之后，instance指向分配好的内存放在了前面，而这段内存的初始化被排在了后面。 在线程A执行这段赋值语句，在初始化分配对象之前就已经将其赋值给instance引用，恰好另一个线程进入方法判断instance引用不为null，然后就将其返回使用，导致出错。 3 防止指令重排除了前面内存可见性中讲到的volatile关键字可以保证变量修改的可见性之外，还有另一个重要的作用：在JDK1.5之后，可以使用volatile变量禁止指令重排序。 解决方案：例子1中的inited和例子2中的instance以关键字volatile修饰之后，就会阻止JVM对其相关代码进行指令重排，这样就能够按照既定的顺序指执行。 volatile关键字通过提供“内存屏障”的方式来防止指令被重排序，为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 （三）总结volatile是轻量级同步机制相对于synchronized块的代码锁，volatile应该是提供了一个轻量级的针对共享变量的锁，当我们在多个线程间使用共享变量进行通信的时候需要考虑将共享变量用volatile来修饰。 volatile是一种稍弱的同步机制，在访问volatile变量时不会执行加锁操作，也就不会执行线程阻塞，因此volatilei变量是一种比synchronized关键字更轻量级的同步机制。 volatile使用建议使用建议：在两个或者更多的线程需要访问的成员变量上使用volatile。当要访问的变量已在synchronized代码块中，或者为常量时，没必要使用volatile。 由于使用volatile屏蔽掉了JVM中必要的代码优化，所以在效率上比较低，因此一定在必要时才使用此关键字。 volatile和synchronized区别1、volatile不会进行加锁操作： volatile变量是一种稍弱的同步机制，在访问volatile变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile变量是一种比synchronized关键字更轻量级的同步机制。 2、volatile变量作用类似于同步变量读写操作： 从内存可见性的角度看，写入volatile变量相当于退出同步代码块，而读取volatile变量相当于进入同步代码块。 3、volatile不如synchronized安全： 在代码中如果过度依赖volatile变量来控制状态的可见性，通常会比使用锁的代码更脆弱，也更难以理解。仅当volatile变量能简化代码的实现以及对同步策略的验证时，才应该使用它。一般来说，用同步机制会更安全些。 4、volatile无法同时保证内存可见性和原子性： 加锁机制（即同步机制）既可以确保可见性又可以确保原子性，而volatile变量只能确保可见性，原因是声明为volatile的简单变量如果当前值与该变量以前的值相关，那么volatile关键字不起作用，也就是说如下的表达式都不是原子操作：“count++”、“count = count+1”。 当且仅当满足以下所有条件时，才应该使用volatile变量： 1231、对变量的写入操作不依赖变量的当前值，或者你能确保只有单个线程更新变量的值。2、该变量没有包含在具有其他变量的不变式中。 总结：在需要同步的时候，第一选择应该是synchronized关键字，这是最安全的方式，尝试其他任何方式都是有风险的。尤其在、jdK1.5之后，对synchronized同步机制做了很多优化，如：自适应的自旋锁、锁粗化、锁消除、轻量级锁等，使得它的性能明显有了很大的提升。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式]]></title>
    <url>%2F2019%2F06%2F17%2Fjava-design-pattern%2F</url>
    <content type="text"><![CDATA[以下文章来自于：Java 设计模式 一直想写一篇介绍设计模式的文章，让读者可以很快看完，而且一看就懂，看懂就会用，同时不会将各个模式搞混。自认为本文还是写得不错的???，花了不少心思来写这文章和做图，力求让读者真的能看着简单同时有所收获。 设计模式是对大家实际工作中写的各种代码进行高层次抽象的总结，其中最出名的当属 1234Gang of Four ( GoF ) 的分类了，他们将设计模式分类为 23 种经典的模式，根据用途我们又可以分为三大类，分别为创建型模式、结构型模式和行为型模式。是的，我不善于扯这些有的没的，还是少点废话吧~ 有一些重要的设计原则在开篇和大家分享下，这些原则将贯通全文： 面向接口编程，而不是面向实现。这个很重要，也是优雅的、可扩展的代码的第一步，这就不需要多说了吧。 职责单一原则。每个类都应该只有一个单一的功能，并且该功能应该由这个类完全封装起来。 对修改关闭，对扩展开放。对修改关闭是说，我们辛辛苦苦加班写出来的代码，该实现的功能和该修复的 bug 都完成了，别人可不能说改就改；对扩展开放就比较好理解了，也就是说在我们写好的代码基础上，很容易实现扩展。 创建型模式创建型模式的作用就是创建对象，说到创建一个对象，最熟悉的就是 new 一个对象，然后 set 相关属性。但是，在很多场景下，我们需要给客户端提供更加友好的创建对象的方式，尤其是那种我们定义了类，但是需要提供给其他开发者用的时候。 简单工厂模式和名字一样简单，非常简单，直接上代码吧： 1234567891011121314151617public class FoodFactory &#123; public static Food makeFood(String name) &#123; if (name.equals("noodle")) &#123; Food noodle = new LanZhouNoodle(); noodle.addSpicy("more"); return noodle; &#125; else if (name.equals("chicken")) &#123; Food chicken = new HuangMenChicken(); chicken.addCondiment("potato"); return chicken; &#125; else &#123; return null; &#125; &#125;&#125;复制代码 其中，LanZhouNoodle 和 HuangMenChicken 都继承自 Food。 简单地说，简单工厂模式通常就是这样，一个工厂类 XxxFactory，里面有一个静态方法，根据我们不同的参数，返回不同的派生自同一个父类（或实现同一接口）的实例对象。 我们强调职责单一原则，一个类只提供一种功能，FoodFactory 的功能就是只要负责生产各种 Food。 工厂模式简单工厂模式很简单，如果它能满足我们的需要，我觉得就不要折腾了。之所以需要引入工厂模式，是因为我们往往需要使用两个或两个以上的工厂。 123456789101112131415161718192021222324252627282930public interface FoodFactory &#123; Food makeFood(String name);&#125;public class ChineseFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new ChineseFoodA(); &#125; else if (name.equals("B")) &#123; return new ChineseFoodB(); &#125; else &#123; return null; &#125; &#125;&#125;public class AmericanFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new AmericanFoodA(); &#125; else if (name.equals("B")) &#123; return new AmericanFoodB(); &#125; else &#123; return null; &#125; &#125;&#125;复制代码 其中，ChineseFoodA、ChineseFoodB、AmericanFoodA、AmericanFoodB 都派生自 Food。 客户端调用： 123456789public class APP &#123; public static void main(String[] args) &#123; // 先选择一个具体的工厂 FoodFactory factory = new ChineseFoodFactory(); // 由第一步的工厂产生具体的对象，不同的工厂造出不一样的对象 Food food = factory.makeFood("A"); &#125;&#125;复制代码 虽然都是调用 makeFood(“A”) 制作 A 类食物，但是，不同的工厂生产出来的完全不一样。 第一步，我们需要选取合适的工厂，然后第二步基本上和简单工厂一样。 核心在于，我们需要在第一步选好我们需要的工厂。比如，我们有 LogFactory 接口，实现类有 FileLogFactory 和 KafkaLogFactory，分别对应将日志写入文件和写入 Kafka 中，显然，我们客户端第一步就需要决定到底要实例化 FileLogFactory 还是 KafkaLogFactory，这将决定之后的所有的操作。 虽然简单，不过我也把所有的构件都画到一张图上，这样读者看着比较清晰： 抽象工厂模式当涉及到产品族的时候，就需要引入抽象工厂模式了。 一个经典的例子是造一台电脑。我们先不引入抽象工厂模式，看看怎么实现。 因为电脑是由许多的构件组成的，我们将 CPU 和主板进行抽象，然后 CPU 由 CPUFactory 生产，主板由 MainBoardFactory 生产，然后，我们再将 CPU 和主板搭配起来组合在一起，如下图： 这个时候的客户端调用是这样的： 1234567891011// 得到 Intel 的 CPUCPUFactory intelCPUFactory = new IntelCPUFactory();CPU cpu = intelCPUFactory.makeCPU();// 得到 AMD 的主板MainBoardFactory mainBoardFactory = new AmdMainBoardFactory();MainBoard mainBoard = mainBoardFactory.make();// 组装 CPU 和主板Computer computer = new Computer(cpu, mainBoard);复制代码 单独看 CPU 工厂和主板工厂，它们分别是前面我们说的工厂模式。这种方式也容易扩展，因为要给电脑加硬盘的话，只需要加一个 HardDiskFactory 和相应的实现即可，不需要修改现有的工厂。 但是，这种方式有一个问题，那就是如果 Intel 家产的 CPU 和 AMD 产的主板不能兼容使用，那么这代码就容易出错，因为客户端并不知道它们不兼容，也就会错误地出现随意组合。 下面就是我们要说的产品族的概念，它代表了组成某个产品的一系列附件的集合： 当涉及到这种产品族的问题的时候，就需要抽象工厂模式来支持了。我们不再定义 CPU 工厂、主板工厂、硬盘工厂、显示屏工厂等等，我们直接定义电脑工厂，每个电脑工厂负责生产所有的设备，这样能保证肯定不存在兼容问题。 这个时候，对于客户端来说，不再需要单独挑选 CPU厂商、主板厂商、硬盘厂商等，直接选择一家品牌工厂，品牌工厂会负责生产所有的东西，而且能保证肯定是兼容可用的。 1234567891011121314public static void main(String[] args) &#123; // 第一步就要选定一个“大厂” ComputerFactory cf = new AmdFactory(); // 从这个大厂造 CPU CPU cpu = cf.makeCPU(); // 从这个大厂造主板 MainBoard board = cf.makeMainBoard(); // 从这个大厂造硬盘 HardDisk hardDisk = cf.makeHardDisk(); // 将同一个厂子出来的 CPU、主板、硬盘组装在一起 Computer result = new Computer(cpu, board, hardDisk);&#125;复制代码 当然，抽象工厂的问题也是显而易见的，比如我们要加个显示器，就需要修改所有的工厂，给所有的工厂都加上制造显示器的方法。这有点违反了对修改关闭，对扩展开放这个设计原则。 单例模式单例模式用得最多，错得最多。 饿汉模式最简单： 123456789101112131415public class Singleton &#123; // 首先，将 new Singleton() 堵死 private Singleton() &#123;&#125;; // 创建私有静态实例，意味着这个类第一次使用的时候就会进行创建 // 这个代码还可以放在静态代码块中 private static Singleton instance = new Singleton(); public static Singleton getInstance() &#123; return instance; &#125; // 瞎写一个静态方法。这里想说的是，如果我们只是要调用 Singleton.getDate(...)， // 本来是不想要生成 Singleton 实例的，不过没办法，已经生成了 public static Date getDate(String mode) &#123;return new Date();&#125;&#125;复制代码 很多人都能说出饿汉模式的缺点(浪费内存空间)，可是我觉得生产过程中，很少碰到这种情况：你定义了一个单例的类，不需要其实例，可是你却把一个或几个你会用到的静态方法塞到这个类中。 饱汉模式最容易出错： 123456789101112131415161718192021public class Singleton &#123; // 首先，也是先堵死 new Singleton() 这条路 private Singleton() &#123;&#125; // 和饿汉模式相比，这边不需要先实例化出来，注意这里的 volatile，它是必须的 private static volatile Singleton instance = null; public static Singleton getInstance() &#123; if (instance == null) &#123; // 加锁 synchronized (Singleton.class) &#123; // 这一次判断也是必须的，不然会有并发问题 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125;复制代码 volatile 关键字可以保证 instance 的内存可见性和防止指令重排序，使得 instance 在多线程环境下依然可以正确的被初始化。详情请移步这里：java中volatile关键字的作用。 双重检查，指的是两次检查 instance 是否为 null。 volatile 在这里是需要的，希望能引起读者的关注。 很多人不知道怎么写，直接就在 getInstance() 方法签名上加上 synchronized，这就不多说了，性能太差。 嵌套类最经典，以后大家就用它吧： 123456789101112public class Singleton3 &#123; private Singleton3() &#123;&#125; // 主要是使用了 嵌套类可以访问外部类的静态属性和静态方法 的特性 private static class Holder &#123; private static Singleton3 instance = new Singleton3(); &#125; public static Singleton3 getInstance() &#123; return Holder.instance; &#125;&#125;复制代码 注意，很多人都会把这个嵌套类说成是静态内部类，严格地说，内部类和嵌套类是不一样的，它们能访问的外部类权限也是不一样的。 最后，一定有人跳出来说用枚举实现单例，是的没错，枚举类很特殊，它在类加载的时候会初始化里面的所有的实例，而且 JVM 保证了它们不会再被实例化，所以它天生就是单例的。不说了，读者自己看着办吧，不建议使用。 建造者模式经常碰见的 XxxBuilder 的类，通常都是建造者模式的产物。建造者模式其实有很多的变种，但是对于客户端来说，我们的使用通常都是一个模式的： 123Food food = new FoodBuilder().a().b().c().build();Food food = Food.builder().a().b().c().build();复制代码 套路就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 来一个中规中矩的建造者模式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class User &#123; // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) &#123; this.name = name; this.password = password; this.nickName = nickName; this.age = age; &#125; // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() &#123; return new UserBuilder(); &#125; public static class UserBuilder &#123; // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() &#123; &#125; // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) &#123; this.name = name; return this; &#125; public UserBuilder password(String password) &#123; this.password = password; return this; &#125; public UserBuilder nickName(String nickName) &#123; this.nickName = nickName; return this; &#125; public UserBuilder age(int age) &#123; this.age = age; return this; &#125; // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() &#123; if (name == null || password == null) &#123; throw new RuntimeException("用户名和密码必填"); &#125; if (age &lt;= 0 || age &gt;= 150) &#123; throw new RuntimeException("年龄不合法"); &#125; // 还可以做赋予”默认值“的功能 if (nickName == null) &#123; nickName = name; &#125; return new User(name, password, nickName, age); &#125; &#125;&#125;复制代码 核心是：先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。 看看客户端的调用： 12345678910public class APP &#123; public static void main(String[] args) &#123; User d = User.builder() .name("foo") .password("pAss12345") .age(25) .build(); &#125;&#125;复制代码 说实话，建造者模式的链式写法很吸引人，但是，多写了很多“无用”的 builder 的代码，感觉这个模式没什么用。不过，当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 题外话，强烈建议读者使用 lombok，用了 lombok 以后，上面的一大堆代码会变成如下这样: 12345678@Builderclass User &#123; private String name; private String password; private String nickName; private int age;&#125;复制代码 怎么样，省下来的时间是不是又可以干点别的了。 当然，如果你只是想要链式写法，不想要建造者模式，有个很简单的办法，User 的 getter 方法不变，所有的 setter 方法都让其 return this 就可以了，然后就可以像下面这样调用： 12User user = new User().setName("").setPassword("").setAge(20);复制代码 原型模式这是我要说的创建型模式的最后一个设计模式了。 原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 12protected native Object clone() throws CloneNotSupportedException;复制代码 java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 原型模式了解到这里我觉得就够了，各种变着法子说这种代码或那种代码是原型模式，没什么意义。 创建型模式总结创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 123456简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式;单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源；建造者模式专门对付属性很多的那种类，为了让代码更优美；原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 理解代理这个词，这个模式其实就简单了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken("1kg"); f.setSpicy("1g"); f.setSalt("3g"); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle("500g"); f.setSalt("5g"); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println("我们马上要开始制作鸡肉了"); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println("鸡肉制作完成啦，加点胡椒粉"); // 增强 food.addCondiment("pepper"); return food; &#125; public Food makeNoodle() &#123; System.out.println("准备制作拉面~"); Food food = foodService.makeNoodle(); System.out.println("制作完成啦") return food; &#125;&#125;复制代码 客户端调用，注意，我们要用代理来实例化接口： 1234// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken();复制代码 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 1234567891011public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125;复制代码 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 123456789101112131415161718192021222324252627public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125;复制代码 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 123456789101112public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125;复制代码 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 12345678910111213141516171819public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println("咕咕叫"); &#125; public void fly() &#123; System.out.println("鸡也会飞哦"); &#125;&#125;复制代码 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 12345678910111213141516171819202122// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125;复制代码 客户端调用很简单了： 12345678public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125;复制代码 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 1234public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125;复制代码 然后是一系列实现类： 12345678910111213141516171819public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;复制代码 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 123456789public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125;复制代码 定义抽象类的子类： 12345678910111213141516171819202122232425262728// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125;复制代码 最后，我们来看客户端演示： 12345678public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125;复制代码 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 本节引用了这里的例子，并对其进行了修改。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。 首先，定义饮料抽象基类： 1234567public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125;复制代码 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 123456789101112131415161718public class BlackTea extends Beverage &#123; public String getDescription() &#123; return "红茶"; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return "绿茶"; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略复制代码 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 12345// 调料public abstract class Condiment extends Beverage &#123;&#125;复制代码 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 123456789101112131415161718192021222324252627282930public class Lemon extends Condiment &#123; private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + ", 加柠檬"; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + ", 加芒果"; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类复制代码 看客户端调用： 1234567891011public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + " 价格：￥" + beverage.cost()); //"绿茶, 加柠檬, 加芒果 价格：￥16"&#125;复制代码 如果我们需要芒果珍珠双份柠檬红茶： 12Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea()))));复制代码 是不是很变态？ 看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 12InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream("")));复制代码 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 1234LineNumberInputStream is = new LineNumberInputStream( new BufferedInputStream( new FileInputStream("")));复制代码 所以说嘛，要找到纯的严格符合设计模式的代码还是比较难的。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 1234public interface Shape &#123; void draw();&#125;复制代码 定义几个实现类： 12345678910111213141516public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println("Circle::draw()"); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println("Rectangle::draw()"); &#125;&#125;复制代码 客户端调用： 12345678910public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125;复制代码 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： 1234567891011121314151617181920212223242526public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125;复制代码 看看现在客户端怎么调用： 123456789public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125;复制代码 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 123456789101112131415161718192021222324252627282930public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return ("Employee :[ Name : " + name + ", dept : " + dept + ", salary :" + salary+" ]"); &#125; &#125;复制代码 通常，这种类需要定义 add(node)、remove(node)、getChildren() 这些方法。 这说的其实就是组合模式，这种简单的模式我就不做过多介绍了，相信各位读者也不喜欢看我写废话。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是共享元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 这种简单的代码我就不演示了。 结构型模式总结前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。读者是否可以分别把这几个模式说清楚了呢？在说到这些模式的时候，心中是否有一个清晰的图或处理流程在脑海里呢？ 1234567代理模式是做方法增强的;适配器模式是把鸡包装成鸭这种用来适配接口的;桥梁模式做到了很好的解耦;装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景；门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可；组合模式用于描述具有层次结构的数据；享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。 行为型模式行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式策略模式太常用了，所以把它放到最前面进行介绍。它比较简单，我就不废话，直接用代码说事吧。 下面设计的场景是，我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。 首先，先定义一个策略接口： 1234public interface Strategy &#123; public void draw(int radius, int x, int y);&#125;复制代码 然后我们定义具体的几个策略： 12345678910111213141516171819public class RedPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;复制代码 使用策略的类： 123456789101112public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeDraw(int radius, int x, int y)&#123; return strategy.draw(radius, x, y); &#125;&#125;复制代码 客户端演示： 12345public static void main(String[] args) &#123; Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);&#125;复制代码 放到一张图上，让大家看得清晰些： 这个时候，大家有没有联想到结构型模式中的桥梁模式，它们其实非常相似，我把桥梁模式的图拿过来大家对比下： 要我说的话，它们非常相似，桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式观察者模式对于我们来说，真是再简单不过了。无外乎两个操作，观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 首先，需要定义主题，每个主题需要持有观察者列表的引用，用于在数据变更的时候通知各个观察者： 123456789101112131415161718192021222324252627public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; // 通知观察者们 public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125;复制代码 定义观察者接口： 12345public abstract class Observer &#123; protected Subject subject; public abstract void update();&#125;复制代码 其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 我们来定义具体的几个观察者类： 12345678910111213141516171819202122232425262728293031public class BinaryObserver extends Observer &#123; // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) &#123; this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); &#125; // 该方法由主题类在数据变更的时候进行调用 @Override public void update() &#123; String result = Integer.toBinaryString(subject.getState()); System.out.println("订阅的数据发生变化，新的数据处理为二进制值为：" + result); &#125;&#125;public class HexaObserver extends Observer &#123; public HexaObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println("订阅的数据发生变化，新的数据处理为十六进制值为：" + result); &#125;&#125;复制代码 客户端使用也非常简单： 1234567891011public static void main(String[] args) &#123; // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);&#125;复制代码 output: 123订阅的数据发生变化，新的数据处理为二进制值为：1011订阅的数据发生变化，新的数据处理为十六进制值为：B复制代码 当然，jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。 实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 如果产品给你这个需求的话，我想大部分人一开始肯定想的就是，用一个 List 来存放所有的规则，然后 foreach 执行一下每个规则就好了。不过，读者也先别急，看看责任链模式和我们说的这个有什么不一样？ 首先，我们要定义流程上节点的基类： 123456789101112131415public abstract class RuleHandler &#123; // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) &#123; this.successor = successor; &#125; public RuleHandler getSuccessor() &#123; return successor; &#125;&#125;复制代码 接下来，我们需要定义具体的每个节点了。 校验用户是否是新用户： 123456789101112131415public class NewUserRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; if (context.isNewUser()) &#123; // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException("该活动仅限新用户参与"); &#125; &#125;&#125;复制代码 校验用户所在地区是否可以参与： 12345678910111213public class LocationRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException("非常抱歉，您所在的地区无法参与本次活动"); &#125; &#125;&#125;复制代码 校验奖品是否已领完： 12345678910111213public class LimitRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(userInfo); &#125; &#125; else &#123; throw new RuntimeException("您来得太晚了，奖品被领完了"); &#125; &#125;&#125;复制代码 客户端： 12345678910public static void main(String[] args) &#123; RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);&#125;复制代码 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 至于它和我们前面说的用一个 List 存放需要执行的规则的做法有什么异同，留给读者自己琢磨吧。 模板方法模式在含有继承结构的代码中，模板方法模式是非常常用的，这也是在开源代码中大量被使用的。 通常会有一个抽象类： 12345678910111213141516public abstract class AbstractTemplate &#123; // 这就是模板方法 public void templateMethod()&#123; init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 &#125; protected void init() &#123; System.out.println("init 抽象层已经实现，子类也可以选择覆写"); &#125; // 留给子类实现 protected abstract void apply(); protected void end() &#123; &#125;&#125;复制代码 模板方法中调用了 3 个方法，其中 apply() 是抽象方法，子类必须实现它，其实模板方法中有几个抽象方法完全是自由的，我们也可以将三个方法都设置为抽象方法，让子类来实现。也就是说，模板方法只负责定义第一步应该要做什么，第二步应该做什么，第三步应该做什么，至于怎么做，由子类来实现。 我们写一个实现类： 123456789public class ConcreteTemplate extends AbstractTemplate &#123; public void apply() &#123; System.out.println("子类实现抽象方法 apply"); &#125; public void end() &#123; System.out.println("我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了"); &#125;&#125;复制代码 客户端调用演示： 123456public static void main(String[] args) &#123; AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();&#125;复制代码 代码其实很简单，基本上看到就懂了，关键是要学会用到自己的代码中。 状态模式废话我就不说了，我们说一个简单的例子。商品库存中心有个最基本的需求是减库存和补库存，我们看看怎么用状态模式来写。 核心在于，我们的关注点不再是 Context 是该进行哪种操作，而是关注在这个 Context 会有哪些操作。 定义状态接口： 1234public interface State &#123; public void doAction(Context context);&#125;复制代码 定义减库存的状态： 1234567891011121314public class DeductState implements State &#123; public void doAction(Context context) &#123; System.out.println("商品卖出，准备减库存"); context.setState(this); //... 执行减库存的具体操作 &#125; public String toString()&#123; return "Deduct State"; &#125;&#125;复制代码 定义补库存状态： 123456789101112public class RevertState implements State &#123; public void doAction(Context context) &#123; System.out.println("给此商品补库存"); context.setState(this); //... 执行加库存的具体操作 &#125; public String toString() &#123; return "Revert State"; &#125;&#125;复制代码 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 123456789101112131415public class Context &#123; private State state; private String name; public Context(String name) &#123; this.name = name; &#125; public void setState(State state) &#123; this.state = state; &#125; public void getState() &#123; return this.state; &#125;&#125;复制代码 我们来看下客户端调用，大家就一清二楚了： 12345678910111213141516public static void main(String[] args) &#123; // 我们需要操作的是 iPhone X Context context = new Context("iPhone X"); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();&#125;复制代码 读者可能会发现，在上面这个例子中，如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。 不过，商品库存这个例子毕竟只是个例，我们还有很多实例是需要知道当前 context 处于什么状态的。 行为型模式总结行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限，而且本文篇幅也挺大了，我就不进行介绍了。 总结学习设计模式的目的是为了让我们的代码更加的优雅、易维护、易扩展。 作者：JavaDoop 链接：https://juejin.im/post/5bc96afff265da0aa94a4493 来源：掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是restful？]]></title>
    <url>%2F2019%2F06%2F17%2Fwhat-is-resuful%2F</url>
    <content type="text"><![CDATA[以下文章来源于：如何给老婆解释什么是RESTful 老婆经常喜欢翻看我订阅的技术杂志，她总能从她的视角提出很多有趣的问题。 一个悠闲的周日下午，她午觉醒来，又习惯性的抓起这个月的杂志，饶有兴趣地看了起来。 果不其然，看着看着，她又对我发难了，“Restful是什么呀，老公？是restaurant的形容词吗，突然就觉得好饿了啊……” 作为一个合格的程序员，我一直把能够将一项技术讲给老婆听，并且能给她讲懂，作为我已经掌握了这项技术的标准。 如果我直接回答说，“REST就是Representational State Transfer的缩写呀，翻译为中文就是‘表述性状态转移’”，那她今晚肯定得罚我跪键盘。我必须找个合适的机会，把Restful的来龙去脉给她形象的描述一遍。 “走，咱们去楼下咖啡厅吃个下午茶吧”，我对老婆说。 “一个芝士蛋糕，一杯拿铁，两条吸管，谢谢”，我对前台的服务员说，然后我们找了个角落坐了下来。 Level 0 - 面向前台“刚才我们向前台点了一杯拿铁，这个过程可以用这段文字来描述”，说着，我在纸上写下了这段JSON，虽然她不知道什么叫JSON，但理解这段文字对于英语专业8级的她，实在再简单不过。 12345&#123; &quot;addOrder&quot;: &#123; &quot;orderName&quot;: &quot;latte&quot; &#125;&#125; “我们通过这段文字，告诉前台，新增一笔订单，订单是一杯拿铁咖啡”，接着，前台给我们返回这么一串回复： 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “订单ID？还是订单编号？” “恩恩，就是订单编号” “那我们就等着前台喊‘订单123456的客户可以取餐了’，然后就可以开吃了！” “哈哈，你真聪明，不过，在这之前，假设我们有一张会员卡，我们想查询一下这张会员卡的余额，这时候，要向前台发起另一个询问”，我继续在纸上写着： 12345&#123; &quot;queryBalance&quot;: &#123; &quot;cardId&quot;: &quot;886333&quot; &#125;&#125; “查询卡号为886333的卡的余额？” “真棒！接着，查询的结果返回来了” 123&#123; &quot;balance&quot;: &quot;0&quot;&#125; “切，没钱……” “哈哈，没钱，现在我们要跟前台说，这杯咖啡不要了”，我在纸上写到： 12345&#123; &quot;deleteOrder&quot;: &#123; &quot;orderId&quot;: &quot;123456&quot; &#125;&#125; “哼，这就把订单取消啦？” Level 1 - 面向资源“现在这家咖啡店越做越大，来喝咖啡的人越来越多，单靠前台显然是不行的，店主决定进行分工，每个资源都有专人负责，我们可以直接面向资源操作。” “面向资源？” “是的，比如还是下单，请求的内容不变，但是我们多了一条消息”，我在纸上画出这次的模型： 1234567/orders&#123; &quot;addOrder&quot;: &#123; &quot;orderName&quot;: &quot;latte&quot; &#125;&#125; “多了一个斜杠和orders？这是什么意思？” “这个表示我们这个请求是发给哪个资源的，订单是一种资源，我们可以理解为是咖啡厅专门管理订单的人，他可以帮我们处理所有有关订单的操作，包括新增订单、修改订单、取消订单等操作” “Soga…” “接着还是会返回订单的编号给我们” 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “下面，我们还是要查询会员卡余额，这次请求的资源变成了cards” 1234567/cards&#123; &quot;queryBalance&quot;: &#123; &quot;cardId&quot;: &quot;886333&quot; &#125;&#125; “接下来是取消订单” “这个我会”，说着，她抢走我手上的笔，在纸上写了起来： 1234567/orders&#123; &quot;deleteOrder&quot;: &#123; &quot;orderId&quot;: &quot;123456&quot; &#125;&#125; Level 2 - 打上标签“接下来，店主还想继续优化他的咖啡厅的服务流程，他发现负责处理订单的员工，每次都要去订单内容里面看是新增订单还是删除订单，还是其他的什么操作，十分不方便，于是规定，所有新增资源的请求，都在请求上面写上大大的‘POST’，表示这是一笔新增资源的请求” “其他种类的请求，比如查询类的，用‘GET’表示，删除类的，用‘DELETE’表示” “还有修改类的，修改分为两种，第一种，如果这个修改，无论发送多少次，最后一次修改后的资源，总是和第一次修改后的一样，比如将拿铁改为猫屎，那么用‘PUT’表示；第二种，如果这个修改，每次修改都会让这个资源和前一次的不一样，比如是加一杯咖啡，那么这种请求用‘PATCH’或者‘POST’表示”，一口气讲了这么多，发现她有点似懂非懂。 “来，我们再来重复上面那个过程，来一杯拿铁”，我边说边画着： 12345POST /orders&#123; &quot;orderName&quot;: &quot;latte&quot;&#125; “请求的内容简洁多啦，不用告诉店员是addOrder，看到POST就知道是新增”，她听的很认真，理解的也很透彻。 “恩恩，返回的内容还是一样” 123&#123; &quot;orderId&quot;: &quot;123456&quot;&#125; “接着是查询会员卡余额，这次也简化了很多” 12345GET /cards&#123; &quot;cardId&quot;: &quot;886333&quot;&#125; “这个请求我们还可以进一步优化为这样” 1GET /cards/886333 “Soga，直接把要查询的卡号写在后面了” “没错，接着，取消订单” 1DELETE /orders/123456 Level 3 - 完美服务“忽然有一天，有个顾客抱怨说，他买了咖啡后，不知道要怎么取消订单，咖啡厅一个店员回了一句，你不会看我们的宣传单吗，上面不写着： 1DELETE /orders/&#123;orderId&#125; 顾客反问道，谁会去看那个啊，店员不服，又说到，你瞎了啊你……据说后面两人吵着吵着还打了起来…” “噗，真是悲剧…” “有了这次教训，店长决定，顾客下了单之后，不仅给他们返回订单的编号，还给顾客返回所有可以对这个订单做的操作，比如告诉用户如何删除订单。现在，我们还是发出请求，请求内容和上一次一样” 12345POST /orders&#123; &quot;orderName&quot;: &quot;latte&quot;&#125; “但是这次返回时多了些内容” 1234567&#123; &quot;orderId&quot;: &quot;123456&quot;, &quot;link&quot;: &#123; &quot;rel&quot;: &quot;cancel&quot;, &quot;url&quot;: &quot;/order/123456&quot; &#125;&#125; “这次返回时多了一项link信息，里面包含了一个rel属性和url属性，rel是relationship的意思，这里的关系是cancel，url则告诉你如何执行这个cancel操作，接着你就可以这样子来取消订单啦” 1DELETE /orders/123456 “哈哈，这服务真是贴心，以后再也不用担心店员和顾客打起来了” “订单123456的客户可以取餐了”，伴随着咖啡厅的广播，我们吃起了下午茶，一杯拿铁，两支吸管…… 对程序员的话用了大白话，给老婆讲明白了RESTful的来龙去脉，当然，我还是有些话想说的，只是怕老婆听完一脸懵逼，没给她说： 1、 上面讲的Level0~Level3，来自Leonard Richardson提出的Richardson Maturity Model： Level0和Level1最大的区别，就是Level1拥有了Restful的第一个特征——面向资源，这对构建可伸缩、分布式的架构是至关重要的。同时，如果把Level0的数据格式换成Xml，那么其实就是SOAP，SOAP的特点是关注行为和处理，和面向资源的RESTful有很大的不同。 Level0和Level1，其实都很挫，他们都只是把HTTP当做一个传输的通道，没有把HTTP当做一种传输协议。 Level2，真正将HTTP作为了一种传输协议，最直观的一点就是Level2使用了HTTP动词，GET/PUT/POST/DELETE/PATCH….,这些都是HTTP的规范，规范的作用自然是重大的，用户看到一个POST请求，就知道它不是幂等的，使用时要小心，看到PUT，就知道他是幂等的，调用多几次都不会造成问题，当然，这些的前提都是API的设计者和开发者也遵循这一套规范，确保自己提供的PUT接口是幂等的。 Level3，关于这一层，有一个古怪的名词，叫HATEOAS（Hypertext As The Engine Of Application State），中文翻译为“将超媒体格式作为应用状态的引擎”，核心思想就是每个资源都有它的状态，不同状态下，可对它进行的操作不一样。理解了这一层，再来看看REST的全称，Representational State Transfer，中文翻译为“表述性状态转移”，是不是好理解多了？ Level3的Restful API，给使用者带来了很大的便利，使用者只需要知道如何获取资源的入口，之后的每个URI都可以通过请求获得，无法获得就说明无法执行那个请求。 现在绝大多数的RESTful接口都做到了Level2的层次，做到Level3的比较少。当然，这个模型并不是一种规范，只是用来理解Restful的工具。所以，做到了Level2，也就是面向资源和使用Http动词，就已经很Restful了。Restful本身也不是一种规范，我比较倾向于用“风格“来形容它。如果你想深入了解Level3，可以阅读《Rest in Practice》第五章。 2、 我跟老婆讲的时候，用的数据格式是JSON，但是要强调一点，Restful对数据格式没有限制，就算你用的是XML或者其他格式，只要符合上面提到的几个特征，也算Restful。]]></content>
      <categories>
        <category>Restful</category>
      </categories>
      <tags>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出理解基于 Kafka 和 ZooKeeper 的分布式消息队列]]></title>
    <url>%2F2019%2F06%2F17%2Fkafka-zookeeper%2F</url>
    <content type="text"><![CDATA[消息队列中间件是分布式系统中重要的组件，主要解决应用耦合(使消息的生产者和消费者解耦)，异步消息（消息的生产和消费是异步的），流量削峰等问题。实现高性能，高可用，可伸缩和最终一致性架构，是大型分布式系统不可缺少的中间件。 本场 Chat 主要内容： Kafka 的架构解读； Kafka 为什么要将 Topic 进行分区； Kafka 高可靠性实现基础解读； Kafka 复制原理和同步方式； Leader 选举机制，及如何确保新选举出的 Leader 是优选； 同步副本 ISR； Kafka 数据可靠性和持久性保证； 深入解读 HW 机制； Kafka 架构中 ZooKeeper 以怎样的形式存在； 全程解析：Producer -&gt; kafka -&gt; consumer。 相关内容链接： 《分布式中间件实践之路》 《Python 快速入门实战教程》 《分布式锁的最佳实践之：基于 Etcd 的分布式锁》 《基于 Redis 的分布式锁实现及踩坑案例》 《一个高可靠性商用 Redis 集群方案介绍》 1. Kafka 总体架构基于 Kafka-ZooKeeper 的分布式消息队列系统总体架构如下： 如上图所示，一个典型的 Kafka 体系架构包括若干 Producer（消息生产者），若干 broker（作为 Kafka 节点的服务器），若干 Consumer（Group），以及一个 ZooKeeper 集群。Kafka通过 ZooKeeper 管理集群配置、选举 Leader 以及在 consumer group 发生变化时进行 Rebalance（即消费者负载均衡，在下一课介绍）。Producer 使用 push（推）模式将消息发布到 broker，Consumer 使用 pull（拉）模式从 broker 订阅并消费消息。 上图仅描摹了一个总体架构，并没有对作为 Kafka 节点的 broker 进行深入刻画，事实上，它的内部细节相当复杂，如下图所示，Kafka 节点涉及 Topic、Partition 两个重要概念。 在 Kafka 架构中，有几个术语： Producer：生产者，即消息发送者，push 消息到 Kafka 集群中的 broker（就是 server）中； Broker：Kafka 集群由多个 Kafka 实例（server） 组成，每个实例构成一个 broker，说白了就是服务器； Topic：producer 向 kafka 集群 push 的消息会被归于某一类别，即Topic，这本质上只是一个逻辑概念，面向的对象是 producer 和 consumer，producer 只需要关注将消息 push 到哪一个 Topic 中，而 consumer 只需要关心自己订阅了哪个 Topic； Partition：每一个 Topic 又被分为多个 Partitions，即物理分区；出于负载均衡的考虑，同一个 Topic 的 Partitions 分别存储于 Kafka 集群的多个 broker 上；而为了提高可靠性，这些 Partitions 可以由 Kafka 机制中的 replicas 来设置备份的数量；如上面的框架图所示，每个 partition 都存在两个备份； Consumer：消费者，从 Kafka 集群的 broker 中 pull 消息、消费消息； Consumer group：high-level consumer API 中，每个 consumer 都属于一个 consumer-group，每条消息只能被 consumer-group 中的一个 Consumer 消费，但可以被多个 consumer-group 消费； replicas：partition 的副本，保障 partition 的高可用； leader：replicas 中的一个角色， producer 和 consumer 只跟 leader 交互； follower：replicas 中的一个角色，从 leader 中复制数据，作为副本，一旦 leader 挂掉，会从它的 followers 中选举出一个新的 leader 继续提供服务； controller：Kafka 集群中的其中一个服务器，用来进行 leader election 以及 各种 failover(故障转移)； ZooKeeper：Kafka 通过 ZooKeeper 来存储集群的 meta 信息等，文中将详述。 1.1 Topic &amp; Partition一个 topic 可以认为是一类消息，每个 topic 将被分成多个 partition，每个 partition 在存储层面是 append log 文件。任何发布到此 partition 的消息都会被追加到log文件的尾部，每条消息在文件中的位置称为 offset（偏移量），offset 为一个 long 型的数字，它唯一标记一条消息。 Kafka 机制中，producer push 来的消息是追加（append）到 partition 中的，这是一种顺序写磁盘的机制，效率远高于随机写内存，如下示意图： 1.2 Kafka 为什么要将 Topic 进行分区？1简而言之：负载均衡 + 水平扩展。 前已述及，Topic 只是逻辑概念，面向的是 producer 和 consumer；而 Partition 则是物理概念。可以想象，如果 Topic 不进行分区，而将 Topic 内的消息存储于一个 broker，那么关于该 Topic 的所有读写请求都将由这一个 broker 处理，吞吐量很容易陷入瓶颈，这显然是不符合高吞吐量应用场景的。有了 Partition 概念以后，假设一个 Topic 被分为 10 个 Partitions，Kafka 会根据一定的算法将 10 个 Partition 尽可能均匀的分布到不同的 broker（服务器）上，当 producer 发布消息时，producer 客户端可以采用 random、key-hash 及 轮询 等算法选定目标 partition，若不指定，Kafka 也将根据一定算法将其置于某一分区上。Partiton 机制可以极大的提高吞吐量，并且使得系统具备良好的水平扩展能力。 在创建 topic 时可以在 $KAFKA_HOME/config/server.properties 中指定这个 partition 的数量（如下所示），当然可以在 topic 创建之后去修改 partition 的数量。 1234# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=3 在发送一条消息时，可以指定这个消息的 key，producer 根据这个 key 和 partition 机制来判断这个消息发送到哪个partition。partition 机制可以通过指定 producer 的 partition.class 这一参数来指定（即支持自定义），该 class 必须实现 kafka.producer.Partitioner 接口。 有关 topic 与 partition 的更多细节，可以参考下面的“Kafka 文件存储机制”这一节。 2. Kafka 高可靠性实现基础解读谈及可靠性，最常规、最有效的策略就是 “副本（replication）机制” ，Kafka 实现高可靠性同样采用了该策略。通过调节副本相关参数，可使 Kafka 在性能和可靠性之间取得平衡。本节先从 Kafka 文件存储机制入手，从最底层了解 Kafka 的存储细节，进而对消息的存储有个微观的认知。之后通过介绍 Kafka 的复制原理和同步方式来阐述宏观层面的概念。最后介绍 ISR，HW 和 leader 选举。 2.1 Kafka 文件存储机制Kafka 中消息是以 topic 进行分类的，生产者通过 topic 向 Kafka broker 发送消息，消费者通过 topic 读取数据。然而 topic 在物理层面又能以 partition 为分组，一个 topic 可以分成若干个 partition。事实上，partition 并不是最终的存储粒度，partition 还可以细分为 segment，一个 partition 物理上由多个 segment 组成，那么这些 segment 又是什么呢？ 为了便于说明问题，假设这里只有一个 Kafka 集群，且这个集群只有一个 Kafka broker，即只有一台物理机。在这个 Kafka broker 中配置 log.dirs=/tmp/kafka-logs，以此来设置 Kafka 消息文件存储目录；与此同时，通过命令创建一个 topic：mytopic_test，partition 的数量配置为 4（创建 topic 的命令请见上一课）。之后，可以在 /tmp/kafka-logs 目录中可以看到生成了 4 个目录： 1234drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-0drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-1drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-2drwxr-xr-x 2 root root 4096 Apr 15 13:21 mytopic_test-3 在 Kafka 文件存储中，同一个 topic 下有多个不同的 partition，每个 partiton 为一个目录，partition 的名称规则为：topic 名称 + 有序序号，第一个序号从 0 开始计，最大的序号为 partition 数量减 1，partition 是实际物理上的概念，而 topic 是逻辑上的概念。 问题 1：为什么不能以 partition 作为存储单位？ 上面提到 partition 还可以细分为 segment，这个 segment 又是什么？如果就以 partition 为最小存储单位，可以想象，当 Kafka producer 不断发送消息，必然会引起 partition 文件的无限扩张，将对消息文件的维护以及已消费的消息的清理带来严重的影响，因此，需以 segment 为单位将 partition 进一步细分。每个 partition（目录）相当于一个巨型文件被平均分配到多个大小相等的 segment（段）数据文件中（每个 segment 文件中消息数量不一定相等）这种特性也方便 old segment 的删除，即方便已被消费的消息的清理，提高磁盘的利用率。每个 partition 只需要支持顺序读写就行，segment 的文件生命周期由服务端配置参数（log.segment.bytes，log.roll.{ms,hours} 等若干参数）决定。 问题 2：segment 的工作原理是怎样的？ segment 文件由两部分组成，分别为 “.index” 文件和 “.log” 文件，分别表示为 segment 索引文件和数据文件。这两个文件的命令规则为：partition 全局的第一个 segment 从 0 开始，后续每个 segment 文件名为上一个 segment 文件最后一条消息的 offset 值，数值大小为 64 位，20 位数字字符长度，没有数字用 0 填充，如下： 12345600000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.log 以上面的 segment 文件为例，展示出 segment：00000000000000170410 的 “.index” 文件和 “.log” 文件的对应的关系，如下图： 如上图，“.index” 索引文件存储大量的元数据，“.log” 数据文件存储大量的消息，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。其中以 “.index” 索引文件中的元数据 [3, 348] 为例，在 “.log” 数据文件表示第 3 个消息，即在全局 partition 中表示 170410+3=170413 个消息，该消息的物理偏移地址为 348。 问题 3：如何从 partition 中通过 offset 查找 message 呢？ 以上图为例，读取 offset=170418 的消息，首先查找 segment 文件，其中 00000000000000000000.index 为最开始的文件，第二个文件为 00000000000000170410.index（起始偏移为 170410+1=170411），而第三个文件为 00000000000000239430.index（起始偏移为 239430+1=239431），所以这个 offset=170418 就落到了第二个文件之中。其它后续文件可以依次类推，以其偏移量命名并排列这些文件，然后根据二分查找法就可以快速定位到具体文件位置。其次根据 00000000000000170410.index 文件中的 [8,1325] 定位到 00000000000000170410.log 文件中的 1325 的位置进行读取。 要是读取 offset=170418 的消息，从 00000000000000170410.log 文件中的 1325 的位置进行读取，那么，如何确定何时读完本条消息呢？（否则就读到下一条消息的内容了） 这个问题由消息的物理结构解决，消息都具有固定的物理结构，包括：offset（8 Bytes）、消息体的大小（4 Bytes）、crc32（4 Bytes）、magic（1 Byte）、attributes（1 Byte）、key length（4 Bytes）、key（K Bytes）、payload（N Bytes）等等字段，可以确定一条消息的大小，即读取到哪里截止。 2.2 复制原理和同步方式Kafka 中 topic 的每个 partition 有一个预写式的日志文件，虽然 partition 可以继续细分为若干个 segment 文件，但是对于上层应用来说，仍然可以将 partition 看成最小的存储单元（一个有多个 segment 文件拼接的 “巨型” 文件），每个 partition 都由一些列有序的、不可变的消息组成，这些消息被连续的追加到 partition 中。 上图中有两个新名词：HW 和 LEO。这里先介绍下 LEO，LogEndOffset 的缩写，表示每个 partition 的 log 最后一条 Message 的位置。HW 是 HighWatermark 的缩写，是指 consumer 能够看到的此 partition 的位置，这个涉及到多副本的概念，这里先提及一下，下文再详述。 言归正传，为了提高消息的可靠性，Kafka 每个 topic 的 partition 有 N 个副本（replicas），其中 N（大于等于 1）是 topic 的复制因子（replica fator）的个数。Kafka 通过多副本机制实现故障自动转移，当 Kafka 集群中出现 broker 失效时，副本机制可保证服务可用。对于任何一个 partition，它的 N 个 replicas 中，其中一个 replica 为 leader，其他都为 follower，leader 负责处理 partition 的所有读写请求，follower 则负责被动地去复制 leader 上的数据。如下图所示，Kafka 集群中有 4 个 broker，某 topic 有 3 个 partition，且复制因子即副本个数也为 3： 如果 leader 所在的 broker 发生故障或宕机，对应 partition 将因无 leader 而不能处理客户端请求，这时副本的作用就体现出来了：一个新 leader 将从 follower 中被选举出来并继续处理客户端的请求。 如何确保新选举出的 leader 是优选呢？ 一个 partition 有多个副本（replicas），为了提高可靠性，这些副本分散在不同的 broker 上，由于带宽、读写性能、网络延迟等因素，同一时刻，这些副本的状态通常是不一致的：即 followers 与 leader 的状态不一致。那么，如何保证新选举出的 leader 是优选呢？ Kafka 机制中，leader 将负责维护和跟踪一个 ISR（In-Sync Replicas）列表，即同步副本队列，这个列表里面的副本与 leader 保持同步，状态一致。如果新的 leader 从 ISR 列表中的副本中选出，那么就可以保证新 leader 为优选。当然，这不是唯一的策略，下文将继续解读。 2.3 同步副本 ISR上一节中讲到了同步副本队列 ISR（In-Sync Replicas）。虽然副本极大的增强了可用性，但是副本数量对 Kafka 的吞吐率有一定影响。默认情况下 Kafka 的 replica 数量为 1，即每个 partition 都只有唯一的 leader，无 follower，没有容灾能力。为了确保消息的可靠性，生产环境中，通常将其值（由 broker 的参数 offsets.topic.replication.factor 指定）大小设置为大于 1，比如 3。 所有的副本（replicas）统称为 Assigned Replicas，即 AR。ISR 是 AR 中的一个子集，由 leader 维护 ISR 列表，follower 从 leader 同步数据有一些延迟（由参数 replica.lag.time.max.ms 设置超时阈值），超过阈值的 follower 将被剔除出 ISR， 存入 OSR（Outof-Sync Replicas）列表，新加入的 follower 也会先存放在 OSR 中。AR=ISR+OSR。 1注：ISR中包括：leader + 与leader保持同步的followers。 上面一节还涉及到一个概念，即 HW。HW 俗称高水位，HighWatermark 的缩写，取一个 partition 对应的 ISR 中最小的 LEO 作为 HW，consumer 最多只能消费到 HW 所在的位置。另外每个 replica 都有 HW，leader 和 follower 各自负责更新自己的 HW 的状态。对于 leader 新写入的消息，consumer 不能立刻消费，leader 会等待该消息被所有 ISR 中的 replicas 同步后更新 HW，此时消息才能被 consumer 消费。这样就保证了如果 leader 所在的 broker 失效，该消息仍然可以从新选举的 leader 中获取。对于来自内部 broker 的读取请求，没有 HW 的限制。 下图详细的说明了当 producer 生产消息至 broker 后，ISR 以及 HW 和 LEO 的流转过程： 由此可见，Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 都复制完，这条消息才会被 commit，这种复制方式受限于复制最慢的 follower，会极大的影响吞吐率。而异步复制方式下，follower 异步的从 leader 复制数据，数据只要被 leader 写入 log 就被认为已经 commit，这种情况下如果 follower 都还没有复制完，落后于 leader 时，突然 leader 宕机，则会丢失数据，降低可靠性。而 Kafka 使用 ISR 的策略则在可靠性和吞吐率方面取得了较好的平衡。 Kafka 的 ISR 的管理最终都会反馈到 ZooKeeper 节点上，具体位置为： 1/brokers/topics/[topic]/partitions/[partition]/state 目前，有两个地方会对这个 ZooKeeper 的节点进行维护。 Controller 来维护：Kafka 集群中的其中一个 Broker 会被选举为 Controller，主要负责 Partition 管理和副本状态管理，也会执行类似于重分配 partition 之类的管理任务。在符合某些特定条件下，Controller 下的 LeaderSelector 会选举新的 leader，ISR 和新的 leader_epoch 及 controller_epoch 写入 ZooKeeper 的相关节点中。同时发起 LeaderAndIsrRequest 通知所有的 replicas。 leader 来维护：leader 有单独的线程定期检测 ISR 中 follower 是否脱离 ISR，如果发现 ISR 变化，则会将新的 ISR 的信息返回到 ZooKeeper 的相关节点中。 2.4 数据可靠性和持久性保证当 producer 向 leader 发送数据时，可以通过 request.required.acks 参数来设置数据可靠性的级别： 1. request.required.acks = 1 这是默认情况，即：producer 发送数据到 leader，leader 写本地日志成功，返回客户端成功；此时 ISR 中的其它副本还没有来得及拉取该消息，如果此时 leader 宕机了，那么此次发送的消息就会丢失。 2. request.required.acks = 0 producer 不停向leader发送数据，而不需要 leader 反馈成功消息，这种情况下数据传输效率最高，但是数据可靠性确是最低的。可能在发送过程中丢失数据，可能在 leader 宕机时丢失数据。 3. request.required.acks = -1（all） producer 发送数据给 leader，leader 收到数据后要等到 ISR 列表中的所有副本都同步数据完成后（强一致性），才向生产者返回成功消息，如果一直收不到成功消息，则认为发送数据失败会自动重发数据。这是可靠性最高的方案，当然，性能也会受到一定影响。 *注意：参数 min.insync.replicas * 如果要提高数据的可靠性，在设置 request.required.acks=-1 的同时，还需参数 min.insync.replicas 配合，如此才能发挥最大的功效。min.insync.replicas 这个参数用于设定 ISR 中的最小副本数，默认值为1，当且仅当 request.required.acks 参数设置为-1时，此参数才生效。当 ISR 中的副本数少于 min.insync.replicas 配置的数量时，客户端会返回异常：org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。不难理解，如果 min.insync.replicas 设置为 2，当 ISR 中实际副本数为 1 时（只有leader），将无法保证可靠性，此时拒绝客户端的写请求以防止消息丢失。 2.5 深入解读 HW 机制考虑这样一种场景：acks=-1，部分 ISR 副本完成同步，此时leader挂掉，如下图所示：follower1 同步了消息 4、5，follower2 同步了消息 4，与此同时 follower2 被选举为 leader，那么此时 follower1 中的多出的消息 5 该做如何处理呢？ 这里就需要 HW 的协同配合了。如前所述，一个 partition 中的 ISR 列表中，leader 的 HW 是所有 ISR 列表里副本中最小的那个的 LEO。类似于木桶原理，水位取决于最低那块短板。 如上图，某个 topic 的某 partition 有三个副本，分别为 A、B、C。A 作为 leader 肯定是 LEO 最高，B 紧随其后，C 机器由于配置比较低，网络比较差，故而同步最慢。这个时候 A 机器宕机，这时候如果 B 成为 leader，假如没有 HW，在 A 重新恢复之后会做同步（makeFollower) 操作，在宕机时 log 文件之后直接做追加操作，而假如 B 的 LEO 已经达到了 A 的 LEO，会产生数据不一致的情况，所以使用 HW 来避免这种情况。 A 在做同步操作的时候，先将 log 文件截断到之前自己的 HW 的位置，即 3，之后再从 B 中拉取消息进行同步。 如果失败的 follower 恢复过来，它首先将自己的 log 文件截断到上次 checkpointed 时刻的 HW 的位置，之后再从 leader 中同步消息。leader 挂掉会重新选举，新的 leader 会发送 “指令” 让其余的 follower 截断至自身的 HW 的位置然后再拉取新的消息。 当 ISR 中的个副本的 LEO 不一致时，如果此时 leader 挂掉，选举新的 leader 时并不是按照 LEO 的高低进行选举，而是按照 ISR 中的顺序选举。 2.6 Leader 选举为了保证可靠性，对于任意一条消息，只有它被 ISR 中的所有 follower 都从 leader 复制过去才会被认为已提交，并返回信息给 producer。如此，可以避免因部分数据被写进 leader，而尚未被任何 follower 复制就宕机的情况下而造成数据丢失。对于 producer 而言，它可以选择是否等待消息 commit，这可以通过参数 request.required.acks 来设置。这种机制可以确保：只要 ISR 中有一个或者以上的 follower，一条被 commit 的消息就不会丢失。 问题 1：如何在保证可靠性的前提下避免吞吐量下降？ 有一个很重要的问题是当 leader 宕机了，怎样在 follower 中选举出新的 leader，因为 follower 可能落后很多或者直接 crash 了，所以必须确保选择 “最新” 的 follower 作为新的 leader。一个基本的原则就是，如果 leader 挂掉，新的 leader 必须拥有原来的 leader 已经 commit 的所有消息，这不就是 ISR 中副本的特征吗？ 但是，存在一个问题，ISR 列表维持多大的规模合适呢？换言之，leader 在一个消息被 commit 前需要等待多少个 follower 确认呢？等待 follower 的数量越多，与 leader 保持同步的 follower 就越多，可靠性就越高，但这也会造成吞吐率的下降。 少数服从多数的选举原则 一种常用的选举 leader 的策略是 “少数服从多数” ，不过，Kafka 并不是采用这种方式。这种模式下，如果有 2f+1 个副本，那么在 commit 之前必须保证有 f+1 个 replica 复制完消息，同时为了保证能正确选举出新的 leader，失败的副本数不能超过 f 个。这种方式有个很大的优势，系统的延迟取决于最快的几台机器，也就是说比如副本数为 3，那么延迟就取决于最快的那个 follower 而不是最慢的那个。 “少数服从多数” 的策略也有一些劣势，为了保证 leader 选举的正常进行，它所能容忍的失败的 follower 数比较少，如果要容忍 1 个 follower 挂掉，那么至少要 3 个以上的副本，如果要容忍 2 个 follower 挂掉，必须要有 5 个以上的副本。也就是说，在生产环境下为了保证较高的容错率，必须要有大量的副本，而大量的副本又会在大数据量下导致性能的急剧下降。这种算法更多用在 ZooKeeper 这种共享集群配置的系统中，而很少在需要大量数据的系统中使用。 Kafka 选举 leader 的策略是怎样的？ 实际上，leader 选举的算法非常多，比如 ZooKeeper 的 Zab、Raft 以及 Viewstamped Replication。而 Kafka 所使用的 leader 选举算法更像是微软的 PacificA 算法。 Kafka 在 ZooKeeper 中为每一个 partition 动态的维护了一个 ISR，这个 ISR 里的所有 replica 都与 leader 保持同步，只有 ISR 里的成员才能有被选为 leader 的可能（通过参数配置：unclean.leader.election.enable=false）。在这种模式下，对于 f+1 个副本，一个 Kafka topic 能在保证不丢失已经 commit 消息的前提下容忍 f 个副本的失败，在大多数使用场景下，这种模式是十分有利的。事实上，对于任意一条消息，只有它被 ISR 中的所有 follower 都从 leader 复制过去才会被认为已提交，并返回信息给 producer，从而保证可靠性。但与 “少数服从多数” 策略不同的是，Kafka ISR 列表中副本的数量不需要超过副本总数的一半，即不需要满足 “多数派” 原则，通常，ISR 列表副本数大于等于 2 即可，如此，便在可靠性和吞吐量方面取得平衡。 极端情况下的 leader 选举策略 前已述及，当 ISR 中至少有一个 follower 时（ISR 包括 leader），Kafka 可以确保已经 commit 的消息不丢失，但如果某一个 partition 的所有 replica 都挂了，自然就无法保证数据不丢失了。这种情况下如何进行 leader 选举呢？通常有两种方案： 等待 ISR 中任意一个 replica 恢复过来，并且选它作为 leader； 选择第一个恢复过来的 replica（并不一定是在 ISR 中）作为leader。（默认） 如何选择呢？这就需要在可用性和一致性当中作出抉择。如果一定要等待 ISR 中的 replica 恢复过来，不可用的时间就可能会相对较长。而且如果 ISR 中所有的 replica 都无法恢复了，或者数据丢失了，这个 partition 将永远不可用。 选择第一个恢复过来的 replica 作为 leader，如果这个 replica 不是 ISR 中的 replica，那么，它可能并不具备所有已经 commit 的消息，从而造成消息丢失。默认情况下，Kafka 采用第二种策略，即 unclean.leader.election.enable=true，也可以将此参数设置为 false 来启用第一种策略。 unclean.leader.election.enable 这个参数对于 leader 的选举、系统的可用性以及数据的可靠性都有至关重要的影响。生产环境中应慎重权衡。 3. Kafka 架构中 ZooKeeper 以怎样的形式存在？ZooKeeper 是一个分布式的、开放源码的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现。分布式应用程序可以基于它实现统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等工作。在基于 Kafka 的分布式消息队列中，ZooKeeper 的作用有：broker 注册、topic 注册、producer 和 consumer 负载均衡、维护 partition 与 consumer 的关系、记录消息消费的进度以及 consumer 注册等。 3.1 broker 在 ZooKeeper 中的注册 为了记录 broker 的注册信息，在 ZooKeeper 上，专门创建了属于 Kafka 的一个节点，其路径为 /brokers； Kafka 的每个 broker 启动时，都会到 ZooKeeper 中进行注册，告诉 ZooKeeper 其 broker.id，在整个集群中，broker.id 应该全局唯一，并在 ZooKeeper 上创建其属于自己的节点，其节点路径为 /brokers/ids/{broker.id}； 创建完节点后，Kafka 会将该 broker 的 broker.name 及端口号记录到该节点； 另外，该 broker 节点属性为临时节点，当 broker 会话失效时，ZooKeeper 会删除该节点，这样，我们就可以很方便的监控到broker 节点的变化，及时调整负载均衡等。 3.2 Topic 在 ZooKeeper 中的注册在 Kafka 中，所有 topic 与 broker 的对应关系都由 ZooKeeper 进行维护，在 ZooKeeper 中，建立专门的节点来记录这些信息，其节点路径为 /brokers/topics/{topic_name}。 前面说过，为了保障数据的可靠性，每个 Topic 的 Partitions 实际上是存在备份的，并且备份的数量由 Kafka 机制中的 replicas 来控制。那么问题来了：如下图所示，假设某个 TopicA 被分为 2 个 Partitions，并且存在两个备份，由于这 2 个 Partitions（1-2）被分布在不同的 broker 上，同一个 partiton 与其备份不能（也不应该）存储于同一个 broker 上。以 Partition1 为例，假设它被存储于 broker2，其对应的备份分别存储于 broker1 和 broker4，有了备份，可靠性得到保障，但数据一致性却是个问题。 为了保障数据的一致性，ZooKeeper 机制得以引入。基于 ZooKeeper，Kafka 为每一个 partition 找一个节点作为 leader，其余备份作为 follower；接续上图的例子，就 TopicA 的 partition1 而言，如果位于 broker2（Kafka 节点）上的 partition1 为 leader，那么位于 broker1 和 broker4 上面的 partition1 就充当 follower，则有下图： 基于上图的架构，当 producer push 的消息写入 partition（分区) 时，作为 leader 的 broker（Kafka 节点） 会将消息写入自己的分区，同时还会将此消息复制到各个 follower，实现同步。如果，某个follower 挂掉，leader 会再找一个替代并同步消息；如果 leader 挂了，follower 们会选举出一个新的 leader 替代，继续业务，这些都是由 ZooKeeper 完成的。 3.3 consumer 在 ZooKeeper 中的注册注册新的消费者分组 当新的消费者组注册到 ZooKeeper 中时，ZooKeeper 会创建专用的节点来保存相关信息，其节点路径为 ls/consumers/{group_id}，其节点下有三个子节点，分别为 [ids, owners, offsets]。 ids 节点：记录该消费组中当前正在消费的消费者； owners 节点：记录该消费组消费的 topic 信息； offsets 节点：记录每个 topic 的每个分区的 offset。 注册新的消费者 当新的消费者注册到 Kafka 中时，会在 /consumers/{group_id}/ids 节点下创建临时子节点，并记录相关信息。 监听消费者分组中消费者的变化 每个消费者都要关注其所属消费者组中消费者数目的变化，即监听 /consumers/{group_id}/ids 下子节点的变化。一旦发现消费者新增或减少，就会触发消费者的负载均衡。 3.4 Producers 负载均衡对于同一个 topic 的不同 partition，Kafka会尽力将这些 partition 分布到不同的 broker 服务器上，这种均衡策略实际上是基于 ZooKeeper 实现的。在一个 broker 启动时，会首先完成 broker 的注册过程，并注册一些诸如 “有哪些可订阅的 topic” 之类的元数据信息。producers 启动后也要到 ZooKeeper 下注册，创建一个临时节点来监听 broker 服务器列表的变化。由于在 ZooKeeper 下 broker 创建的也是临时节点，当 brokers 发生变化时，producers 可以得到相关的通知，从改变自己的 broker list。其它的诸如 topic 的变化以及broker 和 topic 的关系变化，也是通过 ZooKeeper 的这种 Watcher 监听实现的。 在生产中，必须指定 topic；但是对于 partition，有两种指定方式： 明确指定 partition(0-N)，则数据被发送到指定 partition； 设置为 RD_KAFKA_PARTITION_UA，则 Kafka 会回调 partitioner 进行均衡选取，partitioner 方法需要自己实现。可以轮询或者传入 key 进行 hash。未实现则采用默认的随机方法 rd_kafka_msg_partitioner_random 随机选择。 3.5 Consumer 负载均衡Kafka 保证同一 consumer group 中只有一个 consumer 可消费某条消息，实际上，Kafka 保证的是稳定状态下每一个 consumer 实例只会消费某一个或多个特定的数据，而某个 partition 的数据只会被某一个特定的 consumer 实例所消费。这样设计的劣势是无法让同一个 consumer group 里的 consumer 均匀消费数据，优势是每个 consumer 不用都跟大量的 broker 通信，减少通信开销，同时也降低了分配难度，实现也更简单。另外，因为同一个 partition 里的数据是有序的，这种设计可以保证每个 partition 里的数据也是有序被消费。 consumer 数量不等于 partition 数量 如果某 consumer group 中 consumer 数量少于 partition 数量，则至少有一个 consumer 会消费多个 partition 的数据；如果 consumer 的数量与 partition 数量相同，则正好一个 consumer 消费一个 partition 的数据，而如果 consumer 的数量多于 partition 的数量时，会有部分 consumer 无法消费该 topic 下任何一条消息。 借助 ZooKeeper 实现负载均衡 关于负载均衡，对于某些低级别的 API，consumer 消费时必须指定 topic 和 partition，这显然不是一种友好的均衡策略。基于高级别的 API，consumer 消费时只需制定 topic，借助 ZooKeeper 可以根据 partition 的数量和 consumer 的数量做到均衡的动态配置。 consumers 在启动时会到 ZooKeeper 下以自己的 conusmer-id 创建临时节点 /consumer/[group-id]/ids/[conusmer-id]，并对 /consumer/[group-id]/ids 注册监听事件，当消费者发生变化时，同一 group 的其余消费者会得到通知。当然，消费者还要监听 broker 列表的变化。librdkafka 通常会将 partition 进行排序后，根据消费者列表，进行轮流的分配。 3.6 记录消费进度 Offset在 consumer 对指定消息 partition 的消息进行消费的过程中，需要定时地将 partition 消息的消费进度 Offset 记录到 ZooKeeper上，以便在该 consumer 进行重启或者其它 consumer 重新接管该消息分区的消息消费权后，能够从之前的进度开始继续进行消息消费。Offset 在 ZooKeeper 中由一个专门节点进行记录，其节点路径为： 12#节点内容就是Offset的值。/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id] PS：Kafka 已推荐将 consumer 的 Offset 信息保存在 Kafka 内部的 topic 中，即： 1__consumer_offsets(/brokers/topics/__consumer_offsets) 并且默认提供了 kafka_consumer_groups.sh 脚本供用户查看consumer 信息（命令：sh kafka-consumer-groups.sh –bootstrap-server * –describe –group *）。在当前版本中，offset 存储方式要么存储在本地文件中，要么存储在 broker 端，具体的存储方式取决 offset.store.method 的配置，默认是存储在 broker 端。 3.7 记录 Partition 与 Consumer 的关系consumer group 下有多个 consumer（消费者），对于每个消费者组（consumer group），Kafka都会为其分配一个全局唯一的 group ID，group 内部的所有消费者共享该 ID。订阅的 topic 下的每个分区只能分配给某个 group 下的一个consumer（当然该分区还可以被分配给其它 group）。同时，Kafka 为每个消费者分配一个 consumer ID，通常采用 hostname:UUID 形式表示。 在Kafka中，规定了每个 partition 只能被同组的一个消费者进行消费，因此，需要在 ZooKeeper 上记录下 partition 与 consumer 之间的关系，每个 consumer 一旦确定了对一个 partition 的消费权力，需要将其 consumer ID 写入到 ZooKeeper 对应消息分区的临时节点上，例如： 1/consumers/[group_id]/owners/[topic]/[broker_id-partition_id] 其中，[broker_id-partition_id] 就是一个消息分区的标识，节点内容就是该消息分区 消费者的 consumer ID。 4. 全程解析（Producer-kafka-consumer）4.1 producer 发布消息producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。 其路由机制为： 指定了 patition，则直接使用； 未指定 patition 但指定 key，通过对 key 进行 hash 选出一个 patition； patition 和 key 都未指定，使用轮询选出一个 patition。 写入流程： producer 先从 ZooKeeper 的 “/brokers/…/state” 节点找到该 partition 的leader； producer 将消息发送给该 leader； leader 将消息写入本地 log； followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK； leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK； 4.2 Broker 存储消息物理上把 topic 分成一个或多个 patition，每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件） 4.3 Consumer 消费消息high-level consumer API 提供了 consumer group 的语义，一个消息只能被 group 内的一个 consumer 所消费，且 consumer 消费消息时不关注 offset，最后一个 offset 由 ZooKeeper 保存（下次消费时，该group 中的consumer将从offset记录的位置开始消费）。 注意： 如果消费线程大于 patition 数量，则有些线程将收不到消息； 如果 patition 数量大于消费线程数，则有些线程多收到多个 patition 的消息； 如果一个线程消费多个 patition，则无法保证你收到的消息的顺序，而一个 patition 内的消息是有序的。 consumer 采用 pull 模式从 broker 中读取数据。 push 模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。 对于 Kafka 而言，pull 模式更合适，它可简化 broker 的设计，consumer 可自主控制消费消息的速率，同时 consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 上述文章参考自：深入浅出理解基于 Kafka 和 ZooKeeper 的分布式消息队列]]></content>
      <categories>
        <category>MQ</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中sleep和wait的区别]]></title>
    <url>%2F2019%2F06%2F17%2Fjava-sleep-wait%2F</url>
    <content type="text"><![CDATA[sleep() &amp; wait() sleep()方法 wait()方法 sleep()使当前线程进入停滞状态（阻塞当前线程，即会发生上下文的切换，会有很大的开销），让出CUP的使用，目的是不让当前线程独自霸占该进程所获的CPU资源，以留一定时间给其他线程执行的机会 wait()方法是Object类里的方法；当一个线程执行到wait()方法时，它就进入到一个和该对象相关的等待池中，同时失去（释放）了对象的机锁（暂时失去机锁，wait(long timeout)超时时间到后还需要返还对象锁）；其他线程可以访问 sleep()是Thread类的static(静态)的方法；因此它不能改变对象的锁，所以当在一个synchronized块中调用sleep()方法是，线程虽然休眠了，但是对象的机锁并没有被释放，其他线程无法获取对象锁（即使睡着也持有对象锁） wait()使用notify或者notifyAlll或者指定睡眠时间来唤醒当前等待池中的线程 在sleep()休眠时间期满后，该线程不一定会立即执行，这是因为其它线程可能正在运行而且没有被调度为放弃执行，除非此线程具有更高的优先级 wait()必须放在synchronized block中，否则会在program runtime时抛出”java.lang.IllegalMonitorStateException“异常 ## 区别 1sleep（）和wait()这两个函数被调用之后线程都应该放弃执行权，不同的是sleep（）不释放锁而wait（）的话是释放锁。直白的意思是一个线程调用Sleep（）之后进入了阻塞状态，它的意思就是当sleep()状态超时，线程重新转入可运行(runnable)状态。而Wait（）在释放执行权之后也把锁释放了,通过notify()或者notifyAll()或者指定睡眠时间来唤醒后，它要运行的话还是要和其他的线程去竞争锁，之后才可以获得执行权。 1234所以sleep()和wait()方法的最大区别是： sleep()睡眠时，保持对象锁，仍然占有该锁； 而wait()睡眠时，释放对象锁。 但是wait()和sleep()都可以通过interrupt()方法打断线程的暂停状态，从而使线程立刻抛出InterruptedException（但不建议使用该方法）。 Java中sleep方法的几个注意点 Thread.sleep()方法用来暂停线程的执行，将CPU放给线程调度器。 Thread.sleep()方法是一个静态方法，它暂停的是当前执行的线程。 Java有两种sleep方法，一个只有一个毫秒参数，另一个有毫秒和纳秒两个参数。 与wait方法不同，sleep方法不会释放锁。 如果其他的线程中断了一个休眠的线程，sleep方法会抛出Interrupted Exception。 休眠的线程在唤醒之后不保证能获取到CPU，它会先进入就绪态，与其他线程竞争CPU。 有一个易错的地方，当调用t.sleep()的时候，会暂停线程t。这是不对的，因为Thread.sleep是一个静态方法，它会使当前线程而不是线程t进入休眠状态。 wait方法必须正在同步环境下使用，比如synchronized方法或者同步代码块。如果你不在同步条件下使用，会抛出IllegalMonitorStateException异常。另外，sleep方法不需要再同步条件下调用，你可以任意正常的使用。 wait方法用于和定义于Object类的，而sleep方法操作于当前线程，定义在java.lang.Thread类里面。 参考自： Java sleep和wait的区别 在阻塞式io中，如果一个线程在等待io操作，那么cpu还会分配时间片给该线程吗？]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>sleep</tag>
        <tag>wait</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reactor线程模型]]></title>
    <url>%2F2019%2F06%2F17%2FReactor-thread-model%2F</url>
    <content type="text"><![CDATA[Reactor是什么? The reactor design_pattern is an event_handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to the associated request handlers. from wiki 通过wiki中的定义我们可以发现Reactor的重点 事件驱动 可以处理一个或多个输入源 通过多路复用将请求的事件分发给对应的处理器处理 根据大神Doug Lea 在 《Scalable IO in Java 》中的介绍，Reacotr模型主要分为三个角色 Reactor：把IO事件分配给对应的handler处理 Acceptor：处理客户端连接事件 Handler：处理非阻塞的任务 为什么使用Reactor？ 传统阻塞IO模型的不足 每个连接都需要独立线程处理，当并发数大时，创建线程数多，占用资源 采用阻塞IO模型，连接建立后，若当前线程没有数据可读，线程会阻塞（阻塞会有切换进程或线程上下文的开销）在读操作上，造成资源浪费 针对传统阻塞IO模型的两个问题，可以采用如下的方案 基于池化思想，避免为每个连接创建线程，连接完成后将业务处理交给线程池处理 基于IO复用模型，多个连接共用同一个阻塞对象，不用等待所有的连接。遍历到有新数据可以处理时，操作系统会通知程序，线程跳出阻塞状态，进行业务逻辑处理 Reactor线程模型的思想就是基于IO复用和线程池（线程复用）的结合 Reactor线程模型分类根据Reactor的数量和处理资源的线程数量的不同，分为三类： 单Reactor单线程模型 单Reactor多线程模型 多Reactor多线程模型 单Reactor单线程模型 这种模型在Reactor中处理事件，并分发事件，如果是连接事件交给acceptor处理，如果是读写事件和业务处理就交给handler处理，但始终只有一个线程执行所有的事情 该线程模型的不足 仅用一个线程处理请求，对于多核资源机器来说是有点浪费的 当处理读写任务的线程负载过高后，处理速度下降，事件会堆积，严重的会超时，可能导致客户端重新发送请求，性能越来越差 单线程也会有可靠性的问题 针对上面的种种不足，就有了下面的线程模型 单Reactor多线程模型 这种模型和第一种模型到的主要区别是把业务处理从之前的单一线程脱离出来，换成线程池处理，也就是Reactor线程只处理连接事件和读写事件，业务处理交给线程池处理，充分利用多核机器的资源、提高性能并且增加可靠性 该线程模型的不足 Reactor线程承担所有的事件，例如监听和响应，高并发场景下单线程存在性能问题 多Reactor多线程模型 这种模型下和第二种模型相比是把Reactor线程拆分了mainReactor和subReactor两个部分，mainReactor只处理连接事件，读写事件交给subReactor来处理。业务逻辑还是由线程池来处理 mainRactor只处理连接事件，用一个线程来处理就好。处理读写事件的subReactor个数一般和CPU数量相等，一个subReactor对应一个线程，业务逻辑由线程池处理 这种模型使各个模块职责单一，降低耦合度，性能和稳定性都有提高 这种模型在许多项目中广泛应用，比如Netty的主从线程模型等 Reactor三种模式形象比喻餐厅一般有接待员和服务员，接待员负责在门口接待顾客，服务员负责全程服务顾客 Reactor的三种线程模型可以用接待员和服务员类比 单Reactor单线程模型：接待员和服务员是同一个人，一直为顾客服务。客流量较少适合 单Reactor多线程模型：一个接待员，多个服务员。客流量大，一个人忙不过来，由专门的接待员在门口接待顾客，然后安排好桌子后，由一个服务员一直服务，一般每个服务员负责一片中的几张桌子 多Reactor多线程模型：多个接待员，多个服务员。这种就是客流量太大了，一个接待员忙不过来了 参考资料 《Scalable IO in Java》 -Doug Lea 【关注公众号，回复“Doug Lea” 获取该pdf】 1文章来源微信公众号：每天晒白牙]]></content>
      <categories>
        <category>IO模型</category>
      </categories>
      <tags>
        <tag>Reactor</tag>
        <tag>IO模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux grep命令详解]]></title>
    <url>%2F2019%2F06%2F16%2Flinux-grep%2F</url>
    <content type="text"><![CDATA[一篇讲解grep很好的文章，以便日后查阅。 文章来源：grep命令详解]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中永久代的垃圾回收]]></title>
    <url>%2F2019%2F06%2F16%2Fjava-perm-GC%2F</url>
    <content type="text"><![CDATA[今天面试被问到jvm中永久代会发生垃圾回收吗？ 首先，关于永久代的内容可以看这个：[jvm中方法区和永久代的关系] 垃圾回收不会出现在永久代，但是如果永久代满了会触发完全垃圾回收（Full GC）。 Hotspot的永久代是在方法区，主要存储的是类加载信息，静态变量以及常量，方法（字节码）等等，可以进行常量池回收和类型卸载。 如果这个常量在其它任何对象都没被引用，则可以被回收。 而类型卸载有点复杂，有以下三点要求： 该类型的所有实例都已经被回收 该类型的ClassLoader已经被回收 该类型的java.lang.Class没有在任何地方被引用，该类型不能在任何地方以反射的方式实例化一个对象。在java8中，已经取消了永久代，但是引入了一个元数据区的navite内存区。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>永久代</tag>
        <tag>metaspace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2019%2F06%2F16%2Fmarkdown-syntax%2F</url>
    <content type="text"><![CDATA[前言学习一下Markdown的语法，以便更好地组织文章结构。在此记录，以便日后的查阅。 主要内容 Markdown是什么？谁创造了它？为什么要使用它？怎么使用？谁在用？尝试一下 正文1. Markdown是什么？Markdown是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，并最终以HTML格式发布。Markdown也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。 2. 谁创造了它？它由Aaron Swartz和John Gruber共同设计，Aaron Swartz就是那位于去年（2013年1月11日）自杀,有着开挂一般人生经历的程序员。维基百科对他的介绍是：软件工程师、作家、政治组织者、互联网活动家、维基百科人。 他有着足以让你跪拜的人生经历： 14岁参与RSS 1.0规格标准的制订。 2004年入读斯坦福，之后退学。 2005年创建Infogami，之后与Reddit合并成为其合伙人。 2010年创立求进会（Demand Progress），积极参与禁止网络盗版法案（SOPA）活动，最终该提案被撤回。 2011年7月19日，因被控从MIT和JSTOR下载480万篇学术论文并以免费形式上传于网络被捕。 2013年1月自杀身亡。 天才都有早逝的归途。 3. 为什么要使用它？ 它是易读（看起来舒服）、易写（语法简单）、易更改纯文本。处处体现着极简主义的影子。 兼容HTML，可以转换为HTML格式发布。 跨平台使用。 越来越多的网站支持Markdown。 更方便清晰地组织你的电子邮件。（Markdown-here, Airmail） 摆脱Word（我不是认真的）。 4. 怎么使用？如果不算扩展，Markdown的语法绝对简单到让你爱不释手。 Markdown语法主要分为如下几大部分： 标题，段落，区块引用，代码区块，强调，列表，分割线，链接，图片，反斜杠 \，符号’`’。 4.1 标题两种形式：1）使用=和-标记一级和二级标题。 一级标题=========二级标题--------- 效果： 一级标题二级标题 2）使用#，可表示1-6级标题。 一级标题二级标题三级标题四级标题五级标题六级标题 效果： 一级标题二级标题三级标题四级标题五级标题六级标题 4.2 段落段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用两个以上空格加上回车（引用中换行省略回车）。 4.3 区块引用在段落的每行或者只在第一行使用符号&gt;,还可使用多个嵌套引用，如： &gt; 区块引用&gt;&gt; 嵌套引用 效果： 区块引用 嵌套引用 4.4 代码区块代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如普通段落： void main(){ printf(“Hello, Markdown.”);} 代码区块： 1234void main()&#123; printf("Hello, Markdown.");&#125; 注意:需要和普通段落之间存在空行。 4.5 强调在强调内容两侧分别加上*或者_，如： 斜体，斜体粗体，粗体 效果： 斜体，斜体粗体，粗体 4.6 列表使用·、+、或-标记无序列表，如： -（+） 第一项 -（+） 第二项 - （+*）第三项 注意：标记有一个_空格或制表符_。若不在引用区块中，必须和前方段落之间存在空行。 效果： 第一项 第二项 第三项 有序列表的标记方式是将上述的符号换成数字,并辅以.，如： 1 . 第一项2 . 第二项3 . 第三项 效果： 第一项 第二项 第三项 4.7 分割线分割线最常使用就是三个或以上*，还可以使用-和_。 4.8 链接链接可以由两种形式生成：行内式和参考式。 行内式： younghz的Markdown库。 效果： younghz的Markdown库。 参考式： younghz的Markdown库1younghz的Markdown库2 效果： younghz的Markdown库1younghz的Markdown库2 注意：上述的[1]:https:://github.com/younghz/Markdown &quot;Markdown&quot;不出现在区块中。 4.9 图片添加图片的形式和链接相似，只需在链接的基础上前方加一个！。 4.10 反斜杠\相当于反转义作用。使符号成为普通符号。 4.11 符号’`’起到标记作用。如： 12&gt; ctrl+a&gt; 效果： 12&gt; ctrl+a&gt; 4.12 上标、下标markdown是支持HTML语法的，所以可以用HTML的语法表示下标、下标等。 上标： &lt;sup&gt;xxx&lt;/sup&gt;语法：2&lt;sup&gt;32&lt;/sup&gt; - 1效果：232 - 1 下标： &lt;sub&gt;xxx&lt;/sub&gt;语法：a=log&lt;sub&gt;2&lt;/sub&gt;b效果：a=log2b 5. 谁在用？Markdown的使用者： GitHub 简书 Stack Overflow Apollo Moodle Reddit 等等 6. 尝试一下 Chrome下的插件诸如stackedit与markdown-here等非常方便，也不用担心平台受限。 在线的dillinger.io评价也不错 Windowns下的MarkdownPad也用过，不过免费版的体验不是很好。 Mac下的Mou是国人贡献的，口碑很好。 Linux下的ReText不错。 当然，最终境界永远都是笔下是语法，心中格式化 :)。 注意：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。 虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber] (http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/ )。 以上基本是所有traditonal markdown的语法。 其它：列表的使用(非traditonal markdown)： 用|表示表格纵向边界，表头和表内容用-隔开，并可用:进行对齐设置，两边都有:则表示居中，若不加:则默认左对齐。 代码库 链接 MarkDown https://github.com/younghz/Markdown MarkDownCopy https://github.com/younghz/Markdown 关于其它扩展语法可参见具体工具的使用说明。 1以上内容来源自：https://github.com/younghz/Markdown]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>syntax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm中方法区和永久代的关系]]></title>
    <url>%2F2019%2F06%2F16%2Fjvm-MethodArea-PERM%2F</url>
    <content type="text"><![CDATA[前言知道有方法区，知道里面存放的是什么东西。知道有永久代，也知道它在jdk1.7和jdk1.8的区别。但是对他们的关系有点稀里糊涂。。。 什么是方法区？方法区（Method Area）是jvm规范里面的运行时数据区的一个组成部分，jvm规范中的运行时数据区还包含了：pc寄存器、虚拟机栈、堆、方法区、运行时常量池、本地方法栈，还应该有堆外内存。 方法区存储什么？主要用来存储class、运行时常量池、字段、方法、代码、JIT代码等。 注意： 运行时数据区跟内存不是一个概念。 方法区是运行时数据区的一部分。 方法区是jvm规范中的一部分，并不是实际的实现，切忌将规范跟实现混为一谈。 永久代（Perm区）永久代又叫Perm区，只存在于hotspot jvm中，并且只存在于jdk7和之前的版本中，jdk8中已经彻底移除了永久代，jdk8中引入了一个新的内存区域叫metaspace。 并不是所有的jvm中都有永久代，ibm的j9，oracle的JRocket都没有永久代。 永久代是实现层面的东西。 永久代里面存的东西基本上就是方法区规定的那些东西。 因此，我们可以说，永久代是方法区的一种实现，当然，在hotspot jdk8中metaspace可以看成是方法区的一种实现。 下面我们来看下hotspot jdk8中移除了永久代以后的内存结构： 结论 方法区是规范层面的东西，规定了这一个区域要存放哪些东西 永久代（Hotspot虚拟机特有的概念）或者是metaspace是对方法区的不同实现，是实现层面的东西。 1234作者：若鱼1919链接：https://www.imooc.com/article/47149来源：慕课网本文原创发布于慕课网 ，转载请注明出处，谢谢合作]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>永久代</tag>
        <tag>metaspace</tag>
        <tag>jvm</tag>
        <tag>方法区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产者-消费者问题详解]]></title>
    <url>%2F2019%2F06%2F15%2Fproducer-consumer%2F</url>
    <content type="text"><![CDATA[文章参考自：生产者-消费者问题详解 一、明确定义要理解生产消费者问题，首先应弄清PV操作的含义：PV操作是由P操作原语和V操作原语组成（原语是不可中断的过程），对信号量进行操作，具体定义如下： P（S）：①将信号量S的值减1，即S=S-1； ​ ②如果S&gt;=0，则该进程继续执行；否则该进程置为等待状态，排入等待队列。 1解读：P操作可以理解为申请资源。P操作每次申请一个资源（S = S - 1，可用资源个数减少1），如果可用资源的个数大于等于0（S&gt;=0），那么说明本次申请资源操作成功，继续执行后续程序。否则说明没有足够的资源供该进程使用，该进程置为等待状态，加入等待该资源的等待队列。 V（S）：①将信号量S的值加1，即S=S+1； ​ ②如果S&gt;0，则该进程继续执行；否则释放队列中第一个等待信号量的进程。 1解读：V操作可以理解为释放资源。V操作每次释放一个资源（S = S + 1，可用资源个数增加1），如果可用资源的个数大于0（S &gt; 0），说明本次释放资源操作成功，继续执行后续程序。否则释放该资源的等待队列中第一个等待信号量的进程。 这只是书本的定义，对于这部分内容，老师先不要急于解释上面的程序流程，而是应该让学生首先知道P操作与V操作到底有什么作用。 P操作相当于申请资源，而V操作相当于释放资源。所以要学生记住以下几个关键字： 123P操作-----&gt;申请资源V操作-----&gt;释放资源 二、形象启发为此举两个生活中的例子： 例一：在公共电话厅打电话 公共电话厅里有多个电话，如某人要打电话，首先要进行申请，看是否有电话空闲，若有，则可以使用电话，如果电话亭里所有电话都有人正在使用，那后来的人只有排队等候。当某人用完电话后，则有空电话腾出，正在排队的第一个人就可以使用电话。这就相当于PV操作： 123某人要打电话，首先要进行申请，相当于执行一次P操作，申请一个可用资源（电话）；某人用完电话，则有空电话腾出，相当于执行一次V操作，释放一个可用资源（电话）。 三、分层解剖在理解了PV操作的的含义后，就必须讲解利用PV操作可以实现进程的两种情况：互斥和同步。根据互斥和同步不同的特点，就有利用PV操作实现互斥与同步相对固定的结构模式。这里就不详细讲解了。但生产者-消费者问题是一个有代表性的进程同步问题。但是如果我们将问题细分成三种情况进行讲解，理解难度将大大降低。 1）一个生产者，一个消费者，公用一个缓冲区。可以作以下比喻：将一个生产者比喻为一个生产厂家，如伊利牛奶厂家，而一个消费者，比喻是学生小明，而一个缓冲区则比喻成一间好又多(类似于商店)。第一种情况，可以理解成伊利牛奶生产厂家生产一盒牛奶，把它放在好又多一分店进行销售，而小明则可以从那里买到这盒牛奶。只有当厂家把牛奶放在商店里面后，小明才可以从商店里买到牛奶。所以很明显这是最简单的同步问题。 解题如下： 定义两个同步信号量： 123empty——表示缓冲区是否为空，初值为1。full——表示缓冲区中是否为满，初值为0。 生产者进程 123456while(TRUE)&#123; 生产一个产品; P(empty); 产品送往Buffer; V(full);&#125; 生产者行为： 生成一个商品 拿到 empty 同步信号量，执行P操作（empty -= 1，empty现在等于0，表示不为空），表示缓冲区有数据 产品运送Buffer 拿到 full 同步信号量，执行V操作（full += 1，full现在等于1，表示满），表示缓冲区有数据 消费者进程 123456while(TRUE)&#123; P(full); 从Buffer取出一个产品; V(empty); 消费该产品;&#125; 消费者行为： 拿到 full 同步信号量，执行P操作（full -= 1，full现在等于0，表示不满），表示缓冲区无数据 从Buffer取出一个产品 拿到 empty 同步信号量，执行V操作（empty += 1，empty现在等于1，表示空），表示缓冲区无数据 消费该产品 2）一个生产者，一个消费者，公用n个环形缓冲区。第二种情况可以理解为伊利牛奶生产厂家可以生产好多牛奶，并将它们放在多个好又多分店进行销售，而小明可以从任一间好又多分店中购买到牛奶。同样，只有当厂家把牛奶放在某一分店里，小明才可以从这间分店中买到牛奶。不同于第一种情况的是，第二种情况有N个分店（即N个缓冲区形成一个环形缓冲区），所以要利用指针，要求厂家必须按一定的顺序将商品依次放到每一个分店中。缓冲区的指向则通过模运算得到。 解题如下： 定义两个同步信号量： 123empty——表示缓冲区是否为空，初值为n。full——表示缓冲区中是否为满，初值为0。 设缓冲区的编号为1～n-1，定义两个指针in和out，分别是生产者进程和消费者进程使用的指针，指向下一个可用的缓冲区。 生产者进程 1234567while(TRUE)&#123; 生产一个产品; P(empty); 产品送往buffer（in）； in=(in+1)mod n； V(full);&#125; 消费者进程 1234567while(TRUE)&#123; P(full); 从buffer（out）中取出产品； out=(out+1)mod n； V(empty); 消费该产品;&#125; 3）一组生产者，一组消费者，公用n个环形缓冲区第三种情况，可以理解成有多间牛奶生产厂家，如蒙牛，达能，光明等，消费者也不只小明一人，有许许多多消费者。不同的牛奶生产厂家生产的商品可以放在不同的好又多分店中销售，而不同的消费者可以去不同的分店中购买。当某一分店已放满某个厂家的商品时，下一个厂家只能把商品放在下一间分店。所以在这种情况中，生产者与消费者存在同步关系，而且各个生产者之间、各个消费者之间存在互斥关系,他们必须互斥地访问缓冲区。（得好好揣摩这句话） 解题如下： 定义四个信号量： 1234567empty——表示缓冲区是否为空，初值为n。full——表示缓冲区中是否为满，初值为0。mutex1——生产者之间的互斥信号量，初值为1。mutex2——消费者之间的互斥信号量，初值为1。 设缓冲区的编号为1～n-1，定义两个指针in和out，分别是生产者进程和消费者进程使用的指针，指向下一个可用的缓冲区。 生产者进程 123456789while(TRUE)&#123; 生产一个产品; P(empty); //生成产品，那么缓冲区一定不为空 P(mutex1)；//同一时间只能有一个生产者生成商品 产品送往buffer（in）； in=(in+1)mod n； V(mutex1); V(full);&#125; 消费者进程 12345678while(TRUE)&#123; P(full); P(mutex2)； 从buffer（out）中取出产品； out=(out+1)mod n； V（mutex2）； V(empty);&#125;]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>生产者&amp;消费者</tag>
        <tag>PV操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitignore]]></title>
    <url>%2F2019%2F06%2F15%2Fgitignore%2F</url>
    <content type="text"><![CDATA[下面文章参考自：Git 忽略提交 .gitignore 前言在使用Git的过程中，有的文件比如日志，临时文件，编译的中间文件等不需要提交到代码仓库，这时就要设置相应的忽略规则，来忽略这些文件的提交。 规则 作用 /mtk 过滤整个文件夹 *.zip 过滤所有.zip文件 /mtk/do.c 过滤某个具体文件 !/mtk/one.txt 追踪（不过滤）某个具体文件 注意：如果你创建.gitignore文件之前就push了某一文件，那么即使你在.gitignore文件中写入过滤该文件的规则，该规则也不会起作用，git仍然会对该文件进行版本管理。 配置语法以斜杠“/”开头表示目录； 以星号“*”通配多个字符； 以问号“?”通配单个字符 以方括号“[]”包含单个字符的匹配列表； 以叹号“!”表示不忽略(跟踪)匹配到的文件或目录。 注意： git 对于 .gitignore配置文件是按行从上到下进行规则匹配的 Git 忽略文件提交的方法有三种方法可以实现忽略Git中不想提交的文件。 在Git项目中定义 .gitignore 文件这种方式通过在项目的某个文件夹下定义 .gitignore 文件，在该文件中定义相应的忽略规则，来管理当前文件夹下的文件的Git提交行为。 .gitignore 文件是可以提交到共有仓库中，这就为该项目下的所有开发者都共享一套定义好的忽略规则。 在 .gitingore 文件中，遵循相应的语法，在每一行指定一个忽略规则。如： 123*.log*.temp/vendor 在Git项目的设置中指定排除文件这种方式只是临时指定该项目的行为，需要编辑当前项目下的 .git/info/exclude 文件，然后将需要忽略提交的文件写入其中。 需要注意的是，这种方式指定的忽略文件的根目录是项目根目录。 定义Git全局的 .gitignore 文件除了可以在项目中定义 .gitignore 文件外，还可以设置全局的 git .gitignore 文件来管理所有Git项目的行为。这种方式在不同的项目开发者之间是不共享的，是属于项目之上Git应用级别的行为。 这种方式也需要创建相应的 .gitignore 文件，可以放在任意位置。然后在使用以下命令配置Git： 1git config --global core.excludesfile ~/.gitignore Git 忽略规则详细的忽略规则可以参考官方英文文档 Git 忽略规则优先级在 .gitingore 文件中，每一行指定一个忽略规则，Git 检查忽略规则的时候有多个来源，它的优先级如下（由高到低）： 从命令行中读取可用的忽略规则 当前目录定义的规则 父级目录定义的规则，依次地推 $GIT_DIR/info/exclude 文件中定义的规则 core.excludesfile中定义的全局规则 Git 忽略规则匹配语法在 .gitignore 文件中，每一行的忽略规则的语法如下： 空格不匹配任意文件，可作为分隔符，可用反斜杠转义 # 开头的模式标识注释，可以使用反斜杠进行转义 ! 开头的模式标识否定，该文件将会再次被包含，如果排除了该文件的父级目录，则使用 ! 也不会再次被包含。可以使用反斜杠进行转义 / 结束的模式只匹配文件夹以及在该文件夹路径下的内容，但是不匹配该文件 / 开始的模式匹配项目跟目录 如果一个模式不包含斜杠，则它匹配相对于当前 .gitignore 文件路径的内容，如果该模式不在 .gitignore 文件中，则相对于项目根目录 **匹配多级目录，可在开始，中间，结束 ?通用匹配单个字符 []通用匹配单个字符列表 常用匹配示例： bin/: 忽略当前路径下的bin文件夹，该文件夹下的所有内容都会被忽略，不忽略 bin 文件 /bin: 忽略根目录下的bin文件 /*.c: 忽略 cat.c，不忽略 build/cat.c debug/*.obj: 忽略 debug/io.obj，不忽略 debug/common/io.obj 和 tools/debug/io.obj **/foo: 忽略/foo, a/foo, a/b/foo等 a/**/b: 忽略a/b, a/x/b, a/x/y/b等 !/bin/run.sh: 不忽略 bin 目录下的 run.sh 文件 *.log: 忽略所有 .log 文件 config.php: 忽略当前路径的 config.php 文件 .gitignore规则不生效.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。 解决方法就是先把本地缓存删除（改变成未track状态），然后再提交: 123git rm -r --cached .git add .git commit -m 'update .gitignore' 作者：王伟desire链接：https://www.jianshu.com/p/74bd0ceb6182来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js css 404]]></title>
    <url>%2F2019%2F06%2F15%2Fjs-404%2F</url>
    <content type="text"><![CDATA[问题描述本地调试Hexo的过程中，无意间打开google浏览器的开发者模式，然后发现很多有关 css、js404的错误： 问题分析首先打开Hexo所使用的主题所在目录（我用的是next6，点击这里下载），依次进入source/lib目录下，发现并没有 canvas-nest等文件夹，所以也并没有canvas-nest.min.js等文件，问题锁定。 问题解决打开Hexo所用主题所在目录，找到主题配置文件_config.yml，将其打开，然后搜索canvas_nest，结果如下图： canvas_nest下面 enable 属性为 true，说明开启了canvas_nest，但是并没有对应的lib支持，所以需要安装对应的lib。 安装方法切换到Hexo主题根目录下，我的是：D:\work\hexo\changsk\themes\next6。 看上图，写明了lib的下载地址：Dependencies: https://github.com/theme-next/theme-next-canvas-nest 只需下载即可，在这里利用 git clone，执行以下命令： 1git clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest note：source/lib/canvas-nest 表示下载的lib存放的位置，next6的lib资源文件都放在source/lib下，canvas-nest文件夹的名称要和报错信息里面文件夹名称一致，不然还是会报404错误，找不到该资源文件。 当然也可以手动进行下载，然后放到正确的位置。执行完成后，可以发现lib下面多了个文件夹，里面有前端所需要的canvas-nest.min.js然后执行 123hexo cleanhexo ghexo s 进行本地调试，发现canvas-nest.min.js 404错误已经消失，其他类似的错误都可以通过这种方式解决。 后记执行git clone *之后，下载目录下面会有 *.git文件夹，最好把它删掉。 因为如果进行hexo源文件备份的话，会把整个hexo源文件push到github仓库，包括theme文件夹，因为theme文件夹/source/lib/的一些lib是通过*git clone的方式获取的。这种方式下载的lib会在文件夹下面生成.git。那么进行hexo备份的时候，会发现有多个.git**文件夹存在，就会报错，详情看这里]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>404</tag>
        <tag>css</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 创建分支]]></title>
    <url>%2F2019%2F06%2F12%2Fgit-command%2F</url>
    <content type="text"><![CDATA[搭建好本地 Hexo ，然后链接到了 github pages，也绑定了域名changsk.top 。但是 github 博客仓库里面中的文件和本地文件不一样，有些差异，这是因为本地文件经 hexo g 命令生成静态页面后，然后经hexo d deploy（部署）到github上，所以github仓库是没有本地hexo源文件。如果某一天不小心把本地仓库文件删了，或者换了电脑等原因，致使hexo源文件丢失，那么会造成一定的损失。所以可以把本地 hexo 源文件也同步到 github 上面。方法是在原仓库另创建一个分支，专门用于同步本地 Hexo 源文件。 执行以下命令的前提：当前主机已经可以通过SSH连接到 github 博客仓库（即本机生成的SSH KEY放入到 github 博客仓库当中去） 新建 git 仓库首先新建一个文件夹，比如起名为 changsk_backup ，在此文件夹内打开Git Bash，输入命令： 1git init # 在当前目录创建新的 Git 仓库 可以看到在当前文件夹里面会生成隐藏文件夹 .git，表示当前文件夹是一个git仓库 添加远程仓库1git remote add origin https://github.com/Tkzccsk/changsk.git https://github.com/Tkzccsk/changsk.git 是博客仓库的地址，获取方式是登录GitHub，找到自己的博客的仓库。远程库的名字就是origin，这是Git默认的叫法，可以起其他名。 查看远程仓库的名称 1git remote 下载远程仓库将GitHub上的博客仓库完全下载下来 1git pull origin master # 将远程仓库 origin 的 master 分支 pull 到本地仓库 创建新分支创建并切换到一个新分支（原来的分支名为master），输入命令： 1git checkout -b changsk changsk 为新分支名上述命令相当于两条命令 12git branch changsk # 创建分支git checkout changsk # 切换分支 上传本地hexo源文件到github博客仓库的changsk分支这样就在我们的博客仓库中新建了一个名为changsk的分支，接下来把生成的.git文件复制到本地 hexo 仓库中去，现在changsk_backup就没有用了，可以删了。接下来我们把 Hexo 本地博客仓库源文件中上传到GitHub的博客仓库的changsk分支中。 123git add .git commit -m "backup"git push origin changsk git add . ： 使用它会把工作时的所有变化提交到暂存区，包括文件内容修改(modified)以及新文件(new)，但不包括被删除的文件git commit -m “backup” ： 主要是将暂存区里的改动给提交到本地的版本库。-m 表示使用消息参数， “backup” 为 -m 的内容，用来表示这次提交的简要说明。git push origin changsk ： 将本地仓库的代码推送到changsk分支。 进入到GitHub的博客仓库中，可以发现出现了一个新的分支changsk，并且里面是我们的博客原件。最好在上传备份之前写一份README.md 文件，作为一项说明（因为这是GitHub建议的）。通过以上方式，我们就完成了备份啦，下一次更新了博客，首先执行 1hexo clean &amp; hexo d -g 注意：部署本地 Hexo 到 github 用的是 master 分支(__config.yml中的声明)生成及部署hexo，然后执行 123git add .git commit -m "backup"git push origin changsk 进行本地 Hexo 源文件的备份。参考 ：https://blog.csdn.net/qq_34229391/article/details/82251852]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[changes not staged for commit]]></title>
    <url>%2F2019%2F06%2F12%2Fchanges-not-staged-for-commit%2F</url>
    <content type="text"><![CDATA[在使用 git 的过程中，要把当前文件夹下的内容 push 到 changsk 分支，先执行 1git add . 然后执行 1git commit -m "backup" 最后执行 push 操作，将当前本地 git 仓库提交到 origin 远程仓库的 changsk 分支 1git push origin changsk # changsk 是我创建的一个分支 然后就收到报错，报错信息如下：大概意思是： 1更新（push操作）被拒绝，因为github远程仓库changsk分支的有些内容在本地仓库没有。 然后我想起了我昨天在 github 仓库 changsk 分支里面创建了一个 README.md，所以本地仓库是没有的，造成了远方仓库和本地仓库的不一致（精确来说是远方仓库有，但是本地仓库没有），所以 push 之前应该先把远程仓库的内容pull（拉取）到本地，然后才可以进行push。 所以先应该执行 1git pull origin changsk 将远程仓库 origin 的 chagnsk 分支和当前本地 git 仓库进行合并，使它们保持一致然后执行 1git add . 然后执行 1git commit -m "backup" 又收到一个不同的错误经过网上查阅，大部分人都说是因为没有执行 git add .，但我显然不是这个问题。原因是我要 push 的本地仓库里面还有另外的clone过来的git仓库，我查看文件夹，就像报错信息里面说的， themes/next（Hexo 的一个主题，也是本网站使用的主题） 里面是我 git clone 下来的一个仓库。解决办法是删除 themes/next 文件夹里面的隐藏文件夹 .git然后再执行就没有问题了。 1git commit -m "backup" 最后执行 push 操作 1git push origin changsk 问题解决。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CRLF CR LF]]></title>
    <url>%2F2019%2F06%2F12%2FCRLF-CR-LF%2F</url>
    <content type="text"><![CDATA[deploy 本地 Hexo 到 github pages 的时候，遇到了两陌生的单词：LF和CRLF ，本着不求甚解的态度，去网上找了相关的资料，特此记录，以便日后查看。其实前几天在安装 Hexo 的过程中有过一个设置LF和CRLF 的转换的选项，当时没在意。 名词解释CR：Carriage Return，对应ASCII中转义字符\r，表示回车LF：Linefeed，对应ASCII中转义字符\n，表示换行CRLF：Carriage Return &amp; Linefeed，\r\n，表示回车并换行 Windows操作系统采用两个字符来进行换行，即CRLF Unix/Linux/Mac OS X操作系统采用单个字符LF来进行换行 野史据野史记载，在很久以前的机械打字机时代，CR和LF分别具有不同的作用：LF会将打印纸张上移一行位置，但是保持当前打字的水平位置不变；CR则会将“Carriage”（打字机上的滚动托架）滚回到打印纸张的最左侧，但是保持当前打字的垂直位置不变，即还是在同一行。当CR和LF组合使用时，则会将打印纸张上移一行，且下一个打字位置将回到该行的最左侧，也就是我们今天所理解的换行操作。 随着时间的推移，机械打字机渐渐地退出了历史舞台，当初的纸张变成了今天的显示器，打字机的按键也演变为了如今的键盘。在操作系统出现的年代，受限于内存和软盘空间的不足，一些操作系统的设计者决定采用单个字符来表示换行符，如Unix的LF、MacIntosh的CR。他们的意图都是为了进行换行操作，只是当初并没有一个国际标准，所以才有这样字符上的不同。参考：https://www.jianshu.com/p/b03ad01acd69]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 的常用命令]]></title>
    <url>%2F2019%2F06%2F11%2Fhexo-command%2F</url>
    <content type="text"><![CDATA[常用 hexo 命令hexo new “postName” #新建文章hexo new page “pageName” #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help #查看帮助hexo version #查看Hexo的版本 缩写hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 组合命令：hexo s -g #生成并本地预览hexo d -g #生成并上传hexo clear # 删除无用 tags 和 categorieshexo clean &amp; hexo d -g # 清除缓存 生成静态页面并发布 给文章添加标签和分类1234567title: hexo 的常用命令date: 2019-06-11 11:43:56tags: - hexo # 文章标签- aaa- bbbcategories: hexo # 该文章类别为 categories\hexo 效果图：]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error JAVA_HOME is incorrectly set]]></title>
    <url>%2F2019%2F06%2F11%2FError_JAVA_HOME_is%20_incorrectly_set%2F</url>
    <content type="text"><![CDATA[在安装、调试、运行Hexo的过程中，出现了以下错误： Error: JAVA_HOME is incorrectly set. Please update D:\software\software\hadoop3\hadoop-3.0.2\etc\hadoop\hadoop-env.cmd 然后找到对应的目录，打开hadoop-env.cmd，发现其中的 JAVA_HOME 是这样的： 然后打开 terminal，查询 java 版本 以及 JAVA_HOME 环境变量： 发现 JAVA_HOME 已正确配置。那么问题究竟出在哪里？经网上查阅，因为Program Files中存在空格，所以出现错误，只需要用PROGRA~1代替Program Files即可。如图： 或者也可以将jdk装到其他不存在空格的目录下。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
